---
id: 6
title: Advanced Topics
description: Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.
navOrder: 6
category: Core
---

import {Icon} from "@iconify/react";
import {Tab, Tabs} from "@heroui/react";
import { Steps, Step } from '/src/components/Shared/Steps';

This section delves into more advanced topics related to SoundFlow, including extending the engine with custom components, optimizing performance, and understanding threading considerations.

<Tabs color="primary" variant="bordered" aria-label="Advanced Topics">
    <Tab
        key="extending"
        title={
            <div className="flex items-center gap-2">
                <Icon icon='ph:puzzle-piece-bold' />
                <span>Extending SoundFlow</span>
            </div>
        }
    >
        ## Extending SoundFlow

        One of SoundFlow's key strengths is its extensibility. You can tailor the engine to your specific needs by creating custom:

        *   **Sound Components (`SoundComponent`)**
        *   **Sound Modifiers (`SoundModifier`)**
        *   **Audio Analyzers (`AudioAnalyzer`)** and **Visualizers (`IVisualizer`)**
        *   **Audio Backends (`AudioEngine`)**
        *   **Sound Data Providers (`ISoundDataProvider`)**
        *   **Extensions (e.g., for specific DSP libraries)**: SoundFlow supports integration with external audio processing libraries. For instance, the `SoundFlow.Extensions.WebRtc.Apm` package provides features like noise suppression and echo cancellation by wrapping the WebRTC Audio Processing Module. You can create similar extensions for other libraries.

        ### Custom Sound Components

        Creating custom `SoundComponent` classes allows you to implement unique audio processing logic and integrate it seamlessly into the SoundFlow audio graph.

        <Steps layout='vertical'>
            <Step title="Inherit from SoundComponent" icon='ph:git-fork-bold'>
                Create a new class that inherits from the abstract `SoundComponent` class. Your constructor must accept `AudioEngine` and `AudioFormat` parameters and pass them to the base constructor.
            </Step>
            <Step title="Implement GenerateAudio" icon='lucide:audio-lines'>
                Override the `GenerateAudio(Span<float> buffer, int channels)` method. This is where you'll write the core audio processing code for your component. The `buffer` passed to this method already contains the mixed output from all of the component's inputs.
                *   If your component **generates** new audio (e.g., an oscillator), it should add its generated samples to the `buffer`.
                *   If your component **modifies** incoming audio, it should process the samples within the `buffer` in-place.
            </Step>
            <Step title="Override other methods (optional)" icon='icon-park-outline:switch-one'>
                You can override methods like `ConnectInput`, `AddAnalyzer`, `AddModifier`, etc., to customize how your component interacts with the audio graph.
            </Step>
            <Step title="Add properties (optional)" icon='material-symbols:settings-outline'>
                Add properties to your component to expose configurable parameters that users can adjust.
            </Step>
        </Steps>

        **Example:**

        ```csharp
        using SoundFlow.Abstracts;
        using SoundFlow.Structs;
        using System;

        public class CustomGainComponent : SoundComponent
        {
            public float Gain { get; set; } = 1.0f; // Default gain

            public override string Name { get; set; } = "Custom Gain";
            
            // The constructor must match the base class requirements.
            public CustomGainComponent(AudioEngine engine, AudioFormat format) : base(engine, format) { }

            protected override void GenerateAudio(Span<float> buffer, int channels)
            {
                // The buffer already contains mixed audio from any connected inputs.
                // We simply modify it in-place.
                for (int i = 0; i < buffer.Length; i++)
                {
                    buffer[i] *= Gain;
                }
            }
        }
        ```

        **Usage:**

        ```csharp
        using SoundFlow.Backends.MiniAudio;
        using SoundFlow.Components;
        using SoundFlow.Enums;
        using SoundFlow.Providers;
        using SoundFlow.Structs;
        using System.Linq;
        
        // 1. Create an instance of an audio engine.
        using var engine = new MiniAudioEngine();

        // 2. Define the desired audio format.
        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };

        // 3. Get the default playback device info from the engine.
        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);

        // 4. Initialize a playback device.
        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);

        // 5. Create the player and your custom component, passing the engine and format.
        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead("audio.wav"));
        var player = new SoundPlayer(engine, audioFormat, dataProvider);
        var gainComponent = new CustomGainComponent(engine, audioFormat) { Gain = 0.5f };

        // 6. Connect the player as an input to the gain component.
        gainComponent.ConnectInput(player);
        
        // 7. Add the final component in the chain to the device's master mixer.
        device.MasterMixer.AddComponent(gainComponent);
        
        // 8. Start the device and play the sound.
        device.Start();
        player.Play();
        // ...
        ```

        ### Custom Sound Modifiers

        Custom `SoundModifier` classes allow you to implement your own audio effects.

        <Steps layout='vertical'>
            <Step title="Inherit from SoundModifier" icon='ph:git-fork-bold'>
                Create a new class that inherits from the abstract `SoundModifier` class.
            </Step>
            <Step title="Implement ProcessSample" icon='icon-park-outline:sound-wave'>
                Implement `ProcessSample(float sample, int channel)` (or override `Process` for buffer-level):
                *   `ProcessSample(float sample, int channel)`: This method takes a single audio sample and the channel index as input and returns the modified sample.
                *   `Process(Span<float> buffer, int channels)`: Override this for more complex effects that operate on entire buffers (e.g., FFT-based effects). By default, it calls `ProcessSample` for each sample.
            </Step>
            <Step title="Add properties (optional)" icon='material-symbols:settings-outline'>
                Add properties to your modifier to expose configurable parameters.
            </Step>
            <Step title="Use 'Enabled' property" icon='material-symbols:toggle-on-outline'>
                Your modifier will have an `Enabled` property (defaulting to `true`) to allow toggling its effect.
            </Step>
        </Steps>

        **Example:**

        ```csharp
        using SoundFlow.Abstracts;
        using System;

        public class CustomDistortionModifier : SoundModifier
        {
            public float Threshold { get; set; } = 0.5f;

            public override string Name { get; set; } = "Custom Distortion";

            public override float ProcessSample(float sample, int channel)
            {
                // Simple hard clipping distortion
                if (sample > Threshold)
                {
                    return Threshold;
                }
                else if (sample < -Threshold)
                {
                    return -Threshold;
                }
                else
                {
                    return sample;
                }
            }
        }
        ```

        **Usage:**

        ```csharp
        using SoundFlow.Backends.MiniAudio;
        using SoundFlow.Components;
        using SoundFlow.Enums;
        using SoundFlow.Providers;
        using SoundFlow.Structs;
        using System.Linq;

        // 1. Create an engine and define the audio format.
        using var engine = new MiniAudioEngine();
        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };
        
        // 2. Get device info and initialize a playback device.
        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);
        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);
        
        // 3. Create the data provider and player.
        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead("audio.wav"));
        var player = new SoundPlayer(engine, audioFormat, dataProvider);

        // 4. Create an instance of your custom modifier.
        var distortion = new CustomDistortionModifier { Threshold = 0.7f };
        // distortion.Enabled = false; // To disable it

        // 5. Add the modifier to a SoundComponent (like a player).
        player.AddModifier(distortion);
        
        // 6. Add the player to the device's master mixer.
        device.MasterMixer.AddComponent(player);
        
        // 7. Start the device and play.
        device.Start();
        player.Play();
        // ...
        ```

        ### Custom Visualizers

        Custom `IVisualizer` classes allow you to create unique visual representations of audio data.

        <Steps layout='vertical'>
            <Step title="Implement IVisualizer" icon='ph:plugs-connected-bold'>
                Create a new class that implements the `IVisualizer` interface.
            </Step>
            <Step title="Implement ProcessOnAudioData" icon='carbon:data-vis-4'>
                This method receives a `Span<float>` containing audio data. You should process this data and store the relevant information needed for rendering.
            </Step>
            <Step title="Implement Render" icon='material-symbols:draw-outline'>
                This method receives an `IVisualizationContext`. Use the drawing methods provided by the context (e.g., `DrawLine`, `DrawRectangle`) to render your visualization.
            </Step>
            <Step title="Raise VisualizationUpdated" icon='mdi:bell-ring-outline'>
                When the visualization data changes (e.g., after processing new audio data), raise the `VisualizationUpdated` event to notify the UI to update the display.
            </Step>
            <Step title="Implement Dispose" icon='material-symbols:delete-outline'>
                Release any unmanaged resources or unsubscribe from events.
            </Step>
        </Steps>

        **Example:**

        ```csharp
        using SoundFlow.Interfaces;
        using System;
        using System.Numerics;

        public class CustomBarGraphVisualizer : IVisualizer
        {
            private float _level;

            public string Name => "Custom Bar Graph";

            public event EventHandler? VisualizationUpdated;

            public void ProcessOnAudioData(Span<float> audioData)
            {
                if (audioData.IsEmpty) return;
                // Calculate the average level (simplified for this example)
                float sum = 0;
                for (int i = 0; i < audioData.Length; i++)
                {
                    sum += Math.Abs(audioData[i]);
                }
                _level = sum / audioData.Length;

                // Notify that the visualization needs to be updated
                VisualizationUpdated?.Invoke(this, EventArgs.Empty);
            }

            public void Render(IVisualizationContext context)
            {
                // Clear the drawing area
                context.Clear();

                // Draw a simple bar graph based on the calculated level
                float barHeight = _level * 200; // Scale the level for visualization
                context.DrawRectangle(10, 200 - barHeight, 30, barHeight, new Color(0, 1, 0));
            }

            public void Dispose()
            {
                // Unsubscribe from events, release resources if any
                VisualizationUpdated = null;
            }
        }
        ```

        ### Adding Audio Backends

        SoundFlow is designed to support multiple audio backends. Currently, it includes a `MiniAudio` backend. You can add support for other audio APIs (e.g., WASAPI, ASIO, CoreAudio) by creating a new backend.

        <Steps layout='vertical'>
            <Step title="Inherit from AudioEngine" icon='ph:engine-bold'>
                Create a new class that inherits from the abstract `AudioEngine`. This class will manage the entire lifecycle of your custom backend.
            </Step>
            <Step title="Implement Abstract Methods" icon='material-symbols:function'>
                Implement all the abstract methods from `AudioEngine`:
                *   `InitializeBackend()` and `CleanupBackend()`: Handle global setup/teardown of your native backend's context.
                *   `InitializePlaybackDevice()`, `InitializeCaptureDevice()`, `InitializeFullDuplexDevice()`, `InitializeLoopbackDevice()`: These methods are the core of device management. You will implement the logic to initialize a device using your backend's API and return an instance of a class that inherits from the appropriate abstract device class.
                *   `SwitchDevice(...)` (3 overloads): Implement logic to tear down an old device and initialize a new one while preserving the state (audio graph, event listeners).
                *   `CreateEncoder(...)` and `CreateDecoder(...)`: Return your backend-specific implementations of the `ISoundEncoder` and `ISoundDecoder` interfaces.
                *   `UpdateDevicesInfo()`: Implement logic to enumerate playback and capture devices using the backend's API, populating `PlaybackDevices` and `CaptureDevices`.
            </Step>
            <Step title="Create Device Wrappers" icon='ph:package-bold'>
                Create concrete classes inheriting from `AudioPlaybackDevice` and `AudioCaptureDevice`. These will wrap the native device handles and logic specific to your backend, including the audio callback that drives the processing graph.
            </Step>
            <Step title="Implement Codec Interfaces" icon='mdi:file-code-outline'>
                Create classes that implement `ISoundEncoder` and `ISoundDecoder` to handle audio encoding and decoding for your chosen backend.
            </Step>
        </Steps>

        **Example (Skeleton):**

        ```csharp
        using SoundFlow.Abstracts;
        using SoundFlow.Abstracts.Devices;
        using SoundFlow.Interfaces;
        using SoundFlow.Enums;
        using SoundFlow.Structs;
        using System;
        using System.IO;
        
        // Custom Backend Engine
        public class MyNewAudioEngine : AudioEngine
        {
            private nint _context; // Example native context handle
            
            // The parameterless constructor calls InitializeBackend() automatically.
            public MyNewAudioEngine() { }

            protected override void InitializeBackend()
            {
                // _context = NativeApi.InitContext();
                // UpdateDevicesInfo(); // Initial device enumeration
                Console.WriteLine("MyNewAudioEngine: Backend initialized.");
            }

            protected override void CleanupBackend()
            {
                // NativeApi.UninitContext(_context);
                Console.WriteLine("MyNewAudioEngine: Backend cleaned up.");
            }
            
            public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)
            {
                // Create and return your backend-specific playback device wrapper
                return new MyNewPlaybackDevice(this, _context, deviceInfo, format, config);
            }

            public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)
            {
                // Similar logic for capture devices...
                throw new NotImplementedException();
            }

            // Implement other abstract methods...
            public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();
            public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();
            public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format) => throw new NotImplementedException();
            public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format) => throw new NotImplementedException();
            public override void UpdateDevicesInfo() { /* NativeApi.GetDevices(...); */ }
            public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();
            public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();
            public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null) => throw new NotImplementedException();
        }

        // Custom Playback Device Wrapper
        public class MyNewPlaybackDevice : AudioPlaybackDevice
        {
            private nint _deviceHandle;
            
            public MyNewPlaybackDevice(AudioEngine engine, nint context, DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config)
                : base(engine, format, config ?? new MyDeviceConfig())
            {
                // _deviceHandle = NativeApi.InitDevice(context, deviceInfo?.Id, OnAudioCallback);
                // this.Info = ... // Populate DeviceInfo from native API
                this.Capability = Capability.Playback;
            }

            public override void Start() { /* NativeApi.StartDevice(_deviceHandle); */ IsRunning = true; }
            public override void Stop() { /* NativeApi.StopDevice(_deviceHandle); */ IsRunning = false; }
            public override void Dispose() 
            {
                if (IsDisposed) return;
                Stop();
                // NativeApi.UninitDevice(_deviceHandle);
                OnDisposedHandler();
                IsDisposed = true;
            }
            
            // This callback is invoked by the native backend on the audio thread.
            private void OnAudioCallback(Span<float> buffer)
            {
                // 1. Clear the buffer
                buffer.Clear();
                // 2. Process the master mixer or a soloed component
                var soloed = Engine.GetSoloedComponent();
                if (soloed != null)
                    soloed.Process(buffer, Format.Channels);
                else
                    MasterMixer.Process(buffer, Format.Channels);
            }
        }
        
        // Custom Device Configuration (Optional)
        public class MyDeviceConfig : DeviceConfig { /* ... */ }
        ```
    </Tab>

    <Tab
        key="performance"
        title={
            <div className="flex items-center gap-2">
                <Icon icon='ic:round-speed' />
                <span>Performance Optimization</span>
            </div>
        }
    >
        ## Performance Optimization

        Here are some tips for optimizing the performance of your SoundFlow applications:

        *   **Buffer Sizes:** Choose appropriate buffer sizes for your use case. Smaller buffers reduce latency but increase CPU overhead. Larger buffers can improve efficiency but may introduce latency. With the `MiniAudio` backend, you can specify this via `MiniAudioDeviceConfig` when initializing a device.
        *   **SIMD:** SoundFlow uses SIMD instructions (when available) in critical paths like the `Mixer`, `SoundComponent` (for volume and panning), `MathHelper` (for FFTs and windowing), and in the `DeviceBufferHelper` for audio format conversions. Ensure your target platform supports SIMD for the best performance.
        *   **Profiling:** Use a profiler (like the one built into Visual Studio) to identify performance bottlenecks in your audio processing pipeline.
        *   **Asynchronous Operations:** For long-running operations (e.g., loading large files, network requests in `NetworkDataProvider`, project saving/loading), use asynchronous programming (`async` and `await`) to avoid blocking the main thread or the audio thread.
        *   **Avoid Allocations:** Minimize memory allocations within the `GenerateAudio` method of `SoundComponent` and the `ProcessSample` or `Process` method of `SoundModifier`. SoundFlow internally uses `ArrayPool<T>.Shared` for many temporary buffers to reduce GC pressure.
        *   **Efficient Algorithms:** Use efficient algorithms for audio processing, especially in performance-critical sections.
        *   **Modifier Overhead:** Each `SoundModifier` added to a component or to the editing hierarchy (`AudioSegment`, `Track`, `Composition`) introduces some overhead. For very simple operations, integrating them directly into a custom `SoundComponent` might be more performant than using many tiny modifiers. However, modifiers offer better reusability and modularity.
        *   **Effect Toggling:** Use the `Enabled` property on `SoundModifier`, `AudioAnalyzer`, `SoundComponent`, `AudioSegmentSettings`, and `TrackSettings` to non-destructively disable effects or entire processing paths instead of removing and re-adding them, which is more efficient.
    </Tab>

    <Tab
        key="threading"
        title={
            <div className="flex items-center gap-2">
                <Icon icon='carbon:thread' />
                <span>Threading Considerations</span>
            </div>
        }
    >
        ## Threading Considerations

        SoundFlow uses a dedicated, high-priority thread (managed by the audio backend, e.g., MiniAudio) for audio processing. This ensures that audio is processed in real time and minimizes the risk of glitches or dropouts.

        **Key Considerations:**

        *   **Audio Thread:** The audio processing logic is executed on a dedicated audio thread. This thread is responsible for calling the audio callback provided to the backend. In SoundFlow's `MiniAudio` implementation, this callback triggers the `Process` method on the appropriate `MiniAudioPlaybackDevice` or `MiniAudioCaptureDevice`, which in turn traverses the `SoundComponent` graph (e.g., calling `MasterMixer.Process(...)`). Therefore, all code within `GenerateAudio` (for `SoundComponent`) and `Process`/`ProcessSample` (for `SoundModifier`) runs on this critical audio thread. Avoid performing any long-running or blocking operations (like I/O, complex non-audio computations, or direct UI updates) on this thread.
        *   **UI Thread:** Never perform audio processing directly on the UI thread. This can lead to unresponsiveness and glitches. Use the `AudioEngine`'s audio thread for all audio-related operations. For UI updates based on audio events (e.g., from an `IVisualizer`'s `VisualizationUpdated` event), you must marshal the calls to the UI thread (e.g., using `Dispatcher.Invoke` in WPF/Avalonia, or `Control.Invoke` in WinForms).
        *   **Thread Safety:** If you need to access or modify shared data from both the audio thread and another thread (e.g., the UI thread updating a `SoundModifier`'s property), use appropriate synchronization mechanisms (like `lock`, `Monitor`, or thread-safe collections) to ensure data integrity and prevent race conditions. Many properties on `SoundComponent` and `SoundModifier` are internally locked for thread-safe access from different threads.
    </Tab>
</Tabs>