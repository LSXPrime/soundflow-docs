---
id: 5
title: API Reference
description: A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.
navOrder: 5
category: Core
---

# API Reference

This section provides a detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members. It serves as a comprehensive reference for developers working with the SoundFlow library.

## Namespaces

SoundFlow is organized into the following namespaces:

*   **`SoundFlow.Abstracts`:** Contains abstract classes and interfaces that define the core framework of SoundFlow. This includes base classes for the audio engine, audio devices, sound components, modifiers, and analyzers.
*   **`SoundFlow.Backends`:** Provides backend-specific implementations for audio input/output.
*   **`SoundFlow.Backends.MiniAudio`:** The primary audio backend, which uses the `miniaudio` library.
*   **`SoundFlow.Codecs`:** Contains official codec extensions for the SoundFlow engine.
*   **`SoundFlow.Codecs.FFMpeg`:** Provides a codec factory using the FFmpeg library to support a wide range of audio formats like MP3, AAC, OGG, Opus, and more.
*   **`SoundFlow.Codecs.FFMpeg.Enums`:** Contains enumerations specific to the FFmpeg codec wrapper, such as `FFmpegResult`.
*   **`SoundFlow.Codecs.FFMpeg.Exceptions`:** Contains exception classes specific to the FFmpeg codec wrapper, such as `FFmpegException`.
*   **`SoundFlow.Components`:** Contains concrete `SoundComponent` classes that provide various audio processing functionalities, including playback, mixing, synthesis, and analysis. It also includes standalone components like the `Recorder`.
*   **`SoundFlow.Editing`:** Contains classes for non-destructive audio editing, including `Composition`, `Track`, `MidiTrack`, `AudioSegment`, `MidiSegment`, `MidiSequence`, and their respective settings classes.
*   **`SoundFlow.Editing.Mapping`:** Contains classes for real-time MIDI mapping and control of `IMidiMappable` objects within a `Composition`.
*   **`SoundFlow.Editing.Persistence`:** Contains classes for saving and loading audio compositions, such as `CompositionProjectManager` and various project data DTOs.
*   **`SoundFlow.Enums`:** Contains enumerations used throughout the SoundFlow library to represent different states, options, and capabilities. The original `EncodingFormat` enum is removed as it is superseded by the codec factory system using format strings.
*   **`SoundFlow.Exceptions`:** Contains custom exception classes used for error handling within SoundFlow.
*   **`SoundFlow.Interfaces`:** Contains interfaces that define contracts for various functionalities, such as audio data providers, encoders, decoders, codec factories, and MIDI backends.
*   **`SoundFlow.Metadata`:** Contains classes for reading and writing metadata (tags, format info) from various audio file formats.
*   **`SoundFlow.Metadata.Midi`:** Contains classes for parsing and generating Standard MIDI Files (SMF).
*   **`SoundFlow.Midi`:** Contains core classes and abstractions for handling MIDI data and devices.
*   **`SoundFlow.Midi.Abstracts`:** Contains base classes for MIDI devices and modifiers.
*   **`SoundFlow.Midi.Devices`:** Contains abstract representations of MIDI input and output devices.
*   **`SoundFlow.Midi.Enums`:** Contains enumerations for MIDI commands and related types.
*   **`SoundFlow.Midi.Interfaces`:** Contains interfaces for MIDI-controllable components and routing graph nodes.
*   **`SoundFlow.Midi.Modifier`:** Contains concrete MIDI modifier (effect) classes, such as `TransposeModifier`.
*   **`SoundFlow.Midi.PortMidi`:** Provides a MIDI backend implementation using the cross-platform PortMidi library.
*   **`SoundFlow.Midi.PortMidi.Enums`:** Contains enumerations for PortMidi errors and synchronization states.
*   **`SoundFlow.Midi.PortMidi.Exceptions`:** Contains exception classes specific to the PortMidi backend.
*   **`SoundFlow.Midi.Routing`:** Contains classes for managing MIDI routing, including the `MidiManager` and `MidiRoute`.
*   **`SoundFlow.Midi.Structs`:** Contains struct types for MIDI data representation, like `MidiMessage`.
*   **`SoundFlow.Modifiers`:** Contains concrete `SoundModifier` classes that implement various audio effects.
*   **`SoundFlow.Providers`:** Contains classes that implement the `ISoundDataProvider` interface, providing ways to load audio data from different sources.
*   **`SoundFlow.Structs`:** Contains custom struct types used within SoundFlow, including `AudioFormat`, `DeviceInfo`, `Result`, and a comprehensive set of error types.
*   **`SoundFlow.Synthesis`:** Contains a polyphonic, multi-timbral synthesizer engine, including `Synthesizer`, `Sequencer`, and related classes for instrument banks and voices.
*   **`SoundFlow.Utils`:** Contains utility classes and extension methods that provide helpful functionalities for working with audio data and performing common operations.
*   **`SoundFlow.Visualization`:** Contains classes related to audio visualization, including analyzers and visualizers.
*   **`SoundFlow.Extensions`:** Namespace for official extensions.
*   **`SoundFlow.Extensions.WebRtc.Apm`:** Provides integration with the WebRTC Audio Processing Module for features like echo cancellation, noise suppression, and automatic gain control.
*   **`SoundFlow.Extensions.WebRtc.Apm.Components`:** Contains components utilizing the WebRTC APM, like `NoiseSuppressor`.
*   **`SoundFlow.Extensions.WebRtc.Apm.Modifiers`:** Contains modifiers utilizing the WebRTC APM, like `WebRtcApmModifier`.

## Key Classes and Interfaces

Below is a summary of the key classes and interfaces in SoundFlow.

### Abstracts

| Class/Interface                                         | Description                                                                                                                                                                           |
|---------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [`AudioAnalyzer`](#abstracts-audioanalyzer)             | Abstract base class for audio analysis components. Implements `IMidiMappable`.                                                                                                    |
| [`AudioEngine`](#abstracts-audioengine)                 | Abstract base class for an audio engine. Manages audio and MIDI devices, provides a pluggable codec system, and is the root context.                                                         |
| [`AudioDevice`](#abstracts-audiodevice)                 | Abstract base class for an initialized audio device (playback or capture).                                                                                                            |
| [`AudioPlaybackDevice`](#abstracts-audioplaybackdevice) | Abstract class representing an initialized output/playback device. Contains a `MasterMixer`.                                                                                          |
| [`AudioCaptureDevice`](#abstracts-audiocapturedevice)   | Abstract class representing an initialized input/capture device. Exposes an `OnAudioProcessed` event.                                                                                 |
| [`FullDuplexDevice`](#abstracts-fullduplexdevice)       | A high-level abstraction managing a paired playback and capture device for simultaneous I/O.                                                                                          |
| [`DeviceConfig`](#abstracts-deviceconfig)               | Abstract base class for backend-specific device configuration objects.                                                                                                                |
| [`SoundComponent`](#abstracts-soundcomponent)           | Abstract base class for all audio processing units in SoundFlow. Represents a node in the audio graph. Implements `IMidiMappable`.                                                    |
| [`SoundModifier`](#abstracts-soundmodifier)             | Abstract base class for audio effects that modify audio samples. Implements `IMidiMappable` and `IMidiControllable`.                                                                  |
| [`SoundPlayerBase`](#abstracts-soundplayerbase)         | Abstract base class providing common functionality for sound playback components. Inherits from `SoundComponent` and implements `ISoundPlayer`. Includes support for time-stretching. |

### Backends.MiniAudio

| Class/Interface                                                     | Description                                                                                                                  |
|---------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| [`MiniAudioEngine`](#backendsminiaudio-miniaudioengine)             | `AudioEngine` implementation that uses the `miniaudio` library for audio I/O.                                                |
| [`MiniAudioDeviceConfig`](#backendsminiaudio-miniaudiodeviceconfig) | `DeviceConfig` implementation for MiniAudio, providing detailed, backend-specific settings for WASAPI, CoreAudio, ALSA, etc. |
| [`MiniAudioCodecFactory`](#backendsminiaudio-miniaudiocodecfactory) | `ICodecFactory` implementation for codecs natively supported by `miniaudio` (WAV, MP3, FLAC). Registered by default at low priority. |
| [`MiniaudioException`](#exceptions-miniaudioexception) | Exception thrown for errors originating from the MiniAudio backend.                                   |

### Codecs.FFMpeg
| Class/Interface                                          | Description                                                                                                                   |
|----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| [`FFmpegCodecFactory`](#codecsffmpeg-ffmpegcodecfactory) | An `ICodecFactory` that uses FFmpeg to provide decoding and encoding for a wide range of formats (MP3, AAC, OGG, Opus, etc.). |
| [`FFmpegException`](#codecsffmpeg-ffmpegexception)       | Exception thrown for errors originating from the native FFmpeg wrapper library.                                               |

### Components

| Class/Interface                                                | Description                                                                                                                                            |
| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`EnvelopeGenerator`](#components-envelopegenerator)           | `SoundComponent` that generates an ADSR (Attack, Decay, Sustain, Release) envelope signal.                                                             |
| [`LowFrequencyOscillator`](#components-lowfrequencyoscillator) | `SoundComponent` that generates a low-frequency oscillator (LFO) signal with various waveforms.                                                        |
| [`Mixer`](#components-mixer)                                   | `SoundComponent` that mixes multiple audio streams together. The `Mixer.Master` property provides access to the default root mixer.                    |
| [`Oscillator`](#components-oscillator)                         | `SoundComponent` that generates various band-limited waveforms (sine, square, sawtooth, etc.).                                                    |
| [`Recorder`](#components-recorder)                             | A component that captures audio from a device and writes it to a file or stream, with support for metadata tagging. |
| [`SoundPlayer`](#components-soundplayer)                       | `SoundPlayerBase` implementation that plays audio from an `ISoundDataProvider`.                                                                        |
| [`SurroundPlayer`](#components-surroundplayer)                 | `SoundPlayerBase` implementation that extends `SoundPlayer` to support surround sound configurations with customizable speaker positions, delays, and panning methods. |
| [`VoiceActivityDetector`](#components-voiceactivitydetector)   | An `AudioAnalyzer` that detects human voice in an audio stream, featuring configurable activation and hangover times to prevent rapid state changes.   |
| [`WsolaTimeStretcher`](#components-wsolatimestretcher)	 | Implements WSOLA algorithm for real-time, pitch-preserved time stretching. Used internally by `AudioSegment`. |

### Editing

| Class/Interface                                       | Description                                                                                                                                            |
| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`Composition`](#editing-composition)                 | Top-level container for audio and MIDI tracks, representing a complete project. Provides access to `Renderer`, `Editor`, and `Recorder` services. `IDisposable`. |
| [`CompositionRenderer`](#editing-compositionrenderer) | Renders a `Composition` to an audio stream. Implements `ISoundDataProvider`.                                                                           |
| [`CompositionEditor`](#editing-compositioneditor)     | Provides methods for manipulating the structure of a `Composition` (tracks, segments, tempo, etc.).                                                      |
| [`CompositionRecorder`](#editing-compositionrecorder) | Manages the MIDI recording workflow for a `Composition`, including track arming and punch-in/out.                                                      |
| [`Track`](#editing-track)                             | Represents a single audio track within a `Composition`, containing `AudioSegment`s and track-level settings. Implements `IMidiMappable`.             |
| [`MidiTrack`](#editing-miditrack)                     | Represents a single MIDI track within a `Composition`, containing `MidiSegment`s and routing to a MIDI target.                                         |
| [`AudioSegment`](#editing-audiosegment)               | Represents a single audio clip on a `Track`'s timeline, referencing a portion of an audio source and applying playback settings. `IDisposable`.      |
| [`MidiSegment`](#editing-midisegment)                 | Represents a single MIDI clip on a `MidiTrack`, containing an editable `MidiSequence`. `IDisposable`.                                                    |
| [`MidiSequence`](#editing-midisequence)               | A mutable container for editable MIDI data, including notes and automation, within a `MidiSegment`.                                                      |
| [`AudioSegmentSettings`](#editing-audiosegmentsettings) | Configurable settings for an `AudioSegment` (volume, pan, fades, loop, reverse, speed, time stretch, modifiers, analyzers). Implements `IMidiMappable`. |
| [`TrackSettings`](#editing-tracksettings)             | Configurable settings for a `Track` or `MidiTrack` (volume, pan, mute, solo, enabled, modifiers). Implements `IMidiMappable`.                                 |
| [`LoopSettings`](#editing-loopsettings)               | (struct) Defines looping behavior for an `AudioSegment` (repetitions, target duration).                                                                |
| [`FadeCurveType`](#editing-fadecurvetype)             | (enum) Defines curve types for fade effects (Linear, Logarithmic, SCurve).                                                                             |
| [`MidiNote`](#editing-midinote)                       | Represents a single, editable MIDI note with start time, duration, pitch, and velocity.                                                                |
| [`ControlPoint`](#editing-controlpoint)               | Represents a single, editable MIDI automation point for CC or Pitch Bend data.                                                                         |
| [`TempoMarker`](#editing-tempomarker)                 | (record struct) Represents a tempo change at a specific point in the `Composition` timeline.                                                             |
| [`QuantizationSettings`](#editing-quantizationsettings) | (record) Defines settings for MIDI note quantization (Grid, Strength, Swing, etc.).                                                                    |
| [`MidiExporter`](#editing-midiexporter)               | Static utility for exporting a `Composition` to a Standard MIDI File (.mid).                                                                           |

### Editing.Mapping

| Class/Interface                                            | Description                                                                       |
|------------------------------------------------------------|-----------------------------------------------------------------------------------|
| [`MidiMappingManager`](#editingmapping-midimappingmanager) | Manages and executes a collection of real-time MIDI mappings for a `Composition`. |
| [`MidiMapping`](#editingmapping-midimapping)               | Represents a complete link between a MIDI message and a controllable parameter.   |

### Editing.Persistence

| Class/Interface                                                        | Description                                                                                                                            |
| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| [`CompositionProjectManager`](#editingpersistence-compositionprojectmanager) | Static class for saving and loading `Composition` projects to/from `.sfproj` files. Handles media consolidation and relinking.       |
| [`ProjectSaveOptions`](#editingpersistence-projectsaveoptions)           | Provides configurable options for saving a project, such as media consolidation, embedding, and folder names.                          |
| [`ProjectData`](#editingpersistence-projectdata)                         | DTO representing the root of a saved project file.                                                                                     |
| [`ProjectTrack`](#editingpersistence-projecttrack)                       | DTO for a `Track` within a saved project.                                                                                              |
| [`ProjectMidiTrack`](#editingpersistence-projectmiditrack)               | DTO for a `MidiTrack` within a saved project.                                                                                          |
| [`ProjectSegment`](#editingpersistence-projectsegment)                   | DTO for an `AudioSegment` within a saved project.                                                                                      |
| [`ProjectMidiSegment`](#editingpersistence-projectmidisegment)           | DTO for a `MidiSegment` within a saved project.                                                                                        |
| [`ProjectAudioSegmentSettings`](#editingpersistence-projectaudiosegmentsettings) | DTO for `AudioSegmentSettings` within a saved project.                                                                                 |
| [`ProjectTrackSettings`](#editingpersistence-projecttracksettings)       | DTO for `TrackSettings` within a saved project.                                                                                        |
| [`ProjectSourceReference`](#editingpersistence-projectsourcereference)   | DTO representing how an audio source is referenced in a project (file path, embedded data, consolidation).                             |
| [`ProjectEffectData`](#editingpersistence-projecteffectdata)             | DTO for serializing `SoundModifier`, `MidiModifier`, or `AudioAnalyzer` instances.                                                     |
| [`ProjectMidiMapping`](#editingpersistence-projectmidimapping)           | DTO for serializing a `MidiMapping`.                                                                                                   |
| [`ProjectTempoMarker`](#editingpersistence-projecttempomarker)           | DTO for serializing a `TempoMarker`.                                                                                                   |

### Enums

| Enum                                             | Description                                                                                                                                 |
| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |
| [`Capability`](#enums-capability)                | Specifies the capabilities of an audio device (Playback, Record, Mixed, Loopback).                                                          |
| [`ChannelLayout`](#enums-channellayout)          | Defines the physical or logical arrangement of channels in an audio stream (Mono, Stereo, Quad, Surround51, Surround71).                     |
| [`DeviceType`](#enums-devicetype)                | Specifies the type of audio device (Playback, Capture).                                                                                     |
| [`LogLevel`](#enums-loglevel)                    | Defines severity levels for log messages (Debug, Info, Warning, Error).                                                                     |
| [`MiniAudioBackend`](#enums-miniaudiobackend)    | Represents the available low-level audio backends that MiniAudio can use (e.g., WASAPI, CoreAudio, ALSA).                                     |
| [`PlaybackState`](#enums-playbackstate)          | Specifies the current playback state of a player or recorder (Stopped, Playing, Paused).                                                      |
| [`SampleFormat`](#enums-sampleformat)            | Specifies the format of audio samples (e.g., U8, S16, S24, S32, F32).                                                                     |
| [`FilterType`](#enums-filtertype)                | Specifies the type of filter to use in the `Filter` and `ParametricEqualizer` components (Peaking, LowShelf, HighShelf, BandPass, Notch, LowPass, HighPass) |
| [`EnvelopeGenerator.EnvelopeState`](#enums-envelopegenerator-envelopestate) | Specifies the current state of the envelope generator (Idle, Attack, Decay, Sustain, Release) |
| [`EnvelopeGenerator.TriggerMode`](#enums-envelopegenerator-triggermode) | Specifies how the envelope generator is triggered (NoteOn, Gate, Trigger) |
| [`LowFrequencyOscillator.WaveformType`](#enums-lowfrequencyoscillator-waveformtype) | Specifies the waveform type for the low-frequency oscillator (Sine, Square, Triangle, Sawtooth, ReverseSawtooth, Random, SampleAndHold) |
| [`LowFrequencyOscillator.TriggerMode`](#enums-lowfrequencyoscillator-triggermode) | Specifies how the LFO is triggered (FreeRunning, NoteTrigger) |
| [`Oscillator.WaveformType`](#enums-oscillator-waveformtype) | Specifies the waveform type for the oscillator (Sine, Square, Sawtooth, Triangle, Noise, Pulse) |
| [`SurroundPlayer.SpeakerConfiguration`](#enums-surroundplayer-speakerconfiguration) | Specifies the speaker configuration for the surround player (Stereo, Quad, Surround51, Surround71, Custom) |
| [`SurroundPlayer.PanningMethod`](#enums-surroundplayer-panningmethod) | Specifies the panning method for the surround player (Linear, EqualPower, Vbap) |
| [`FadeCurveType`](#editing-fadecurvetype)	   | Specifies curve types for fade effects (Linear, Logarithmic, SCurve). |
| **MIDI Enums** (`SoundFlow.Midi...`)             |                                                                                                                                             |
| [`MidiCommand`](#midi-enums-midicommand)         | Represents the command portion of a MIDI status byte (e.g., NoteOn, ControlChange).                                                           |
| [`PortMidiError`](#midi-portmidi-enums-portmidierror) | Error codes returned by the PortMidi library.                                                                                               |
| [`SyncMode`](#midi-portmidi-enums-syncmode)      | Defines the MIDI synchronization mode (Off, Master, Slave).                                                                                   |
| [`SyncSource`](#midi-portmidi-enums-syncsource)  | Defines the source of synchronization when in Slave mode (Internal, MidiClock, MTC).                                                          |
| [`SyncStatus`](#midi-portmidi-enums-syncstatus)  | Represents the current lock status of MIDI synchronization (Unlocked, Locked).                                                                |
| **`SoundFlow.Extensions.WebRtc.Apm` Enums**          |                                                                                                                                             |
| [`ApmError`](#extensions-webrtc-apm-apmerror)    | Error codes returned by the WebRTC Audio Processing Module.                                                                                |
| [`NoiseSuppressionLevel`](#extensions-webrtc-apm-noisesuppressionlevel) | Specifies noise suppression levels (Low, Moderate, High, VeryHigh).                                                                  |
| [`GainControlMode`](#extensions-webrtc-apm-gaincontrolmode) | Specifies gain controller modes (AdaptiveAnalog, AdaptiveDigital, FixedDigital).                                                        |
| [`DownmixMethod`](#extensions-webrtc-apm-downmixmethod) | Specifies methods for downmixing audio channels (AverageChannels, UseFirstChannel).                                                    |
| [`RuntimeSettingType`](#extensions-webrtc-apm-runtimesettingtype) | Specifies types of runtime settings for the WebRTC APM.                                                                        |

### Exceptions

| Class                                           | Description                                                                                   |
| ----------------------------------------------- | --------------------------------------------------------------------------------------------- |
| [`BackendException`](#exceptions-backendexception) | Base class for exceptions thrown by backend libraries (e.g., MiniAudio, PortMidi, FFmpeg).   |
| [`MiniaudioException`](#exceptions-miniaudioexception) | Thrown for errors originating from the MiniAudio backend.                                   |
| [`FFmpegException`](#exceptions-ffmpegexception)     | Thrown for errors originating from the FFmpeg codec backend.                                |
| [`PortBackendException`](#exceptions-portbackendexception) | Thrown for errors originating from the PortMidi backend.                                  |

### Extensions.WebRtc.Apm

| Class/Interface                                                                 | Description                                                                                                                                       |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`AudioProcessingModule`](#extensions-webrtc-apm-audioprocessingmodule)         | Provides access to the native WebRTC Audio Processing Module for advanced audio processing like AEC, NS, AGC.                                     |
| [`ApmConfig`](#extensions-webrtc-apm-apmconfig)                                 | Represents a configuration for the `AudioProcessingModule`, allowing enabling/disabling and setting parameters for various APM features.            |
| [`StreamConfig`](#extensions-webrtc-apm-streamconfig)                           | Represents a stream configuration (sample rate, channels) for audio processing within the APM.                                                    |
| [`ProcessingConfig`](#extensions-webrtc-apm-processingconfig)                   | Holds multiple `StreamConfig` instances for input, output, and reverse streams for the APM.                                                       |
| **Components Namespace**                                                        |                                                                                                                                                   |
| [`NoiseSuppressor`](#extensions-webrtc-apm-components-noisesuppressor)          | A component for offline/batch noise suppression using WebRTC APM, processing audio from an `ISoundDataProvider`.                                  |
| **Modifiers Namespace**                                                         |                                                                                                                                                   |
| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier)       | A `SoundModifier` that applies WebRTC APM features (AEC, NS, AGC, etc.) in real-time to an audio stream within the SoundFlow graph. Configurable. |

### Interfaces

| Interface                                           | Description                                                                                                                                  |
| --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| [`ICodecFactory`](#interfaces-icodecfactory)         | Defines a factory for creating `ISoundDecoder` and `ISoundEncoder` instances, enabling a pluggable codec system.                             |
| [`IMidiBackend`](#interfaces-imidibackend)           | Defines the contract for a pluggable MIDI backend, responsible for creating and managing MIDI devices.                                       |
| [`IMidiMappable`](#interfaces-imidimappable)         | Marks a class as being a target for real-time MIDI mapping. Requires a unique `Guid Id`.                                                       |
| [`IMidiControllable`](#interfaces-imidicontrollable) | Defines an interface for components that can be directly controlled by MIDI messages.                                                        |
| [`ISoundDataProvider`](#interfaces-isounddataprovider) | Defines a standard way to access audio data from various sources. Implements `IDisposable`.                                                |
| [`ISoundDecoder`](#interfaces-isounddecoder)         | Defines the contract for decoding audio data from a specific format into raw audio samples.                                                 |
| [`ISoundEncoder`](#interfaces-isoundencoder)         | Defines the contract for encoding raw audio samples into a specific format.                                                                 |
| [`ISoundPlayer`](#interfaces-isoundplayer)           | Defines the contract for controlling audio playback (Play, Pause, Stop, Seek, Looping, Speed, Volume).                                       |
| [`IVisualizationContext`](#interfaces-ivisualizationcontext) | Provides drawing methods for rendering audio visualizations. The implementation depends on the specific UI framework used.               |
| [`IVisualizer`](#interfaces-ivisualizer)             | Defines the contract for components that visualize audio data.                                                                            |

### Metadata
| Class/Interface                                        | Description                                                                                                  |
|--------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| [`SoundMetadataReader`](#metadata-soundmetadatareader) | Static class to read format information and metadata tags (artist, title, album art, etc.) from audio files. |
| [`SoundMetadataWriter`](#metadata-soundmetadatawriter) | Static class to write or remove metadata tags from audio files.                                              |
| [`SoundFormatInfo`](#metadata-soundformatinfo)         | Holds detailed information about an audio file's format, codec, duration, and tags.                          |

### Midi
| Class/Interface                      | Description                                                                                                                  |
|--------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| [`MidiManager`](#midi-midimanager)   | Central hub for managing MIDI devices, routing (`MidiRoute`), and MPE configuration. Accessed via `AudioEngine.MidiManager`. |
| [`MidiRoute`](#midi-midiroute)       | Represents a single connection from a MIDI source to a destination, with an optional chain of `MidiModifier`s.               |
| [`MidiMessage`](#midi-midimessage)   | (record struct) Represents a standard MIDI channel message (Note On, CC, Pitch Bend, etc.).                                  |
| [`MidiModifier`](#midi-midimodifier) | Abstract base class for real-time MIDI processing components (MIDI effects).                                                 |

### Midi.Modifier

| Class                                                          | Description                                                                                                         |
|----------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| [`ArpeggiatorModifier`](#midimodifier-arpeggiatormodifier)     | A stateful, temporal modifier that generates rhythmic patterns from held notes. Implements `ITemporalMidiModifier`. |
| [`ChannelFilterModifier`](#midimodifier-channelfiltermodifier) | Filters messages, allowing only those on a specific MIDI channel to pass.                                           |
| [`HarmonizerModifier`](#midimodifier-harmonizermodifier)       | Generates chords from single notes based on a list of intervals.                                                    |
| [`RandomizerModifier`](#midimodifier-randomizermodifier)       | Introduces randomness to note events, affecting probability, velocity, and pitch.                                   |
| [`TransposeModifier`](#midimodifier-transposemodifier)         | Transposes the pitch of Note On/Off messages by a fixed semitone amount.                                            |
| [`VelocityModifier`](#midimodifier-velocitymodifier)           | Reshapes velocity data using clamping, offsets, and non-linear curves.                                              |

### Midi.PortMidi

| Class/Interface                                        | Description                                                                                                  |
|--------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| [`PortMidiBackend`](#midiportmidi-portmidibackend)     | Concrete implementation of `IMidiBackend` using the PortMidi library. Provides device I/O and MIDI synchronization. |
| [`PortMidiExtensions`](#midiportmidi-portmidiextensions) | Provides the `UsePortMidi()` extension method for easy registration with the `AudioEngine`.                 |

### Modifiers

| Class                                                                                                                        | Description                                                                                                                                                                                           |
|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [`AlgorithmicReverbModifier`](#modifiers-algorithmicreverbmodifier)                                                          | Simulates reverberation using a network of comb and all-pass filters. Now supports multi-channel processing and MIDI control.                                                                         |
| [`BassBoosterModifier`](#modifiers-bassboostmodifier)                                                                        | Enhances low-frequency content using a resonant low-pass filter. Supports MIDI control.                                                                                                               |
| [`ChorusModifier`](#modifiers-chorusmodifier)                                                                                | Creates a chorus effect by mixing delayed and modulated copies of the signal. Supports MIDI control.                                                                                                  |
| [`CompressorModifier`](#modifiers-compressormodifier)                                                                        | Reduces the dynamic range of the audio signal using a compressor algorithm.                                                                                                                           |
| [`DelayModifier`](#modifiers-delaymodifier)                                                                                  | Applies a delay effect with feedback and optional low-pass filtering of the delayed signal. Supports MIDI control.                                                                                    |
| [`Filter`](#modifiers-filter)                                                                                                | `SoundModifier` that applies a digital filter (low-pass, high-pass, band-pass, notch) to the audio signal. Supports MIDI control.                                                                     |
| [`FrequencyBandModifier`](#modifiers-frequencybandmodifier)                                                                  | Allows boosting or cutting specific frequency bands using a combination of low-pass and high-pass filters.                                                                                            |
| [`ParametricEqualizer`](#modifiers-parametricequalizer)                                                                      | Provides precise control over the frequency spectrum with multiple configurable bands, each of which can be set as a peaking, low-shelf, high-shelf, band-pass, notch, low-pass, or high-pass filter. |
| [`MultiChannelChorusModifier`](#modifiers-multichannelchorusmodifier)                                                        | Creates a chorus effect with independent processing for each channel, allowing for rich spatial effects.                                                                                              |
| [`ResamplerModifier`](#modifiers-resamplermodifier)                                                                          | A real-time resampling modifier that changes the playback speed and pitch of an audio signal.                                                                                                         |
| [`TrebleBoostModifier`](#modifiers-trebleboostmodifier)                                                                      | Enhances high-frequency content using a high-pass filter. Supports MIDI control.                                                                                                                      |
| [`VocalExtractorModifier`](#modifiers-vocalextractormodifier)                                                                | An advanced vocal extraction effect that isolates vocals using spatial (Mid-Side) processing for stereo pairs and spectral gating for mono channels.                                                  |
| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier) (from `SoundFlow.Extensions.WebRtc.Apm.Modifiers`) | Applies WebRTC APM features like echo cancellation, noise suppression, and AGC in real-time.                                                                                                          |

### Providers

| Class                                                         | Description                                                                                                                                  |
| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| [`AssetDataProvider`](#providers-assetdataprovider)           | `ISoundDataProvider` that decodes and loads an entire audio file into memory. Automatically detects format. Implements `IDisposable`.           |
| [`StreamDataProvider`](#providers-streamdataprovider)         | `ISoundDataProvider` that reads and decodes audio from a `Stream` on-demand. Automatically detects format. Implements `IDisposable`.            |
| [`MicrophoneDataProvider`](#providers-microphonedataprovider) | `ISoundDataProvider` that captures and provides audio data from an `AudioCaptureDevice` in real-time. Implements `IDisposable`.                 |
| [`ChunkedDataProvider`](#providers-chunkeddataprovider)       | `ISoundDataProvider` that reads and decodes audio in chunks, improving efficiency for large files. Automatically detects format. Implements `IDisposable`. |
| [`NetworkDataProvider`](#providers-networkdataprovider)       | `ISoundDataProvider` that provides audio data from a network source (direct URL or HLS playlist). Automatically detects format. Implements `IDisposable`. |
| [`QueueDataProvider`](#providers-queuedataprovider)           | `ISoundDataProvider` fed by an external source in real-time, ideal for generated or procedural audio. Implements `IDisposable`.                |
| [`RawDataProvider`](#providers-rawdataprovider)               | `ISoundDataProvider` for reading raw PCM audio data from a stream or array. Implements `IDisposable`.                                        |
| [`MidiDataProvider`](#providers-mididataprovider)             | Processes a `MidiFile` into a single, time-ordered sequence of events for playback by a `Sequencer`.                                         |


### Structs

| Struct                                       | Description                                                                                    |
| -------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| [`AudioFormat`](#structs-audioformat)        | (record struct) Represents the format of an audio stream (sample format, channels, layout, sample rate). |
| [`DeviceInfo`](#structs-deviceinfo)          | Represents information about an audio device, including ID, name, and supported formats.         |
| [`MidiDeviceInfo`](#structs-midideviceinfo)  | (record struct) Represents information about a MIDI device, including its backend-specific ID and name. |
| [`NativeDataFormat`](#structs-nativedataformat) | Represents a native data format supported by an audio device (format, channels, sample rate). |
| [`Result`](#structs-result)                  | (struct) Represents the outcome of an operation, which can be a success or a failure with an `IError`. |
| [`IError`](#structs-ierror)                  | (interface) and various record implementations (`ValidationError`, `NotFoundError`, etc.) define a structured error handling system. |

### Synthesis

| Class/Interface | Description |
|---|---|
| [`Synthesizer`](#synthesis-synthesizer) | A polyphonic, multi-timbral `SoundComponent` that generates audio from MIDI messages using an `IInstrumentBank`. |
| [`Sequencer`](#synthesis-sequencer) | A `SoundComponent` that plays back MIDI events from a `MidiDataProvider` with sample-accurate timing. |
| [`IInstrumentBank`](#synthesis-iinstrumentbank) | Interface for a collection of instruments, accessible via MIDI bank and program numbers. |
| [`SoundFontBank`](#synthesis-soundfontbank) | An `IInstrumentBank` implementation that loads instruments from a SoundFont 2 (SF2) file. |
| [`BasicInstrumentBank`](#synthesis-basicinstrumentbank) | A simple, procedural `IInstrumentBank` implementation useful for testing and fallbacks. |
| [`MultiInstrumentBank`](#synthesis-multiinstrumentbank) | A composite `IInstrumentBank` that layers multiple banks, allowing for stacked sounds and complex fallback chains. |

### Synthesis.Instruments

| Class                                         | Description                                                                                                                  |
|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| [`Instrument`](#synthesis-instruments-instrument) | Represents a single patch containing mappings that link key/velocity ranges to voice definitions.                            |
| [`VoiceDefinition`](#synthesis-instruments-voicedefinition) | A factory blueprint for creating synthesizer voices with specific parameters (oscillator type, ADSR, sample data).           |
| [`VoiceMapping`](#synthesis-instruments-voicemapping) | Defines the criteria (Min/Max Key, Min/Max Velocity) under which a specific `VoiceDefinition` is used.                       |

### Synthesis.Interfaces

| Interface                                   | Description                                                                                             |
|---------------------------------------------|---------------------------------------------------------------------------------------------------------|
| [`IGenerator`](#synthesis-interfaces-igenerator) | Interface for components in the voice signal chain (oscillators, samplers, envelopes) that generate audio or control signals. |
| [`IVoice`](#synthesis-interfaces-ivoice)         | Interface representing an active, sounding voice within the synthesizer.                                |

### Utils

| Class                                           | Description                                                                                                                                             |
|-------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
| [`ChannelMixer`](#utils-channelmixer)           | High-performance static class for mixing audio channels between different layouts (e.g., stereo to mono).                                               |
| [`Extensions`](#utils-extensions)               | Provides extension methods for working with audio data and other utility functions, including `ReadArray<T>` for reading structures from native memory. |
| [`Log`](#utils-log)                             | Provides a centralized, decoupled logging mechanism. Subscribe to `Log.OnLog` to capture library messages.                                              |
| [`MathHelper`](#utils-mathhelper)               | Provides mathematical functions, including optimized FFT, window functions, and a new `ResampleLinear` method.                                          |
| [`MidiTimeConverter`](#utils-miditimeconverter) | Static utility to convert between MIDI ticks and `TimeSpan` based on a tempo map.                                                                       |
| [`ControllableParameterAttribute`](#utils-controllableparameterattribute) | Attribute used to expose properties of `IMidiMappable` objects to the MIDI mapping system. |

### Visualization

| Class/Interface                                         | Description                                                                                                                                                                                      |
| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`LevelMeterAnalyzer`](#visualization-levelmeteranalyzer) | `AudioAnalyzer` that calculates the RMS (root mean square) and peak levels of an audio signal.                                                                                                  |
| [`LevelMeterVisualizer`](#visualization-levelmetervisualizer) | `IVisualizer` that displays a level meter showing the current RMS or peak level of the audio.                                                                                                 |
| [`SpectrumAnalyzer`](#visualization-spectrumanalyzer)    | `AudioAnalyzer` that computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).                                                                                         |
| [`SpectrumVisualizer`](#visualization-spectrumvisualizer)  | `IVisualizer` that renders a bar graph representing the frequency spectrum of the audio.                                                                                                        |
| [`WaveformVisualizer`](#visualization-waveformvisualizer)  | `IVisualizer` that draws the waveform of the audio signal.                                                                                                                                     |


## Detailed Class and Interface Documentation

This section provides more in-depth information about some of the key classes and interfaces.

### Abstracts `AudioAnalyzer`

```csharp
public abstract class AudioAnalyzer : IMidiMappable
{
    protected AudioAnalyzer(AudioFormat format, IVisualizer? visualizer = null);

    public Guid Id { get; }
    public virtual string Name { get; set; }
    public bool Enabled { get; set; } = true;
    public AudioFormat Format { get; }

    public void Process(Span<float> buffer, int channels);
    protected abstract void Analyze(Span<float> buffer, int channels);
}
```

**Properties:**

*   `Id`: A unique identifier for the analyzer instance, used for MIDI mapping.
*   `Name`: The name of the analyzer.
*   `Enabled`: Gets or sets whether the analyzer is active. If false, `Analyze` is skipped.
*   `Format`: The audio format the analyzer is configured to process.

**Methods:**

*   `Process(Span<float> buffer, int channels)`: Processes the audio data, calling `Analyze` and then sending data to the attached visualizer.
*   `Analyze(Span<float> buffer, int channels)`: Abstract method to be implemented by derived classes to perform audio analysis on the provided buffer.

### Abstracts `AudioEngine`

```csharp
public abstract class AudioEngine : IDisposable
{
    // Properties
    public DeviceInfo[] PlaybackDevices { get; protected set; }
    public DeviceInfo[] CaptureDevices { get; protected set; }
    public MidiDeviceInfo[] MidiInputDevices { get; protected set; }
    public MidiDeviceInfo[] MidiOutputDevices { get; protected set; }
    public MidiManager MidiManager { get; }
    public bool IsDisposed { get; private set; }

    // Events
    public event EventHandler<DeviceEventArgs>? DeviceStarted;
    public event EventHandler<DeviceEventArgs>? DeviceStopped;
    public event EventHandler<AudioFramesRenderedEventArgs>? AudioFramesRendered;

    // Codec Management
    public void RegisterCodecFactory(ICodecFactory factory);
    public bool UnregisterCodecFactory(string factoryId);
    public bool SetCodecPriority(string factoryId, int newPriority);
    public IReadOnlyList<ICodecFactory> GetRegisteredCodecs(string formatId);
    public ISoundEncoder CreateEncoder(Stream stream, string formatId, AudioFormat format);
    public ISoundDecoder CreateDecoder(Stream stream, string formatId, AudioFormat format);
    public ISoundDecoder CreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);

    // MIDI
    public void UseMidiBackend(IMidiBackend midiBackend);
    public virtual void UpdateMidiDevicesInfo();

    // Device Management
    public abstract AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);
    public abstract AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);
    public abstract FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);
    public abstract AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);
    public abstract AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);
    public abstract AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);
    public abstract FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);
    public abstract void UpdateAudioDevicesInfo();

    // Component Management
    public void SoloComponent(SoundComponent component);
    public void UnsoloComponent(SoundComponent component);
    public SoundComponent? GetSoloedComponent();

    public void Dispose();
}
```

**Properties:**

*   `PlaybackDevices`: An array of available audio playback devices.
*   `CaptureDevices`: An array of available audio capture devices.
*   `MidiInputDevices`: An array of available MIDI input devices.
*   `MidiOutputDevices`: An array of available MIDI output devices.
*   `MidiManager`: The central manager for all MIDI devices and routing.
*   `IsDisposed`: Indicates whether the engine has been disposed.

**Events:**

*   `DeviceStarted`: Occurs when an audio device begins processing, useful for synchronization.
*   `DeviceStopped`: Occurs when an audio device stops processing.
*   `AudioFramesRendered`: Occurs after a block of audio is rendered, providing a sample-accurate clock for master synchronization.

**Methods:**

*   `SoloComponent(SoundComponent component)`: Solos a specific component, muting all others.
*   `UnsoloComponent(SoundComponent component)`: Removes the solo status from a component.
*   `GetSoloedComponent()`: Returns the currently soloed component, if any.
*   `UseMidiBackend(IMidiBackend midiBackend)`: Configures the engine to use a specific MIDI backend implementation (e.g., `PortMidiBackend`).
*   `RegisterCodecFactory(ICodecFactory factory)`: Registers a codec factory to add support for new audio formats.
*   `UnregisterCodecFactory(string factoryId)`: Unregisters a codec factory by its unique ID.
*   `SetCodecPriority(string factoryId, int newPriority)`: Overrides the priority of a registered codec factory.
*   `GetRegisteredCodecs(string formatId)`: Gets a list of registered factories for a specific format, ordered by priority.
*   `CreateEncoder(Stream stream, string formatId, ...)`: Creates a sound encoder by querying registered factories for the specified format ID (e.g., "wav", "mp3").
*   `CreateDecoder(Stream stream, string formatId, ...)`: Creates a sound decoder for a known format ID.
*   `CreateDecoder(Stream stream, out AudioFormat, ...)`: Creates a sound decoder by probing the stream with all available codecs to automatically detect the format.
*   `InitializePlaybackDevice(...)`: Initializes and returns a new playback device.
*   `InitializeCaptureDevice(...)`: Initializes and returns a new capture device.
*   `InitializeFullDuplexDevice(...)`: Initializes a high-level full-duplex device for simultaneous I/O.
*   `InitializeLoopbackDevice(...)`: Initializes a device for loopback recording of system audio.
*   `SwitchDevice(...)`: Switches an active device to a new physical device, preserving its state (components or subscribers).
*   `UpdateAudioDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.
*   `UpdateMidiDevicesInfo()`: Refreshes the `MidiInputDevices` and `MidiOutputDevices` lists from the configured MIDI backend.
*   `Dispose()`: Disposes the engine and all associated resources.

### Abstracts `AudioDevice`

```csharp
public abstract class AudioDevice : IDisposable
{
    public AudioEngine Engine { get; }
    public DeviceInfo? Info { get; protected init; }
    public DeviceConfig Config { get; protected init; }
    public Capability Capability { get; protected init; }
    public AudioFormat Format { get; }
    public bool IsRunning { get; protected set; }
    public bool IsDisposed { get; protected set; }

    public event EventHandler? OnDisposed;

    public abstract void Start();
    public abstract void Stop();
    public abstract void Dispose();
}
```

**Description:** An abstract base class representing an initialized audio device, which is managed by an `AudioEngine`. This class encapsulates the common state and behavior for both playback and capture devices, serving as the foundation for all device interactions within the framework.

**Properties:**

*   `Engine`: Gets the parent `AudioEngine` that manages this device instance.
*   `Info`: Gets the informational struct (`DeviceInfo`) for the physical device being used. This contains details like the device name and ID. `null` if a default device was used without specific info.
*   `Config`: Gets the `DeviceConfig` object used to initialize the device. This holds any backend-specific configuration settings.
*   `Capability`: Gets the capability of this device (e.g., `Playback`, `Record`, `Loopback`).
*   `Format`: Gets the `AudioFormat` (sample rate, channels, bit depth) that the device was initialized with.
*   `IsRunning`: Gets a value indicating whether the device is currently started and processing audio.
*   `IsDisposed`: Gets a value indicating whether this device instance has been disposed and can no longer be used.

**Events:**

*   `OnDisposed`: This event is raised when the device's `Dispose()` method is called, signaling that it is being shut down and its resources are being released.

**Methods:**

*   `Start()`: Abstract method that must be implemented by derived classes to start the audio stream for the device.
*   `Stop()`: Abstract method that must be implemented by derived classes to stop the audio stream for the device.
*   `Dispose()`: Abstract method to release all resources used by the audio device.

### Abstracts `AudioPlaybackDevice`

```csharp
public abstract class AudioPlaybackDevice : AudioDevice
{
    public Mixer MasterMixer { get; }
}
```

**Description:** An abstract class that represents an initialized playback (output) audio device. It inherits from `AudioDevice` and extends it with functionality specific to audio output.

**Properties:**

*   `MasterMixer`: Gets the master `Mixer` for this device. All audio to be played on this device must be routed through this mixer. You can connect various `SoundComponent`s (like `SoundPlayer`, `Oscillator`, or other `Mixer`s) to this master mixer to combine them into a final output signal.

### Abstracts `AudioCaptureDevice`

```csharp
public abstract class AudioCaptureDevice : AudioDevice
{
    public event AudioProcessCallback? OnAudioProcessed;
    internal Delegate[] GetEventSubscribers();
}
```

**Description:** An abstract class that represents an initialized capture (input) audio device. It inherits from `AudioDevice` and provides the core mechanism for receiving audio data from an input source like a microphone.

**Events:**

*   `OnAudioProcessed`: This event is the primary way to receive audio data from the capture device. It is raised by the backend whenever a new chunk of audio samples has been captured. Subscribe to this event to process live audio input. The event delegate is `AudioProcessCallback(Span<float> samples, Capability capability)`.

**Methods:**

*   `GetEventSubscribers()`: (Internal) An internal method used by the `AudioEngine` to retrieve the list of subscribers to the `OnAudioProcessed` event. This is crucial for the device switching functionality, allowing event subscriptions to be preserved when moving from one physical device to another.

### Abstracts `FullDuplexDevice`

```csharp
public sealed class FullDuplexDevice : AudioDevice, IDisposable
{
    public AudioPlaybackDevice PlaybackDevice { get; }
    public AudioCaptureDevice CaptureDevice { get; }
    public Mixer MasterMixer => PlaybackDevice.MasterMixer;

    public event AudioProcessCallback? OnAudioProcessed;

    public override void Start();
    public override void Stop();
    public override void Dispose();
}
```

**Description:** A high-level, sealed class that simplifies full-duplex (simultaneous input and output) audio operations. It internally manages a paired `AudioPlaybackDevice` and `AudioCaptureDevice`, making it ideal for applications like live effects processing, VoIP, or real-time instrument monitoring where you need to listen to an input while producing an output.

**Properties:**

*   `PlaybackDevice`: Gets the underlying `AudioPlaybackDevice` instance used for audio output.
*   `CaptureDevice`: Gets the underlying `AudioCaptureDevice` instance used for audio input.
*   `MasterMixer`: A convenient shortcut to access the `MasterMixer` of the underlying `PlaybackDevice`. This is where you should route all audio you want to play out.

**Events:**

*   `OnAudioProcessed`: An event that is raised when audio data is captured from the input device. This event is a direct pass-through to the `OnAudioProcessed` event of the underlying `CaptureDevice`.

**Methods:**

*   `Start()`: Starts both the underlying capture and playback devices simultaneously.
*   `Stop()`: Stops both the underlying capture and playback devices simultaneously.
*   `Dispose()`: Stops and disposes of all resources, including the underlying `PlaybackDevice` and `CaptureDevice`.

### Abstracts `DeviceConfig`

```csharp
public abstract class DeviceConfig;
```
**Description:** A marker base class for creating backend-specific device configuration objects. This allows passing detailed, implementation-specific settings during device initialization. See `MiniAudioDeviceConfig` for an example.

### Abstracts `SoundComponent`

```csharp
public abstract class SoundComponent : IDisposable, IMidiMappable
{
    protected SoundComponent(AudioEngine engine, AudioFormat format);

    public Guid Id { get; }
    public AudioEngine Engine { get; }
    public AudioFormat Format { get; }
    public virtual string Name { get; set; }
    public Mixer? Parent { get; set; }
    public virtual float Volume { get; set; }
    public virtual float Pan { get; set; } // Range 0.0 (Left) to 1.0 (Right)
    public virtual bool Enabled { get; set; }
    public virtual bool Solo { get; set; }
    public virtual bool Mute { get; set; }
    public bool IsDisposed { get; private set; }

    public IReadOnlyList<SoundComponent> Inputs { get; }
    public IReadOnlyList<SoundModifier> Modifiers { get; }
    public IReadOnlyList<AudioAnalyzer> Analyzers { get; }

    public void ConnectInput(SoundComponent input);
    public void DisconnectInput(SoundComponent input);
    public void AddModifier(SoundModifier modifier);
    public void RemoveModifier(SoundModifier modifier);
    public void AddAnalyzer(AudioAnalyzer analyzer);
    public void RemoveAnalyzer(AudioAnalyzer analyzer);
    internal void Process(Span<float> outputBuffer, int channels);
    protected abstract void GenerateAudio(Span<float> buffer, int channels);
    public virtual void Dispose();
}
```

**Properties:**

*   `Id`: A unique identifier for the component instance, used for MIDI mapping.
*   `Engine`: The engine context this component belongs to.
*   `Format`: The audio format of this component.
*   `Name`: The name of the component.
*   `Parent`: The parent mixer of this component.
*   `Inputs`: Read-only list of connected input components.
*   `Modifiers`: Read-only list of applied modifiers.
*   `Analyzers`: Read-only list of attached audio analyzers.
*   `Volume`: The volume of the component's output.
*   `Pan`: The panning of the component's output (0.0 for full left, 0.5 for center, 1.0 for full right).
*   `Enabled`: Whether the component is enabled.
*   `Solo`: Whether the component is soloed.
*   `Mute`: Whether the component is muted.
*   `IsDisposed`: Indicates whether the component has been disposed.

**Methods:**

*   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.
*   `DisconnectInput(SoundComponent input)`: Disconnects an input from this component.
*   `AddModifier(SoundModifier modifier)`: Adds a sound modifier to the component.
*   `RemoveModifier(SoundModifier modifier)`: Removes a sound modifier from the component.
*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an audio analyzer to the component.
*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an audio analyzer from the component.
*   `Process(Span<float> outputBuffer, int channels)`: Processes the component's audio, applying modifiers and handling input/output connections.
*   `GenerateAudio(Span<float> buffer, int channels)`: Abstract method that derived classes must implement to generate or modify audio data.
*   `Dispose()`: Disposes the component and disconnects it from the audio graph.

### Abstracts `SoundModifier`

```csharp
public abstract class SoundModifier : IMidiMappable, IMidiControllable
{
    public Guid Id { get; }
    public virtual string Name { get; set; }
    public bool Enabled { get; set; } = true;

    public virtual void ProcessMidiMessage(MidiMessage message);
    public abstract float ProcessSample(float sample, int channel);
    public virtual void Process(Span<float> buffer, int channels);
}
```

**Properties:**

*   `Id`: A unique identifier for the modifier instance, used for MIDI mapping.
*   `Name`: The name of the modifier.
*   `Enabled`: Gets or sets whether the modifier is active and should process audio. Defaults to true.

**Methods:**

*   `ProcessMidiMessage(MidiMessage message)`: A virtual method that allows derived classes to be controlled by MIDI messages.
*   `ProcessSample(float sample, int channel)`: Abstract method to be implemented by derived classes to process a single audio sample.
*   `Process(Span<float> buffer, int channels)`: Processes a buffer of audio data. Applies `ProcessSample` to each sample if not overridden.

### Abstracts `SoundPlayerBase`

```csharp
public abstract class SoundPlayerBase : SoundComponent, ISoundPlayer
{
    protected SoundPlayerBase(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);

    public float PlaybackSpeed { get; set; }
    public PlaybackState State { get; private set; }
    public bool IsLooping { get; set; }
    public float Time { get; }
    public float Duration { get; }
    public int LoopStartSamples { get; }
    public int LoopEndSamples { get; }
    public float LoopStartSeconds { get; }
    public float LoopEndSeconds { get; }
    // Volume is inherited from SoundComponent

    public event EventHandler<EventArgs>? PlaybackEnded;

    protected override void GenerateAudio(Span<float> output, int channels);
    protected virtual void HandleEndOfStream(Span<float> remainingOutputBuffer, int channels);
    protected virtual void OnPlaybackEnded();

    public void Play();
    public void Pause();
    public void Stop();
    public bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);
    public bool Seek(float time);
    public bool Seek(int sampleOffset);
    public void SetLoopPoints(float startTime, float? endTime = -1f);
    public void SetLoopPoints(int startSample, int endSample = -1);
    public void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);
    public void Dispose();
}
```

**Properties:**

*   `PlaybackSpeed`: Gets or sets the playback speed (1.0 is normal). Values other than 1.0 use a WSOLA time stretcher for pitch preservation.
*   `State`: Gets the current playback state (`Stopped`, `Playing`, `Paused`).
*   `IsLooping`: Gets or sets whether looping is enabled.
*   `Time`: Gets the current playback position in seconds, affected by `PlaybackSpeed`.
*   `Duration`: Gets the total duration of the audio in seconds.
*   `LoopStartSamples`: Gets the loop start point in samples.
*   `LoopEndSamples`: Gets the loop end point in samples (-1 for end of audio).
*   `LoopStartSeconds`: Gets the loop start point in seconds.
*   `LoopEndSeconds`: Gets the loop end point in seconds (-1 for end of audio).
*   `Volume`: (Inherited from `SoundComponent`) Gets or sets the volume of the player.

**Events:**

*   `PlaybackEnded`: Occurs when playback reaches the end of the audio (not raised during looping).

**Methods:**

*   `GenerateAudio(Span<float> output)`: (Protected Override) Core audio generation logic, handles reading from data provider, resampling for playback speed, and looping.
*   `HandleEndOfStream(Span<float> remainingOutputBuffer)`: (Protected Virtual) Handles logic when the data provider reaches its end (looping or stopping).
*   `OnPlaybackEnded()`: (Protected Virtual) Invokes the `PlaybackEnded` event.
*   `Play()`: Starts or resumes playback.
*   `Pause()`: Pauses playback.
*   `Stop()`: Stops playback and resets the position to the beginning.
*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to a specific time using `TimeSpan`. Returns `true` if successful.
*   `Seek(float time)`: Seeks to a specific time in seconds. Returns `true` if successful.
*   `Seek(int sampleOffset)`: Seeks to a specific sample offset. Returns `true` if successful.
*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures loop points using start/end times in seconds.
*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures loop points using start/end sample indices.
*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures loop points using `TimeSpan`.
*   `Dispose()`: Disposes the player and its underlying data provider.

### Abstracts `WsolaTimeStretcher`
```csharp
public class WsolaTimeStretcher
{
    public WsolaTimeStretcher(int initialChannels = 2, float initialSpeed = 1.0f);

    public void SetChannels(int channels);
    public void SetSpeed(float speed);
    public int MinInputSamplesToProcess { get; }
    public void Reset();
    public float GetTargetSpeed();
    public int Process(ReadOnlySpan<float> input, Span<float> output, out int samplesConsumedFromInputBuffer, out int sourceSamplesRepresentedByOutput);
    public int Flush(Span<float> output);
}
```
**Description:** Implements the WSOLA (Waveform Similarity Overlap-Add) algorithm for real-time, pitch-preserved time stretching of audio. Allows changing playback speed without altering pitch. Used internally by `SoundPlayerBase` and `AudioSegment`.

### Backends.MiniAudio `MiniAudioCodecFactory`
```csharp
public sealed class MiniAudioCodecFactory : ICodecFactory
```
**Description:** The built-in codec factory registered by `MiniAudioEngine` with priority 0. Supports `wav`, `mp3`, and `flac` decoding via the native `miniaudio` library.

### Backends.MiniAudio `MiniAudioDecoder`

```csharp
internal sealed unsafe class MiniAudioDecoder : ISoundDecoder
{
    internal MiniAudioDecoder(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);

    public bool IsDisposed { get; private set; }
    public int Length { get; private set; } // Length can be updated after initial check
    public SampleFormat SampleFormat { get; }

    public event EventHandler<EventArgs>? EndOfStreamReached;

    public int Decode(Span<float> samples);
    public void Dispose();
    public bool Seek(int offset);
}
```

**Properties:**

* `IsDisposed`: Indicates whether the decoder has been disposed.
* `Length`: The total length of the decoded audio data in samples. *Note: Can be updated after initial checks if the stream length was not immediately available.*
* `SampleFormat`: The sample format of the decoded audio data.

**Events:**

* `EndOfStreamReached`: Occurs when the end of the audio stream is reached during decoding.

**Methods:**

* `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer. Internally synchronized.
* `Dispose()`: Releases the resources used by the decoder.
* `Seek(int offset)`: Seeks to the specified offset within the audio stream (in samples). Internally synchronized.

### Backends.MiniAudio `MiniAudioEncoder`

```csharp
internal sealed unsafe class MiniAudioEncoder : ISoundEncoder
{
    public MiniAudioEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int channels, int sampleRate);

    public bool IsDisposed { get; private set; }

    public void Dispose();
    public int Encode(Span<float> samples);
}
```

**Properties:**

*   `IsDisposed`: Indicates whether the encoder has been disposed.

**Methods:**

*   `Dispose()`: Releases the resources used by the encoder.
*   `Encode(Span<float> samples)`: Encodes the provided audio samples and writes them to the output file or stream.

### Backends.MiniAudio `MiniAudioEngine`

```csharp
public class MiniAudioEngine : AudioEngine
{
    public MiniAudioEngine(IEnumerable<MiniAudioBackend>? backendPriority = null);

    public static IReadOnlyList<MiniAudioBackend> AvailableBackends { get; }
    public MiniAudioBackend ActiveBackend { get; }

    // Inherits all public methods from AudioEngine
    public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);
    public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);
    public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);
    public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);
    public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format);
    public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format);
    public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);
    public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);
    public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);

    public override void UpdateAudioDevicesInfo();
}
```

**Description:** This is the concrete implementation of the abstract `AudioEngine` class using the powerful `miniaudio` C library as its backend. It is responsible for all low-level audio I/O operations, including device discovery, initialization, and data processing callbacks. It manages the native `miniaudio` context and registers the default `MiniAudioCodecFactory` for WAV, MP3, and FLAC support. Because it handles the native interop, this class ensures that SoundFlow is cross-platform.

**Constructor:**

*   `MiniAudioEngine(IEnumerable<MiniAudioBackend>? backendPriority = null)`: Initializes the engine. Optionally accepts a prioritized list of backends to attempt to use.

**Properties:**

*   `AvailableBackends`: A static list of `MiniAudioBackend` enums that are supported on the current operating system.
*   `ActiveBackend`: The low-level audio backend that was successfully initialized and is currently in use.

**Methods:**

*   `InitializePlaybackDevice(...)`: Creates and returns a playback device connected to a physical sound output device.
*   `InitializeCaptureDevice(...)`: Creates and returns a capture device for a physical sound input device.
*   `InitializeFullDuplexDevice(...)`: Creates and returns a `FullDuplexDevice` that manages both a playback and capture stream simultaneously.
*   `InitializeLoopbackDevice(...)`: Creates a special capture device that records the system's audio output. On Windows, this uses the WASAPI loopback feature.
*   `CreateEncoder(...)`: Returns a `MiniAudioEncoder` instance, currently supporting `.wav` file encoding.
*   `CreateDecoder(...)`: Returns a `MiniAudioDecoder` instance, capable of decoding various audio formats like `.wav`, `.mp3`, and `.flac`.
*   `SwitchDevice(...)`: Implements the logic to seamlessly switch between physical audio devices at runtime, preserving the state of the audio graph.
*   `UpdateAudioDevicesInfo()`: Communicates with the `miniaudio` backend to refresh the lists of available playback and capture devices.

---

### Backends.MiniAudio `MiniAudioDeviceConfig`

```csharp
public class MiniAudioDeviceConfig : DeviceConfig
{
    public uint PeriodSizeInFrames { get; set; }
    public uint PeriodSizeInMilliseconds { get; set; }
    public uint Periods { get; set; }
    public bool NoPreSilencedOutputBuffer { get; set; }
    public bool NoClip { get; set; }
    public bool NoDisableDenormals { get; set; }
    public bool NoFixedSizedCallback { get; set; }
    public DeviceSubConfig Playback { get; set; }
    public DeviceSubConfig Capture { get; set; }
    public WasapiSettings? Wasapi { get; set; }
    public CoreAudioSettings? CoreAudio { get; set; }
    public AlsaSettings? Alsa { get; set; }
    public PulseSettings? Pulse { get; set; }
    public OpenSlSettings? OpenSL { get; set; }
    public AAudioSettings? AAudio { get; set; }
}
```

**Description:** A detailed configuration object that inherits from `DeviceConfig` and is specifically designed for initializing a MiniAudio device. It provides fine-grained control over buffer sizes, performance flags, and exposes nested configuration classes for OS-specific audio backends like WASAPI (Windows), CoreAudio (macOS), ALSA (Linux), and others. This allows developers to tune performance and behavior for specific platforms.

**General Properties:**

*   `PeriodSizeInFrames`: Gets or sets the desired size of the internal processing buffer in frames (a frame is one sample for each channel). This gives precise, sample-level control over buffer latency. Takes precedence over `PeriodSizeInMilliseconds`. Default is 0 (backend default).
*   `PeriodSizeInMilliseconds`: Gets or sets the desired size of the internal processing buffer in milliseconds. A more intuitive way to control latency. Default is 0 (backend default).
*   `Periods`: Gets or sets the number of periods to use for the device's buffer. Default is 0 (backend default).
*   `NoPreSilencedOutputBuffer`: If `true`, the output buffer passed to the audio callback will contain undefined data instead of being cleared to silence. This can be a minor performance optimization if you are always filling the entire buffer. Default is `false`.
*   `NoClip`: If `true`, the backend will not clip F32 sample values that are outside the [-1.0, 1.0] range. Default is `false`.
*   `NoDisableDenormals`: If `true`, the backend will not attempt to disable denormal floating-point numbers, which can slightly improve precision at the cost of performance on some CPUs. Default is `false`.
*   `NoFixedSizedCallback`: If `true`, the backend is not required to provide buffers of a fixed size in every callback. This can be an optimization if your processing logic is flexible. Default is `false`.

**Sub-Configurations:**

*   `Playback`: A `DeviceSubConfig` object for playback-specific settings.
*   `Capture`: A `DeviceSubConfig` object for capture-specific settings.
*   `Wasapi`: A `WasapiSettings` object for Windows-specific settings. Only used on Windows.
*   `CoreAudio`: A `CoreAudioSettings` object for macOS/iOS-specific settings. Only used on Apple platforms.
*   `Alsa`: An `AlsaSettings` object for Linux-specific settings. Only used on Linux with ALSA.
*   `Pulse`: A `PulseSettings` object for Linux-specific settings. Only used on Linux with PulseAudio.
*   `OpenSL`: An `OpenSlSettings` object for Android-specific settings.
*   `AAudio`: An `AAudioSettings` object for modern Android-specific settings.

---

#### `DeviceSubConfig` (Nested Class)

```csharp
public class DeviceSubConfig
{
    public ShareMode ShareMode { get; set; } = ShareMode.Shared;
    internal bool IsLoopback { get; set; }
}
```

**Description:** Contains settings for a specific direction (playback or capture).

**Properties:**

*   `ShareMode`: Specifies how the device is opened. `ShareMode.Shared` (default) allows multiple applications to use the device. `ShareMode.Exclusive` attempts to gain exclusive control for the lowest possible latency, but may not be supported by all devices.
*   `IsLoopback`: (Internal) A flag used to indicate that a capture device should be initialized in loopback mode.

---

#### `WasapiSettings` (Nested Class)

```csharp
public class WasapiSettings
{
    public WasapiUsage Usage { get; set; } = WasapiUsage.Default;
    public bool NoAutoConvertSRC { get; set; }
    public bool NoDefaultQualitySRC { get; set; }
    public bool NoAutoStreamRouting { get; set; }
    public bool NoHardwareOffloading { get; set; }
}
```
**Description:** Contains settings specific to the WASAPI audio backend on Windows.

**Properties:**

*   `Usage`: Hints to the OS about the stream's purpose (`Default`, `Games`, `ProAudio`), which can affect system-level audio processing and prioritization.
*   `NoAutoConvertSRC`: If `true`, disables automatic sample rate conversion by WASAPI, letting MiniAudio handle it instead.
*   `NoDefaultQualitySRC`: If `true`, prevents WASAPI from using its default quality for sample rate conversion.
*   `NoAutoStreamRouting`: If `true`, disables automatic stream routing by the OS.
*   `NoHardwareOffloading`: If `true`, disables WASAPI's hardware offloading feature.

---

#### `CoreAudioSettings` (Nested Class)

```csharp
public class CoreAudioSettings
{
    public bool AllowNominalSampleRateChange { get; set; }
}
```
**Description:** Contains settings specific to the CoreAudio backend on macOS and iOS.

**Properties:**

*   `AllowNominalSampleRateChange`: If `true`, allows the OS to change the device's sample rate to match the stream. Typically used on desktop macOS.

---

#### `AlsaSettings` (Nested Class)

```csharp
public class AlsaSettings
{
    public bool NoMMap { get; set; }
    public bool NoAutoFormat { get; set; }
    public bool NoAutoChannels { get; set; }
    public bool NoAutoResample { get; set; }
}
```
**Description:** Contains settings specific to the ALSA audio backend on Linux.

**Properties:**

*   `NoMMap`: If `true`, disables memory-mapped (MMap) mode for ALSA.
*   `NoAutoFormat`: If `true`, prevents ALSA from performing automatic format conversion.
*   `NoAutoChannels`: If `true`, prevents ALSA from performing automatic channel count conversion.
*   `NoAutoResample`: If `true`, prevents ALSA from performing automatic resampling.

---

#### `PulseSettings` (Nested Class)

```csharp
public class PulseSettings
{
    public string? StreamNamePlayback { get; set; }
    public string? StreamNameCapture { get; set; }
}
```
**Description:** Contains settings specific to the PulseAudio backend on Linux.

**Properties:**

*   `StreamNamePlayback`: Sets a custom name for the playback stream as it appears in PulseAudio volume controls.
*   `StreamNameCapture`: Sets a custom name for the capture stream.

---

#### `OpenSlSettings` (Nested Class)

```csharp
public class OpenSlSettings
{
    public OpenSlStreamType StreamType { get; set; }
    public OpenSlRecordingPreset RecordingPreset { get; set; }
}
```
**Description:** Contains settings specific to the OpenSL ES backend on Android.

**Properties:**

*   `StreamType`: Specifies the type of audio stream (e.g., `Voice`, `Media`, `Alarm`) to help Android manage audio focus and routing.
*   `RecordingPreset`: Optimizes the microphone input for a specific scenario (e.g., `VoiceCommunication`, `Camcorder`).

---

#### `AAudioSettings` (Nested Class)

```csharp
public class AAudioSettings
{
    public AAudioUsage Usage { get; set; }
    public AAudioContentType ContentType { get; set; }
    public AAudioInputPreset InputPreset { get; set; }
    public AAudioAllowedCapturePolicy AllowedCapturePolicy { get; set; }
}
```
**Description:** Contains settings specific to the modern AAudio backend on Android.

**Properties:**

*   `Usage`: Hints to the system about the stream's purpose (e.g., `Media`, `Game`, `Assistant`) for optimized routing and resource management.
*   `ContentType`: Describes the type of content being played (e.g., `Music`, `Speech`, `Sonification`).
*   `InputPreset`: Specifies a configuration for the audio input, optimizing it for scenarios like `VoiceRecognition` or `Camcorder`.
*   `AllowedCapturePolicy`: Controls whether other applications are allowed to capture the audio from this stream.

### Components `EnvelopeGenerator`

```csharp
public class EnvelopeGenerator : SoundComponent
{
    public EnvelopeGenerator(AudioEngine engine, AudioFormat format) : base(engine, format);

    public EnvelopeState State { get; }
    public float AttackTime { get; set; }
    public float DecayTime { get; set; }
    public override string Name { get; set; }
    public float ReleaseTime { get; set; }
    public bool Retrigger { get; set; }
    public float SustainLevel { get; set; }
    public TriggerMode Trigger { get; set; }

    public event Action<float>? LevelChanged;

    protected override void GenerateAudio(Span<float> buffer, int channels);
    public float ProcessSample();
    public void TriggerOff();
    public void TriggerOn();
}
```

**Properties:**

*   `State`: Gets the current stage of the envelope (`Idle`, `Attack`, etc.).
*   `AttackTime`: The attack time of the envelope (in seconds).
*   `DecayTime`: The decay time of the envelope (in seconds).
*   `Name`: The name of the envelope generator.
*   `ReleaseTime`: The release time of the envelope (in seconds).
*   `Retrigger`: Whether to retrigger the envelope on each new trigger.
*   `SustainLevel`: The sustain level of the envelope.
*   `Trigger`: The trigger mode (`NoteOn`, `Gate`, `Trigger`).

**Events:**

*   `LevelChanged`: Occurs when the envelope level changes.

**Methods:**

*   `GenerateAudio(Span<float> buffer, int channels)`: Fills a buffer with the envelope signal.
*   `ProcessSample()`: Processes and returns a single sample of the envelope's output.
*   `TriggerOff()`: Triggers the release stage of the envelope.
*   `TriggerOn()`: Triggers the attack stage of the envelope.

### Components `LowFrequencyOscillator`

```csharp
public class LowFrequencyOscillator : SoundComponent
{
    public LowFrequencyOscillator(AudioEngine engine, AudioFormat format) : base(engine, format);

    public float Depth { get; set; }
    public TriggerMode Mode { get; set; }
    public override string Name { get; set; }
    public float Phase { get; set; }
    public float Rate { get; set; }
    public WaveformType Type { get; set; }

    protected override void GenerateAudio(Span<float> buffer, int channels);
    public float GetLastOutput();
    public void Trigger();
}
```

**Properties:**

*   `Depth`: The depth of the LFO's modulation.
*   `Mode`: The trigger mode (`FreeRunning`, `NoteTrigger`).
*   `Name`: The name of the LFO.
*   `Phase`: The initial phase of the LFO.
*   `Rate`: The rate (frequency) of the LFO.
*   `Type`: The waveform type (`Sine`, `Square`, `Triangle`, `Sawtooth`, `ReverseSawtooth`, `Random`, `SampleAndHold`).

**Methods:**

*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the LFO signal.
*   `GetLastOutput()`: Returns the last generated output sample.
*   `Trigger()`: Triggers the LFO (if in `NoteTrigger` mode).

### Components `Mixer`

```csharp
public sealed class Mixer : SoundComponent
{
    public Mixer(AudioEngine engine, AudioFormat format, bool isMasterMixer = false);

    public IReadOnlyCollection<SoundComponent> Components { get; }
    public AudioPlaybackDevice? ParentDevice { get; internal set; }
    public bool IsMasterMixer { get; }
    public override string Name { get; set; }

    public void AddComponent(SoundComponent component);
    protected override void GenerateAudio(Span<float> buffer, int channels);
    public void RemoveComponent(SoundComponent component);
    public override void Dispose();
}
```

**Properties:**

*   `Components`: A read-only collection of sound components mixed by this mixer.
*   `ParentDevice`: The playback device this mixer is the master for, if any.
*   `IsMasterMixer`: A value indicating whether this is a master mixer for a device.
*   `Name`: The name of the mixer.

**Methods:**

*   `AddComponent(SoundComponent component)`: Adds a component to the mixer.
*   `GenerateAudio(Span<float> buffer, int channels)`: Mixes the audio from all connected components.
*   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.
*   `Dispose()`: Disposes the mixer and all components within it.

### Components `Oscillator`

```csharp
public class Oscillator : SoundComponent
{
    public Oscillator(AudioEngine engine, AudioFormat format) : base(engine, format);

    public float Amplitude { get; set; }
    public float Frequency { get; set; }
    public override string Name { get; set; }
    public float PhaseOffset { get; set; }
    public float PulseWidth { get; set; }
    public WaveformType Type { get; set; }

    protected override void GenerateAudio(Span<float> buffer, int channels);
}
```

**Properties:**

*   `Amplitude`: The amplitude of the oscillator.
*   `Frequency`: The frequency of the oscillator.
*   `Name`: The name of the oscillator.
*   `PhaseOffset`: The phase offset of the waveform in radians.
*   `PulseWidth`: The pulse width (for pulse waveforms).
*   `Type`: The band-limited waveform type (`Sine`, `Square`, `Sawtooth`, `Triangle`, `Noise`, `Pulse`).

**Methods:**

*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the oscillator's output.

### Components `Recorder`

```csharp
public class Recorder : IDisposable
{
    public Recorder(AudioCaptureDevice captureDevice, string filePath, string formatId = "wav");
    public Recorder(AudioCaptureDevice captureDevice, Stream stream, string formatId = "wav");
    public Recorder(AudioCaptureDevice captureDevice, AudioProcessCallback callback);

    public PlaybackState State { get; private set; }
    public readonly SampleFormat SampleFormat;
    public readonly string FormatId;
    public readonly string? FilePath;
    public readonly int SampleRate;
    public readonly int Channels;
    public readonly Stream Stream;
    public AudioProcessCallback? ProcessCallback;
    public ReadOnlyCollection<SoundModifier> Modifiers { get; }
    public ReadOnlyCollection<AudioAnalyzer> Analyzers { get; }

    public void StartRecording(SoundTags? tags = null);
    public void ResumeRecording();
    public void PauseRecording();
    public Task StopRecordingAsync();
    public void StopRecording();
    public void AddModifier(SoundModifier modifier);
    public void RemoveModifier(SoundModifier modifier);
    public void AddAnalyzer(AudioAnalyzer analyzer);
    public void RemoveAnalyzer(AudioAnalyzer analyzer);
    public void Dispose();
}
```

**Properties:**

*   `Analyzers`: Gets a read-only collection of 'AudioAnalyzer" components applied to the recorder. Analyzers are used to process and extract data from the audio stream during recording.
*   `Modifiers`: Gets a read-only collection of "SoundModifier" components applied to the recorder. Modifiers are applied to the audio data before encoding or processing via callback, allowing for real-time audio effects during recording.
*   `Channels`: The number of channels to record.
*   `State`: The current recording state (`Stopped`, `Playing`, `Paused`).
*   `FormatId`: The string identifier for the encoding format (e.g., "wav", "flac").
*   `FilePath`: The final destination file path, if recording to a file.
*   `Stream`: The stream to write encoded recorded audio to.
*   `ProcessCallback`: A callback for processing recorded audio in real time.
*   `SampleRate`: The sample rate for recording.
*   `SampleFormat`: The sample format for recording.

**Methods:**

*   `StartRecording(SoundTags? tags = null)`: Starts the recording. Optionally accepts metadata tags to be written to the file upon completion.
*   `ResumeRecording()`: Resumes a paused recording.
*   `PauseRecording()`: Pauses the recording.
*   `StopRecordingAsync()`: Asynchronously stops the recording, finalizes the file, and writes metadata tags if provided.
*   `StopRecording()`: Synchronously stops the recording.
*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an "AudioAnalyzer" to the recording pipeline. Analyzers process audio data during recording, enabling real-time analysis.
*   `AddModifier(SoundModifier modifier)`: Adds a "SoundModifier" to the recording pipeline. Modifiers apply effects to the audio data in real-time as it's being recorded.
*   `Dispose()`: Releases resources used by the recorder.
*   `PauseRecording()`: Pauses the recording.
*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes a specific "AudioAnalyzer" from the recording pipeline.
*   `RemoveModifier(SoundModifier modifier)`: Removes a specific "SoundModifier" from the recording pipeline.
*   `Dispose()`: Stops recording and releases all resources.

### Components `SoundPlayer`

```csharp
public sealed class SoundPlayer : SoundPlayerBase
{
    public SoundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);
    public override string Name { get; set; } // Overrides SoundPlayerBase's default
}
```
Inherits all playback functionality, properties, and events from `SoundPlayerBase`.

**Properties:**
* `Name`: The name of the sound player component (default: "Sound Player").

### Components `SurroundPlayer`

```csharp
public sealed class SurroundPlayer : SoundPlayerBase
{
    public SurroundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);

    public override string Name { get; set; } // Overrides SoundPlayerBase's default
    public Vector2 ListenerPosition { get; set; }
    public PanningMethod Panning { get; set; }
    public VbapParameters VbapParameters { get; set; }
    public SurroundConfiguration SurroundConfig { get; set; }
    public SpeakerConfiguration SpeakerConfig { get; set; }

    protected override void GenerateAudio(Span<float> output, int channels);
    public void SetSpeakerConfiguration(SpeakerConfiguration config);
    // Inherits Play, Pause, Stop, Seek, IsLooping, etc. from SoundPlayerBase
}
```
Inherits base playback functionality from `SoundPlayerBase` and adds surround-specific features with improved channel mapping for various source layouts (mono, stereo, 5.1, 7.1, etc.).

**Properties:**
*   `Name`: The name of the surround player component (default: "Surround Player").
*   `ListenerPosition`: The position of the listener in the surround sound field (Vector2).
*   `Panning`: Gets or sets the panning method to use for surround sound (`Linear`, `EqualPower`, `VBAP`).
*   `SpeakerConfig`: Gets or sets the speaker configuration (`Stereo`, `Quad`, `Surround51`, `Surround71`, `Custom`).
*   `VbapParameters`: Gets or sets parameters for Vector Base Amplitude Panning (VBAP).
*   `SurroundConfig`: Gets or sets the custom surround configuration when `SpeakerConfig` is set to `Custom`.

**Methods:**
*   `GenerateAudio(Span<float> output, int channels)`: (Overrides `SoundPlayerBase`) Reads audio data, applies resampling, then applies surround processing (including upmixing/downmixing) and looping if enabled.
*   `SetSpeakerConfiguration(SpeakerConfiguration config)`: Sets the speaker configuration for surround sound playback.
*   `Seek(int sampleOffset)`: (Overrides `SoundPlayerBase`) Seeks and re-initializes delay lines for surround processing.


### Components `VoiceActivityDetector`

```csharp
public class VoiceActivityDetector : AudioAnalyzer
{
    public VoiceActivityDetector(AudioFormat format, int fftSize = 1024, float energyThreshold = 5f, IVisualizer? visualizer = null);

    public bool IsVoiceActive { get; private set; }
    public float EnergyThreshold { get; set; }
    public float ActivationTimeMs { get; set; }
    public float HangoverTimeMs { get; set; }
    public int SpeechLowBand { get; set; }
    public int SpeechHighBand { get; set; }

    public override string Name { get; set; }

    public event Action<bool>? SpeechDetected;

    protected override void Analyze(Span<float> buffer, int channels);
}
```

**Properties:**

*   `Name`: Gets or sets the name of the voice activity detector component, useful for identification and debugging.
*   `IsVoiceActive`: Read-only property indicating if voice is currently detected.
*   `EnergyThreshold`: The energy threshold for detection.
*   `ActivationTimeMs`: Time in milliseconds the signal must be considered speech before activation. Helps prevent short noise bursts from triggering.
*   `HangoverTimeMs`: Time in milliseconds to keep the VAD active after the last speech frame. Prevents deactivation during short pauses.
*   `SpeechLowBand`/`SpeechHighBand`: The frequency range (in Hz) to analyze for speech.

**Events:**

*   `SpeechDetected`: An event that is raised whenever the voice activity state changes (i.e., when speech is detected or ceases to be detected). Listeners can subscribe to this event to respond in real-time to changes in voice activity.

**Methods:**

*   `VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null)`: Constructor for the VoiceActivityDetector class. Initializes a new instance of the voice activity detector with configurable FFT size, detection threshold and optional visualizer for audio analysis visualization.
*   `fftSize`: `int`  The size of the FFT (Fast Fourier Transform) window used for spectral analysis. Must be a power of two. Larger FFT sizes provide finer frequency resolution but may increase processing latency. Default is 1024.
*   `energyThreshold`: `float`  The sensitivity threshold for voice detection. A lower value increases sensitivity. Default is 0.01.
*   `visualizer`: `IVisualizer?`  An optional visualizer instance that can be attached to the analyzer for visualizing audio processing data, useful for debugging and tuning. Default is `null`.
*   `Analyze(Span<float> buffer, int channels)`: Checks the audio buffer and updates the `IsVoiceActive` property based on the detection algorithm.
*   `buffer`: `Span<float>`  The audio buffer to analyze for voice activity. The audio data in this buffer is processed to determine if voice is present.

**Remarks:**

*   **Frequency Range:** The `SpeechLowBand` and `SpeechHighBand` properties allow you to customize the frequency range that the VAD focuses on for speech detection. Speech typically falls within the 300Hz to 3400Hz range, but you may need to adjust these values depending on the characteristics of your audio and the type of speech you are detecting.
*   **Threshold Sensitivity:** The `Threshold` property is crucial for controlling the sensitivity of the voice activity detection. Adjusting this threshold may be necessary to achieve optimal performance in different environments and with varying audio input levels.
*   **FFT Size:** The `fftSize` parameter in the constructor determines the FFT window size. A larger FFT size provides better frequency resolution, which can be beneficial in noisy environments or when detecting subtle voice activity. However, it also increases the computational cost and latency. Ensure that the FFT size is always a power of 2 for optimal performance and compatibility with FFT algorithms.
*   **Performance Tuning:** For optimal performance, especially in real-time applications, carefully tune the `fftSize` and `Threshold` parameters. Larger FFT sizes are more computationally intensive but offer better frequency resolution. Adjust the `Threshold` based on the ambient noise level and the desired sensitivity of voice detection.
*   **Environment Considerations:** The ideal settings for `fftSize`, `Threshold`, `SpeechLowBand`, and `SpeechHighBand` may vary depending on the environment in which the voice activity detector is used. In noisy environments, you might need to increase the `fftSize` and adjust the `Threshold` to minimize false positives.
*   **Visualizer for Debugging:** The optional `visualizer` parameter in the constructor is highly useful for debugging and tuning the voice activity detector. By attaching a visualizer, you can visually inspect the audio data and the detector's response, which can help in understanding and adjusting the detector's parameters for optimal performance in your specific use case.

### Enums `LogLevel`

```csharp
public enum LogLevel
{
    Debug,
    Info,
    Warning,
    Error
}
```

**Description:** Defines the severity levels for log messages used by the `SoundFlow.Utils.Log` static class.

**Values:**

*   `Debug`: Detailed information intended for debugging purposes.
*   `Info`: General informational messages about the library's operation.
*   `Warning`: Indicates a potential issue that does not prevent the current operation from completing but may lead to unexpected behavior.
*   `Error`: Indicates a definite error that has occurred, which may affect functionality.

### Enums `MidiCommand`

```csharp
public enum MidiCommand : byte
{
    NoteOff = 0x80,
    NoteOn = 0x90,
    PolyphonicKeyPressure = 0xA0,
    ControlChange = 0xB0,
    ProgramChange = 0xC0,
    ChannelPressure = 0xD0,
    PitchBend = 0xE0,
    SystemExclusive = 0xF0
}
```

**Description:** Represents the command portion of a MIDI status byte, defining the type of MIDI message. This enum covers the most common channel messages.

**Values:**

*   `NoteOff`: Note Off message (e.g., releasing a key).
*   `NoteOn`: Note On message (e.g., pressing a key). A Note On with velocity 0 is often interpreted as a Note Off.
*   `PolyphonicKeyPressure`: Polyphonic Key Pressure (Aftertouch) message.
*   `ControlChange`: Control Change (CC) message, used for adjusting parameters like volume, pan, or effects.
*   `ProgramChange`: Program Change message, used for changing instruments or presets.
*   `ChannelPressure`: Channel Pressure (Aftertouch) message, which applies to the entire channel.
*   `PitchBend`: Pitch Bend message.
*   `SystemExclusive`: System Exclusive (SysEx) message start byte.

### Enums `MiniAudioBackend`

```csharp
public enum MiniAudioBackend
{
    Wasapi = 1,
    DirectSound = 2,
    WinMm = 3,
    CoreAudio = 4,
    Sndio = 5,
    Audio4 = 6,
    Oss = 7,
    PulseAudio = 8,
    Alsa = 9,
    Jack = 10,
    AAudio = 11,
    OpenSl = 12,
    WebAudio = 13,
    Custom = 14,
    Null = 0
}
```

**Description:** Represents the available low-level audio backends that the `MiniAudioEngine` can use for audio I/O. The integer values correspond directly to the native `ma_backend` enum in the `miniaudio` library.

**Values:**

*   `Wasapi`: The modern Windows Audio Session API. (Windows Vista+)
*   `DirectSound`: The legacy DirectSound API. (Windows)
*   `WinMm`: The legacy Windows MultiMedia API (WaveOut). (Windows)
*   `CoreAudio`: The standard audio API for macOS and iOS.
*   `Sndio`: A common audio server on OpenBSD.
*   `Oss`: The Open Sound System, common on BSD and older Linux systems.
*   `PulseAudio`: A common sound server on modern Linux desktops.
*   `Alsa`: The Advanced Linux Sound Architecture, the standard low-level audio API on Linux.
*   `Jack`: The JACK Audio Connection Kit, a professional low-latency audio server for Linux and macOS.
*   `AAudio`: The modern low-latency audio API for Android. (Android 8.0+)
*   `OpenSl`: The legacy audio API for Android.
*   `WebAudio`: The Web Audio API, for use in WebAssembly environments.
*   `Null`: A silent backend that consumes and discards audio data. Used as a fallback.
*   `Custom`: A placeholder for a user-defined backend.

### Enums `Capability`

```csharp
[Flags]
public enum Capability
{
    Playback = 1,
    Record = 2,
    Mixed = Playback | Record,
    Loopback = 4
}
```

**Values:**

*   `Playback`: Indicates playback capability.
*   `Record`: Indicates recording capability.
*   `Mixed`: Indicates both playback and recording capability.
*   `Loopback`: Indicates loopback capability (recording system audio output).

### Enums `DeviceType`

```csharp
public enum DeviceType
{
    Playback,
    Capture
}
```
**Values:**
*   `Playback`: Device used for audio playback.
*   `Capture`: Device used for audio capture.

### Enums `PlaybackState`

```csharp
public enum PlaybackState
{
    Stopped,
    Playing,
    Paused
}
```

**Values:**

*   `Stopped`: Playback is stopped.
*   `Playing`: Playback is currently in progress.
*   `Paused`: Playback is paused.

### Enums `MiniAudioResult`

```csharp
public enum MiniAudioResult
{
    Success = 0,
    Error = -1,
    // ... (other error codes)
    CrcMismatch = -100,
    FormatNotSupported = -200,
    // ... (other backend-specific error codes)
    DeviceNotInitialized = -300,
    // ... (other device-related error codes)
    FailedToInitBackend = -400
    // ... (other backend initialization error codes)
}
```

**Values:**

*   `Success`: The operation was successful.
*   `Error`: A generic error occurred.
*   `CrcMismatch`: CRC checksum mismatch.
*   `FormatNotSupported`: The requested audio format is not supported.
*   `DeviceNotInitialized`: The audio device is not initialized.
*   `FailedToInitBackend`: Failed to initialize the audio backend.
*   **(Many other error codes representing various error conditions)**

### Enums `SampleFormat`

```csharp
public enum SampleFormat
{
    Unknown = 0,
    U8 = 1,
    S16 = 2,
    S24 = 3,
    S32 = 4,
    F32 = 5
}
```

**Values:**

*   `Unknown`: Unknown sample format.
*   `U8`: Unsigned 8-bit integer.
*   `S16`: Signed 16-bit integer.
*   `S24`: Signed 24-bit integer packed in 3 bytes.
*   `S32`: Signed 32-bit integer.
*   `F32`: 32-bit floating-point.

### Enums `FilterType`

```csharp
public enum FilterType
{
    Peaking,
    LowShelf,
    HighShelf,
    BandPass,
    Notch,
    LowPass,
    HighPass
}
```

**Values:**

*   `Peaking`: Peaking filter.
*   `LowShelf`: Low-shelf filter.
*   `HighShelf`: High-shelf filter.
*   `BandPass`: Band-pass filter.
*   `Notch`: Notch filter.
*   `LowPass`: Low-pass filter.
*   `HighPass`: High-pass filter.

### Enums `EnvelopeGenerator.EnvelopeState`

```csharp
public enum EnvelopeState
{
    Idle,
    Attack,
    Decay,
    Sustain,
    Release
}
```

**Values:**

*   `Idle`: The envelope is inactive.
*   `Attack`: The attack stage of the envelope.
*   `Decay`: The decay stage of the envelope.
*   `Sustain`: The sustain stage of the envelope.
*   `Release`: The release stage of the envelope.

### Enums `EnvelopeGenerator.TriggerMode`

```csharp
public enum TriggerMode
{
    NoteOn,
    Gate,
    Trigger
}
```

**Values:**

* `NoteOn`: The envelope will go directly from attack to sustain, without a decay stage.
* `Gate`: The envelope will progress normally, and will only enter release stage when trigger is off.
* `Trigger`: The envelope will always progress to the end, including the release stage.

### Enums `LowFrequencyOscillator.WaveformType`

```csharp
public enum WaveformType
{
    Sine,
    Square,
    Triangle,
    Sawtooth,
    ReverseSawtooth,
    Random,
    SampleAndHold
}
```

**Values:**

*   `Sine`: Sine wave.
*   `Square`: Square wave.
*   `Triangle`: Triangle wave.
*   `Sawtooth`: Sawtooth wave.
*   `ReverseSawtooth`: Reverse sawtooth wave.
*   `Random`: Random values.
*   `SampleAndHold`: Sample and hold random values.

### Enums `LowFrequencyOscillator.TriggerMode`

```csharp
public enum TriggerMode
{
    FreeRunning,
    NoteTrigger
}
```

**Values:**

* `FreeRunning`: The LFO will run continuously without needing a trigger.
* `NoteTrigger`: The LFO will only start when triggered.

### Enums `Oscillator.WaveformType`

```csharp
public enum WaveformType
{
    Sine,
    Square,
    Sawtooth,
    Triangle,
    Noise,
    Pulse
}
```

**Values:**

*   `Sine`: Sine wave.
*   `Square`: Band-limited square wave.
*   `Sawtooth`: Band-limited sawtooth wave.
*   `Triangle`: Triangle wave.
*   `Noise`: White noise.
*   `Pulse`: Band-limited pulse wave.

### Enums `SurroundPlayer.SpeakerConfiguration`

```csharp
public enum SpeakerConfiguration
{
    Stereo,
    Quad,
    Surround51,
    Surround71,
    Custom
}
```

**Values:**

*   `Stereo`: Standard stereo configuration (2 speakers).
*   `Quad`: Quadraphonic configuration (4 speakers).
*   `Surround51`: 5.1 surround sound configuration (6 speakers).
*   `Surround71`: 7.1 surround sound configuration (8 speakers).
* *   `Custom`: A custom speaker configuration defined by the user.

### Enums `SurroundPlayer.PanningMethod`

```csharp
public enum PanningMethod
{
    Linear,
    EqualPower,
    Vbap
}
```

**Values:**

*   `Linear`: Linear panning.
*   `EqualPower`: Equal power panning.
*   `Vbap`: Vector Base Amplitude Panning (VBAP).

### Editing `Composition`

```csharp
public sealed class Composition : ISequencerContext, IDisposable, IMidiMappable
{
    // Service Accessors
    public CompositionRenderer Renderer { get; }
    public CompositionEditor Editor { get; }
    public CompositionRecorder Recorder { get; }
    public MidiMappingManager MappingManager { get; }

    // State
    public Guid Id { get; }
    public string Name { get; set; }
    public AudioFormat Format { get; }
    public int TicksPerQuarterNote { get; set; } // Default 480
    public float MasterVolume { get; set; }
    public bool IsDirty { get; }

    // Data Collections
    public List<Track> Tracks { get; }
    public List<MidiTrack> MidiTracks { get; }
    public List<TempoMarker> TempoTrack { get; }
    public List<IMidiDestinationNode> MidiTargets { get; } // Internal synths/effects
    public List<SoundModifier> Modifiers { get; init; } // Master bus effects
    public List<AudioAnalyzer> Analyzers { get; init; }

    public void MarkDirty();
    public void ClearDirtyFlag();
    public void Dispose();
}
```
**Description:** Represents a complete audio and MIDI composition, acting as the top-level container for multiple tracks. It serves as a faade, providing access to its data model and specialized services for rendering, editing, and recording. In previous versions, this class directly implemented `ISoundDataProvider`; this functionality has now been moved to the `Renderer` property.

**Properties:**
*   `Id`: A unique identifier for the composition instance, used for MIDI mapping.
*   `Renderer`: Gets the `CompositionRenderer` service for this composition, which handles audio output generation and implements `ISoundDataProvider`.
*   `Editor`: Gets the `CompositionEditor` service, which provides methods to manipulate tracks, segments, and other structural elements.
*   `Recorder`: Gets the `CompositionRecorder` service, which manages the MIDI recording workflow.
*   `MappingManager`: Gets the `MidiMappingManager` service, which manages real-time control mappings.
*   `Format`: Gets the base audio format of the composition.
*   `Name`: The name of the composition.
*   `Modifiers`: A list of `SoundModifier` instances applied to the master output of the composition.
*   `Analyzers`: A list of `AudioAnalyzer` instances that process the master output of the composition.
*   `MidiTargets`: A list of internal MIDI-controllable components (like `Synthesizer`s) that can be targeted by `MidiTrack`s.
*   `Tracks`: The list of audio `Track`s contained within this composition.
*   `MidiTracks`: The list of `MidiTrack`s contained within this composition.
*   `TempoTrack`: The master tempo track for the composition, defining tempo changes over time.
*   `MasterVolume`: The master volume level for the entire composition (0.0 to 1.0 or higher for gain).
*   `IsDirty`: A flag that indicates if there are unsaved changes.
*   `SampleRate`: The target sample rate for rendering.
*   `TargetChannels`: The target number of channels for the rendered output.
*   `TicksPerQuarterNote`: The time division for all MIDI tracks in this composition, in ticks per quarter note (e.g., 480).
*   `IsDisposed`: Indicates whether the composition has been disposed.

**Methods:**
*   `MarkDirty()`: Marks the composition as having unsaved changes.
*   `ClearDirtyFlag()`: Resets the `IsDirty` flag to `false`.
*   `Dispose()`: Disposes of all disposable resources owned by the composition and its services.

### Editing `CompositionRenderer`

```csharp
public sealed class CompositionRenderer : ISoundDataProvider
{
    public bool IsSyncDriven { get; set; }
    public TimeSpan? LoopStartTime { get; set; }
    public TimeSpan? LoopEndTime { get; set; }

    public int Position { get; }
    public int Length { get; }
    public bool CanSeek { get; }
    public bool IsDisposed { get; private set; }
    // ... ISoundDataProvider implementation ...

    public event EventHandler<EventArgs>? EndOfStreamReached;
    public event EventHandler<PositionChangedEventArgs>? PositionChanged;

    public float[] Render(TimeSpan startTime, TimeSpan duration);
    public int Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer);
    public int ReadBytes(Span<float> buffer);
    public void Seek(int sampleOffset);
    public void Dispose();

    // Sync Transport Controls
    public void SyncPlay();
    public void SyncStop();
    public void SyncContinue();
    public void SyncSeek(TimeSpan time);
    public void AdvanceBySyncTicks(int tickCount);
    public TempoMarker GetTempoAtCurrentPosition();
}
```
**Description:** Renders a `Composition` into an audio stream. This class now contains the audio generation logic previously in `Composition` and implements `ISoundDataProvider`, allowing it to be used as a source for `SoundPlayer` or other components.

**Properties:**
*   `IsSyncDriven`: Gets or sets whether the renderer's transport is driven by an external sync source (e.g., MIDI Clock).
*   `LoopStartTime`: Gets or sets the start time of the playback loop.
*   `LoopEndTime`: Gets or sets the end time of the playback loop.
*   `Position`: The current read position in samples.
*   `Length`: The total length of the composition in samples.
*   `CanSeek`: Always `true`.
*   `IsDisposed`: Indicates whether the renderer has been disposed.

**Methods:**
*   `Render(...)`: Renders a specific time portion of the composition into a buffer, mixing audio and MIDI tracks.
*   `ReadBytes(Span<float> buffer)`: `ISoundDataProvider` implementation for reading rendered audio.
*   `Seek(int sampleOffset)`: `ISoundDataProvider` implementation for seeking.
*   `SyncPlay()`, `SyncStop()`, `SyncContinue()`, `SyncSeek(...)`, `AdvanceBySyncTicks(...)`, `GetTempoAtCurrentPosition()`: Methods for controlling the transport when slaved to an external synchronization source. Intended for use by backend implementations.

### Editing `CompositionEditor`

```csharp
public sealed class CompositionEditor : IDisposable
{
    public AudioSegment CreateAndAddSegmentFromFile(Track track, string filePath, TimeSpan timelineStartTime, ReadOptions? options = null);
    public AudioSegment CreateSegmentFromFile(string filePath, TimeSpan timelineStartTime, ReadOptions? options = null);
    public void AddTrack(Track track);
    public void AddMidiTrack(MidiTrack midiTrack);
    public bool RemoveTrack(Track track);
    public bool RemoveMidiTrack(MidiTrack midiTrack);
    public TempoMarker GetTempoAtTime(TimeSpan time);
    public void SetTempo(TimeSpan time, double newBpm);
    public bool RemoveTempoChange(TimeSpan time);
    public void SetTicksPerQuarterNote(int ticksPerQuarterNote);
    public Task ExportMidiAsync(string filePath);
    public TimeSpan CalculateTotalDuration();
    public bool ReplaceSegment(...);
    public bool RemoveSegment(...);
    public AudioSegment? SilenceSegment(Track track, TimeSpan rangeStartTime, TimeSpan rangeDuration);
    public void InsertSegment(Track track, TimeSpan insertionPoint, AudioSegment segmentToInsert, bool shiftFollowing = true);

    // MIDI Editing Methods
    public IReadOnlyCollection<MidiNote> GetNotes(MidiSegment segment);
    public void AddNoteToSegment(MidiSegment segment, long startTick, long durationTicks, int noteNumber, int velocity);
    public void ModifyNotesInSegment(MidiSegment segment, IEnumerable<NoteModification> modifications);
    public void RemoveNotesFromSegment(MidiSegment segment, IEnumerable<Guid> noteIds);
    public IReadOnlyCollection<ControlPoint> GetControlPoints(MidiSegment segment, int controllerNumber);
    public void AddControlPointToSegment(MidiSegment segment, int controllerNumber, long tick, int value);
    public void ModifyControlPointsInSegment(MidiSegment segment, int controllerNumber, IEnumerable<ControlPointModification> modifications);
    public void RemoveControlPointsFromSegment(MidiSegment segment, int controllerNumber, IEnumerable<Guid> pointIds);
    public void QuantizeSegment(MidiSegment segment, QuantizationSettings settings);
    public (MidiSegment? part1, MidiSegment? part2)? SplitMidiSegment(MidiTrack track, MidiSegment segmentToSplit, TimeSpan splitTimeOnTimeline);
    public MidiSegment? JoinMidiSegments(MidiTrack track, IEnumerable<MidiSegment> segmentsToJoin);

    public void Dispose();
}
```
**Description:** Provides high-level editing logic that modifies a `Composition` object. This class encapsulates all actions that alter the composition's structure, including track management, audio segment manipulation, and detailed MIDI editing.

### Editing `CompositionRecorder`

```csharp
public sealed class CompositionRecorder : IDisposable
{
    public TimeSpan? PunchInTime { get; set; }
    public TimeSpan? PunchOutTime { get; set; }
    public bool IsRecording { get; }
    public bool IsWaitingForPunchIn { get; }

    public void ArmTrackForRecording(MidiTrack track, MidiInputDevice inputDevice);
    public void DisarmTrack(MidiTrack track);

    // Starts recording immediately or waits for PunchInTime
    public void StartRecording(TimeSpan startTime, RecordingMode mode = RecordingMode.Normal, MidiSegment? targetSegment = null);
    public void StopRecording();
}
```
**Description:** Orchestrates the recording process.
*   `ArmTrackForRecording`: Links a physical input to a track.
*   `PunchIn/Out`: Automates start/stop based on timeline position.
*   `RecordingMode`: `Normal` (creates new segment) or `OverdubMerge` (adds notes to existing segment).

### Editing `Track`

```csharp
public class Track : IMidiMappable
{
    public Track(string name = "Track", TrackSettings? settings = null);

    public Guid Id { get; }
    public string Name { get; set; }
    public List<AudioSegment> Segments { get; }
    public TrackSettings Settings { get; set; }
    internal Composition? ParentComposition { get; set; }

    public void MarkDirty();
    public void AddSegment(AudioSegment segment);
    public bool RemoveSegment(AudioSegment segment, bool shiftSubsequent = false);
    public void InsertSegmentAt(AudioSegment segmentToInsert, TimeSpan insertionTime, bool shiftSubsequent = true);
    public TimeSpan CalculateDuration();
    public int Render(TimeSpan overallStartTime, TimeSpan durationToRender, Span<float> outputBuffer, int targetSampleRate, int targetChannels);
}
```
**Description:** Represents a single audio track within a `Composition`. It now implements `IMidiMappable`, allowing its `TrackSettings` (volume, pan, etc.) to be controlled via MIDI mapping.

**Properties:**
*   `Id`: A unique identifier for the track instance, used for MIDI mapping.

### Editing `AudioSegment`

```csharp
public class AudioSegment : IDisposable
{
    public AudioSegment(
        AudioFormat format,
        ISoundDataProvider sourceDataProvider,
        TimeSpan sourceStartTime,
        TimeSpan sourceDuration,
        TimeSpan timelineStartTime,
        string name = "Segment",
        AudioSegmentSettings? settings = null,
        bool ownsDataProvider = false);

    public string Name { get; set; }
    public ISoundDataProvider SourceDataProvider { get; private set; }
    public TimeSpan SourceStartTime { get; set; }
    public TimeSpan SourceDuration { get; set; }
    public TimeSpan TimelineStartTime { get; set; }
    public AudioSegmentSettings Settings { get; set; }
    internal Track? ParentTrack { get; set; }

    public TimeSpan StretchedSourceDuration { get; }
    public TimeSpan EffectiveDurationOnTimeline { get; }
    public TimeSpan TimelineEndTime { get; }
    public TimeSpan GetTotalLoopedDurationOnTimeline();
    public AudioSegment Clone(TimeSpan? newTimelineStartTime = null);
    internal void ReplaceSource(ISoundDataProvider newSource, TimeSpan newSourceStartTime, TimeSpan newSourceDuration);
    public int ReadProcessedSamples(TimeSpan segmentTimelineOffset, TimeSpan durationToRead, Span<float> outputBuffer, int outputBufferOffset, int targetSampleRate, int targetChannels);
    internal void FullResetState();
    public void Dispose();
    public void MarkDirty();
}
```

### Editing `AudioSegmentSettings`

```csharp
public class AudioSegmentSettings : IMidiMappable
{
    public Guid Id { get; }
    public List<SoundModifier> Modifiers { get; init; }
    public List<AudioAnalyzer> Analyzers { get; init; }
    public float Volume { get; set; }
    public float Pan { get; set; }
    public TimeSpan FadeInDuration { get; set; }
    public FadeCurveType FadeInCurve { get; set; }
    public TimeSpan FadeOutDuration { get; set; }
    public FadeCurveType FadeOutCurve { get; set; }
    public bool IsReversed { get; set; }
    public LoopSettings Loop { get; set; }
    public float SpeedFactor { get; set; }
    public float TimeStretchFactor { get; set; } // Overridden by TargetStretchDuration if set
    public TimeSpan? TargetStretchDuration { get; set; }
    public bool IsEnabled { get; set; }

    public AudioSegmentSettings Clone();
    public void AddModifier(SoundModifier modifier);
    public bool RemoveModifier(SoundModifier modifier);
    public void ReorderModifier(SoundModifier modifier, int newIndex);
    public void AddAnalyzer(AudioAnalyzer analyzer);
    public bool RemoveAnalyzer(AudioAnalyzer analyzer);
}
```

### Editing `TrackSettings`

```csharp
public class TrackSettings : IMidiMappable
{
    public Guid Id { get; }
    public List<SoundModifier> Modifiers { get; init; }
    public List<AudioAnalyzer> Analyzers { get; init; }
    public List<MidiModifier> MidiModifiers { get; init; }
    public float Volume { get; set; }
    public float Pan { get; set; }
    public bool IsMuted { get; set; }
    public bool IsSoloed { get; set; }
    public bool IsEnabled { get; set; }

    public TrackSettings Clone();
    public void AddModifier(SoundModifier modifier);
    public bool RemoveModifier(SoundModifier modifier);
    public void ReorderModifier(SoundModifier modifier, int newIndex);
    public void AddAnalyzer(AudioAnalyzer analyzer);
    public bool RemoveAnalyzer(AudioAnalyzer analyzer);
}
```
**Description:** Configurable settings for a `Track` or `MidiTrack`. Now implements `IMidiMappable` and includes a `MidiModifiers` list for applying real-time MIDI effects to `MidiTrack`s.

### Editing `LoopSettings`

```csharp
public record struct LoopSettings
{
    public int Repetitions { get; }
    public TimeSpan? TargetDuration { get; }
    public LoopSettings(int repetitions = 0, TimeSpan? targetDuration = null);
    public static LoopSettings PlayOnce { get; }
}
```

### Editing `FadeCurveType`

```csharp
public enum FadeCurveType
{
    Linear,
    Logarithmic,
    SCurve
}
```

### Editing.Persistence.DTOs
These are Data Transfer Objects (DTOs) for serialization. See [Editing and Persistence Guide](./editing-engine.mdx#project-persistence) for their purpose.
*   `CompositionProjectManager` (static class): `SaveProjectAsync`, `LoadProjectAsync`, `RelinkMissingMedia`.
*   `ProjectSaveOptions`
*   `ProjectData`
*   `ProjectTrack`, `ProjectMidiTrack`
*   `ProjectSegment`, `ProjectMidiSegment`
*   `ProjectAudioSegmentSettings`, `ProjectTrackSettings`
*   `ProjectSourceReference`
*   `ProjectEffectData`
*   `ProjectMidiMapping`
*   `ProjectTempoMarker`

### Editing `MidiSequence`

```csharp
public class MidiSequence
{
    public MidiSequence(MidiFile midiFile);
    public MidiSequence(int ticksPerQuarterNote, IEnumerable<MidiNote> notes, IEnumerable<(int ccNumber, ControlPoint point)> ccEvents, IEnumerable<ControlPoint> pitchBendEvents, IEnumerable<MidiEvent> otherEvents);

    public int TicksPerQuarterNote { get; }
    public IReadOnlyCollection<MidiNote> Notes { get; }
    public IReadOnlyList<MidiEvent> Events { get; }
    public long LengthTicks { get; }
    public IReadOnlyCollection<ControlPoint> PitchBendEvents { get; }
    public IReadOnlyDictionary<int, IReadOnlyCollection<ControlPoint>> ControlChangeEvents { get; }

    public MidiNote AddNote(long startTick, long durationTicks, int noteNumber, int velocity);
    public void RemoveNotes(IEnumerable<Guid> noteIds);
    public void ModifyNotes(IEnumerable<NoteModification> modifications);
    public ControlPoint AddControlPoint(int controllerNumber, long tick, int value);
    public void RemoveControlPoints(int controllerNumber, IEnumerable<Guid> pointIds);
    public void ModifyControlPoints(int controllerNumber, IEnumerable<ControlPointModification> modifications);
    public MidiFile ToMidiFile();
    public (MidiSequence part1, MidiSequence part2) Split(long splitTick);
    public static MidiSequence Join(IEnumerable<(long tickOffset, MidiSequence sequence)> sequences);
}
```

**Description:**
A mutable container representing the musical content of a MIDI segment. It acts as the primary data model for MIDI editing, bridging the gap between high-level musical concepts (like notes with duration) and low-level MIDI events (like Note On/Off messages).

**Constructors:**
*   `MidiSequence(MidiFile midiFile)`: Initializes the sequence by parsing a standard `MidiFile` object. It automatically converts paired Note On/Off events into `MidiNote` objects.
*   `MidiSequence(int ticksPerQuarterNote, ...)`: Initializes a new sequence from raw collections of notes, control points, and other events. Typically used internally during split/join operations.

**Properties:**
*   **`TicksPerQuarterNote`**: The time resolution of the sequence (e.g., 480 or 960 PPQ).
*   **`Notes`**: A read-only collection of all `MidiNote` objects in the sequence.
*   **`Events`**: A read-only list of all raw `MidiEvent` objects, strictly ordered by time. This list is lazily rebuilt whenever the high-level data (Notes, Control Points) is modified, ensuring it is always up-to-date for playback.
*   **`LengthTicks`**: The total duration of the sequence in ticks, determined by the position of the last event.
*   **`PitchBendEvents`**: A collection of automation points for Pitch Bend.
*   **`ControlChangeEvents`**: A dictionary mapping CC numbers to their respective automation curves.

**Methods:**
*   **`AddNote(...)`**: Creates and adds a new `MidiNote`.
*   **`RemoveNotes(...)`**: Removes notes identified by their unique GUIDs.
*   **`ModifyNotes(...)`**: Applies batch updates to notes (moving, resizing, transposing) efficiently.
*   **`AddControlPoint(...)`**: Adds a new automation point for a CC or Pitch Bend (controller -1).
*   **`ToMidiFile()`**: Serializes the current state of the sequence back into a standard `MidiFile` structure, converting absolute ticks back to delta-times.
*   **`Split(long splitTick)`**: Splits the sequence into two new sequences at the specified tick. Handles note splitting logic (truncating the first part, creating a new start for the second).
*   **`Join(...)`**: Static method that merges multiple sequences into one, applying time offsets.

---

### Editing `MidiSegment`

```csharp
public sealed class MidiSegment : IDisposable
{
    public MidiSegment(MidiSequence sequence, TimeSpan timelineStartTime, string name = "MIDI Segment");

    public string Name { get; set; }
    public MidiSequence Sequence { get; }
    public MidiDataProvider DataProvider { get; }
    public TimeSpan TimelineStartTime { get; set; }
    public TimeSpan SourceDuration { get; }
    public TimeSpan TimelineEndTime { get; }
    public MidiTrack? ParentTrack { get; set; }

    public void MarkDirty();
    public void Dispose();
}
```

**Description:**
Represents a clip of MIDI data placed on a track's timeline. It wraps a `MidiSequence` (the content) and adds placement information (`TimelineStartTime`). Crucially, it manages a cached `MidiDataProvider` optimized for real-time playback.

**Properties:**
*   **`Sequence`**: The mutable data model for this segment.
*   **`DataProvider`**: A read-only, thread-safe view of the sequence used by the audio engine. This provider is automatically regenerated when the sequence is modified (marked dirty).
*   **`TimelineStartTime`**: The absolute time on the composition timeline where this segment begins.
*   **`SourceDuration`**: The duration of the segment in time, calculated from the sequence's tick length and the composition's tempo map.
*   **`ParentTrack`**: Reference to the track containing this segment.

**Methods:**
*   **`MarkDirty()`**: Signals that the underlying `Sequence` has changed. This invalidates the cached `DataProvider`, forcing it to be rebuilt on the next access to ensure playback reflects the edits.

---

### Editing.Mapping `MidiMappingManager`

```csharp
public sealed class MidiMappingManager : IDisposable
{
    internal MidiMappingManager(Composition composition);

    public IReadOnlyList<MidiMapping> Mappings { get; }

    public void AddInputDevice(MidiInputDevice device);
    public void RemoveInputDevice(MidiInputDevice device);
    public void AddMapping(MidiMapping mapping);
    public bool RemoveMapping(Guid mappingId);
    public void Dispose();
}
```

**Description:**
The runtime engine responsible for executing MIDI mappings. It listens to all subscribed MIDI input devices, interprets incoming messages (including complex 14-bit NRPN sequences), and applies them to the target properties or methods within the composition.

**Methods:**
*   **`AddInputDevice(MidiInputDevice device)`**: Subscribes the manager to the specified input device. The manager attaches listeners to `OnMessageReceived` and initializes internal parsers for that device (e.g., High-Res CC parser).
*   **`RemoveInputDevice(...)`**: Unsubscribes from the device and cleans up associated parsers.
*   **`AddMapping(MidiMapping mapping)`**: Registers a new mapping rule.
*   **`RemoveMapping(Guid mappingId)`**: Removes a mapping rule by its ID.

**Internal Logic:**
*   **Caching**: It maintains a cache of reflection metadata (`PropertyInfo`, `MethodInfo`) for resolved mappings to ensure high-performance execution during real-time audio processing.
*   **High-Resolution Parsing**: It tracks the state of RPN/NRPN controller streams per channel/device. When a full 14-bit message sequence is detected, it synthesizes a `HighResolutionControlChange` event for mapping.

---

### Editing.Mapping `MidiMapping` & DTOs

```csharp
public sealed class MidiMapping
{
    public MidiMapping(MidiInputSource source, MidiMappingTarget target, ValueTransformer transformer, MidiMappingBehavior behavior = MidiMappingBehavior.Absolute);

    public Guid Id { get; }
    public MidiInputSource Source { get; set; }
    public MidiMappingTarget Target { get; set; }
    public ValueTransformer Transformer { get; set; }
    public MidiMappingBehavior Behavior { get; set; }
    public int ActivationThreshold { get; set; } = 1;
    public bool IsResolved { get; internal set; }
}
```

**Description:**
Defines a single persistent link between a MIDI source event and a destination parameter.

**Properties:**
*   **`Source`**: Defines the criteria for triggering the mapping (Device, Channel, Message Type, Parameter).
*   **`Target`**: Defines what is being controlled (Object ID, Member Name, Type).
*   **`Transformer`**: Defines how the input value (e.g., 0-127) maps to the target property's range (e.g., 20Hz-20kHz).
*   **`Behavior`**:
*   `Absolute`: Sets the value directly based on the input.
*   `Relative`: Increments/decrements the value (for endless encoders).
*   `Toggle`: Flips a boolean value when the input exceeds the threshold.
*   `Trigger`: Invokes a method when the input exceeds the threshold.
*   **`IsResolved`**: Indicates if the target object and property were successfully found in the current composition. Mappings may become unresolved if target objects are deleted.

**Supporting DTOs:**

*   **`MidiInputSource`**:
*   `DeviceName`: Name of the MIDI device.
*   `Channel`: 1-16, or 0 for Omni.
*   `MessageType`: `ControlChange`, `NoteOn`, `NoteOff`, `PitchBend`, `HighResolutionControlChange`.
*   `MessageParameter`: The note number or CC number.
*   **`MidiMappingTarget`**:
*   `TargetObjectId`: GUID of the `IMidiMappable` object.
*   `TargetMemberName`: Name of the property or method.
*   `TargetType`: `Property` or `Method`.
*   **`ValueTransformer`**:
*   `SourceMin`/`Max`: Input range (e.g., 0-127).
*   `TargetMin`/`Max`: Output range (e.g., 0.0-1.0).
*   `CurveType`: `Linear`, `Exponential`, `Logarithmic`.

---

### Midi `MidiManager`

```csharp
public sealed class MidiManager : IDisposable
{
    public IReadOnlyList<MidiDeviceInfo> AvailableInputs { get; }
    public IReadOnlyList<MidiDeviceInfo> AvailableOutputs { get; }
    public IReadOnlyList<MidiRoute> Routes { get; }
    public IReadOnlyList<MidiRoute> FaultedRoutes { get; }

    public event Action<MidiRoute, IError?>? OnRouteFaulted;

    public void ConfigureMpeZone(MidiDeviceInfo deviceInfo, MpeZone zone);
    public void DeconfigureMpeZone(MidiDeviceInfo deviceInfo);

    public MidiRoute CreateRoute(MidiDeviceInfo inputDevice, IMidiControllable target);
    public MidiRoute CreateRoute(MidiDeviceInfo inputDevice, MidiDeviceInfo outputDevice);
    public void AddRoute(MidiRoute route);
    public bool RemoveRoute(MidiRoute route);

    public MidiInputNode GetOrCreateInputNode(MidiDeviceInfo deviceInfo);
    public MidiOutputNode GetOrCreateOutputNode(MidiDeviceInfo deviceInfo);
    public void Dispose();
}
```

**Description:**
The central hub for all MIDI connectivity. It acts as a "virtual patch bay," managing the lifecycles of physical device connections and routing messages between them and internal software components.

**Key Features:**
*   **Device Abstraction**: Users interact with `MidiDeviceInfo` structs. The manager handles the actual instantiation (`MidiInputDevice`/`MidiOutputDevice`) and ensures devices are shared efficiently.
*   **Routing Graph**: Manages a list of `MidiRoute` objects. Each route connects a source node to a destination node.
*   **MPE Processing**:
*   **`ConfigureMpeZone`**: Configures an input device to be treated as an MPE source. The manager attaches a specialized parser that intercepts messages on member channels and converts them into internal, per-note expression events (Pitch, Pressure, Timbre) before routing them to MPE-aware targets (like `Synthesizer`).
*   This abstracts MPE complexity away from the routing logic.
*   **Fault Handling**: Monitors routes for errors (e.g., device disconnection) and raises `OnRouteFaulted` so the application can respond (e.g., show a UI alert).

---

### Midi.PortMidi `PortMidiBackend`

```csharp
public sealed class PortMidiBackend : IMidiBackend
{
    public PortMidiBackend();

    public SyncMode CurrentSyncMode { get; }
    public SyncStatus CurrentSyncStatus { get; }
    public double DetectedBpm { get; }

    public event Action<SyncStatus>? OnSyncStatusChanged;
    public event Action<double>? OnBpmChanged;

    public void Initialize(AudioEngine engine);
    public MidiInputDevice CreateMidiInputDevice(MidiDeviceInfo deviceInfo);
    public MidiOutputDevice CreateMidiOutputDevice(MidiDeviceInfo deviceInfo);
    public void UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs);

    public void ConfigureSync(SyncMode mode, SyncSource source, MidiDeviceInfo? inputDeviceInfo, MidiDeviceInfo? outputDeviceInfo, CompositionRenderer? renderer);
    public void Dispose();
}
```

**Description:**
The concrete implementation of `IMidiBackend` using the PortMidi library. Beyond basic I/O, it implements a robust synchronization engine.

**Synchronization Features:**
*   **`ConfigureSync`**: Sets up the backend to act as a Clock Master or Slave.
*   **Slave Mode**: Listens to the specified `inputDeviceInfo` for MIDI Clock (F8) or MTC (F1/SysEx). It drives the `CompositionRenderer`'s transport, syncing playback to the external source.
*   **Master Mode**: Uses the `CompositionRenderer`'s position and `AudioFramesRendered` events to generate a stable MIDI Clock signal sent to `outputDeviceInfo`.
*   **`DetectedBpm`**: When in Slave mode, reports the smoothed, detected tempo of the incoming clock signal.
*   **`SyncStatus`**: Reports whether the engine is currently locked to the external clock signal.

---

### Synthesis `Synthesizer`

```csharp
public sealed class Synthesizer : SoundComponent, IMidiControllable
{
    public Synthesizer(AudioEngine engine, AudioFormat format, IInstrumentBank instrumentBank);

    public IReadOnlyList<MidiModifier> MidiModifiers { get; }
    public float Bpm { get; set; } = 120f;
    public bool MpeEnabled { get; set; }

    public void AddMidiModifier(MidiModifier modifier);
    public void RemoveMidiModifier(MidiModifier modifier);
    public void ProcessMidiMessage(MidiMessage message);
    public void Reset();

    protected override void GenerateAudio(Span<float> buffer, int channels);
}
```

**Description:**
A high-performance, polyphonic, multi-timbral synthesizer engine.

**Architecture:**
1.  **Input Stage**: Incoming MIDI messages pass through the **MidiModifier Chain** (Arpeggiators, Transposers, etc.).
2.  **Channel Dispatch**: Processed messages are routed to one of 16 internal `MidiChannel` objects.
3.  **Voice Allocation**: Each channel manages a pool of `IVoice` instances. When a Note On is received, a voice is allocated from the bank's `Instrument`.
4.  **MPE Handling**: If `MpeEnabled` is true, the standard channel dispatch is overridden. The synthesizer tracks active notes and routes per-note expression messages (Pitch Bend, Channel Pressure, CC74) directly to the specific voice instance playing that note, regardless of its assigned MIDI channel.

**Properties:**
*   **`Bpm`**: Sets the master tempo for the synthesizer. This drives any temporal modifiers (like Arpeggiators) in the input chain.
*   **`MidiModifiers`**: The chain of MIDI effects that process input before sound generation.

---

### Synthesis.Banks `SoundFontBank`

```csharp
public sealed class SoundFontBank : IInstrumentBank, IDisposable
{
    public SoundFontBank(string filePath, AudioFormat format);

    public IReadOnlyList<PresetInfo> AvailablePresets { get; }

    public Instrument GetInstrument(int bank, int program);
    public void Dispose();
}
```

**Description:**
Loads and parses standard SoundFont 2 (.sf2) files. It acts as a bridge between the file format and the runtime synthesis engine.

**Functionality:**
*   **Parsing**: Reads the RIFF chunk structure of the SF2 file.
*   **Sample Loading**: Extracts audio samples, converts them to floating-point, and resamples them to match the engine's sample rate.
*   **Instrument Building**: Iterates through Presets, Zones, and Generators in the SF2 metadata to construct `Instrument` objects. It maps velocity layers and key ranges to specific `VoiceDefinition`s containing `SamplerGenerator`s.
*   **`AvailablePresets`**: Exposes a list of all programs found in the file, making it easy to build UI selectors.

---

### Utils `ControllableParameterAttribute`

```csharp
[AttributeUsage(AttributeTargets.Property)]
public sealed class ControllableParameterAttribute : Attribute
{
    public ControllableParameterAttribute(string displayName, double minValue, double maxValue, MappingScale scale = MappingScale.Linear);

    public string DisplayName { get; }
    public double MinValue { get; }
    public double MaxValue { get; }
    public MappingScale Scale { get; }
}
```

**Description:**
A declarative attribute used to expose properties of `SoundModifier`, `TrackSettings`, or any `IMidiMappable` object to the MIDI mapping system.

**Usage:**
When the `MidiMappingManager` needs to map a MIDI value (0-127) to a property, it inspects this attribute to determine the valid range and scaling behavior.

**Example:**
```csharp
[ControllableParameter("Cutoff", 20.0, 20000.0, MappingScale.Logarithmic)]
public float CutoffFrequency { get; set; }
```
This tells the system that MIDI value 0 maps to 20.0, 127 maps to 20000.0, and the mapping should follow a logarithmic curve (standard for frequency controls).

---

### Midi.PortMidi `PortMidiExtensions`

```csharp
public static class PortMidiExtensions
{
    public static PortMidiBackend UsePortMidi(this AudioEngine engine);
}
```

**Description:**
Provides a fluent extension method to easily register the PortMidi backend with an `AudioEngine`.

**Example:**
```csharp
var engine = new MiniAudioEngine();
var midiBackend = engine.UsePortMidi(); // Registers and initializes backend
// midiBackend can now be used to configure sync
```

---

### Synthesis `BasicInstrumentBank`

```csharp
public sealed class BasicInstrumentBank : IInstrumentBank
{
    public BasicInstrumentBank(AudioFormat format);
    public Instrument GetInstrument(int bank, int program);
}
```

**Description:**
A lightweight, zero-dependency instrument bank. It contains a hardcoded set of procedural instruments (using basic waveforms like Sine, Sawtooth, Square).

**Use Cases:**
*   **Testing**: Verifying synthesis logic without needing external files.
*   **Fallback**: Ensuring the synthesizer always has *something* to play if a requested SoundFont fails to load.
*   **Presets**: Typically includes basic General MIDI approximations (e.g., Piano, Strings, Lead).

---

### Synthesis `MultiInstrumentBank`

```csharp
public sealed class MultiInstrumentBank : IInstrumentBank, IDisposable
{
    public MultiInstrumentBank(Instrument masterFallbackInstrument);

    public void AddBank(IInstrumentBank bank);
    public bool RemoveBank(IInstrumentBank bank);
    public void ClearBanks();
    public Instrument GetInstrument(int bank, int program);
    public void Dispose();
}
```

**Description:**
A composite implementation of `IInstrumentBank`. It maintains an ordered list of child banks.

**Resolution Logic:**
When `GetInstrument(bank, program)` is called:
1.  It iterates through the child banks in **reverse order** (LIFO / Priority).
2.  It calls `GetInstrument` on each child bank.
3.  If a child bank returns a valid instrument (i.e., `!instrument.IsFallback`), that instrument is returned immediately.
4.  If no valid instrument is found in any child bank, `masterFallbackInstrument` is returned.

This allows users to "stack" banks. For example, loading a Piano SF2 first, then a Synth SF2. If the Synth SF2 doesn't have a piano patch, the request falls through to the Piano SF2.

---

### Synthesis.Instruments `Instrument`

```csharp
public class Instrument
{
    public Instrument(List<VoiceMapping> mappings, VoiceDefinition fallbackDefinition, bool isFallback = false);

    public bool IsFallback { get; }
    public bool IsEmpty { get; }

    public VoiceDefinition GetVoiceDefinition(int noteNumber, int velocity);
}
```

**Description:**
Represents a single patch (e.g., "Grand Piano"). It is essentially a collection of `VoiceMapping` rules.

**Methods:**
*   **`GetVoiceDefinition(...)`**: The core lookup logic. It scans the internal mappings to find the one that covers the requested Note Number and Velocity range. This enables:
*   **Key Splitting**: Different sounds for different keyboard ranges (e.g., Bass on left hand, Piano on right).
*   **Velocity Layering**: Different samples for soft vs. hard key presses.

---

### Synthesis.Instruments `VoiceDefinition`

```csharp
public class VoiceDefinition
{
    public VoiceDefinition(AudioFormat format, Oscillator.WaveformType oscType, int unison, float detune, float attack, float decay, float sustain, float release, bool useFilter = false);

    public IVoice CreateVoice(VoiceContext context);
}
```

**Description:**
A factory/blueprint for creating voices. It stores the static configuration for a sound (e.g., ADSR times, oscillator type, sample reference). It does *not* hold state for an active note.

**Function:**
When `CreateVoice` is called (on Note On), it instantiates a new `IVoice` object (either `Voice` for synth sounds or `SampleVoice` for sampled sounds), injecting the configuration defined here.

---

### Synthesis.Instruments `VoiceMapping`

```csharp
public class VoiceMapping
{
    public VoiceMapping(VoiceDefinition definition);

    public VoiceDefinition Definition { get; }
    public int MinKey { get; set; }
    public int MaxKey { get; set; }
    public int MinVelocity { get; set; }
    public int MaxVelocity { get; set; }

    // Tuning & Panning Offsets
    public float Pan { get; set; }
    public int Tune { get; set; }

    public bool IsMatch(int noteNumber, int velocity);
}
```

**Description:**
Defines the criteria under which a specific `VoiceDefinition` should be used. It specifies the valid range of MIDI keys and velocities.

---

### Synthesis.Interfaces `IGenerator`

```csharp
public interface IGenerator
{
    int Generate(Span<float> buffer, VoiceContext context);
    void Reset();
}
```

**Description:**
The fundamental building block of the synthesis engine's signal chain.
*   **Generators**: Produce audio (e.g., `OscillatorGenerator`, `SamplerGenerator`) or control signals (e.g., `AdsrGenerator`).
*   **Context**: The `VoiceContext` passed to `Generate` provides current real-time data (frequency, sample rate) needed for signal generation.

---

### Synthesis.Interfaces `IVoice`

```csharp
public interface IVoice
{
    int NoteNumber { get; }
    int Velocity { get; }
    bool IsReleasing { get; }
    bool IsSustained { get; set; }
    bool IsFinished { get; }

    void Render(Span<float> buffer);
    void NoteOff();
    void Kill();
    void ProcessMidiControl(MidiMessage message, float channelPitchBend);

    void SetPerNotePitchBend(float semitones);
    void SetPerNotePressure(float value);
    void SetPerNoteTimbre(float value);
}
```

**Description:**
The interface for an active, sounding voice.

**Lifecycle:**
1.  Created by `Synthesizer` on Note On.
2.  `Render()` is called repeatedly to fill audio buffers.
3.  `NoteOff()` triggers the release envelope.
4.  `IsFinished` becomes true when the envelope completes.
5.  Synthesizer removes/recycles the voice.

**MPE Support**:
The `SetPerNote...` methods allow individual voices to be modulated independently of the global channel state, enabling polyphonic expression.

---

### Editing `MidiExporter`

```csharp
public static class MidiExporter
{
    public static Task ExportAsync(Composition composition, string filePath);
}
```

**Description:**
A utility to export the current `Composition` state to a standard `.mid` file.
*   Converts internal `TempoTrack` markers to MIDI Meta Events (Set Tempo).
*   Converts all `MidiTrack` segments and their absolute timings into standard MIDI Track Chunks.
*   Handles merging multiple segments into single continuous tracks as required by the SMF specification.

---

### Editing `QuantizationSettings`

```csharp
public record QuantizationSettings
{
    public enum GridInterval { WholeNote, ... SixteenthNote, ... }

    public GridInterval Grid { get; init; } = GridInterval.SixteenthNote;
    public double Strength { get; init; } = 1.0;
    public bool QuantizeNoteEnd { get; init; } = false;
    public double Swing { get; init; } = 0.5;
}
```

**Description:**
Parameters for the quantization algorithm.
*   **`Strength`**: (0.0 - 1.0) Interpolates the note position between its original time and the grid time. Allows for "soft" quantization.
*   **`Swing`**: (0.0 - 1.0) Adjusts the grid itself. 0.5 is straight. >0.5 delays every second grid point to create a swing/shuffle feel.

---

### Editing `MidiNote`

```csharp
public sealed class MidiNote
{
    public MidiNote(long startTick, long durationTicks, int noteNumber, int velocity);

    public Guid Id { get; }
    public long StartTick { get; set; }
    public long DurationTicks { get; set; }
    public int NoteNumber { get; set; }
    public int Velocity { get; set; }
}
```

**Description:**
The high-level editing model for a note.
*   **Abstraction**: Unlike raw MIDI (which consists of separate On/Off events), this object represents the *entire note* with a start and duration.
*   **Editing**: Changing properties here (e.g., `StartTick`) automatically updates the underlying raw event stream in the `MidiSequence`.

---

### Editing `ControlPoint`

```csharp
public sealed class ControlPoint
{
    public ControlPoint(long tick, int value);

    public Guid Id { get; }
    public long Tick { get; set; }
    public int Value { get; set; }
}
```

**Description:**
Represents a single node in an automation curve (CC or Pitch Bend). Used by `MidiSequence` to manage continuous controller data in an editable way.

---

### Editing `TempoMarker`

```csharp
public readonly record struct TempoMarker(TimeSpan Time, double BeatsPerMinute);
```

**Description:**
A data structure used in the `Composition.TempoTrack` list. It defines a specific BPM value taking effect at a specific absolute time. The system interpolates time between these markers to calculate MIDI ticks.

---

### Midi.Enums `SyncMode`

```csharp
public enum SyncMode
{
    Off,
    Master,
    Slave
}
```
*   **`Off`**: No external synchronization.
*   **`Master`**: The engine generates MIDI Clock messages based on its internal transport.
*   **`Slave`**: The engine's transport is driven by incoming MIDI messages.

### Midi.Enums `SyncSource`

```csharp
public enum SyncSource
{
    Internal,
    MidiClock,
    Mtc
}
```
*   **`Internal`**: Standard internal clock (used when `SyncMode` is Off/Master).
*   **`MidiClock`**: Sync to MIDI Beat Clock (0xF8) messages. Tempo is derived from the clock rate.
*   **`Mtc`**: Sync to MIDI Time Code (quarter frame) messages. Position is absolute based on timecode.

### Midi.Enums `SyncStatus`

```csharp
public enum SyncStatus
{
    Unlocked,
    Locked
}
```
*   **`Unlocked`**: No valid clock signal is being received, or the signal is unstable.
*   **`Locked`**: The engine has successfully locked onto the external clock source.

### Midi.Enums `PortMidiError`

```csharp
public enum PortMidiError
{
    NoError = 0,
    HostError = -10000,
    InvalidDeviceId,
    InsufficientMemory,
    BufferTooSmall,
    BufferOverflow,
    BadPtr,
    BadData,
    InternalError,
    DeviceIsBusy
}
```
**Description:**
Native error codes returned by the PortMidi library. `PortBackendException` wraps these codes to provide descriptive .NET exceptions.

### Midi.Modifier `ArpeggiatorModifier`

```csharp
public sealed class ArpeggiatorModifier : MidiModifier, ITemporalMidiModifier
{
    public override string Name { get; } // "Arpeggiator (Mode)"

    public ArpMode Mode { get; set; } = ArpMode.Up;
    public int Octaves { get; set; } = 1;
    public double Rate { get; set; } = 0.25;
    public double Gate { get; set; } = 0.9;

    public override IEnumerable<MidiMessage> Process(MidiMessage message);
    public IEnumerable<MidiMessage> Tick(double deltaSeconds, double bpm);
    public void ResetState();
}
```

**Description:**
A stateful, temporal MIDI modifier that transforms held chords into rhythmic arpeggio patterns. Unlike standard modifiers that output messages immediately upon input, the Arpeggiator buffers incoming `NoteOn` messages and generates a stream of new notes over time based on a clock signal.

This class implements `ITemporalMidiModifier`, meaning it requires an external clock source (typically the `Synthesizer`'s audio render loop) to call its `Tick` method to advance its internal timing logic.

**Properties:**

*   `Mode`: Gets or sets the arpeggiation pattern direction.
*   `Up`: Plays notes from lowest to highest.
*   `Down`: Plays notes from highest to lowest.
*   `UpDown`: Plays from lowest to highest, then back down.
*   `Random`: Plays held notes in a random order.
*   **`Octaves`**: Gets or sets the number of octaves the pattern will span. The arpeggiator will cycle through the held notes, then repeat them transposed up by one octave, up to this limit, before wrapping back to the base octave. Default is `1`.
*   **`Rate`**: Gets or sets the step duration in musical beats (Quarter Notes).
*   `1.0` = Quarter Note.
*   `0.5` = Eighth Note.
*   `0.25` = Sixteenth Note (Default).
*   **`Gate`**: Gets or sets the note duration as a fraction of the step `Rate`.
*   Range: `0.0` to `1.0`.
*   Example: A value of `0.5` means the note will sound for half the step time and be silent for the other half (staccato). A value of `1.0` creates a legato feel. Default is `0.9`.

**Methods:**

*   **`Process(MidiMessage message)`**: Intercepts `NoteOn` and `NoteOff` messages to update the internal list of held notes. These messages are consumed and **not** passed through to the output immediately. All other message types (CC, PitchBend) are passed through unchanged.
*   **`Tick(double deltaSeconds, double bpm)`**: Advances the arpeggiator's internal clock.
*   `deltaSeconds`: The time elapsed since the last tick.
*   `bpm`: The current tempo in Beats Per Minute.
*   **Returns**: An enumerable of generated `MidiMessage`s (new Note On/Off events) if a step boundary was crossed during this tick.
*   **`ResetState()`**: Resets the internal playback index, octave counter, and timing accumulator to zero. Useful for re-syncing the pattern when the transport starts or when the first note of a new phrase is played.

---

### Midi.Modifier `ChannelFilterModifier`

```csharp
public sealed class ChannelFilterModifier : MidiModifier
{
    public ChannelFilterModifier(int channel);

    public override string Name { get; } // "Channel Filter (Channel)"
    public int Channel { get; set; }

    public override IEnumerable<MidiMessage> Process(MidiMessage message);
}
```

**Description:**
A utility modifier that filters the MIDI stream, allowing only messages on a specific MIDI channel to pass through. All messages on other channels are dropped. This is useful for creating keyboard splits or isolating specific controllers in a complex routing graph.

**Constructor:**

*   `ChannelFilterModifier(int channel)`: Initializes the filter. `channel` should be 1-16.

**Properties:**

*   **`Channel`**: Gets or sets the target MIDI channel (1-16) to allow.

**Methods:**

*   **`Process(MidiMessage message)`**: Checks the `Channel` property of the incoming message.
*   If `message.Channel == this.Channel`, the message is returned.
*   Otherwise, an empty collection is returned (dropping the message).
*   If `IsEnabled` is false, the message is passed through regardless of channel.

---

### Midi.Modifier `HarmonizerModifier`

```csharp
public sealed class HarmonizerModifier : MidiModifier
{
    public HarmonizerModifier(int[] intervals);

    public override string Name { get; } // "Harmonizer (N Notes)"
    public int[] Intervals { get; set; }

    public override IEnumerable<MidiMessage> Process(MidiMessage message);
}
```

**Description:**
A modifier that automatically generates chords from single input notes. For every incoming `NoteOn` or `NoteOff` message, it generates multiple messages based on a defined list of semitone intervals relative to the root note.

**Constructor:**

*   `HarmonizerModifier(int[] intervals)`: Initializes the harmonizer with a set of intervals.
*   Example: `new int[] { 0, 4, 7 }` creates a Major triad (Root, Major 3rd, Perfect 5th).

**Properties:**

*   **`Intervals`**: Gets or sets the array of intervals in semitones.
*   `0` represents the original root note.
*   Negative values generate intervals below the root.
*   If this array is empty, the original message is passed through unchanged.

**Methods:**

*   **`Process(MidiMessage message)`**:
*   For `NoteOn`/`NoteOff`: Iterates through `Intervals`. For each interval, calculates `NewNote = OriginalNote + Interval`. Resulting notes are clamped to the valid MIDI range (0-127). Returns the collection of generated chord notes.
*   For other messages: Passes them through unchanged.

---

### Midi.Modifier `RandomizerModifier`

```csharp
public sealed class RandomizerModifier : MidiModifier
{
    public RandomizerModifier();

    public override string Name { get; } // "Randomizer"

    public float Chance { get; set; } = 1.0f;
    public float VelocityRandomness { get; set; } = 0.0f;
    public float VelocityBias { get; set; } = 0.0f;
    public float PitchRandomness { get; set; } = 0.0f;
    public int PitchRange { get; set; } = 12;
    public int MinNote { get; set; } = 0;
    public int MaxNote { get; set; } = 127;

    public override IEnumerable<MidiMessage> Process(MidiMessage message);
}
```

**Description:**
A powerful stochastic modifier that introduces controlled randomness to MIDI note data. It can be used to "humanize" sequences (subtle velocity changes) or for generative music (random pitch/probability). It maintains internal state to ensure that if a note's pitch is randomized on `NoteOn`, the corresponding `NoteOff` is randomized to the exact same pitch, preventing stuck notes.

**Properties:**

*   **`Chance`**: The probability that a note will be played (0.0 to 1.0).
*   `1.0`: Always play.
*   `0.5`: 50% chance to play.
*   If a note is skipped based on chance, it is dropped completely.
*   **`VelocityRandomness`**: The amount of random deviation applied to velocity (0.0 to 1.0).
*   `0.0`: No change.
*   `1.0`: Maximum deviation (full range).
*   **`VelocityBias`**: Biases the velocity randomization towards higher or lower values (-1.0 to 1.0).
*   `-1.0`: Tends towards lower velocities (softer).
*   `0.0`: Balanced randomization.
*   `1.0`: Tends towards higher velocities (louder).
*   **`PitchRandomness`**: The probability that a note's pitch will be randomized (0.0 to 1.0).
*   **`PitchRange`**: The maximum range in semitones (+/-) for pitch randomization.
*   Example: `12` means the note can shift up or down by up to one octave.
*   **`MinNote`** / **`MaxNote`**: Defines the key range (0-127) affected by this modifier. Notes outside this range are passed through unmodified.

**Methods:**

*   **`Process(MidiMessage message)`**: Applies randomization logic to `NoteOn` messages. Stores state for pitch-shifted notes to ensure correct `NoteOff` handling.

---

### Midi.Modifier `TransposeModifier`

```csharp
public sealed class TransposeModifier : MidiModifier
{
    public TransposeModifier(int semitones);

    public override string Name { get; } // "Transpose (N st)"
    public int Semitones { get; set; }

    public override IEnumerable<MidiMessage> Process(MidiMessage message);
}
```

**Description:**
A simple modifier that shifts the pitch of all incoming `NoteOn` and `NoteOff` messages by a fixed number of semitones.

**Constructor:**

*   `TransposeModifier(int semitones)`: Initializes the modifier with the specified shift.

**Properties:**

*   **`Semitones`**: Gets or sets the transposition amount.
*   Positive values transpose up (e.g., `12` = up 1 octave).
*   Negative values transpose down (e.g., `-12` = down 1 octave).

**Methods:**

*   **`Process(MidiMessage message)`**: Adds `Semitones` to `message.NoteNumber`. The result is clamped to the valid MIDI note range (0-127). Non-note messages are passed through unchanged.

---

### Midi.Modifier `VelocityModifier`

```csharp
public sealed class VelocityModifier : MidiModifier
{
    public override string Name { get; } // "Velocity"

    public float Curve { get; set; } = 0.0f;
    public int MinVelocity { get; set; } = 1;
    public int MaxVelocity { get; set; } = 127;
    public int Add { get; set; } = 0;

    public override IEnumerable<MidiMessage> Process(MidiMessage message);
}
```

**Description:**
A modifier designed to reshape the dynamic response of MIDI notes. It can compress, expand, offset, or apply non-linear curves to the velocity of incoming `NoteOn` messages.

**Properties:**

*   **`Curve`**: Gets or sets the velocity response curve (-1.0 to 1.0).
*   `0.0`: Linear response (Input = Output).
*   `-1.0` (Logarithmic): Makes it easier to play loud notes (boosts low velocities).
*   `+1.0` (Exponential): Makes it harder to play loud notes (attenuates low velocities), offering more dynamic control for expressive playing.
*   *Math:* The curve uses an exponent derived from $2^{-c}$.
*   **`MinVelocity`**: Clamps the minimum output velocity. Input velocities that would map below this value are set to this value. Range 1-127.
*   **`MaxVelocity`**: Clamps the maximum output velocity. Range 1-127.
*   **`Add`**: A fixed integer value added to the velocity after the curve and range calculations. Can be positive (boost) or negative (attenuate).

**Methods:**

*   **`Process(MidiMessage message)`**:
1.  Normalizes input velocity (0-1).
2.  Applies `Curve`.
3.  Remaps the result to the range [`MinVelocity`, `MaxVelocity`].
4.  Adds `Add`.
5.  Clamps final result to 1-127.
6.  Returns the modified message.

### Extensions.WebRtc.Apm

#### `AudioProcessingModule` (Class)
```csharp
public class AudioProcessingModule : IDisposable
{
    public AudioProcessingModule();
    public ApmError ApplyConfig(ApmConfig config);
    public ApmError Initialize();
    public ApmError ProcessStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);
    public ApmError ProcessReverseStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);
    // ... other methods for setting delays, levels, runtime settings, getting info, AEC dump ...
    public static int GetFrameSize(int sampleRateHz);
    public void Dispose();
}
```
**Description:** Provides low-level access to the WebRTC Audio Processing Module. It's responsible for initializing the APM, applying configurations, and processing audio frames. Generally used internally by `WebRtcApmModifier` and `NoiseSuppressor`.

#### `ApmConfig` (Class)
```csharp
public class ApmConfig : IDisposable
{
    public ApmConfig();
    public void SetEchoCanceller(bool enabled, bool mobileMode);
    public void SetNoiseSuppression(bool enabled, NoiseSuppressionLevel level);
    public void SetGainController1(bool enabled, GainControlMode mode, int targetLevelDbfs, int compressionGainDb, bool enableLimiter);
    public void SetGainController2(bool enabled);
    public void SetHighPassFilter(bool enabled);
    public void SetPreAmplifier(bool enabled, float fixedGainFactor);
    public void SetPipeline(int maxInternalRate, bool multiChannelRender, bool multiChannelCapture, DownmixMethod downmixMethod);
    public void Dispose();
}
```
**Description:** Used to configure the features of the `AudioProcessingModule` such as echo cancellation, noise suppression, gain control, etc.

#### `StreamConfig` (Class)
```csharp
public class StreamConfig : IDisposable
{
    public StreamConfig(int sampleRateHz, int numChannels);
    public int SampleRateHz { get; }
    public int NumChannels { get; }
    public void Dispose();
}
```
**Description:** Defines the properties (sample rate, number of channels) of an audio stream being processed by the APM.

#### `ProcessingConfig` (Class)
This class holds multiple `StreamConfig` instances for different parts of the APM pipeline (input, output, reverse input, reverse output).

#### `NoiseSuppressor` (Component - `SoundFlow.Extensions.WebRtc.Apm.Components`)
```csharp
public class NoiseSuppressor : IDisposable
{
    public NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High, bool useMultichannelProcessing = false);
    public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;
    public float[] ProcessAll();
    public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null);
    public void Dispose();
}
```
**Description:** A component for offline/batch noise suppression using WebRTC APM. It takes an `ISoundDataProvider`, processes its audio, and outputs the cleaned audio either as a whole or in chunks.
**Key Members:**
*   `NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, ...)`: Constructor.
*   `OnAudioChunkProcessed` (event): Raised when a chunk of audio is processed.
*   `ProcessAll()`: Processes the entire audio stream and returns it.
*   `ProcessChunks()`: Processes audio in chunks, raising `OnAudioChunkProcessed`.

#### `WebRtcApmModifier` (Modifier - `SoundFlow.Extensions.WebRtc.Apm.Modifiers`)
```csharp
public sealed class WebRtcApmModifier : SoundModifier, IDisposable
{
    public WebRtcApmModifier(
        bool aecEnabled = false, bool aecMobileMode = false, int aecLatencyMs = 40,
        bool nsEnabled = false, NoiseSuppressionLevel nsLevel = NoiseSuppressionLevel.High,
        // ... other AGC, HPF, PreAmp, Pipeline settings ...
    );

    public override string Name { get; set; }
    public EchoCancellationSettings EchoCancellation { get; }
    public NoiseSuppressionSettings NoiseSuppression { get; }
    public AutomaticGainControlSettings AutomaticGainControl { get; }
    public ProcessingPipelineSettings ProcessingPipeline { get; }
    public bool HighPassFilterEnabled { get; set; }
    public bool PreAmplifierEnabled { get; set; }
    public float PreAmplifierGainFactor { get; set; }
    public float PostProcessGain { get; set; }

    public override void Process(Span<float> buffer, int channels);
    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException
    public void Dispose();
}
```

**Description:** A `SoundModifier` that applies various WebRTC APM features (AEC, NS, AGC, HPF, PreAmp) to an audio stream in real-time.

**Key Members:**
*   Constructor with detailed initial settings.
*   Properties for configuring each APM feature (`EchoCancellation`, `NoiseSuppression`, `AutomaticGainControl`, `ProcessingPipeline`, `HighPassFilterEnabled`, etc.).
*   `Process(Span<float> buffer, int channels)`: Core processing logic.
*   `Dispose()`: Releases native APM resources.

#### Enums for WebRTC APM
*   `ApmError`: Error codes.
*   `NoiseSuppressionLevel`: Low, Moderate, High, VeryHigh.
*   `GainControlMode`: AdaptiveAnalog, AdaptiveDigital, FixedDigital.
*   `DownmixMethod`: AverageChannels, UseFirstChannel.
*   `RuntimeSettingType`: Types for runtime APM settings.

### Codecs.FFMpeg `FFmpegCodecFactory`

```csharp
public sealed class FFmpegCodecFactory : ICodecFactory
{
    public string FactoryId { get; }
    public IReadOnlyCollection<string> SupportedFormatIds { get; }
    public int Priority { get; }

    public ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format);
    public ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);
    public ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format);
}
```
**Description:** An implementation of `ICodecFactory` that uses a native FFmpeg wrapper to provide robust decoding and encoding for a broad spectrum of audio formats, including popular lossy (MP3, AAC, OGG, Opus) and lossless (FLAC, ALAC) codecs. To enable these formats, an instance of this factory must be registered with the `AudioEngine` using `engine.RegisterCodecFactory(new FFmpegCodecFactory())`.

**Properties:**

*   `FactoryId`: A unique string identifier for this factory, which is `"SoundFlow.Codecs.FFMpeg"`.
*   `SupportedFormatIds`: A read-only collection of supported format strings derived from the specific FFmpeg build configuration. This list includes, but is not limited to:
*   **Lossless:** `"wav"`, `"aiff"`, `"flac"`, `"alac"`, `"ape"`, `"wv"`, `"tta"`, `"shn"`
*   **Lossy:** `"mp3"`, `"mp2"`, `"ogg"`, `"opus"`, `"aac"`, `"m4a"`, `"wma"`, `"ac3"`
*   **Container/Other:** `"mka"`, `"mpc"`, `"tak"`, `"ra"`, `"dsf"`, `"au"`, `"gsm"`
*   `Priority`: The priority of this factory, which is `100`. This high value ensures it will be tried before lower-priority built-in codecs (like the `MiniAudioCodecFactory`) for any formats they might both support.

**Methods:**

*   `CreateDecoder(Stream stream, string formatId, AudioFormat format)`: Creates an internal `FFmpegDecoder` instance for the specified format if it is in the `SupportedFormatIds` list.
*   `TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null)`: Probes the input stream using the native FFmpeg format detection capabilities to determine the audio format and create a decoder. On success, `detectedFormat` is populated with the precise format discovered in the stream.
*   `CreateEncoder(Stream stream, string formatId, AudioFormat format)`: Creates an internal `FFmpegEncoder` instance for the specified format. It includes special logic to handle container formats like `"m4a"`, automatically selecting the `alac` encoder for high-bit-depth lossless audio or `aac` for standard lossy audio.

### Codecs.FFMpeg `FFmpegException`

```csharp
public sealed class FFmpegException : BackendException
{
    public FFmpegException(FFmpegResult result, string? message);

    public FFmpegResult Result { get; }
}
```

**Description:** Represents an error that occurred within the native `soundflow-ffmpeg` wrapper library. It inherits from `BackendException` and provides a strongly-typed `FFmpegResult` enum value that corresponds to a specific error code from the native C API, making it easier to diagnose issues.

**Constructor:**

*   `FFmpegException(FFmpegResult result, string? message)`: Initializes the exception with the native result code and an optional custom message. If the message is null, a descriptive error message is generated from the `result` code.

**Properties:**

*   `Result`: Gets the detailed `FFmpegResult` enum value that caused the exception. This provides specific insight into what failed (e.g., `DecoderErrorCodecNotFound`, `EncoderErrorWriteHeader`).


### Interfaces `ICodecFactory`

```csharp
public interface ICodecFactory
{
    string FactoryId { get; }
    IReadOnlyCollection<string> SupportedFormatIds { get; }
    int Priority { get; }

    ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format);
    ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);
    ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format);
}
```

**Description:** Defines a factory for creating custom `ISoundDecoder` and `ISoundEncoder` instances. This allows developers to extend the audio engine with support for new formats or alternative implementations (e.g., using a platform-specific library like FFmpeg).

**Properties:**

*   `FactoryId`: A unique string identifier for the factory.
*   `SupportedFormatIds`: A collection of lowercase string identifiers for the audio formats this factory supports (e.g., "mp3", "flac").
*   `Priority`: The priority level. Higher numbers are tried first when searching for a codec.

**Methods:**

*   `CreateDecoder(...)`: Creates a decoder instance for a known format ID.
*   `TryCreateDecoder(...)`: Attempts to create a decoder by probing an unknown stream. This method should detect the format and return the resulting `AudioFormat`.
*   `CreateEncoder(...)`: Creates an encoder instance for a specific format ID.

### Interfaces `IMidiBackend`

```csharp
public interface IMidiBackend : IDisposable
{
    void Initialize(AudioEngine engine);
    MidiInputDevice CreateMidiInputDevice(MidiDeviceInfo deviceInfo);
    MidiOutputDevice CreateMidiOutputDevice(MidiDeviceInfo deviceInfo);
    void UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs);
}
```

**Description:** Defines the contract for a pluggable MIDI backend, responsible for creating and managing MIDI devices.

**Methods:**

*   `Initialize(AudioEngine engine)`: Initializes the backend with a reference to the parent engine.
*   `CreateMidiInputDevice(MidiDeviceInfo deviceInfo)`: Creates a platform-specific MIDI input device instance.
*   `CreateMidiOutputDevice(MidiDeviceInfo deviceInfo)`: Creates a platform-specific MIDI output device instance.
*   `UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs)`: Retrieves the list of available MIDI input and output devices.

### Interfaces `IMidiMappable`

```csharp
public interface IMidiMappable
{
    Guid Id { get; }
}
```

**Description:** Defines an interface for objects that can be a target for MIDI mapping (e.g., a `SoundModifier` or `TrackSettings`). It ensures that any mappable object has a stable, unique identifier.

### Interfaces `IMidiControllable`

```csharp
public interface IMidiControllable
{
    void ProcessMidiMessage(MidiMessage message);
}
```

**Description:** Defines an interface for components that can be directly controlled by incoming MIDI messages (e.g., a `Synthesizer`).

### Interfaces `ISoundDataProvider`

```csharp
public interface ISoundDataProvider : IDisposable
{
    int Position { get; }
    int Length { get; }
    bool CanSeek { get; }
    SampleFormat SampleFormat { get; }
    int SampleRate { get; }
    bool IsDisposed { get; }
    SoundFormatInfo? FormatInfo { get; }

    event EventHandler<EventArgs>? EndOfStreamReached;
    event EventHandler<PositionChangedEventArgs>? PositionChanged;

    int ReadBytes(Span<float> buffer);
    void Seek(int offset);
}
```

**Properties:**

*   `Position`: The current read position within the audio data (in samples).
*   `Length`: The total length of the audio data (in samples). Can be -1 if length is unknown.
*   `CanSeek`: Indicates whether seeking is supported.
*   `SampleFormat`: The format of the audio samples.
*   `SampleRate`: The sample rate of the audio data.
*   `IsDisposed`: Indicates whether the provider has been disposed.
*   `FormatInfo`: Detailed format and tag information read from the source file. Can be `null`.

**Events:**

*   `EndOfStreamReached`: Raised when the end of the audio data is reached.
*   `PositionChanged`: Raised when the read position changes.

**Methods:**

*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.
*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio data.
*   `Dispose()`: Releases resources held by the data provider.

### Interfaces `ISoundDecoder`

```csharp
public interface ISoundDecoder : IDisposable
{
    bool IsDisposed { get; }
    int Length { get; }
    SampleFormat SampleFormat { get; }
    int Channels { get; }
    int SampleRate { get; }

    event EventHandler<EventArgs>? EndOfStreamReached;

    int Decode(Span<float> samples);
    bool Seek(int offset);
}
```

**Properties:**

*   `IsDisposed`: Indicates whether the decoder has been disposed.
*   `Length`: The total length of the decoded audio data (in samples).
*   `SampleFormat`: The sample format of the decoded audio data.
*   `Channels`: The number of channels in the decoded audio data.
*   `SampleRate`: The sample rate of the decoded audio data.

**Events:**

*   `EndOfStreamReached`: Raised when the end of the audio stream is reached during decoding.

**Methods:**

*   `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer.
*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio stream.
*   `Dispose()`: Releases the resources used by the decoder.

### Interfaces `ISoundEncoder`

```csharp
public interface ISoundEncoder : IDisposable
{
    bool IsDisposed { get; }

    int Encode(Span<float> samples);
}
```

**Properties:**

*   `IsDisposed`: Indicates whether the encoder has been disposed.

**Methods:**

*   `Encode(Span<float> samples)`: Encodes the provided audio samples.
*   `Dispose()`: Releases the resources used by the encoder.


### Interfaces `ISoundPlayer`

```csharp
public interface ISoundPlayer
{
    PlaybackState State { get; }
    bool IsLooping { get; set; }
    float PlaybackSpeed { get; set; }
    float Volume { get; set; }
    float Time { get; }
    float Duration { get; }
    float LoopStartSeconds { get; }
    float LoopEndSeconds { get; }
    int LoopStartSamples { get; }
    int LoopEndSamples { get; }

    void Play();
    void Pause();
    void Stop();
    bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);
    bool Seek(float time);
    bool Seek(int sampleOffset);
    void SetLoopPoints(float startTime, float? endTime = -1f);
    void SetLoopPoints(int startSample, int endSample = -1);
    void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);
}
```

**Properties:**

*   `State`: The current playback state (`Stopped`, `Playing`, `Paused`).
*   `IsLooping`: Whether looping is enabled or disabled (`get`, `set`).
*   `PlaybackSpeed`: Gets or sets the playback speed. 1.0 is normal speed.
*   `Volume`: Gets or sets the volume of the sound player (0.0 to 1.0 or higher for gain).
*   `Time`: The current playback position (in seconds).
*   `Duration`: The total duration of the audio (in seconds).
*   `LoopStartSeconds`: Gets the configured loop start point in seconds.
*   `LoopEndSeconds`: Gets the configured loop end point in seconds.  -1 indicates looping to the natural end.
*   `LoopStartSamples`: Gets the configured loop start point in samples.
*   `LoopEndSamples`: Gets the configured loop end point in samples. -1 indicates looping to the natural end.

**Methods:**

*   `Play()`: Starts or resumes playback.
*   `Pause()`: Pauses playback.
*   `Stop()`: Stops playback and resets the position to the beginning.
*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to the specified time using `TimeSpan`. Returns `true` if successful.
*   `Seek(float time)`: Seeks to the specified time (in seconds). Returns `true` if successful.
*   `Seek(int sampleOffset)`: Seeks to the specified sample offset (in samples). Returns `true` if successful.
*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures custom loop points using start and end times in seconds. `endTime` is optional;  use -1 or `null` to loop to the natural end.
*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures custom loop points using start and end sample indices. `endSample` is optional; use -1 to loop to the natural end.
*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures custom loop points using `TimeSpan`.

### Interfaces `IVisualizationContext`

```csharp
public interface IVisualizationContext
{
    void Clear();
    void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f);
    void DrawRectangle(float x, float y, float width, float height, Color color);
}
```

**Methods:**

*   `Clear()`: Clears the drawing surface.
*   `DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)`: Draws a line from (`x1`, `y1`) to (`x2`, `y2`) with the specified color and thickness.
*   `DrawRectangle(float x, float y, float width, float height, Color color)`: Draws a rectangle with the specified position, dimensions, and color.

### Interfaces `IVisualizer`

```csharp
public interface IVisualizer : IDisposable
{
    string Name { get; }

    event EventHandler VisualizationUpdated;

    void ProcessOnAudioData(Span<float> audioData);
    void Render(IVisualizationContext context);
}
```

**Properties:**

*   `Name`: The name of the visualizer.

**Events:**

*   `VisualizationUpdated`: Raised when the visualization needs to be updated.

**Methods:**

*   `ProcessOnAudioData(Span<float> audioData)`: Processes a chunk of audio data for visualization.
*   `Render(IVisualizationContext context)`: Renders the visualization using the provided `IVisualizationContext`.
*   `Dispose()`: Releases the resources used by the visualizer.

### Metadata `SoundMetadataReader`

```csharp
public static class SoundMetadataReader
{
    public static Result<SoundFormatInfo> Read(string filePath, ReadOptions? options = null);
    public static Task<Result<SoundFormatInfo>> ReadAsync(string filePath, ReadOptions? options = null);
    public static Result<SoundFormatInfo> Read(Stream stream, ReadOptions? options = null);
    public static Task<Result<SoundFormatInfo>> ReadAsync(Stream stream, ReadOptions? options = null, bool leaveOpen = false);
}
```

**Description:** A static class that provides a unified API for reading format information and metadata tags from various audio file formats. It automatically identifies the file type and uses the appropriate internal parser.

**Methods:**

*   `Read(string filePath, ...)`: Synchronously reads metadata from a file at the given path.
*   `ReadAsync(string filePath, ...)`: Asynchronously reads metadata from a file at the given path.
*   `Read(Stream stream, ...)`: Synchronously reads metadata from a stream. The stream must be readable and seekable.
*   `ReadAsync(Stream stream, ...)`: Asynchronously reads metadata from a stream.

All methods return a `Result<SoundFormatInfo>`. On success, the `Value` property contains a `SoundFormatInfo` object with the file's details. On failure, the `Error` property contains an `IError` object describing the issue (e.g., `UnsupportedFormatError`, `HeaderNotFoundError`).

### Metadata `SoundMetadataWriter`

```csharp
public static class SoundMetadataWriter
{
    public static Result WriteTags(string filePath, SoundTags tags);
    public static Task<Result> WriteTagsAsync(string filePath, SoundTags? tags);
    public static Result RemoveTags(string filePath);
    public static Task<Result> RemoveTagsAsync(string filePath);
}
```

**Description:** A static class for writing or removing metadata tags from audio files. These operations are destructive and rewrite the file. The process is atomic: a temporary file is created, and only upon successful writing is the original file replaced.

**Methods:**

*   `WriteTags(string filePath, SoundTags tags)`: Synchronously writes the provided tags to the file, overwriting any existing tags.
*   `WriteTagsAsync(string filePath, SoundTags? tags)`: Asynchronously writes the provided tags to the file.
*   `RemoveTags(string filePath)`: Synchronously removes all recognizable metadata tags from the file.
*   `RemoveTagsAsync(string filePath)`: Asynchronously removes all recognizable metadata tags from the file.

All methods return a `Result` object. `IsSuccess` will be `true` if the operation completed successfully.

### Metadata `SoundFormatInfo` (and supporting models)

```csharp
public class SoundFormatInfo
{
    public string FormatName { get; set; }
    public string FormatIdentifier { get; set; }
    public string CodecName { get; set; }
    public string ContainerVersion { get; set; }
    public TimeSpan Duration { get; set; }
    public int ChannelCount { get; set; }
    public int SampleRate { get; set; }
    public int BitsPerSample { get; set; }
    public int Bitrate { get; set; }
    public BitrateMode BitrateMode { get; set; }
    public bool IsLossless { get; set; }
    public SoundTags? Tags { get; set; }
    public CueSheet? Cues { get; set; }
}

public class SoundTags
{
    public string Title { get; internal set; }
    public string Artist { get; internal set; }
    public string Album { get; internal set; }
    public string Genre { get; internal set; }
    public uint? Year { get; internal set; }
    public uint? TrackNumber { get; internal set; }
    public byte[]? AlbumArt { get; internal set; }
    public string? Lyrics { get; internal set; }
}

public class ReadOptions
{
    public bool ReadTags { get; set; } = true;
    public bool ReadAlbumArt { get; set; }
    public bool ReadCueSheet { get; set; }
    public DurationAccuracy DurationAccuracy { get; set; } = DurationAccuracy.AccurateScan;
}
```

**Description:** A set of classes that model the metadata of an audio file.

*   **`SoundFormatInfo`**: The main container holding all discovered information about a file, including its format, duration, sample rate, and any tags or cue sheets that were read.
*   **`SoundTags`**: Holds common metadata tags like artist, title, album, genre, year, track number, embedded album art, and lyrics.
*   **`ReadOptions`**: A configuration object passed to `SoundMetadataReader` to control what data should be parsed (e.g., whether to read album art or scan for accurate duration).

### Modifiers `AlgorithmicReverbModifier`

```csharp
public sealed class AlgorithmicReverbModifier : SoundModifier
{
    public AlgorithmicReverbModifier(AudioFormat format);

    public override string Name { get; set; }
    [ControllableParameter("Wet", 0.0, 1.0)] public float Wet { get; set; }
    [ControllableParameter("Room Size", 0.0, 1.0)] public float RoomSize { get; set; }
    [ControllableParameter("Damping", 0.0, 1.0)] public float Damp { get; set; }
    [ControllableParameter("Width", 0.0, 1.0)] public float Width { get; set; }
    [ControllableParameter("Pre-Delay", 0.0, 100.0)] public float PreDelay { get; set; }
    [ControllableParameter("Mix", 0.0, 1.0)] public float Mix { get; set; }

    public override void ProcessMidiMessage(MidiMessage message);
    public override float ProcessSample(float sample, int channel);
}
```

**Description:** An algorithmic reverb modifier based on the Freeverb algorithm. Now supports multi-channel processing, applying distinct delay lines and modulation to create a wider spatial effect. Can be controlled via MIDI CC messages.

**Constructor:** `AlgorithmicReverbModifier(AudioFormat format)`: Initializes the modifier for the specified audio format.

**Properties:**

*   `Damp`: The damping factor of the reverb.
*   `Name`: The name of the modifier.
*   `PreDelay`: The pre-delay time (in milliseconds).
*   `RoomSize`: The simulated room size.
*   `Wet`: The wet mix amount.
*   `Width`: The stereo width of the reverb.
*   `Mix`: The wet/dry mix level of the reverb.

**Methods:**

*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control reverb parameters.
*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the reverb effect.

### Modifiers `BassBoostModifier`

```csharp
public class BassBoosterModifier : SoundModifier
{
    public BassBoosterModifier(AudioFormat format, float cutoff = 150f, float boostGainDb = 6f);

    [ControllableParameter("Cutoff", 20.0, 1000.0, MappingScale.Logarithmic)]
    public float Cutoff { get; set; }
    [ControllableParameter("Boost", 0.0, 24.0)]
    public float BoostGainDb { get; set; }

    public override void ProcessMidiMessage(MidiMessage message);
    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `Cutoff`: The cutoff frequency below which the bass boost is applied.
*   `BoostGainDb`: The gain applied to the bass boost in dB.

**Methods:**

*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control boost parameters.
*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the bass boost effect.

### Modifiers `ChorusModifier`

```csharp
public sealed class ChorusModifier : SoundModifier
{
    public ChorusModifier(AudioFormat format, float depthMs = 2f, float rateHz = 0.5f, float feedback = 0.7f, float wetDryMix = 0.5f, float maxDelayMs = 50f);

    [ControllableParameter("Depth", 0.1, 8.0)] public float DepthMs { get; set; }
    [ControllableParameter("Rate", 0.05, 5.0)] public float RateHz { get; set; }
    [ControllableParameter("Feedback", 0.0, 0.95)] public float Feedback { get; } // Read-only
    [ControllableParameter("Mix", 0.0, 1.0)] public float WetDryMix { get; set; }

    public override void ProcessMidiMessage(MidiMessage message);
    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `DepthMs`: The depth of the chorus effect in milliseconds.
*   `RateHz`: The rate of the LFO modulation in Hz.
*   `Feedback`: The feedback amount of the chorus effect.
*   `WetDryMix`: The wet/dry mix of the chorus effect.

**Methods:**

*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control chorus parameters.
*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the chorus effect.

### Modifiers `CompressorModifier`

```csharp
public class CompressorModifier : SoundModifier
{
    public CompressorModifier(AudioFormat format, float thresholdDb, float ratio, float attackMs, float releaseMs, float kneeDb = 0, float makeupGainDb = 0);

    [ControllableParameter("Threshold", -60.0, 0.0)] public float ThresholdDb { get; set; }
    [ControllableParameter("Ratio", 1.0, 20.0)] public float Ratio { get; set; }
    [ControllableParameter("Attack", 0.1, 200.0, MappingScale.Logarithmic)] public float AttackMs { get; set; }
    [ControllableParameter("Release", 5.0, 2000.0, MappingScale.Logarithmic)] public float ReleaseMs { get; set; }
    [ControllableParameter("Knee", 0.0, 12.0)] public float KneeDb { get; set; }
    [ControllableParameter("Makeup Gain", 0.0, 24.0)] public float MakeupGainDb { get; set; }

    public void UpdateParameters();
    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `ThresholdDb`: The threshold (in dB) above which compression is applied.
*   `Ratio`: The compression ratio.
*   `AttackMs`: The attack time (in milliseconds).
*   `ReleaseMs`: The release time (in milliseconds).
*   `KneeDb`: The knee width (in dB).
*   `MakeupGainDb`: The amount of makeup gain to apply after compression.

**Methods:**

*   `UpdateParameters()`: Recalculates internal coefficients after changing properties.
*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the compression effect.

### Modifiers `DelayModifier`

```csharp
public sealed class DelayModifier : SoundModifier
{
    public DelayModifier(int delaySamples, float feedback, float wetMix, float cutoff = 10000);

    [ControllableParameter("Feedback", 0.0, 0.99)] public float Feedback { get; set; }
    [ControllableParameter("Mix", 0.0, 1.0)] public float WetMix { get; set; }
    [ControllableParameter("Cutoff", 100.0, 20000.0, MappingScale.Logarithmic)] public float Cutoff { get; set; }

    public override void ProcessMidiMessage(MidiMessage message);
    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `Feedback`: The feedback amount of the delay.
*   `WetMix`: The wet/dry mix of the delay (0 = dry, 1 = wet).
*   `Cutoff`: The cutoff frequency for the low-pass filter applied to the delayed signal.

**Methods:**

*   `ProcessMidiMessage(MidiMessage message)`: Handles MIDI CC messages to control delay parameters.
*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the delay effect.

### Modifiers `Filter`

```csharp
public class Filter : SoundModifier
{
    public Filter(AudioFormat format);

    public AudioFormat Format { get; set; }
    [ControllableParameter("Filter Type", 0, 3)] public FilterType Type { get; set; }
    [ControllableParameter("Cutoff", 20.0, 20000.0, MappingScale.Logarithmic)] public float CutoffFrequency { get; set; }
    [ControllableParameter("Resonance", 0.0, 1.0)] public float Resonance { get; set; }
    public override string Name { get; set; } = "Filter";

    public override void ProcessMidiMessage(MidiMessage message);
    public override float ProcessSample(float sample, int channel);
}
```

**Description:** A `SoundModifier` that applies a digital biquad filter (low-pass, high-pass, band-pass, notch) to the audio signal and can be controlled via MIDI.

**Properties:**

*   `Format`: The audio format of the filter.
*   `Type`: The filter type (`LowPass`, `HighPass`, `BandPass`, `Notch`).
*   `CutoffFrequency`: The cutoff frequency of the filter.
*   `Resonance`: The resonance of the filter.
*   `Name`: The name of the modifier.

**Methods:**

*   `ProcessMidiMessage(MidiMessage message)`: Handles MIDI CC messages to control cutoff and resonance.
*   `ProcessSample(float sample, int channel)`: Applies the filter to a single audio sample.

### Modifiers `FrequencyBandModifier`

```csharp
public class FrequencyBandModifier : SoundModifier
{
    public FrequencyBandModifier(AudioFormat format, float lowCutoff, float highCutoff);

    [ControllableParameter("High Cut", 20.0, 20000.0, MappingScale.Logarithmic)]
    public float HighCutoffFrequency { get; set; }
    [ControllableParameter("Low Cut", 20.0, 20000.0, MappingScale.Logarithmic)]
    public float LowCutoffFrequency { get; set; }

    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `HighCutoffFrequency`: The high cutoff frequency of the frequency band.
*   `LowCutoffFrequency`: The low cutoff frequency of the frequency band.

**Methods:**

*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the frequency band modification.

### Modifiers `NoiseReductionModifier`

```csharp
public class NoiseReductionModifier : SoundModifier
{
    public NoiseReductionModifier(int fftSize = 2048, float alpha = 3f, float beta = 0.001f, float smoothingFactor = 0.9f, float gain = 1.5f, int noiseFrames = 5, VoiceActivityDetector? vad = null);

    public override string Name { get; set; }

    public override void Process(Span<float> buffer, int channels);
    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `Name`: The name of the modifier.

**Methods:**

*   `Process(Span<float> buffer, int channels)`: Processes an entire buffer of audio, applying noise reduction.
*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `NoiseReductionModifier` operates on buffers, not individual samples.

### Modifiers `ParametricEqualizer`

```csharp
public class ParametricEqualizer : SoundModifier
{
    public ParametricEqualizer(int channels);

    public override string Name { get; set; }
    
    public void AddBand(EqualizerBand band);
    public void AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth);
    public void AddBands(IEnumerable<EqualizerBand> bands);
    public void ClearBands();
    public override void Process(Span<float> buffer, int channels);
    public override float ProcessSample(float sample, int channel);
    public void RemoveBand(EqualizerBand band);
}
```

**Properties:**

*   `Name`: The name of the modifier.

**Methods:**

*   `AddBand(EqualizerBand band)`: Adds an `EqualizerBand` to the equalizer.
*   `AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth)`: Adds an equalizer band with the specified parameters.
*   `AddBands(IEnumerable<EqualizerBand> bands)`: Adds multiple equalizer bands.
*   `ClearBands()`: Removes all equalizer bands.
*   `Process(Span<float> buffer, int channels)`: Processes an entire buffer of audio, applying equalization.
*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `ParametricEqualizer` operates on buffers, not individual samples.
*   `RemoveBand(EqualizerBand band)`: Removes a specific equalizer band.


### Modifiers `MultiChannelChorusModifier`

```csharp
public class MultiChannelChorusModifier : SoundModifier
{
    public MultiChannelChorusModifier(
        AudioFormat format,
        float wetMix,
        int maxDelay,
        params (float depth, float rate, float feedback)[] channelParameters);
        
    [ControllableParameter("Mix", 0.0, 1.0)]
    public float WetMix { get; set; }

    public override void Process(Span<float> buffer, int channels);
    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException
}
```
**Description:** A sound modifier that implements a multi-channel chorus effect, allowing for different chorus parameters (depth, rate, feedback) for each audio channel. This is useful for creating rich, spatial chorus effects.

### Modifiers `ResamplerModifier`

```csharp
public class ResamplerModifier : SoundModifier
{
    public ResamplerModifier(float resampleFactor = 1.0f);
    public ResamplerModifier(int sourceRate, int targetRate);

    [ControllableParameter("Factor", 0.1, 4.0)]
    public float ResampleFactor { get; set; }

    public override void Process(Span<float> buffer, int channels);
    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException
}
```
**Description:** A real-time resampling modifier that changes the playback speed and pitch of an audio signal by modulating the sample consumption rate.

**Constructors:**
*   `ResamplerModifier(float resampleFactor = 1.0f)`: Initializes with a fixed speed factor.
*   `ResamplerModifier(int sourceRate, int targetRate)`: Calculates the factor based on source and target rates.

**Properties:**
*   `ResampleFactor`: Gets or sets the resampling factor. > 1.0 speeds up, < 1.0 slows down.

**Methods:**
*   `Process(Span<float> buffer, int channels)`: Processes and resamples the audio block.
*   `ProcessSample(float sample, int channel)`: **Throws `NotSupportedException`**. Resampling must be done on blocks for interpolation.

### Modifiers `TrebleBoostModifier`

```csharp
public class TrebleBoosterModifier : SoundModifier
{
    public TrebleBoosterModifier(AudioFormat format, float cutoff = 4000f, float boostGainDb = 6f);

    [ControllableParameter("Boost", 0.0, 24.0)]
    public float BoostGainDb { get; set; }
    [ControllableParameter("Cutoff", 1000.0, 20000.0, MappingScale.Logarithmic)]
    public float Cutoff { get; set; }

    public override void ProcessMidiMessage(MidiMessage message);
    public override float ProcessSample(float sample, int channel);
}
```

**Properties:**

*   `BoostGainDb`: The gain of the treble boost in decibels.
*   `Cutoff`: The cutoff frequency above which the treble boost is applied.

**Methods:**
*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control boost parameters.
*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the treble boost effect.

### Modifiers `VocalExtractorModifier`

```csharp
public class VocalExtractorModifier : SoundModifier
{
    public VocalExtractorModifier(int sampleRate, float minFrequency = 100f, float maxFrequency = 10000f, int fftSize = 4096, int hopSize = 1024);

    public override string Name { get; set; }
    public float MinFrequency { get; set; }
    public float MaxFrequency { get; set; }
    public int FftSize { get; }
    public int HopSize { get; }

    public override void Process(Span<float> buffer, int channels);
    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException
}
```

**Description:** An advanced audio modifier designed to isolate vocals from a mixed track. It employs a hybrid approach:
*   **Stereo/Quad Sources:** Uses Spatial Isolation (Mid-Side subtraction) to remove instruments panned to the sides, preserving center-panned vocals.
*   **Mono/Center Sources:** Falls back to Spectral Gating and Bandpass filtering to suppress noise and non-vocal frequencies based on energy thresholds.

**Constructor:**
*   `VocalExtractorModifier(int sampleRate, float minFrequency = 100f, float maxFrequency = 10000f, int fftSize = 4096, int hopSize = 1024)`:
*   `sampleRate`: The sample rate of the audio (must match the engine/source).
*   `minFrequency` / `maxFrequency`: The frequency range to preserve. Defaults to 100Hz-10kHz to capture the human voice while cutting mud and high hiss.
*   `fftSize`: The size of the FFT window. Defaults to 4096, which provides better frequency resolution for isolating bass/low-mid frequencies compared to smaller windows.
*   `hopSize`: The overlap size for the STFT process. Defaults to 1024 (25% overlap with 4096 window).

**Properties:**
*   `MinFrequency`: The lower bound of the frequency range to preserve (in Hz). Setting this avoids processing sub-bass rumble.
*   `MaxFrequency`: The upper bound of the frequency range to preserve (in Hz).
*   `FftSize`: (Read-only) The configured FFT size.
*   `HopSize`: (Read-only) The configured hop size.

**Methods:**
*   `Process(Span<float> buffer, int channels)`: Processes the audio buffer. It handles channel management, pairing L/R channels for spatial processing and treating others as mono.
*   `ProcessSample(float sample, int channel)`: **Throws `NotSupportedException`**. This modifier relies on FFT-based block processing and cannot operate on single samples.

### Providers `AssetDataProvider`

```csharp
public sealed class AssetDataProvider : ISoundDataProvider
{
    public AssetDataProvider(AudioEngine engine, Stream stream, ReadOptions? options = null);
    public AssetDataProvider(AudioEngine engine, AudioFormat format, Stream stream);
    public AssetDataProvider(AudioEngine engine, byte[] data, ReadOptions? options = null);

    public bool CanSeek { get; } = true;
    public int Length { get; }
    public int Position { get; }
    public int SampleRate { get; }
    public SampleFormat SampleFormat { get; }
    public SoundFormatInfo? FormatInfo { get; }
    public bool IsDisposed { get; }

    public event EventHandler<PositionChangedEventArgs>? PositionChanged;
    public event EventHandler<EventArgs>? EndOfStreamReached;

    public void Dispose();
    public int ReadBytes(Span<float> buffer);
    public void Seek(int sampleOffset);
}
```

**Description:** An `ISoundDataProvider` that decodes and loads an entire audio file into an in-memory float array. It automatically detects the audio format by reading metadata or probing with codecs. Ideal for small to medium-sized files where seeking performance is critical.

**Constructors:**

*   `AssetDataProvider(AudioEngine engine, Stream stream, ...)`: Initializes by reading from a stream, automatically detecting the format.
*   `AssetDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes by reading from a stream with a specified target `AudioFormat`.
*   `AssetDataProvider(AudioEngine engine, byte[] data, ...)`: Initializes from a byte array containing the full audio file data.

**Properties:**

*   `FormatInfo`: Contains detailed format and tag information read from the source file.
*   `CanSeek`: Indicates whether seeking is supported (always true for `AssetDataProvider`).
*   `Length`: The total length of the audio data (in samples).
*   `Position`: The current read position within the audio data (in samples).
*   `SampleRate`: The sample rate of the audio data.
*   `SampleFormat`: The format of the audio samples.

**Events:**

*   `PositionChanged`: Raised when the read position changes.
*   `EndOfStreamReached`: Raised when the end of the audio data is reached.


**Methods:**

*   `Dispose()`: Releases the resources used by the provider.
*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.
*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio data.

### Providers `StreamDataProvider`

```csharp
public sealed class StreamDataProvider : ISoundDataProvider
{
    public StreamDataProvider(AudioEngine engine, Stream stream, ReadOptions? options = null);
    public StreamDataProvider(AudioEngine engine, AudioFormat format, Stream stream);
    
    public bool CanSeek { get; }
    public int Length { get; }
    public int Position { get; private set; }
    public int SampleRate { get; }
    public SampleFormat SampleFormat { get; }
    public SoundFormatInfo? FormatInfo { get; }
    public bool IsDisposed { get; private set; }

    public event EventHandler<PositionChangedEventArgs>? PositionChanged;
    public event EventHandler<EventArgs>? EndOfStreamReached;

    public int ReadBytes(Span<float> buffer);
    public void Seek(int sampleOffset);
    public void Dispose();
}
```

**Description:** An `ISoundDataProvider` that reads and decodes audio data directly from a `Stream` on demand. It supports seeking if the underlying stream is seekable. This provider automatically detects the audio format by reading metadata or probing with available codecs.

**Constructors:**

*   `StreamDataProvider(AudioEngine engine, Stream stream, ...)`: Initializes by wrapping a stream, automatically detecting the format.
*   `StreamDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes by wrapping a stream with a specified target `AudioFormat`.

**Properties:**

*   `FormatInfo`: Contains detailed format and tag information read from the source file.
*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).
*   `Length`: The total length of the audio data (in samples).
*   `Position`: The current read position within the audio data (in samples).
*   `SampleRate`: The sample rate of the audio data.
*   `SampleFormat`: The format of the audio samples.

**Events:**

*   `PositionChanged`: Raised when the read position changes.
*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.

**Methods:**

*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the stream into the provided buffer.
*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio stream (if supported).
*   `Dispose()`: Releases resources used by the provider.

### Providers `MicrophoneDataProvider`

```csharp
public class MicrophoneDataProvider : ISoundDataProvider
{
    public MicrophoneDataProvider(AudioCaptureDevice captureDevice, int bufferSize = 8);

    public int Position { get; private set; }
    public int Length { get; } = -1;
    public bool CanSeek { get; } = false;
    public SampleFormat SampleFormat { get; }
    public int SampleRate { get; }
    public SoundFormatInfo? FormatInfo { get; } = null;
    public bool IsDisposed { get; private set; }

    public event EventHandler<EventArgs>? EndOfStreamReached;
    public event EventHandler<PositionChangedEventArgs>? PositionChanged;

    public void StartCapture();
    public void StopCapture();    
    public int ReadBytes(Span<float> buffer);
    public void Seek(int offset); // Throws NotSupportedException
    public void Dispose();
}
```

**Properties:**

*   `Position`: The current read position within the captured audio data (in samples).
*   `Length`: Returns -1.
*   `CanSeek`: Returns `false`.
*   `SampleFormat`: The sample format of the captured audio data.
*   `SampleRate`: The sample rate of the captured audio data.
*   `FormatInfo`: Always `null`.

**Events:**

*   `EndOfStreamReached`: Raised when `StopCapture()` is called, signaling the end of the microphone input stream.
*   `PositionChanged`: Raised after reading data, indicating that the read position has changed.

**Methods:**

*   `MicrophoneDataProvider(AudioCaptureDevice captureDevice, int bufferSize = 8)`: Constructor.
* `bufferSize`: The number of audio sample arrays to hold in internal queue. Higher values will lead to higher latency but will be more resilient to performance spikes.
*   `StartCapture()`: Starts capturing audio data from the microphone.
*   `StopCapture()`: Stops capturing audio data and raises the `EndOfStreamReached` event.
*   `ReadBytes(Span<float> buffer)`: Reads captured audio samples into the provided buffer. If not enough data is available in the queue it will fill the rest of the buffer with silence.
*   `Seek(int offset)`: Throws `NotSupportedException` because seeking is not supported for live microphone input.
*   `Dispose()`: Releases resources used by the `MicrophoneDataProvider`, unsubscribes from the `AudioEngine.OnAudioProcessed` event, and clears the internal buffer queue.

### Providers `ChunkedDataProvider`

```csharp
public sealed class ChunkedDataProvider : ISoundDataProvider
{
    public ChunkedDataProvider(AudioEngine engine, Stream stream, ReadOptions? options = null, int chunkSize = 220500);
    public ChunkedDataProvider(AudioEngine engine, AudioFormat format, Stream stream, int chunkSize = 220500);

    public int Position { get; }
    public int Length { get; }
    public bool CanSeek { get; }
    public SampleFormat SampleFormat { get; }
    public int SampleRate { get; }
    public SoundFormatInfo? FormatInfo { get; }
    public bool IsDisposed { get; private set; }

    public event EventHandler<EventArgs>? EndOfStreamReached;
    public event EventHandler<PositionChangedEventArgs>? PositionChanged;

    public int ReadBytes(Span<float> buffer);
    public void Seek(int sampleOffset);
    public void Dispose();
}
```

**Description:** An `ISoundDataProvider` designed for efficient handling of large audio files by reading and decoding them in manageable chunks. It automatically detects the audio format by reading metadata or probing with codecs.

**Constructors:**

*   `ChunkedDataProvider(AudioEngine engine, Stream stream, ...)`: Initializes by reading from a stream, automatically detecting the format.
*   `ChunkedDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes by reading from a stream with a specified target `AudioFormat`.

**Properties:**

*   `FormatInfo`: Contains detailed format and tag information read from the source file.
*   `Position`: The current read position within the audio data (in samples).
*   `Length`: The total length of the audio data in samples.
*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).
*   `SampleFormat`: The format of the audio samples.
*   `SampleRate`: The sample rate of the audio data.

**Events:**

*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.
*   `PositionChanged`: Raised when the read position changes.

**Methods:**

*   `ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a `Stream`. It creates an `ISoundDecoder` to decode the stream, sets the default chunk size (220500 samples per channel, which is 10 seconds at 44.1 kHz), and starts prefetching data.
*   `ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a file path. It opens a `FileStream` and calls the other constructor.
*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is prefilled with decoded audio. If the buffer runs out, it decodes another chunk from the stream.
*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported by the stream and decoder). It disposes of the current decoder, creates a new one, and seeks within the stream. Then, it clears the internal buffer and refills it from the new position.
*   `Dispose()`: Releases the resources used by the `ChunkedDataProvider`, including the decoder and the stream.

**Remarks:**

The `ChunkedDataProvider` is designed to handle large audio files efficiently by reading and decoding them in chunks. This prevents the entire file from being loaded into memory at once. The default chunk size is set to 10 seconds of audio at 44.1 kHz, but you can adjust this value in the constructor. The class uses an internal buffer (`Queue<float>`) to store decoded audio samples and prefetches data in the background.

### Providers `QueueDataProvider`

```csharp
public enum QueueFullBehavior
{
    /// <summary>
    ///     Throw an <see cref="InvalidOperationException"/> when the queue is full. This is the default behavior.
    /// </summary>
    Throw,

    /// <summary>
    ///     Block the calling thread until space becomes available in the queue.
    /// </summary>
    Block,

    /// <summary>
    ///     Silently drop the incoming samples and return immediately.
    /// </summary>
    Drop
}

public class QueueDataProvider : ISoundDataProvider
{
    public QueueDataProvider(AudioFormat format, int? maxSamples = null, QueueFullBehavior fullBehavior = QueueFullBehavior.Throw);

    public int SamplesAvailable { get; }
    public long TotalSamplesEnqueued { get; }
    public bool IsDisposed { get; }

    public int Position { get; }
    public int Length { get; } = -1;
    public bool CanSeek { get; } = false;
    public SampleFormat SampleFormat { get; }
    public int SampleRate { get; }
    public SoundFormatInfo? FormatInfo { get; } = null;

    public event EventHandler<EventArgs>? EndOfStreamReached;
    public event EventHandler<PositionChangedEventArgs>? PositionChanged;

    public void AddSamples(ReadOnlySpan<float> samples);
    public void Reset();
    public void CompleteAdding();

    public int ReadBytes(Span<float> buffer);
    public void Seek(int sampleOffset);
    public void Dispose();
}
```
**Description:** Provides audio data from an in-memory queue that is fed samples externally. This provider is ideal for scenarios where audio data is generated or received in chunks. It supports configurable behavior for when the queue becomes full, specified by the `QueueFullBehavior` enum.

**Properties:**

*   `Position`: The current read position within the audio data (in samples).
*   `Length`: Always returns -1, indicating the total length is unknown as it's a dynamic queue.
*   `CanSeek`: Always returns `false`, as seeking is not supported by this provider.
*   `SampleFormat`: The format of the audio samples.
*   `SampleRate`: The sample rate of the audio data.
*   `SamplesAvailable`: Gets the number of samples currently available in the queue.
*   `TotalSamplesEnqueued`: Gets the total number of samples that have been enqueued into the provider since its creation or last reset.
*   `IsDisposed`: Gets a value indicating whether the provider has been disposed.
*   `FormatInfo`: Always `null`.

**Events:**

*   `EndOfStreamReached`: Raised when the end of the audio stream is reached, which occurs when `CompleteAdding()` has been called and all samples have been read from the queue.
*   `PositionChanged`: Raised when the read position changes.

**Methods:**

*   `AddSamples(ReadOnlySpan<float> samples)`: Adds audio samples to the internal queue. The behavior when the queue is full is determined by the `QueueFullBehavior` set during construction:
*   If `QueueFullBehavior.Throw` (default), an `InvalidOperationException` is thrown if adding samples would exceed the maximum size.
*   If `QueueFullBehavior.Block`, the calling thread will block until space becomes available in the queue for the entire block of samples.
*   If `QueueFullBehavior.Drop`, the incoming samples are silently discarded.
*   Throws `InvalidOperationException` if called after `CompleteAdding()` has been invoked.
*   `Reset()`: Resets the provider to its initial state, clearing the sample queue, resetting the position, and allowing samples to be added again. Any threads previously blocked in `AddSamples` (if `QueueFullBehavior.Block` was active) will be unblocked.
*   `CompleteAdding()`: Marks that no more samples will be added to the queue. Once called, subsequent calls to `AddSamples` will throw an `InvalidOperationException`. The `EndOfStreamReached` event will be raised when all remaining samples in the queue have been read.
*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the internal queue into the provided buffer. Returns the number of samples read. If the queue is empty, it returns 0. If `CompleteAdding()` has been called and the queue becomes empty, the `EndOfStreamReached` event will be fired.
*   `Seek(int sampleOffset)`: Throws an `InvalidOperationException` because seeking is not supported by this queue-based provider.
*   `Dispose()`: Releases the resources used by the `QueueDataProvider`, clears the sample queue, and unblocks any waiting threads.

### Providers `NetworkDataProvider`

```csharp
public sealed class NetworkDataProvider : ISoundDataProvider
{
    public NetworkDataProvider(AudioEngine engine, string url, ReadOptions? options = null);
    public NetworkDataProvider(AudioEngine engine, AudioFormat format, string url, string? hlsSegmentFormatId = null);

    public int Position { get; }
    public int Length { get; }
    public bool CanSeek { get; private set; }
    public SampleFormat SampleFormat { get; private set; }
    public int SampleRate { get; private set; }
    public SoundFormatInfo? FormatInfo { get; }
    public bool IsDisposed { get; private set; }

    public event EventHandler<EventArgs>? EndOfStreamReached;
    public event EventHandler<PositionChangedEventArgs>? PositionChanged;

    public int ReadBytes(Span<float> buffer);
    public void Seek(int sampleOffset);
    public void Dispose();
}
```

**Description:** An `ISoundDataProvider` that provides audio data from a network source, handling both direct URLs and HLS playlists. It automatically detects the format of direct streams.

**Constructors:**

*   `NetworkDataProvider(AudioEngine engine, string url, ...)`: Initializes for a direct stream, automatically detecting the format. Not recommended for HLS.
*   `NetworkDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes with a specific target format. Required for HLS streams where the segment format must be known.

**Properties:**

*   `Length`: The total length of the audio data (in samples). Returns -1 for live HLS streams.
*   `CanSeek`: Indicates whether seeking is supported.
*   `SampleRate`: The sample rate of the audio data.
*   `FormatInfo`: Contains detailed format and tag information read from the source stream.
*   `Position`: The current read position within the audio data (in samples).
*   `SampleFormat`: The format of the audio samples. Determined after the initial connection to the stream.

**Events:**

*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.
*   `PositionChanged`: Raised when the read position changes.

**Methods:**

*   `NetworkDataProvider(string url, int? sampleRate = null)`: Constructor that initializes the `NetworkDataProvider` with a network URL. It determines whether the URL points to a direct audio file or an HLS playlist.
*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is filled asynchronously.
*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported). The behavior differs for direct URLs and HLS streams:
*   **Direct URLs:** Performs an HTTP range request to fetch data starting from the desired offset.
*   **HLS Streams:** Locates the HLS segment containing the desired time offset and starts downloading from that segment.
*   `Dispose()`: Releases the resources used by the `NetworkDataProvider`, including the `HttpClient`, decoder, and stream.

**Remarks:**

The `NetworkDataProvider` can handle both direct audio URLs and HLS (HTTP Live Streaming) playlists. It automatically detects the stream type and behaves accordingly.

**Direct Audio URLs:**

*   It uses `HttpClient` to make requests to the URL.
*   It supports seeking if the server responds with an "Accept-Ranges: bytes" header.
*   It creates an `ISoundDecoder` to decode the audio stream.
*   It buffers audio data asynchronously in a background thread.

**HLS Playlists:**

*   It downloads and parses the M3U(8) playlist file.
*   It identifies the individual media segments (e.g., `.ts` files).
*   It downloads and decodes segments sequentially.
*   It refreshes the playlist periodically for live streams.
*   It supports seeking by selecting the appropriate segment based on the desired time offset.
*   It determines whether the playlist has a defined end by checking for the `#EXT-X-ENDLIST` tag, which affects whether `Length` is known and `CanSeek` is true.

The class uses an internal `Queue<float>` to buffer audio samples. The `ReadBytes` method waits for data to become available in the buffer if it's empty.

### Providers `RawDataProvider`

```csharp
public sealed class RawDataProvider : ISoundDataProvider
{
    public RawDataProvider(float[] rawSamples);
    public RawDataProvider(Stream pcmStream, SampleFormat sampleFormat);
    public RawDataProvider(byte[] rawBytes, SampleFormat sampleFormat);
    public RawDataProvider(int[] rawSamples);
    public RawDataProvider(short[] rawSamples);

    public int Position { get; }
    public int Length { get; }
    public bool CanSeek { get; } = true;
    public SampleFormat SampleFormat { get; }
    public int SampleRate { get; } = 48000;
    public SoundFormatInfo? FormatInfo { get; } = null;
    public bool IsDisposed { get; private set; }

    public event EventHandler<EventArgs>? EndOfStreamReached;
    public event EventHandler<PositionChangedEventArgs>? PositionChanged;

    public int ReadBytes(Span<float> buffer);
    public void Seek(int offset);
    public void Dispose();
}
```
**Description:** Provides audio data directly from raw PCM arrays or streams. Assumes a sample rate of 48000 Hz.

**Properties:**
*   `Position`: The current read position in samples.
*   `Length`: The total length of the stream in samples.
*   `CanSeek`: Indicates if the underlying stream is seekable.
*   `SampleFormat`: The sample format of the raw audio data.
*   `SampleRate`: The sample rate of the raw audio data.
**Events:**
*   `EndOfStreamReached`: Raised when the end of the stream is reached.
*   `PositionChanged`: Raised when the read position changes.
**Methods:**
*   `ReadBytes(Span<float> buffer)`: Reads raw PCM data from the stream and converts it to `float` if necessary.
*   `Seek(int offset)`: Seeks to the specified offset in the underlying stream if `CanSeek` is true.
*   `Dispose()`: Disposes the underlying stream.

### Providers `MidiDataProvider`

```csharp
public sealed class MidiDataProvider
{
    public MidiDataProvider(MidiFile midiFile);
    public MidiDataProvider(Stream stream);
    public MidiDataProvider(MidiSequence sequence);

    public IReadOnlyList<TimedMidiEvent> Events { get; }
    public int TicksPerQuarterNote { get; }
    public long LengthTicks { get; }
    public TimeSpan Duration { get; }

    public IEnumerable<TimedMidiEvent> GetEvents(long startTick, long endTick);
    public TimeSpan GetTimeSpanForTick(long tick);
    public long GetTickForTimeSpan(TimeSpan time);
    public MidiFile ToMidiFile();
}
```

**Description:** A specialized provider that processes raw MIDI data into a time-ordered, linear sequence of events suitable for playback. It acts as the data source for the `Sequencer` component.

**Key Capabilities:**
*   **Time Ordering:** It merges events from multiple tracks in a `MidiFile` into a single chronological list, simplifying playback logic.
*   **Tempo Mapping:** It scans the MIDI file for tempo change events to build an internal tempo map.
*   **Time Conversion:** It provides crucial methods (`GetTimeSpanForTick` and `GetTickForTimeSpan`) that use the internal tempo map to accurately convert between MIDI ticks and real-world time (`TimeSpan`). This handles complex variable tempo changes seamlessly.

**Constructors:**
*   `MidiDataProvider(MidiFile midiFile)`: Creates a provider from a parsed `MidiFile`.
*   `MidiDataProvider(Stream stream)`: Parses a MIDI file directly from a stream.
*   `MidiDataProvider(MidiSequence sequence)`: Creates a provider from an editable `MidiSequence` (used by the editing engine).

**Methods:**
*   `GetEvents(long startTick, long endTick)`: Efficiently retrieves all events occurring within a specific tick range. Used by the `Sequencer` during the audio render loop.
*   `ToMidiFile()`: Converts the processed sequence back into a standard `MidiFile` object.

### Structs

### `AudioFormat`
```csharp
public record struct AudioFormat
{
    public SampleFormat Format;
    public int Channels;
    public ChannelLayout Layout;
    public int SampleRate;
    public float InverseSampleRate { get; }

    public static readonly AudioFormat Cd;
    public static readonly AudioFormat Dvd;
    public static readonly AudioFormat DvdHq;
    public static readonly AudioFormat Surround51;
    public static readonly AudioFormat Surround71;
    public static readonly AudioFormat Studio;
    public static readonly AudioFormat StudioHq;
    public static readonly AudioFormat Broadcast;
    public static readonly AudioFormat Telephony;

    public static AudioFormat? GetFormatFromStream(Stream stream);
    public static ChannelLayout GetLayoutFromChannels(int channels);
}
```
**Description:** A record struct representing the format of an audio stream, including sample format, channel count, channel layout, and sample rate.

**Fields:**

*   `Format`: The sample format.
*   `Channels`: The number of audio channels.
*   `Layout`: The physical or logical arrangement of channels.
*   `SampleRate`: The sample rate in Hertz.

**Properties:**

*   `InverseSampleRate`: Gets the inverse of the sample rate, useful for calculations involving sample duration.

**Static Presets:**

*   `Cd`: Standard Compact Disc (CD) audio format (S16, 2 Channels, 44100 Hz).
*   `Dvd`: Standard DVD-Video audio format (S16, 2 Channels, 48000 Hz).
*   `DvdHq`: High-quality DVD-Video audio format (F32, 2 Channels, 48000 Hz).
*   `Studio`: Common studio recording format (S24, 2 Channels, 96000 Hz).
*   `StudioHq`: High-quality studio recording format (F32, 2 Channels, 96000 Hz).
*   `Broadcast`: Standard broadcast audio format (S16, 1 Channel, 48000 Hz).
*   `Telephony`: Telephony and VoIP audio format (U8, 1 Channel, 8000 Hz).

**Static Methods:**

*   `GetFormatFromStream(Stream stream)`: Infers an `AudioFormat` by reading a stream's metadata.
*   `GetLayoutFromChannels(int channels)`: Infers a `ChannelLayout` from a channel count.

### Codecs.FFMpeg `FFmpegCodecFactory`

```csharp
public sealed class FFmpegCodecFactory : ICodecFactory
{
    public string FactoryId { get; } = "SoundFlow.Codecs.FFMpeg";
    public IReadOnlyCollection<string> SupportedFormatIds { get; }
    public int Priority { get; } = 100;

    public ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format);
    public ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);
    public ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format);
}
```

**Description:** An implementation of `ICodecFactory` that uses a native FFmpeg wrapper to provide robust decoding and encoding for a broad spectrum of audio formats, including popular lossy (MP3, AAC, OGG, Opus) and lossless (FLAC, ALAC) codecs. This factory is designed to be registered manually with the `AudioEngine`.

**Properties:**

*   `FactoryId`: The unique identifier, `"SoundFlow.Codecs.FFMpeg"`.
*   `SupportedFormatIds`: A read-only list of supported format strings, including: `"wav"`, `"aiff"`, `"flac"`, `"alac"`, `"ape"`, `"wv"`, `"tta"`, `"shn"`, `"mp3"`, `"mp2"`, `"ogg"`, `"opus"`, `"aac"`, `"m4a"`, `"wma"`, `"ac3"`, `"mka"`, `"mpc"`, `"tak"`, `"ra"`, `"dsf"`, `"au"`, `"gsm"`.
*   `Priority`: Set to `100` to ensure it takes precedence over lower-priority built-in or default codecs (like the `MiniAudioCodecFactory`).

**Methods:**

*   `CreateDecoder(...)`: Creates an `FFmpegDecoder` instance for the specified format.
*   `TryCreateDecoder(...)`: Probes the input stream using the native FFmpeg format detection capabilities to determine the format and create a decoder.
*   `CreateEncoder(...)`: Creates an `FFmpegEncoder` instance. Handles logic for formats like `"m4a"` (which may map to `alac` or `aac` depending on the input quality).

### Codecs.FFMpeg `FFmpegException`

```csharp
public sealed class FFmpegException : BackendException
{
    public FFmpegException(FFmpegResult result, string? message);

    public FFmpegResult Result { get; }
}
```

**Description:** Represents an error that occurred within the native FFmpeg Codecs library. It inherits from `BackendException` and carries the specific `FFmpegResult` enum value returned by the native function.

**Constructor:**

*   `FFmpegException(FFmpegResult result, string? message)`: Initializes the exception with the native result code and an optional custom message.

**Properties:**

*   `Result`: Gets the detailed result code from the native FFmpeg library.

### Structs `DeviceInfo`
```csharp
public record struct DeviceInfo
{
    public IntPtr Id;
    public bool IsDefault;
    public uint NativeDataFormatCount;
    public IntPtr NativeDataFormats; // Pointer to an array of NativeDataFormat

    public string Name { get; }
    public NativeDataFormat[] SupportedDataFormats { get; }
}
```
**Description:** Represents information about an audio device, including its native ID, name, whether it's the default system device, and a count/pointer to its supported native data formats. The `Name` is now a property that correctly decodes the UTF-8 name from the native byte array.

#### `MidiDeviceInfo`
```csharp
public readonly record struct MidiDeviceInfo
{
    public int Id { get; init; }
    public string Name { get; init; }
}
```
**Description:** Represents informational details for a MIDI device, unique to its configured backend.

### Structs `NativeDataFormat`
```csharp
[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]
public struct NativeDataFormat
{
    public SampleFormat Format;
    public uint Channels;
    public uint SampleRate;
    public uint Flags;
}
```
**Description:** Represents a specific native data format (sample format, channels, sample rate) supported by an audio device. Accessed via the `NativeDataFormats` pointer in `DeviceInfo`.

### Structs `Result`
```csharp
public readonly struct Result
{
    public bool IsSuccess { get; }
    public bool IsFailure { get; }
    public IError? Error { get; }

    public static Result Ok();
    public static Result Fail(IError error);
}

public readonly struct Result<T>
{
    public bool IsSuccess { get; }
    public bool IsFailure { get; }
    public T? Value { get; }
    public IError? Error { get; }

    public static Result<T> Ok(T value);
    public static Result<T> Fail(IError error);
}
```
**Description:** Implements the Railway-Oriented Programming (ROP) pattern using lightweight record structs, representing the outcome of an operation. This pattern encourages explicit error handling via the contained `IError` instead of relying on exceptions for expected failure paths.

### Structs `IError`

The `IError` interface and its various record-based implementations form a structured, railway-oriented error handling system. Instead of throwing exceptions for predictable failures, methods return a `Result` struct containing an object that implements `IError`. This allows for robust and explicit error handling.

```csharp
public interface IError
{
    string Message { get; }
    Exception? InnerException { get; }
}
```

**Implementations:**

The framework provides a comprehensive set of specific error types to clearly identify the nature of a failure.

*   **`Error(string Message, Exception? InnerException = null)`**
A base record implementation for a generic error.

*   **`ValidationError(string Message)`**
Represents errors related to invalid input arguments or preconditions.

*   **`NotFoundError(string ResourceName, string Message)`**
Represents an error when a required resource (like a file) is not found.

*   **`FileFormatError(string Message, Exception? InnerException = null)`**
An abstract base record for errors that occur during the parsing or writing of a specific file format.

*   **`UnsupportedFormatError(string FormatDetails)`**
The error that occurs when attempting to read an audio format that is not supported by the library.

*   **`HeaderNotFoundError(string HeaderDescription)`**
The error that occurs when a mandatory header, marker, or chunk is missing from a file (e.g., "RIFF chunk", "ID3v2 tag").

*   **`CorruptChunkError(string ChunkId, string Reason, Exception? InnerException = null)`**
The error that occurs when a recognized audio file's structural component (chunk, atom, etc.) is malformed.

*   **`CorruptFrameError(string FrameDescription, string Reason, Exception? InnerException = null)`**
The error that occurs when a specific frame within a stream-based format (like MP3 or AAC) is malformed.

*   **`ObjectDisposedError(string ObjectDescription)`**
The error that occurs when an operation is attempted on an object that has already been disposed.

*   **`DeviceError(string Message, Exception? InnerException = null)`**
An abstract base record for errors related to an audio or MIDI device.

*   **`DeviceStateError(string Reason)`**
The error that occurs when an operation is performed on a device that is in an invalid state for that operation.

*   **`DeviceOperationError(string Operation, string Reason, Exception? InnerException = null)`**
The error that occurs when a core device operation (like open, start, or stop) fails.

*   **`DeviceNotFoundError(string DeviceIdentifier)`**
The error that occurs when a requested audio device cannot be found.

*   **`BackendNotFoundError(string? BackendName = null)`**
The error that occurs when a required audio backend (like WASAPI, CoreAudio, etc.) is not available or enabled.

*   **`ResourceBusyError(string ResourceName)`**
The error that occurs when an operation attempts to use a resource that is already in use.

*   **`OutOfMemoryError()`**
The error that occurs when an attempt to allocate memory fails.

*   **`InvalidOperationError(string Message)`**
The error that occurs when a method call is invalid for the object's current state, analogous to `System.InvalidOperationException`.

*   **`NotImplementedError(string FeatureName)`**
The error that occurs when a requested feature or method is not implemented.

*   **`HostError(string Reason, Exception? InnerException = null)`**
Represents a generic error reported by the underlying operating system or host environment.

*   **`InternalLibraryError(string LibraryName, string Reason, Exception? InnerException = null)`**
Represents an unexpected error within a wrapped native library (e.g., "PortMidi", "miniaudio").

*   **`IOError(string OperationDescription, Exception? InnerException = null)`**
Represents errors that occur during an I/O operation (e.g., "reading from file stream").

*   **`TimeoutError(string OperationDescription)`**
The error that occurs when an operation times out.

*   **`AccessDeniedError(string ResourceIdentifier)`**
The error that occurs when access to a requested resource is denied.

### Utils `Extensions`

```csharp
public static class Extensions
{
    public static int GetBytesPerSample(this SampleFormat sampleFormat);
    public static SampleFormat GetSampleFormatFromBitsPerSample(this int bitsPerSample);
    public static unsafe Span<T> GetSpan<T>(nint ptr, int length) where T : unmanaged;
    public static T[] ReadArray<T>(this nint pointer, int count) where T : struct;
}
```
**Methods:**
*   `GetBytesPerSample(this SampleFormat sampleFormat)`: Returns the number of bytes per sample for a given sample format.
*   `GetSampleFormatFromBitsPerSample(this int bitsPerSample)`: Converts a bit depth to the corresponding `SampleFormat`.
*   `GetSpan<T>(nint ptr, int length)`: Creates a span of type `T` from a native memory pointer.
*   `ReadArray<T>(this nint pointer, int count) where T : struct`: Reads an array of structures of type `T` from a native memory pointer.

### Utils `Log`

```csharp
public static class Log
{
    public static event Action<LogLevel, string>? OnLog;

    public static void Debug(string message);
    public static void Info(string message);
    public static void Warning(string message);
    public static void Error(string message);
}
```

**Description:** Provides a centralized, decoupled logging mechanism for the entire SoundFlow library. Your application can subscribe to the static `OnLog` event to capture and handle all log messages generated by the library, allowing you to route them to a console, file, UI, or any other logging framework.

**Event:**

*   `OnLog`: A static event that is raised whenever a log message is generated. The delegate provides the `LogLevel` and the `string` message.

**Static Methods:**

*   `Debug(string message)`: Invokes the `OnLog` event with a `Debug` level message.
*   `Info(string message)`: Invokes the `OnLog` event with an `Info` level message.
*   `Warning(string message)`: Invokes the `OnLog` event with a `Warning` level message.
*   `Error(string message)`: Invokes the `OnLog` event with an `Error` level message.

**Example Usage (in your application):**
```csharp
Log.OnLog += (level, message) => {
    Console.WriteLine($"[{level}] {message}");
};
```

### `MathHelper`
```csharp
public static class MathHelper
{
    public static bool EnableAvx { get; set; }
    public static bool EnableSse { get; set; }
    
    public static float[] ResampleLinear(float[] inputData, int channels, int sourceRate, int targetRate);
    public static void InverseFft(Complex[] data);
    public static void Fft(Complex[] data);

    public static float[] HammingWindow(int size);
    public static float[] HanningWindow(int size);

    public static float Lerp(float a, float b, float t);
    public static bool IsPowerOfTwo(long n);
    public static double Mod(this double x, double y);
    public static float PrincipalAngle(float angle);
}
```
**Description:** Provides static helper methods for common mathematical operations, with a particular focus on signal processing functions like Fast Fourier Transforms (FFT/IFFT) and windowing functions. These methods leverage SIMD (Single Instruction, Multiple Data) instructions (AVX and SSE) for optimized performance on compatible hardware, with fallbacks to scalar implementations.

**Properties:**

*   `EnableAvx`: Gets or sets a value indicating whether to use AVX (Advanced Vector Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of AVX, forcing a fallback to SSE or scalar code.
*   `EnableSse`: Gets or sets a value indicating whether to use SSE (Streaming SIMD Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of SSE, forcing a fallback to scalar code. This also affects AVX routines that may utilize SSE for specific operations.

**Methods:**

*   `ResampleLinear(...)`: Resamples an array of audio data using linear interpolation, affecting speed and pitch.
*   `InverseFft(Complex[] data)`: Computes the Inverse Fast Fourier Transform (IFFT) of a complex array.
*   `Fft(Complex[] data)`: Computes the Fast Fourier Transform (FFT) of a complex array.
*   `HammingWindow(int size)`: Generates a Hamming window of the specified `size`.
*   `HanningWindow(int size)`: Generates a Hanning window of the specified `size`.
*   `Lerp(float a, float b, float t)`: Performs linear interpolation between two float values `a` and `b`.
*   `IsPowerOfTwo(long n)`: Checks if a given long integer `n` is a power of two.
*   `Mod(this double x, double y)`: An extension method for `double` that returns the remainder of the division of `x` by `y`.
*   `PrincipalAngle(float angle)`: Calculates the principal angle for a given `angle` in radians.

### Utils `ChannelMixer`
```csharp
public static class ChannelMixer
{
    public static bool EnableAvx { get; set; } = true;
    public static bool EnableSse { get; set; } = true;
    public static float[] Mix(float[] samples, int sourceChannels, int targetChannels);
}
```
**Description:** Highly optimized SIMD (AVX/SSE) utility for upmixing (mono->stereo/surround) and downmixing (stereo->mono) interleaved audio buffers.

**Properties:**
*   `EnableAvx`: Gets or sets a value indicating whether to use AVX (Advanced Vector Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of AVX, forcing a fallback to SSE or scalar code.
*   `EnableSse`: Gets or sets a value indicating whether to use SSE (Streaming SIMD Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of SSE, forcing a fallback to scalar code. This also affects AVX routines that may utilize SSE for specific operations.

**Methods:**
*   `Mix(...)`: Mixes audio samples from `sourceChannels` to `targetChannels` using SIMD instructions.

### Utils `MidiTimeConverter`

```csharp
public static class MidiTimeConverter
{
    public static TimeSpan GetTimeSpanForTick(long tick, int ticksPerQuarterNote, IReadOnlyList<TempoMarker> tempoTrack);
    public static long GetTickForTimeSpan(TimeSpan time, int ticksPerQuarterNote, IReadOnlyList<TempoMarker> tempoTrack);
}
```
**Description:** A static utility class providing essential methods to convert between MIDI ticks and real time (`TimeSpan`). These conversions are tempo-aware, using a composition's tempo track to accurately calculate timing across tempo changes.

**Methods:**
*   `GetTimeSpanForTick(...)`: Converts an absolute time in MIDI ticks to a `TimeSpan`.
*   `GetTickForTimeSpan(...)`: Converts a `TimeSpan` to an absolute time in MIDI ticks.

### Visualization `LevelMeterAnalyzer`

```csharp
public class LevelMeterAnalyzer : AudioAnalyzer
{
    public LevelMeterAnalyzer(AudioFormat format, IVisualizer? visualizer = null);

    public override string Name { get; set; }
    public float Peak { get; }
    public float Rms { get; }

    protected override void Analyze(Span<float> buffer, int channels);
}
```

**Properties:**

*   `Name`: The name of the analyzer.
*   `Peak`: The peak level of the audio signal.
*   `Rms`: The RMS (root mean square) level of the audio signal.

**Methods:**

*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to calculate the RMS and peak levels.

### Visualization `LevelMeterVisualizer`

```csharp
public class LevelMeterVisualizer : IVisualizer
{
    public LevelMeterVisualizer(LevelMeterAnalyzer levelMeterAnalyzer);

    public Color BarColor { get; set; }
    public string Name { get; }
    public Color PeakHoldColor { get; set; }
    public static Vector2 Size { get; }

    public event EventHandler? VisualizationUpdated;

    public void Dispose();
    public void ProcessOnAudioData(Span<float> audioData);
    public void Render(IVisualizationContext context);
}
```

**Properties:**

*   `BarColor`: The color of the level meter bar.
*   `Name`: The name of the visualizer.
*   `PeakHoldColor`: The color of the peak hold indicator.
*   `Size`: The size of the level meter.

**Events:**

*   `VisualizationUpdated`: Raised when the visualization needs to be updated.

**Methods:**

*   `Dispose()`: Releases resources used by the visualizer.
*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the level meter.
*   `Render(IVisualizationContext context)`: Renders the level meter visualization.

### Visualization `SpectrumAnalyzer`

```csharp
public class SpectrumAnalyzer : AudioAnalyzer
{
    public SpectrumAnalyzer(AudioFormat format, int fftSize, IVisualizer? visualizer = null);

    public override string Name { get; set; }
    public ReadOnlySpan<float> SpectrumData { get; }

    protected override void Analyze(Span<float> buffer, int channels);
}
```

**Properties:**

*   `Name`: The name of the analyzer.
*   `SpectrumData`: The calculated frequency spectrum data.

**Methods:**

*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to compute the frequency spectrum using an FFT.

### Visualization `SpectrumVisualizer`

```csharp
public class SpectrumVisualizer : IVisualizer
{
    public SpectrumVisualizer(SpectrumAnalyzer spectrumAnalyzer);

    public Color BarColor { get; set; }
    public string Name { get; }
    public static Vector2 Size { get; }

    public event EventHandler? VisualizationUpdated;

    public void Dispose();
    public void ProcessOnAudioData(Span<float> audioData);
    public void Render(IVisualizationContext context);
}
```

**Properties:**

*   `BarColor`: The color of the spectrum bars.
*   `Name`: The name of the visualizer.
*   `Size`: The size of the spectrum visualizer.

**Events:**

*   `VisualizationUpdated`: Raised when the visualization needs to be updated.

**Methods:**

*   `Dispose()`: Releases resources used by the visualizer.
*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the spectrum.
*   `Render(IVisualizationContext context)`: Renders the spectrum visualization.

### Visualization `WaveformVisualizer`

```csharp
public class WaveformVisualizer : IVisualizer
{
    public WaveformVisualizer();

    public string Name { get; }
    public List<float> Waveform { get; }
    public Color WaveformColor { get; set; }
    public Vector2 Size { get; }

    public event EventHandler? VisualizationUpdated;

    public void Dispose();
    public void ProcessOnAudioData(Span<float> audioData);
    public void Render(IVisualizationContext context);
}
```

**Properties:**

*   `Name`: The name of the visualizer.
*   `Waveform`: The waveform data.
*   `WaveformColor`: The color of the waveform.
*   `Size`: The size of the waveform visualizer.

**Events:**

*   `VisualizationUpdated`: Raised when the visualization needs to be updated.

**Methods:**

*   `Dispose()`: Releases resources used by the visualizer.
*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the waveform.
*   `Render(IVisualizationContext context)`: Renders the waveform visualization.