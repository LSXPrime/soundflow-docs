[
  {
    "id": 11,
    "slug": "webrtc-apm",
    "version": "1.2.0",
    "title": "WebRTC Audio Processing Module (APM) Extension",
    "description": "Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.",
    "navOrder": 11,
    "category": "Extensions",
    "content": "﻿---\r\nid: 11\r\ntitle: WebRTC Audio Processing Module (APM) Extension\r\ndescription: Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.\r\nnavOrder: 11\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# WebRTC Audio Processing Module (APM) Extension for SoundFlow\r\n\r\nThe `SoundFlow.Extensions.WebRtc.Apm` package integrates a native library based on the high-quality WebRTC Audio Processing Module (APM) into the SoundFlow audio engine. This extension provides advanced voice processing features essential for real-time communication and audio enhancement.\r\n\r\n## Features\r\n\r\nThe WebRTC APM extension offers several key audio processing features:\r\n\r\n*   **Acoustic Echo Cancellation (AEC):** Reduces or eliminates echoes that occur when audio played through speakers is picked up by the microphone.\r\n*   **Noise Suppression (NS):** Attenuates steady-state background noise (e.g., fans, hums) to improve speech clarity. Multiple suppression levels are available.\r\n*   **Automatic Gain Control (AGC):** Dynamically adjusts the microphone input volume to maintain a consistent audio level, preventing clipping or overly quiet audio. Supports different modes and target levels.\r\n*   **High Pass Filter (HPF):** Removes low-frequency components (typically below 80Hz) to reduce rumble and DC offset.\r\n*   **Pre-Amplifier:** Applies a configurable fixed gain to the audio signal before other APM processing steps.\r\n*   **Multi-channel Processing Configuration:** Allows specifying how multi-channel audio is handled and downmixed.\r\n\r\nThese features can be configured and applied primarily through the `WebRtcApmModifier` for real-time processing within the SoundFlow audio graph, or using the `NoiseSuppressor` component for offline batch processing.\r\n\r\n**Important Note on Sample Rates:** The WebRTC APM native library primarily supports specific sample rates: **8000 Hz, 16000 Hz, 32000 Hz, and 48000 Hz**. Ensure your SoundFlow `AudioEngine` is initialized with one of these sample rates when using this extension for optimal performance and compatibility.\r\n\r\n## Installation\r\n\r\nTo use this extension, you need to have the core `SoundFlow` library installed. Then, add the `SoundFlow.Extensions.WebRtc.Apm` package to your project:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\nThis package includes the necessary C# wrapper and the native WebRTC APM binaries for supported platforms.\r\n\r\n## Usage\r\n\r\n### Real-time Processing with `WebRtcApmModifier`\r\n\r\nThe `WebRtcApmModifier` is a `SoundModifier` that can be added to any `SoundComponent` to process its audio output in real-time. This is ideal for applications like voice chat, live audio input processing, or any scenario requiring real-time audio enhancement. The following steps demonstrate a typical full-duplex setup for echo cancellation.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine & Devices\" description=\"Create the engine and a full-duplex device\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine` and Devices\r\n        First, create an instance of the `AudioEngine`. Then, define an `AudioFormat` compatible with WebRTC (e.g., 48kHz mono). Finally, initialize a `FullDuplexDevice`, which manages both a capture (microphone) and a playback (speaker) device, making it perfect for AEC scenarios.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Abstracts.Devices;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Structs;\r\n\r\n        // Initialize the audio engine\r\n        var audioEngine = new MiniAudioEngine();\r\n\r\n        // Define a WebRTC APM compatible audio format\r\n        var audioFormat = new AudioFormat\r\n        {\r\n            SampleRate = 48000,\r\n            Channels = 1, // Mono for typical voice processing\r\n            Format = SampleFormat.F32\r\n        };\r\n\r\n        // Initialize a full-duplex device using the system's default microphone and speakers\r\n        var fullDuplexDevice = audioEngine.InitializeFullDuplexDevice(\r\n            playbackDeviceInfo: audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault),\r\n            captureDeviceInfo: audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault),\r\n            format: audioFormat\r\n        );\r\n\r\n        // Start the devices so they are ready for processing\r\n        fullDuplexDevice.Start();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create SoundComponent for Mic\" description=\"Set up a data provider and player for the mic\" icon='ph:microphone-bold'>\r\n        ### 2. Set Up the Microphone Input Component\r\n        Create a `MicrophoneDataProvider` to receive live audio from the capture device, and a `SoundPlayer` to process this data through the SoundFlow graph. This `SoundPlayer` will act as our microphone source component.\r\n\r\n        ```csharp\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Providers;\r\n\r\n        // Create a data provider that reads from our duplex device\r\n        var microphoneDataProvider = new MicrophoneDataProvider(fullDuplexDevice);\r\n\r\n        // Create a sound player to process the live mic data\r\n        // We will add the APM modifier to this player\r\n        var micAudioComponent = new SoundPlayer(audioEngine, audioFormat, microphoneDataProvider);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Configure APM Modifier\" description=\"Instantiate and set up the modifier\" icon='material-symbols:settings-outline'>\r\n        ### 3. Instantiate and Configure `WebRtcApmModifier`\r\n        The modifier's constructor allows setting initial states for all features. You can also adjust them dynamically via its public properties.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Modifiers;\r\n\r\n        var apmModifier = new WebRtcApmModifier(\r\n            // Echo Cancellation (AEC) settings\r\n            aecEnabled: true,\r\n            aecMobileMode: false, // Desktop mode is generally more robust\r\n            aecLatencyMs: 40,     // Estimated system latency for AEC (tune this)\r\n\r\n            // Noise Suppression (NS) settings\r\n            nsEnabled: true,\r\n            nsLevel: NoiseSuppressionLevel.High,\r\n\r\n            // Automatic Gain Control (AGC) - Version 1 (legacy)\r\n            agc1Enabled: true,\r\n            agcMode: GainControlMode.AdaptiveDigital,\r\n            agcTargetLevel: -3,   // Target level in dBFS (0 is max, typical is -3 to -18)\r\n            agcCompressionGain: 9, // Only for FixedDigital mode\r\n            agcLimiter: true,\r\n\r\n            // Automatic Gain Control (AGC) - Version 2 (newer, often preferred)\r\n            agc2Enabled: false, // Set to true to use AGC2, potentially disable AGC1\r\n\r\n            // High Pass Filter (HPF)\r\n            hpfEnabled: true,\r\n\r\n            // Pre-Amplifier\r\n            preAmpEnabled: false,\r\n            preAmpGain: 1.0f,\r\n\r\n            // Pipeline settings for multi-channel audio (if numChannels > 1)\r\n            useMultichannelCapture: false, // Process capture (mic) as mono/stereo as configured by AudioEngine\r\n            useMultichannelRender: false,  // Process render (playback for AEC) as mono/stereo\r\n            downmixMethod: DownmixMethod.AverageChannels // Method if downmixing is needed\r\n        );\r\n\r\n        // Example of changing a setting dynamically:\r\n        // apmModifier.NoiseSuppression.Level = NoiseSuppressionLevel.VeryHigh;\r\n        ```\r\n    </Step>\r\n    <Step title=\"Add Modifier\" description=\"Attach the modifier to the mic component\" icon='ic:baseline-plus'>\r\n        ### 4. Add the Modifier to the Microphone Component\r\n\r\n        ```csharp\r\n        micAudioComponent.AddModifier(apmModifier);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Start Processing\" description=\"Add to mixer and start playback\" icon='mdi:play-box-outline'>\r\n        ### 5. Add the Component to the Mixer and Start Processing\r\n        To complete the loop for testing, add the processed microphone component to the playback device's `MasterMixer`. This will route the cleaned microphone audio to the speakers.\r\n\r\n        ```csharp\r\n        // Add the processed mic component to the playback device's master mixer\r\n        fullDuplexDevice.MasterMixer.AddComponent(micAudioComponent);\r\n\r\n        // Start the microphone provider and the player component\r\n        microphoneDataProvider.StartCapture();\r\n        micAudioComponent.Play();\r\n\r\n        Console.WriteLine(\"WebRTC APM processing microphone input. Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        // Cleanup\r\n        fullDuplexDevice.Dispose(); // Disposes underlying devices and their components\r\n        apmModifier.Dispose();      // Important to release native resources\r\n        microphoneDataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n**AEC Far-End (Playback) Signal:** For Acoustic Echo Cancellation to work, the processor needs both the microphone signal (near-end) and the speaker signal (far-end). The `WebRtcApmModifier` is designed to integrate deeply with SoundFlow's architecture. It automatically detects and captures the audio being processed by the master mixer of any active playback device within the same `AudioEngine` context. This allows it to correlate the audio being sent to the speakers with the audio coming from the microphone to perform echo cancellation seamlessly.\r\n\r\n### Offline Processing with `NoiseSuppressor`\r\n\r\nThe `NoiseSuppressor` component is designed for batch processing of audio from an `ISoundDataProvider` (e.g., an audio file). It applies only the WebRTC Noise Suppression feature.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Required for encoding/decoding\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        The engine is required for its decoding and encoding capabilities, even if not playing audio back.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Structs;\r\n\r\n        var audioEngine = new MiniAudioEngine();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create Data Provider\" description=\"Load your noisy audio file\" icon='mdi:file-music-outline'>\r\n        ### 2. Create an `ISoundDataProvider` for your noisy audio file\r\n        Use a `StreamDataProvider` to read from an audio file.\r\n\r\n        ```csharp\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Define the format of the source file. This is needed by the data provider.\r\n        var fileFormat = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        string noisyFilePath = \"path/to/your/noisy_audio.wav\";\r\n        var dataProvider = new StreamDataProvider(audioEngine, fileFormat, File.OpenRead(noisyFilePath));\r\n        ```\r\n    </Step>\r\n    <Step title=\"Instantiate NoiseSuppressor\" description=\"Set up the offline processor\" icon='icon-park-outline:sound-wave'>\r\n        ### 3. Instantiate `NoiseSuppressor`\r\n        Provide the data source and its audio parameters. These MUST match the actual properties of the audio from the data provider.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Components;\r\n\r\n        // Parameters: dataProvider, sampleRate, numChannels, suppressionLevel\r\n        var noiseSuppressor = new NoiseSuppressor(dataProvider, 48000, 1, NoiseSuppressionLevel.VeryHigh);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Process the Audio\" description=\"Process all at once or in chunks\" icon='carbon:cics-transaction-server-zos'>\r\n        ### 4. Process the audio\r\n        You can process all audio at once (for smaller files) or chunk by chunk.\r\n\r\n        **Option A: Process All (returns `float[]`)**\r\n        ```csharp\r\n        float[] cleanedAudio = noiseSuppressor.ProcessAll();\r\n        // Now 'cleanedAudio' contains the noise-suppressed audio data.\r\n        \r\n        // You can save it using an ISoundEncoder:\r\n        var outputFormat = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        using var fileStream = new FileStream(\"cleaned_audio.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        using var encoder = audioEngine.CreateEncoder(fileStream, EncodingFormat.Wav, outputFormat);\r\n        encoder.Encode(cleanedAudio.AsSpan());\r\n        ```\r\n\r\n        **Option B: Process Chunks (via event or direct handler)**\r\n        ```csharp\r\n        var outputFormatChunked = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        using var chunkFileStream = new FileStream(\"cleaned_audio_chunked.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        using var chunkEncoder = audioEngine.CreateEncoder(chunkFileStream, EncodingFormat.Wav, outputFormatChunked);\r\n\r\n        noiseSuppressor.OnAudioChunkProcessed += (processedChunk) =>\r\n        {\r\n            if (!chunkEncoder.IsDisposed)\r\n            {\r\n                chunkEncoder.Encode(processedChunk.ToArray());\r\n            }\r\n        };\r\n\r\n        // ProcessChunks is a blocking call until the entire provider is processed.\r\n        noiseSuppressor.ProcessChunks();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Dispose Resources\" description=\"Clean up all IDisposable objects\" icon='material-symbols:delete-outline'>\r\n        ### 5. Dispose resources\r\n\r\n        ```csharp\r\n        noiseSuppressor.Dispose();\r\n        dataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n## Configuration Details\r\n\r\n### `WebRtcApmModifier` Properties:\r\n\r\n*   **`Enabled` (bool):** Enables/disables the entire APM modifier.\r\n*   **`EchoCancellation` (`EchoCancellationSettings`):**\r\n*   `Enabled` (bool): Enables/disables AEC.\r\n*   `MobileMode` (bool): Toggles between desktop (false) and mobile (true) AEC modes.\r\n*   `LatencyMs` (int): Estimated system audio latency in milliseconds. Crucial for AEC performance. Tune this value for your setup.\r\n*   **`NoiseSuppression` (`NoiseSuppressionSettings`):**\r\n*   `Enabled` (bool): Enables/disables NS.\r\n*   `Level` (`NoiseSuppressionLevel`): Sets the aggressiveness (Low, Moderate, High, VeryHigh).\r\n*   **`AutomaticGainControl` (`AutomaticGainControlSettings`):**\r\n*   `Agc1Enabled` (bool): Enables/disables the legacy AGC1.\r\n*   `Mode` (`GainControlMode`): Sets the mode for AGC1 (AdaptiveAnalog, AdaptiveDigital, FixedDigital).\r\n*   `TargetLevelDbfs` (int): Target level for AGC1 AdaptiveDigital mode (-31 to 0 dBFS).\r\n*   `CompressionGainDb` (int): Gain for AGC1 FixedDigital mode (0 to 90 dB).\r\n*   `LimiterEnabled` (bool): Enables/disables the limiter for AGC1.\r\n*   `Agc2Enabled` (bool): Enables/disables the newer AGC2.\r\n*   **`HighPassFilterEnabled` (bool):** Enables/disables the HPF.\r\n*   **`PreAmplifierEnabled` (bool):** Enables/disables the pre-amplifier.\r\n*   **`PreAmplifierGainFactor` (float):** Gain factor for the pre-amplifier (e.g., 1.0 is no change, 2.0 is +6dB).\r\n*   **`ProcessingPipeline` (`ProcessingPipelineSettings`):**\r\n*   `UseMultichannelCapture` (bool): If true and input is multi-channel, APM processes it as such. Otherwise, it might downmix.\r\n*   `UseMultichannelRender` (bool): Similar to capture, but for the far-end/render signal for AEC.\r\n*   `DownmixMethod` (`DownmixMethod`): Specifies how to downmix if multi-channel processing is disabled for a stream (AverageChannels, UseFirstChannel).\r\n*   **`PostProcessGain` (float):** A final gain applied after all APM processing (default 1.0f).\r\n\r\n### `NoiseSuppressor` Constructor:\r\n\r\n*   `dataProvider` (`ISoundDataProvider`): The audio source.\r\n*   `sampleRate` (int): Sample rate of the source audio (must be 8k, 16k, 32k, or 48k).\r\n*   `numChannels` (int): Number of channels in the source audio.\r\n*   `suppressionLevel` (`NoiseSuppressionLevel`): Desired noise suppression level.\r\n*   `useMultichannelProcessing` (bool): If true and `numChannels > 1`, attempts to process channels independently.\r\n\r\n## Licensing\r\n\r\n*   The C# code (`SoundFlow.Extensions.WebRtc.Apm` wrapper and components) is licensed under the **MIT License**.\r\n*   The native `webrtc-apm` library used by this extension is based on the WebRTC Audio Processing Module, which is typically licensed under the **BSD 3-Clause \"New\" or \"Revised\" License**. The specific version included is derived from the [PulseAudio project's extraction](https://gitlab.freedesktop.org/pulseaudio/webrtc-audio-processing).\r\n\r\n**Users must comply with the terms of both licenses.** This generally involves including the copyright notice and license text of the WebRTC code if distributing applications using this extension. Please consult the native library's specific distribution for exact requirements.\r\n\r\n## Troubleshooting\r\n\r\n*   **No effect or poor quality:**\r\n*   Verify the `AudioEngine` sample rate (set in the `AudioFormat` struct) matches one supported by WebRTC APM (8k, 16k, 32k, 48k Hz).\r\n*   For AEC, ensure `aecLatencyMs` is tuned appropriately for your system. Too low or too high values can degrade performance.\r\n*   Ensure the far-end signal is correctly being captured if AEC is enabled (usually is handled automatically by the modifier, which monitors active playback devices in the same engine context).\r\n*   **Errors during initialization:** Check the console output for any specific error messages from the native APM library. Ensure the native binaries are correctly deployed with your application.\r\n*   **Performance issues:** While WebRTC APM is optimized, processing many channels or enabling all features at very high settings can be CPU intensive. Monitor performance and adjust settings if needed."
  },
  {
    "id": 8,
    "slug": "tutorials-recording",
    "version": "1.2.0",
    "title": "Recording Audio",
    "description": "Comprehensive tutorials for recording audio from devices and processing it in real-time with SoundFlow.",
    "navOrder": 8,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 8\r\ntitle: Recording Audio\r\ndescription: Comprehensive tutorials for recording audio from devices and processing it in real-time with SoundFlow.\r\nnavOrder: 8\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Recording with SoundFlow\r\n\r\nWelcome to the SoundFlow audio recording tutorials! This guide will walk you through the essential steps to integrate audio recording capabilities into your .NET applications using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to record from the default device, perform custom real-time audio processing, or monitor microphone input, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Recording tutorials\">\r\n    <Tab key=\"basic-recording\" title=\"Basic Recording\">\r\n        This tutorial demonstrates how to record audio from the default recording device and save it to a WAV\r\n        file.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o BasicRecording\r\n                cd BasicRecording\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the basic recorder\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace BasicRecording;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default capture (recording) device.\r\n                        var defaultCaptureDevice = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultCaptureDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default capture device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define the audio format for recording. The backend will capture in this format.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 1 // Mono recording\r\n                        };\r\n                        \r\n                        // Initialize the capture device.\r\n                        using var device = audioEngine.InitializeCaptureDevice(defaultCaptureDevice, audioFormat);\r\n                        \r\n                        // Set up the output file stream.\r\n                        string outputFilePath = Path.Combine(Directory.GetCurrentDirectory(), \"output.wav\");\r\n                        using var fileStream = new FileStream(outputFilePath, FileMode.Create, FileAccess.Write, FileShare.None);\r\n                        \r\n                        // Create a recorder, linking it to the capture device and the output stream.\r\n                        using var recorder = new Recorder(device, fileStream, EncodingFormat.Wav);\r\n\r\n                        Console.WriteLine(\"Recording... Press any key to stop.\");\r\n                        device.Start(); // Start the device to begin capturing data.\r\n                        recorder.StartRecording(); // Start the recorder to begin encoding and writing.\r\n                        \r\n                        Console.ReadKey();\r\n                        \r\n                        recorder.StopRecording();\r\n                        device.Stop();\r\n\r\n                        Console.WriteLine($\"Recording stopped. Saved to {outputFilePath}\");\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        First, an `AudioEngine` is initialized. We find the `default capture device` from the engine's list of available recording devices. An `AudioFormat` is specified for the recording session (e.g., 48kHz, mono, 32-bit float).\r\n        The default capture device is then initialized with this format, creating an `AudioCaptureDevice`. This `device` instance is now the source of our audio data.\r\n        A `Recorder` is created, taking the `device` and an output `FileStream` as arguments. The recorder listens to the audio data processed by the device.\r\n        To begin, `device.Start()` is called to activate the hardware, and `recorder.StartRecording()` is called to start encoding the incoming audio from the device and writing it to the specified WAV file. After the user presses a key, both the recorder and the device are stopped, and all resources are automatically disposed by their `using` statements.\r\n    </Tab>\r\n\r\n    <Tab key=\"custom-processing\" title=\"Custom Processing\">\r\n        This tutorial demonstrates using a callback to process recorded audio in real-time.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o CustomProcessing\r\n                cd CustomProcessing\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement real-time processing\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n\r\n                namespace CustomProcessing;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultCaptureDevice = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultCaptureDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default capture device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 1\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializeCaptureDevice(defaultCaptureDevice, audioFormat);\r\n                        \r\n                        // Create a recorder that uses a callback instead of a file stream.\r\n                        using var recorder = new Recorder(device, ProcessAudio);\r\n\r\n                        Console.WriteLine(\"Recording with custom processing... Press any key to stop.\");\r\n                        device.Start();\r\n                        recorder.StartRecording();\r\n                        \r\n                        Console.ReadKey();\r\n                        \r\n                        recorder.StopRecording();\r\n                        device.Stop();\r\n                        Console.WriteLine(\"Recording stopped.\");\r\n                    }\r\n\r\n                    // This method will be called for each chunk of recorded audio.\r\n                    private static void ProcessAudio(Span<float> samples, Capability capability)\r\n                    {\r\n                        // Perform custom processing on the audio samples.\r\n                        // For example, calculate the average level:\r\n                        float sum = 0;\r\n                        for (int i = 0; i < samples.Length; i++)\r\n                        {\r\n                            sum += Math.Abs(samples[i]);\r\n                        }\r\n                        float averageLevel = sum / samples.Length;\r\n\r\n                        Console.WriteLine($\"Average level: {averageLevel:F4}\");\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This example demonstrates how to process audio data in real-time as it's being recorded. After the standard setup of initializing the `AudioEngine` and the default `AudioCaptureDevice`, a `Recorder` is created.\r\n        Instead of providing a file stream, this `Recorder` is given a callback method, `ProcessAudio`. The `Recorder` subscribes to the `device`'s `OnAudioProcessed` event. When the recorder is started, it will invoke our `ProcessAudio` method every time the device provides a new chunk of audio data.\r\n        The `ProcessAudio` method receives a `Span<float>` containing the latest audio samples, allowing for immediate analysis or processing, such as calculating the average level shown in this example. This approach is ideal for applications that need to react to live audio input without writing to a file, like voice activity detection, real-time visualizations, or triggering events based on sound.\r\n    </Tab>\r\n\r\n    <Tab key=\"mic-playback\" title=\"Mic Playback (Monitor)\">\r\n        This tutorial demonstrates capturing microphone audio and playing it back in real-time.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o MicrophonePlayback\r\n                cd MicrophonePlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement microphone loopback\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n\r\n                namespace MicrophonePlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback and capture devices.\r\n                        var defaultPlayback = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        var defaultCapture = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlayback.Id == IntPtr.Zero || defaultCapture.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"Default playback and/or capture device not found.\");\r\n                            return;\r\n                        }\r\n                        \r\n                        // Define a common audio format for both input and output.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2 // Stereo for playback\r\n                        };\r\n\r\n                        // Use FullDuplexDevice for simplified simultaneous input and output.\r\n                        using var fullDuplexDevice = audioEngine.InitializeFullDuplexDevice(defaultPlayback, defaultCapture, audioFormat);\r\n                        \r\n                        // Create a data provider that reads from the microphone.\r\n                        // It subscribes to the capture device's audio events.\r\n                        using var microphoneDataProvider = new MicrophoneDataProvider(fullDuplexDevice.CaptureDevice);\r\n                        \r\n                        // Create a SoundPlayer to play back the microphone data.\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, microphoneDataProvider);\r\n\r\n                        // Add the player to the playback device's master mixer.\r\n                        fullDuplexDevice.PlaybackDevice.MasterMixer.AddComponent(player);\r\n\r\n                        // Start capturing and playing.\r\n                        fullDuplexDevice.Start();\r\n                        microphoneDataProvider.StartCapture();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing live microphone audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop everything.\r\n                        fullDuplexDevice.Stop();\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This example creates a real-time microphone monitoring system. The key component is the `FullDuplexDevice`, a high-level abstraction that simplifies managing simultaneous input and output.\r\n        After initializing the `AudioEngine`, we find the default playback and capture devices and define a common `AudioFormat`. `audioEngine.InitializeFullDuplexDevice` is then called to create and manage a paired `AudioCaptureDevice` and `AudioPlaybackDevice`.\r\n        A `MicrophoneDataProvider` is created and linked to the capture part of the full duplex device (`fullDuplexDevice.CaptureDevice`). This provider listens for incoming audio data.\r\n        A `SoundPlayer` is instantiated with the `MicrophoneDataProvider` as its source. This player is then added to the `MasterMixer` of the playback part of the full duplex device (`fullDuplexDevice.PlaybackDevice.MasterMixer`).\r\n        Starting the `fullDuplexDevice` activates both the capture and playback hardware streams. `microphoneDataProvider.StartCapture()` begins queuing the incoming audio, and `player.Play()` starts reading from that queue and sending the audio to the output, creating a live monitoring effect. All resources are managed with `using` statements for automatic cleanup.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for recording audio with SoundFlow!\r\nNext, explore using audio effects with SoundFlow in our [Modifiers Tutorial](./tutorials-modifiers)."
  },
  {
    "id": 7,
    "slug": "tutorials-playback",
    "version": "1.2.0",
    "title": "Playback Fundamentals",
    "description": "Comprehensive tutorials for playing audio files, streams, and controlling playback with the SoundFlow C# audio library.",
    "navOrder": 7,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 7\r\ntitle: Playback Fundamentals\r\ndescription: Comprehensive tutorials for playing audio files, streams, and controlling playback with the SoundFlow C# audio library.\r\nnavOrder: 7\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Playback with SoundFlow\r\n\r\nWelcome to the SoundFlow audio playback tutorials! This guide will walk you through the essential steps to integrate audio playback into your .NET applications using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to play local files, stream from the web, control playback dynamics, implement looping, experiment with surround sound, or efficiently handle large audio assets, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Playback tutorials\">\r\n    <Tab key=\"basic-playback\" title=\"Basic Playback\">\r\n        This tutorial demonstrates how to play an audio file from disk using `SoundPlayer` and\r\n        `StreamDataProvider`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application:\r\n                ```bash\r\n                dotnet new console -o BasicPlayback\r\n                cd BasicPlayback\r\n                ```\r\n            </Step>\r\n            <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                ### 2. Install the SoundFlow NuGet package:\r\n                ```bash\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the basic player\" icon='ph:code-bold'>\r\n                ### 3. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace BasicPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine with the MiniAudio backend.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback device.\r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // The audio format for processing. We'll use 32-bit float, which is standard for processing.\r\n                        // The data provider will handle decoding the source file to this format.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        // Initialize the playback device. This manages the connection to the physical audio hardware.\r\n                        // The 'using' statement ensures it's properly disposed of.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a data provider for the audio file.\r\n                        // Replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n\r\n                        // Create a SoundPlayer, linking the engine, format, and data provider.\r\n                        // The player is also IDisposable.\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Add the player to the device's master mixer to route its audio for playback.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device. This opens the audio stream to the hardware.\r\n                        device.Start();\r\n\r\n                        // Start playback.\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until playback finishes or the user presses a key.\r\n                        Console.WriteLine(\"Playing audio... Press any key to exit.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop the device. This stops the audio stream.\r\n                        device.Stop();\r\n\r\n                        // The `using` statements for `audioEngine`, `device`, `dataProvider`, and `player`\r\n                        // will automatically handle disposal and resource cleanup.\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file on your\r\n                computer.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 4. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        First, a `MiniAudioEngine` is initialized to manage audio operations. We then identify the `default playback device` from the engine's available devices. An `AudioFormat` struct is defined, specifying the desired internal processing format (e.g., 32-bit float, 48kHz sample rate, 2 channels).\r\n        The chosen device is then initialized using `audioEngine.InitializePlaybackDevice`, creating an `AudioPlaybackDevice`. This device handles the interaction with the audio hardware and exposes a `MasterMixer` where `SoundFlow` components can be added.\r\n        A `StreamDataProvider` is created to load the audio file. Crucially, both `StreamDataProvider` and `SoundPlayer` are now instantiated with `audioEngine` and `audioFormat` to provide them with the necessary context for decoding and processing audio in the specified format. The `player` is added to the `device.MasterMixer`.\r\n        Finally, `device.Start()` is called to open the audio stream to the hardware, enabling sound output. The `player.Play()` then begins playback. The program waits for user input, after which the `using` statements automatically handle the proper disposal and cleanup of all allocated resources (`audioEngine`, `device`, `dataProvider`, and `player`).\r\n    </Tab>\r\n\r\n    <Tab key=\"web-playback\" title=\"Web Playback\">\r\n        This tutorial demonstrates how to play an audio stream from a URL using `SoundPlayer` and\r\n        `NetworkDataProvider`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application:\r\n                ```bash\r\n                dotnet new console -o WebPlayback\r\n                cd WebPlayback\r\n                ```\r\n            </Step>\r\n            <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                ### 2. Install the SoundFlow NuGet package:\r\n                ```bash\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the network player\" icon='ph:code-bold'>\r\n                ### 3. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n                using System.Threading.Tasks;\r\n\r\n                namespace WebPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static async Task Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback device.\r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define the audio format for processing.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        // Initialize the default playback device.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a NetworkDataProvider. Replace \"your-audio-stream-url\"\r\n                        // with the actual URL (direct audio file or HLS .m3u8 playlist).\r\n                        using var dataProvider = new NetworkDataProvider(audioEngine, audioFormat, \"your-audio-stream-url\");\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing stream... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop the device.\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"your-audio-stream-url\"` with the actual URL of an audio stream (e.g., direct\r\n                MP3/WAV or an HLS .m3u8 playlist).***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 4. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example follows the same foundational setup as the basic playback tutorial: initializing an `AudioEngine`, identifying a `default playback device`, defining an `AudioFormat`, and initializing the `AudioPlaybackDevice`.\r\n        The key distinction lies in using `NetworkDataProvider` instead of `StreamDataProvider`. `NetworkDataProvider` is designed to stream and decode audio directly from a web URL. Like other data providers and players in the new API, its constructor now requires both the `audioEngine` and `audioFormat`. It intelligently handles various web audio sources, including direct MP3/WAV files and HLS `.m3u8` playlists.\r\n        After the `SoundPlayer` is created and added to the `device.MasterMixer`, `device.Start()` and `player.Play()` initiate the audio stream. The `using` statements ensure all resources are properly managed and disposed upon program exit.\r\n    </Tab>\r\n\r\n    <Tab key=\"playback-control\" title=\"Playback Control\">\r\n        This tutorial demonstrates how to control audio playback using `Play`, `Pause`, `Stop`, `Seek`, and\r\n        `PlaybackSpeed`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o PlaybackControl\r\n                cd PlaybackControl\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the interactive player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace PlaybackControl;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider) { Volume = 0.8f }; // Example: set initial volume\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n                        Console.WriteLine(\"Playing audio... (p: pause/play, s: seek, +/-: speed, v/m: volume, any other: stop)\");\r\n\r\n                        // Handle user input for playback control.\r\n                        while (player.State != PlaybackState.Stopped)\r\n                        {\r\n                            var keyInfo = Console.ReadKey(true);\r\n                            switch (keyInfo.Key)\r\n                            {\r\n                                case ConsoleKey.P:\r\n                                    if (player.State == PlaybackState.Playing)\r\n                                        player.Pause();\r\n                                    else\r\n                                        player.Play();\r\n                                    Console.WriteLine(player.State == PlaybackState.Paused ? \"Paused\" : \"Playing\");\r\n                                    break;\r\n                                case ConsoleKey.S:\r\n                                    Console.Write(\"Enter seek time (in seconds, e.g., 10.5): \");\r\n                                    if (float.TryParse(Console.ReadLine(), out var seekTimeSeconds))\r\n                                    {\r\n                                        if (player.Seek(TimeSpan.FromSeconds(seekTimeSeconds)))\r\n                                            Console.WriteLine($\"Seeked to {seekTimeSeconds:F1}s. Current time: {player.Time:F1}s\");\r\n                                        else\r\n                                            Console.WriteLine(\"Seek failed.\");\r\n                                    }\r\n                                    else\r\n                                        Console.WriteLine(\"Invalid seek time.\");\r\n                                    break;\r\n                                case ConsoleKey.OemPlus:\r\n                                case ConsoleKey.Add:\r\n                                    player.PlaybackSpeed = Math.Min(2.0f, player.PlaybackSpeed + 0.1f);\r\n                                    Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                case ConsoleKey.OemMinus:\r\n                                case ConsoleKey.Subtract:\r\n                                    player.PlaybackSpeed = Math.Max(0.1f, player.PlaybackSpeed - 0.1f);\r\n                                    Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                case ConsoleKey.V:\r\n                                    player.Volume = Math.Min(1.5f, player.Volume + 0.1f); // Allow gain up to 150%\r\n                                    Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                case ConsoleKey.M:\r\n                                    player.Volume = Math.Max(0.0f, player.Volume - 0.1f);\r\n                                    Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                default:\r\n                                    player.Stop();\r\n                                    Console.WriteLine(\"Stopped\");\r\n                                    break;\r\n                            }\r\n                        }\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates interactive control over a `SoundPlayer` by extending the basic playback setup. After initializing the `AudioEngine`, selecting the `default playback device`, defining the `AudioFormat`, and initializing the `AudioPlaybackDevice`, a `StreamDataProvider` loads the audio file, and a `SoundPlayer` is instantiated (again, with `audioEngine` and `audioFormat`). The player is then added to the `device.MasterMixer`.\r\n        Once `device.Start()` and `player.Play()` are called, the application enters a loop to process user input from the console. The `SoundFlow` API provides intuitive properties and methods for common playback controls:\r\n        *   `P` toggles between `Play()` and `Pause()`.\r\n        *   `S` prompts for a time and calls `player.Seek(TimeSpan)`. The `Seek` method returns a boolean, indicating if the seek operation was successful (which depends on the underlying data provider's `CanSeek` capability).\r\n        *   `+` and `-` keys adjust the `player.PlaybackSpeed` property.\r\n        *   `V` and `M` keys control the `player.Volume` property, allowing for dynamic gain adjustment.\r\n        *   Any other key press calls `player.Stop()`, terminating playback and exiting the loop.\r\n        The `using` statements guarantee all resources are correctly released when the program concludes.\r\n    </Tab>\r\n\r\n    <Tab key=\"looping\" title=\"Looping\">\r\n        This tutorial demonstrates how to enable looping for a `SoundPlayer` and how to set custom loop points.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LoopingPlayback\r\n                cd LoopingPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the looping player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LoopingPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Enable looping.\r\n                        player.IsLooping = true;\r\n\r\n                        // Optional: Set custom loop points\r\n\r\n                        // Example 1: Loop from 2.5 seconds to 7.0 seconds (using float seconds)\r\n                        // player.SetLoopPoints(2.5f, 7.0f);\r\n\r\n                        // Example 2: Loop from sample 110250 to sample 308700 (using samples)\r\n                        // player.SetLoopPoints(110250, 308700); // Assuming 44.1kHz stereo, these are example values\r\n\r\n                        // Example 3: Loop from 1.5 seconds to the natural end of the audio (using TimeSpan, end point is optional)\r\n                        player.SetLoopPoints(TimeSpan.FromSeconds(1.5));\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio in a loop... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code extends the fundamental playback example to showcase audio looping capabilities. After the standard initialization of the `AudioEngine`, `AudioPlaybackDevice`, and `SoundPlayer` (including passing `audioEngine` and `audioFormat` to the data provider and player), the player's looping behavior is configured:\r\n        *   `player.IsLooping = true;`: This boolean property is the primary control to enable continuous looping. When set to `true`, upon reaching the end of the audio (or the defined loop end point), the player will automatically reset its position to the loop start point and continue playing.\r\n        *   `player.SetLoopPoints(...)`:: This method offers precise control over the section of audio that will loop. It provides multiple overloads, allowing you to specify the loop start and end points using `float` seconds, `int` samples, or `TimeSpan` values, catering to different precision requirements. If the `endTime` (or equivalent) parameter is omitted or set to a default indicating \"to end,\" the loop will extend to the natural conclusion of the audio data.\r\n        The player is added to the `device.MasterMixer`, and after starting the `device` and `player`, the application enters a wait state until the user presses a key, after which resources are automatically cleaned up.\r\n    </Tab>\r\n\r\n    <Tab key=\"surround-sound\" title=\"Surround Sound\">\r\n        This tutorial demonstrates how to use `SurroundPlayer` to play audio with surround sound configurations.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SurroundPlayback\r\n                cd SurroundPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the surround player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n                using System.Numerics;\r\n\r\n                namespace SurroundPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define an audio format with 8 channels for 7.1 surround sound.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 8\r\n                        };\r\n\r\n                        // Initialize the playback device with the 8-channel format.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a SurroundPlayer. It will upmix mono/stereo sources.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SurroundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Configure the SurroundPlayer for 7.1 surround sound.\r\n                        player.SpeakerConfig = SurroundPlayer.SpeakerConfiguration.Surround71;\r\n\r\n                        // Set the panning method (VBAP is often good for surround).\r\n                        player.Panning = SurroundPlayer.PanningMethod.Vbap;\r\n\r\n                        // Set the listener position (optional, (0,0) is center).\r\n                        player.ListenerPosition = new Vector2(0, 0);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing surround sound audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file (mono, stereo,\r\n                or multi-channel).***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example illustrates how to configure `SoundFlow` for surround sound playback.\r\n        The crucial first step, after initializing the `AudioEngine`, is to define an `AudioFormat` that specifies the desired number of output channels for your surround setup (e.g., 8 channels for 7.1 surround sound). The `AudioPlaybackDevice` is then initialized using this multi-channel `AudioFormat`, ensuring the audio hardware is configured to output to all relevant speakers.\r\n        Instead of `SoundPlayer`, a `SurroundPlayer` is instantiated, taking the `audioEngine`, `audioFormat`, and `dataProvider` as arguments. The `SurroundPlayer` is specialized for spatial audio. If the input `audiofile.wav` is mono or stereo, the `SurroundPlayer` will intelligently upmix and pan the audio across the configured speaker layout.\r\n        Key properties for configuring surround playback include:\r\n        *   `player.SpeakerConfig`: Set this to one of the predefined speaker layouts (e.g., `SurroundPlayer.SpeakerConfiguration.Surround71`).\r\n        *   `player.Panning`: Choose a panning algorithm, such as `SurroundPlayer.PanningMethod.Vbap` (Vector-Based Amplitude Panning), which often provides excellent spatialization for surround sound.\r\n        *   `player.ListenerPosition`: (Optional) Adjust the virtual listener's position within the soundfield using a `Vector2`.\r\n        The `SurroundPlayer` is added to the `device.MasterMixer`, and after `device.Start()` and `player.Play()`, the application will output sound through the configured surround channels. Resources are automatically managed by `using` statements.\r\n    </Tab>\r\n\r\n    <Tab key=\"chunked-data\" title=\"Chunked Data\">\r\n        This tutorial demonstrates how to use the `ChunkedDataProvider` for efficient playback of large audio\r\n        files.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ChunkedPlayback\r\n                cd ChunkedPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the chunked data player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ChunkedPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a ChunkedDataProvider and load a large audio file.\r\n                        // Replace \"path/to/your/large/audiofile.wav\" with the actual path.\r\n                        using var dataProvider = new ChunkedDataProvider(audioEngine, audioFormat, \"path/to/your/large/audiofile.wav\");\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio with ChunkedDataProvider... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/large/audiofile.wav\"` with the path to a large audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This tutorial showcases the `ChunkedDataProvider`, a specialized data provider designed for efficient playback of very large audio files. Unlike `StreamDataProvider` or `AssetDataProvider` which might load an entire file into memory or process it sequentially without optimized chunking, `ChunkedDataProvider` reads and decodes audio data in smaller, manageable chunks. This significantly reduces memory footprint and improves responsiveness, especially for long recordings or high-resolution audio.\r\n        The setup follows the familiar pattern: initialize `AudioEngine`, select `default playback device`, define `AudioFormat`, and initialize `AudioPlaybackDevice`. The `ChunkedDataProvider` is then instantiated, requiring `audioEngine`, `audioFormat`, and the path to the large audio file. A `SoundPlayer` is created with this provider, added to the `device.MasterMixer`, and playback begins.\r\n        The integration is seamless; simply swap `StreamDataProvider` (or `AssetDataProvider`) with `ChunkedDataProvider` in your setup. The underlying chunking mechanism works transparently, ensuring smooth playback while optimizing resource usage. All resources are released automatically via `using` statements.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for playing audio with SoundFlow!\r\nNext, explore recording audio with SoundFlow in our [Recording Audio Tutorial](./tutorials-recording)."
  },
  {
    "id": 9,
    "slug": "tutorials-modifiers",
    "version": "1.2.0",
    "title": "Audio Modifiers & Effects",
    "description": "Comprehensive tutorials for applying various audio effects and modifiers (reverb, EQ, compression, etc.) with SoundFlow.",
    "navOrder": 9,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 9\r\ntitle: Audio Modifiers & Effects\r\ndescription: Comprehensive tutorials for applying various audio effects and modifiers (reverb, EQ, compression, etc.) with SoundFlow.\r\nnavOrder: 9\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Applying Audio Modifiers & Effects with SoundFlow\r\n\r\nWelcome to the SoundFlow audio modifiers and effects tutorials! This guide will walk you through applying various digital audio effects to your playback streams using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to add reverb, equalize frequencies, compress dynamics, or mix multiple sources, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Modifier tutorials\">\r\n    <Tab key=\"reverb\" title=\"Reverb\">\r\n        Demonstrates how to apply a reverb effect using the `AlgorithmicReverbModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ReverbEffect\r\n                cd ReverbEffect\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with reverb\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ReverbEffect;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create an AlgorithmicReverbModifier. It now needs the audio format.\r\n                        var reverb = new AlgorithmicReverbModifier(audioFormat)\r\n                        {\r\n                            RoomSize = 0.8f,\r\n                            Damp = 0.5f,\r\n                            Wet = 0.3f, // Wet mix (0=dry, 1=fully wet)\r\n                            Width = 1f,\r\n                            PreDelay = 20f // Pre-delay in milliseconds\r\n                        };\r\n\r\n                        // Add the reverb modifier to the player.\r\n                        player.AddModifier(reverb);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with reverb... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        After initializing the `AudioEngine` and the `AudioPlaybackDevice`, an `AlgorithmicReverbModifier` is created. In the new API, the modifier's constructor requires an `AudioFormat` instance to correctly configure its internal delay lines and filters based on the sample rate and channel count. The `Wet` property now controls the blend between the original (dry) and processed (wet) signals, where 0 is fully dry and 1 is fully wet. We add this `reverb` modifier to the `player`, which is then added to the `device.MasterMixer` for playback. Experiment with `RoomSize`, `Damp`, `Wet`, `Width`, and `PreDelay` to shape the reverb's character.\r\n    </Tab>\r\n\r\n    <Tab key=\"equalization\" title=\"Equalization\">\r\n        Demonstrates how to use the `ParametricEqualizer` to adjust frequency balance.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Equalization\r\n                cd Equalization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with EQ\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Collections.Generic;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Equalization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a ParametricEqualizer, which now requires the audio format.\r\n                        var equalizer = new ParametricEqualizer(audioFormat);\r\n\r\n                        // Add some equalizer bands.\r\n                        var bands = new List<EqualizerBand>\r\n                        {\r\n                            // Boost low frequencies (bass)\r\n                            new(FilterType.LowShelf, 100, 6, 0.7f),\r\n                            // Cut mid frequencies\r\n                            new(FilterType.Peaking, 1000, -4, 2f),\r\n                            // Boost high frequencies (treble)\r\n                            new(FilterType.HighShelf, 10000, 5, 0.7f)\r\n                        };\r\n                        equalizer.AddBands(bands);\r\n\r\n                        // Add the equalizer to the player.\r\n                        player.AddModifier(equalizer);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with equalization... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust bands\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        Following the new API structure, this code sets up the `AudioEngine` and an `AudioPlaybackDevice`. A `ParametricEqualizer` is instantiated, and like the reverb modifier, its constructor now requires an `AudioFormat` to correctly calculate filter coefficients based on the sample rate.\r\n        We define a `List<EqualizerBand>` to configure our EQ settings: a low-shelf filter to boost bass, a peaking filter to cut a specific mid-range frequency, and a high-shelf filter to boost treble. The `equalizer.AddBands()` method applies these settings. The `equalizer` is then added as a modifier to the `player`, which is subsequently added to the `device.MasterMixer`. You will hear the audio with the equalization applied. Experiment with different `FilterType` enums, frequencies, gain values, and Q factors to shape the sound to your liking.\r\n    </Tab>\r\n\r\n    <Tab key=\"chorus-delay\" title=\"Chorus & Delay\">\r\n        Demonstrates how to apply chorus and delay effects using `ChorusModifier` and `DelayModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ChorusDelay\r\n                cd ChorusDelay\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with effects\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ChorusDelay;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a ChorusModifier.\r\n                        var chorus = new ChorusModifier(\r\n                            audioFormat,\r\n                            depthMs: 2f,           // Depth (in milliseconds)\r\n                            rateHz: 0.8f,          // LFO Rate (in Hz)\r\n                            feedback: 0.5f,      // Feedback amount\r\n                            wetDryMix: 0.5f      // Wet/dry mix (0 = dry, 1 = wet)\r\n                        );\r\n\r\n                        // Create a DelayModifier.\r\n                        var delaySamples = (int)(audioFormat.SampleRate * 0.5); // 500ms delay\r\n                        var delay = new DelayModifier(\r\n                            audioFormat,\r\n                            delaySamples: delaySamples, \r\n                            feedback: 0.6f,      \r\n                            wetMix: 0.4f,       \r\n                            cutoff: 4000f \r\n                        );\r\n\r\n                        // Add the chorus and delay modifiers to the player.\r\n                        player.AddModifier(chorus);\r\n                        player.AddModifier(delay);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with chorus and delay... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust effects\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates chaining multiple effects. After the standard `AudioEngine` and `AudioPlaybackDevice` setup, we instantiate `ChorusModifier` and `DelayModifier`. Both modifiers now require an `AudioFormat` object in their constructors to configure internal buffers and timing calculations correctly based on the sample rate. The delay length for the `DelayModifier` is now specified in samples instead of milliseconds.\r\n        Both `chorus` and `delay` modifiers are added to the `player` instance. They will be processed in the order they were added: the audio will first pass through the chorus effect, and the output of the chorus will then be fed into the delay effect. This chain is then added to the `device.MasterMixer` for playback.\r\n    </Tab>\r\n\r\n    <Tab key=\"compression\" title=\"Compression\">\r\n        Demonstrates how to use the `CompressorModifier` to reduce the dynamic range of an audio stream.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Compression\r\n                cd Compression\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with compressor\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Compression;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a CompressorModifier.\r\n                        var compressor = new CompressorModifier(\r\n                            audioFormat,\r\n                            thresholdDb: -20f, // Threshold (in dB)\r\n                            ratio: 4f,        // Compression ratio\r\n                            attackMs: 10f,      // Attack time (in milliseconds)\r\n                            releaseMs: 100f,    // Release time (in milliseconds)\r\n                            kneeDb: 5f,         // Knee width (in dB)\r\n                            makeupGainDb: 6f   // Makeup gain (in dB)\r\n                        );\r\n\r\n                        // Add the compressor to the player.\r\n                        player.AddModifier(compressor);\r\n                        \r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n                        \r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with compression... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code showcases dynamic range compression. After initializing the `AudioEngine` and an `AudioPlaybackDevice`, a `CompressorModifier` is created. Its constructor now requires an `AudioFormat` to accurately time its attack and release stages based on the sample rate. The compressor is configured with a -20dB threshold, a 4:1 ratio, a soft knee of 5dB, and 6dB of makeup gain to compensate for the volume reduction. This modifier is then added to the `player`, which is routed to the `device.MasterMixer`. When played, the audio's dynamic range will be reduced, making quiet parts louder and loud parts quieter, resulting in a more consistent overall volume level.\r\n    </Tab>\r\n\r\n    <Tab key=\"mixing\" title=\"Mixing\">\r\n        Demonstrates how to use the `Mixer` to combine multiple audio sources.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Mixing\r\n                cd Mixing\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Combine multiple audio sources\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Mixing;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create two SoundPlayer instances and load different audio files.\r\n                        using var dataProvider1 = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile1.wav\"));\r\n                        using var player1 = new SoundPlayer(audioEngine, audioFormat, dataProvider1);\r\n\r\n                        using var dataProvider2 = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile2.wav\"));\r\n                        using var player2 = new SoundPlayer(audioEngine, audioFormat, dataProvider2);\r\n\r\n                        // Create an Oscillator that generates a sine wave.\r\n                        using var oscillator = new Oscillator(audioEngine, audioFormat)\r\n                        {\r\n                            Frequency = 440, // 440 Hz (A4 note)\r\n                            Amplitude = 0.5f,\r\n                            Type = Oscillator.WaveformType.Sine\r\n                        };\r\n\r\n                        // Add the players and the oscillator to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player1);\r\n                        device.MasterMixer.AddComponent(player2);\r\n                        device.MasterMixer.AddComponent(oscillator);\r\n\r\n                        // Start playback for both players.\r\n                        device.Start();\r\n                        player1.Play();\r\n                        player2.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing mixed audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile1.wav\"` and `\"path/to/your/audiofile2.wav\"` with the actual\r\n                paths to two different audio files.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust levels\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the core mixing capability of SoundFlow. After initializing the `AudioEngine` and an `AudioPlaybackDevice`, we create multiple `SoundComponent` instances: two `SoundPlayer`s for audio files and one `Oscillator` for a synthesized sine wave. Note that the `Oscillator` now also requires the `audioEngine` and `audioFormat` in its constructor.\r\n        Instead of a static master mixer, all components are added directly to the `device.MasterMixer`. The mixer automatically sums the audio output from all its added components. When the device is started, you will hear both audio files and the sine wave playing simultaneously, mixed together. You can control the individual contribution of each component to the mix by adjusting its `Volume` and `Pan` properties.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for applying audio effects with SoundFlow!\r\nNext, explore using audio analyzers with SoundFlow in our [Analysis Tutorial](./tutorials-analysis)."
  },
  {
    "id": 10,
    "slug": "tutorials-analysis",
    "version": "1.2.0",
    "title": "Audio Analysis & Visualization",
    "description": "Comprehensive tutorials for analyzing audio streams (level, spectrum, VAD) and visualizing data with SoundFlow.",
    "navOrder": 10,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 10\r\ntitle: Audio Analysis & Visualization\r\ndescription: Comprehensive tutorials for analyzing audio streams (level, spectrum, VAD) and visualizing data with SoundFlow.\r\nnavOrder: 10\r\ncategory: Tutorials and Examples\r\n---\r\nimport { Icon } from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Analysis & Visualization with SoundFlow\r\n\r\nWelcome to the SoundFlow audio analysis and visualization tutorials! This guide will walk you through extracting valuable data from audio streams, such as level information, frequency spectrum, and voice activity, as well as visualizing this data, using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're building a custom meter, a real-time spectrum display, or a smart voice assistant, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Audio Analysis and Visualization tutorials\">\r\n    <Tab key=\"level-metering\" title=\"Level Metering\">\r\n        This tutorial demonstrates how to use the `LevelMeterAnalyzer` to measure the RMS and peak levels of an\r\n        audio stream.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LevelMetering\r\n                cd LevelMetering\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LevelMetering;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a LevelMeterAnalyzer, passing the audio format.\r\n                        var levelMeter = new LevelMeterAnalyzer(audioFormat);\r\n\r\n                        // Attach the analyzer to the player.\r\n                        player.AddAnalyzer(levelMeter);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Create a timer to periodically display the RMS and peak levels.\r\n                        var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            Console.WriteLine($\"RMS Level: {levelMeter.Rms:F4}, Peak Level: {levelMeter.Peak:F4}\");\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop playback and clean up.\r\n                        timer.Stop();\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        After setting up the `AudioEngine` and initializing an `AudioPlaybackDevice`, a `SoundPlayer` is created. A `LevelMeterAnalyzer` is then instantiated, requiring an `AudioFormat` in its constructor. The analyzer is attached directly to the `player` using `player.AddAnalyzer(levelMeter)`.\r\n        When the `player` processes audio, it automatically passes its audio data to the attached `levelMeter`. A timer then periodically reads the `Rms` and `Peak` properties from the analyzer and displays them in the console, providing a real-time level readout.\r\n    </Tab>\r\n\r\n    <Tab key=\"spectrum-analysis\" title=\"Spectrum Analysis\">\r\n        This tutorial demonstrates how to use the `SpectrumAnalyzer` to analyze frequency content using FFT.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SpectrumAnalysis\r\n                cd SpectrumAnalysis\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace SpectrumAnalysis;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                        var spectrumAnalyzer = new SpectrumAnalyzer(audioFormat, fftSize: 2048);\r\n\r\n                        // Attach the spectrum analyzer to the player.\r\n                        player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Create a timer to periodically display the spectrum data.\r\n                        var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            // Get the spectrum data from the analyzer.\r\n                            var spectrumData = spectrumAnalyzer.SpectrumData;\r\n\r\n                            // Print the magnitude of the first few frequency bins.\r\n                            if (spectrumData.Length > 0)\r\n                            {\r\n                                Console.Write(\"Spectrum: \");\r\n                                for (int i = 0; i < Math.Min(10, spectrumData.Length); i++)\r\n                                {\r\n                                    Console.Write($\"{spectrumData[i]:F2} \");\r\n                                }\r\n                                Console.WriteLine();\r\n                            }\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying spectrum data... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop playback and clean up.\r\n                        timer.Stop();\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code follows a similar pattern to level metering. A `SpectrumAnalyzer` is created, requiring the `audioFormat` and an `fftSize` (which must be a power of two). It is then attached to the `player` using `AddAnalyzer`. As the `player` processes audio, it feeds the data to the `spectrumAnalyzer`, which performs a Fast Fourier Transform (FFT).\r\n        A timer periodically accesses the `spectrumAnalyzer.SpectrumData` property, which contains the magnitudes of the frequency bins, and prints the first few values to the console for a simple real-time display of the frequency content.\r\n    </Tab>\r\n\r\n    <Tab key=\"vad\" title=\"Voice Activity Detection\">\r\n        This tutorial demonstrates how to use the `VoiceActivityDetector` to detect the presence of human voice.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o VoiceActivityDetection\r\n                cd VoiceActivityDetection\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement VAD on a source\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace VoiceActivityDetection;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file with speech.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/speechfile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a VoiceActivityDetector.\r\n                        var vad = new VoiceActivityDetector(audioFormat);\r\n\r\n                        // Attach the VAD as an analyzer to the player's output.\r\n                        player.AddAnalyzer(vad);\r\n\r\n                        // Subscribe to the SpeechDetected event.\r\n                        vad.SpeechDetected += isDetected => Console.WriteLine($\"Speech detected: {isDetected}\");\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Analyzing audio for voice activity... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/speechfile.wav\"` with the path to an audio file containing speech.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates how to detect voice in an audio stream. After initializing the `AudioEngine`, `AudioPlaybackDevice`, and `SoundPlayer`, a `VoiceActivityDetector` is created, passing the `audioFormat` to its constructor. It is then attached to the `player` with `AddAnalyzer`.\r\n        The key part of this example is subscribing to the `vad.SpeechDetected` event. This event fires whenever the VAD's state changes (from silence to speech, or vice versa), providing a boolean value. The event handler simply prints the new state to the console. This event-driven approach is efficient for building applications that need to react to the presence or absence of speech.\r\n    </Tab>\r\n\r\n    <Tab key=\"level-meter-viz\" title=\"Console Level Meter\">\r\n        Demonstrates creating a console-based level meter using the `LevelMeterAnalyzer` and\r\n        `LevelMeterVisualizer`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LevelMeterVisualization\r\n                cd LevelMeterVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Interfaces;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LevelMeterVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard engine and device setup.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a player for an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the LevelMeterAnalyzer and LevelMeterVisualizer.\r\n                        // The visualizer is linked to the analyzer.\r\n                        var levelMeterAnalyzer = new LevelMeterAnalyzer(audioFormat);\r\n                        var levelMeterVisualizer = new LevelMeterVisualizer(levelMeterAnalyzer);\r\n                        \r\n                        // Attach the analyzer to the player. The analyzer will automatically\r\n                        // pass its data to the linked visualizer.\r\n                        player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                        // Add the player to the mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        levelMeterVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawLevelMeter(levelMeterAnalyzer.Rms, levelMeterAnalyzer.Peak);\r\n                        };\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            levelMeterVisualizer.ProcessOnAudioData(System.Array.Empty<float>());\r\n                            levelMeterVisualizer.Render(new ConsoleVisualizationContext());\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        levelMeterVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based level meter.\r\n                    private static void DrawLevelMeter(float rms, float peak)\r\n                    {\r\n                        int barLength = (int)(rms * 40); \r\n                        int peakMarkerPos = (int)(peak * 40);\r\n\r\n                        Console.SetCursorPosition(0, 0);\r\n                        Console.Write(\"RMS:  [\");\r\n                        Console.Write(new string('#', barLength));\r\n                        Console.Write(new string(' ', 40 - barLength));\r\n                        Console.Write(\"]\\n\");\r\n\r\n                        Console.SetCursorPosition(0, 1);\r\n                        Console.Write(\"Peak: [\");\r\n                        Console.Write(new string(' ', 40));\r\n                        Console.Write(\"]\\r\"); // Carriage return to move back\r\n                        Console.Write(\"Peak: [\");\r\n                        if(peakMarkerPos < 40) Console.SetCursorPosition(7 + peakMarkerPos, 1);\r\n                        Console.Write(\"|\");\r\n                        \r\n                        Console.SetCursorPosition(0, 3);\r\n                    }\r\n                }\r\n\r\n                // Simple IVisualizationContext implementation for console output.\r\n                public class ConsoleVisualizationContext : IVisualizationContext\r\n                {\r\n                    public void Clear() { }\r\n                    public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1) { }\r\n                    public void DrawRectangle(float x, float y, float width, float height, Color color) { }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example demonstrates the analyzer-visualizer pattern. A `LevelMeterAnalyzer` is created to process the audio, and a `LevelMeterVisualizer` is created, linked to the analyzer. When `player.AddAnalyzer(levelMeterAnalyzer)` is called, the analyzer is attached to the player's audio stream. Inside its processing logic, the analyzer automatically calls its linked visualizer's `ProcessOnAudioData` method.\r\n        We subscribe to the `levelMeterVisualizer.VisualizationUpdated` event, which is fired by the visualizer whenever it receives new data. The event handler calls our `DrawLevelMeter` helper function to render a simple text-based meter in the console, providing a direct visual representation of the audio levels calculated by the analyzer.\r\n    </Tab>\r\n\r\n    <Tab key=\"waveform-viz\" title=\"Console Waveform\">\r\n        Demonstrates using `WaveformVisualizer` to display an audio waveform.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o WaveformVisualization\r\n                cd WaveformVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement waveform visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.Collections.Generic;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace WaveformVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard setup\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a LevelMeterAnalyzer or any analyzer you want.\r\n                        var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                        // Create a WaveformVisualizer.\r\n                        var waveformVisualizer = new WaveformVisualizer();\r\n\r\n                        // Add the player to the master mixer.\r\n                        Mixer.Master.AddComponent(player);\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        waveformVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawWaveform(waveformVisualizer.Waveform);\r\n                        };\r\n\r\n                        // Connect the player's output to the level meter analyzer's input.\r\n                        player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                        // Add the player to the master mixer.\r\n                        Mixer.Master.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        player.Play();\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            waveformVisualizer.Render(new ConsoleVisualizationContext()); // ConsoleVisualizationContext is just a placeholder\r\n                        };\r\n                        timer.Start();\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio and displaying waveform... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        waveformVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based waveform.\r\n                    private static void DrawWaveform(IReadOnlyList<float> waveform)\r\n                    {\r\n                        Console.Clear();\r\n                        int consoleWidth = Console.WindowWidth;\r\n                        int consoleHeight = Console.WindowHeight;\r\n\r\n                        if (waveform.Count == 0) return;\r\n\r\n                        for (int i = 0; i < consoleWidth; i++)\r\n                        {\r\n                            int waveformIndex = (int)(i * (waveform.Count / (float)consoleWidth));\r\n                            waveformIndex = Math.Clamp(waveformIndex, 0, waveform.Count - 1);\r\n\r\n                            float sampleValue = waveform[waveformIndex];\r\n                            int consoleY = (int)((sampleValue + 1) * 0.5 * (consoleHeight - 1));\r\n                            consoleY = Math.Clamp(consoleY, 0, consoleHeight - 1);\r\n\r\n                            if (i < consoleWidth && (consoleHeight - consoleY - 1) < consoleHeight)\r\n                            {\r\n                                Console.SetCursorPosition(i, consoleHeight - consoleY - 1);\r\n                                Console.Write(\"*\");\r\n                            }\r\n                        }\r\n                        Console.SetCursorPosition(0, consoleHeight - 1);\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        In this example, the `WaveformVisualizer` is used to directly visualize the audio samples. While it implements `IVisualizer`, it doesn't need a separate `AudioAnalyzer`. It is attached directly to the `player` using `AddAnalyzer`. When the player processes its audio buffer, it passes a copy of that buffer to the `WaveformVisualizer`, which stores it.\r\n        We subscribe to the `VisualizationUpdated` event, which the visualizer raises after receiving a new buffer. The event handler calls `DrawWaveform` to render a simple ASCII representation of the waveform to the console, providing a real-time oscilloscope-like view of the audio signal.\r\n    </Tab>\r\n\r\n    <Tab key=\"spectrum-viz\" title=\"Console Spectrum\">\r\n        Demonstrates creating a console-based spectrum analyzer using the `SpectrumAnalyzer` and\r\n        `SpectrumVisualizer`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SpectrumAnalyzerVisualization\r\n                cd SpectrumAnalyzerVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement spectrum visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Interfaces;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace SpectrumAnalyzerVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard setup.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the SpectrumAnalyzer and SpectrumVisualizer.\r\n                        var spectrumAnalyzer = new SpectrumAnalyzer(audioFormat, fftSize: 2048);\r\n                        var spectrumVisualizer = new SpectrumVisualizer(spectrumAnalyzer);\r\n\r\n                        // Attach the analyzer to the player.\r\n                        player.AddAnalyzer(spectrumAnalyzer);\r\n                        \r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        spectrumVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawSpectrum(spectrumAnalyzer.SpectrumData);\r\n                        };\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            spectrumVisualizer.ProcessOnAudioData(Array.Empty<float>());\r\n                            spectrumVisualizer.Render(new ConsoleVisualizationContext());\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying spectrum analyzer... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        spectrumVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based spectrum analyzer.\r\n                    private static void DrawSpectrum(ReadOnlySpan<float> spectrumData)\r\n                    {\r\n                        Console.Clear();\r\n                        int consoleWidth = Console.WindowWidth;\r\n                        int consoleHeight = Console.WindowHeight -1;\r\n\r\n                        if (spectrumData.IsEmpty) return;\r\n\r\n                        for (int i = 0; i < consoleWidth; i++)\r\n                        {\r\n                            // Logarithmic mapping of frequency bins to console columns for better visualization\r\n                            double logIndex = Math.Log10(1 + 9 * ((double)i / consoleWidth));\r\n                            int spectrumIndex = (int)(logIndex * (spectrumData.Length - 1));\r\n                            \r\n                            float magnitude = spectrumData[spectrumIndex];\r\n                            int barHeight = (int)(magnitude * consoleHeight);\r\n                            barHeight = Math.Clamp(barHeight, 0, consoleHeight);\r\n\r\n                            for (int j = 0; j < barHeight; j++)\r\n                            {\r\n                                Console.SetCursorPosition(i, consoleHeight - 1 - j);\r\n                                Console.Write(\"█\");\r\n                            }\r\n                        }\r\n                        Console.SetCursorPosition(0, consoleHeight - 1);\r\n                    }\r\n                }\r\n\r\n                // Simple IVisualizationContext implementation for console output.\r\n                public class ConsoleVisualizationContext : IVisualizationContext\r\n                {\r\n                    public void Clear() { }\r\n                    public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f) { }\r\n                    public void DrawRectangle(float x, float y, float width, float height, Color color) { }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the analyzer-visualizer pattern for frequency analysis. A `SpectrumAnalyzer` is created to perform the FFT, and a `SpectrumVisualizer` is linked to it. The analyzer is attached to the `player`, which feeds it audio data. The analyzer processes this data and automatically informs the visualizer.\r\n        We subscribe to the `spectrumVisualizer.VisualizationUpdated` event. In the handler, `DrawSpectrum` is called, which reads the `spectrumAnalyzer.SpectrumData` and renders a simple bar chart of the frequency magnitudes to the console. The drawing logic uses a logarithmic scale for the frequency axis, which provides a more musically intuitive display of the spectrum.\r\n    </Tab>\r\n\t\r\n\t<Tab key=\"ui-integration\" title={<div className=\"flex items-center gap-2\">\r\n\t\t<Icon icon=\"lucide:layout-template\"/><span>UI Integration</span></div>}>\r\n        These examples use basic console output for simplicity. To integrate SoundFlow's visualizers with a GUI\r\n        framework (like WPF, WinForms, Avalonia, or MAUI), you'll need to:\r\n        <Steps layout='vertical'>\r\n            <Step title=\"Implement IVisualizationContext\" description=\"Wrap your UI framework's drawing primitives\"\r\n                  icon='material-symbols:draw-outline'>\r\n                This class will wrap the drawing primitives of your chosen UI framework. For example, in WPF, you might\r\n                use `DrawingContext` methods to draw shapes on a `Canvas`.\r\n            </Step>\r\n            <Step title=\"Update UI from Event\" description=\"Trigger a redraw on the UI thread\" icon='mdi:update'>\r\n                In the `VisualizationUpdated` event handler, trigger a redraw of your UI element that hosts the\r\n                visualization. Make sure to marshal the update to the UI thread using `Dispatcher.Invoke` or a similar\r\n                mechanism if the event is raised from a different thread.\r\n            </Step>\r\n            <Step title=\"Call Render Method\" description=\"Pass your context to the visualizer\" icon='lucide:render'>\r\n                In your UI's rendering logic, call the `Render` method of the visualizer, passing your\r\n                `IVisualizationContext` implementation.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Example (Conceptual WPF):**\r\n\r\n        ```csharp\r\n        // In your XAML:\r\n        // <Canvas x:Name=\"VisualizationCanvas\"/>\r\n\r\n        // In your code-behind:\r\n        public partial class MainWindow : Window\r\n        {\r\n            private readonly WaveformVisualizer _visualizer;\r\n\r\n            public MainWindow()\r\n            {\r\n                InitializeComponent();\r\n\r\n                // ... Initialize AudioEngine, SoundPlayer, etc. ...\r\n\r\n                _visualizer = new WaveformVisualizer();\r\n                _visualizer.VisualizationUpdated += OnVisualizationUpdated;\r\n\r\n                // ...\r\n            }\r\n\r\n            private void OnVisualizationUpdated(object? sender, EventArgs e)\r\n            {\r\n                // Marshal the update to the UI thread\r\n                Dispatcher.Invoke(() =>\r\n                {\r\n                    VisualizationCanvas.Children.Clear(); // Clear previous drawing\r\n\r\n                    // Create a custom IVisualizationContext that wraps the Canvas\r\n                    var context = new WpfVisualizationContext(VisualizationCanvas);\r\n\r\n                    // Render the visualization\r\n                    _visualizer.Render(context);\r\n                });\r\n            }\r\n\r\n            // ...\r\n        }\r\n\r\n        // IVisualizationContext implementation for WPF\r\n        public class WpfVisualizationContext : IVisualizationContext\r\n        {\r\n            private readonly Canvas _canvas;\r\n\r\n            public WpfVisualizationContext(Canvas canvas)\r\n            {\r\n                _canvas = canvas;\r\n            }\r\n\r\n            public void Clear()\r\n            {\r\n                _canvas.Children.Clear();\r\n            }\r\n\r\n            public void DrawLine(float x1, float y1, float x2, float y2, SoundFlow.Interfaces.Color color, float thickness = 1f)\r\n            {\r\n                var line = new Line\r\n                {\r\n                    X1 = x1,\r\n                    Y1 = y1,\r\n                    X2 = x2,\r\n                    Y2 = y2,\r\n                    Stroke = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255))),\r\n                    StrokeThickness = thickness\r\n                };\r\n                _canvas.Children.Add(line);\r\n            }\r\n\r\n            public void DrawRectangle(float x, float y, float width, float height, SoundFlow.Interfaces.Color color)\r\n            {\r\n                var rect = new Rectangle\r\n                {\r\n                    Width = width,\r\n                    Height = height,\r\n                    Fill = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255)))\r\n                };\r\n                Canvas.SetLeft(rect, x);\r\n                Canvas.SetTop(rect, y);\r\n                _canvas.Children.Add(rect);\r\n            }\r\n        }\r\n        ```\r\n\r\n        Remember to adapt this conceptual example to your specific UI framework and project structure.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for audio analysis and visualization with SoundFlow!"
  },
  {
    "id": 1,
    "slug": "getting-started",
    "version": "1.2.0",
    "title": "Getting Started with SoundFlow",
    "description": "Learn how to install the library, set up your development environment, and write your first SoundFlow application.",
    "navOrder": 1,
    "category": "Core",
    "content": "---\nid: 1\ntitle: Getting Started with SoundFlow\ndescription: Learn how to install the library, set up your development environment, and write your first SoundFlow application.\nnavOrder: 1\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\n# Getting Started with SoundFlow\n\nThis guide will help you get up and running with SoundFlow quickly. You'll learn how to install the library, set up your development environment, and write your first SoundFlow application.\n\n## Prerequisites\n\nBefore you begin, make sure you have the following installed:\n\n*   **[.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0) or later:** SoundFlow is built on .NET 8.0, so you'll need the corresponding SDK to build and run SoundFlow projects.\n*   **An IDE or code editor:** You can use any IDE or code editor that supports .NET development. Popular choices include:\n    *   [Visual Studio](https://visualstudio.microsoft.com/) (Recommended for Windows)\n    *   [Visual Studio Code](https://code.visualstudio.com/) (Cross-platform)\n    *   [JetBrains Rider](https://www.jetbrains.com/rider/) (Cross-platform)\n*   **Basic knowledge of C# and .NET:** Familiarity with C# programming and .NET concepts will be helpful.\n\n**Supported Operating Systems:**\n\n*   Windows\n*   macOS\n*   Linux\n*   Android\n*   iOS\n*   FreeBSD\n\n## Installation\n\nYou can install SoundFlow in several ways:\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\n    <Tab\n        key=\"nuget\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:nuget' />\n                <span>NuGet Package Manager</span>\n            </div>\n        }\n    >\n        ### Option 1: Using the NuGet Package Manager (Recommended)\n\n        This is the easiest and recommended way to add SoundFlow to your .NET projects.\n\n        1. **Open the NuGet Package Manager Console:** In Visual Studio, go to `Tools` > `NuGet Package Manager` > `Package Manager Console`.\n        2. **Run the installation command:**\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        This command will download and install the latest version of SoundFlow and its dependencies into your current project.\n    </Tab>\n\n    <Tab\n        key=\"cli\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:dotnet' />\n                <span>.NET CLI</span>\n            </div>\n        }\n    >\n        ### Option 2: Using the .NET CLI\n\n        1. **Open your terminal or command prompt.**\n        2. **Navigate to your project directory.**\n        3. **Run the following command:**\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n\n        This command will add a reference to the SoundFlow package in your project file (`.csproj`).\n    </Tab>\n\n    <Tab\n        key=\"source\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:git' />\n                <span>Building from Source</span>\n            </div>\n        }\n    >\n        ### Option 3: Building from Source\n\n        If you want to use the latest development version of SoundFlow or contribute to the project, you can build it from source:\n\n        1. **Clone the SoundFlow repository:**\n\n        ```bash\n        git clone https://github.com/LSXPrime/SoundFlow.git\n        ```\n\n        2. **Navigate to the cloned directory:**\n\n        ```bash\n        cd SoundFlow\n        ```\n\n        3. **Build the project using the .NET CLI:**\n\n        ```bash\n        dotnet build\n        ```\n    </Tab>\n</Tabs>\n\n## Basic Usage Example\n\nLet's create a simple console application that plays an audio file using SoundFlow.\n\n<Steps layout='horizontal' nextLabel='Got it, Next' finishLabel='All Done!' resetLabel='Start Again' summaryMessage=\"Congratulations! You've built and run your first audio application with SoundFlow. Feel free to experiment with the code or start over.\">\n    <Step title=\"Create Project\" description=\"Use VS or .NET CLI\" icon='ic:outline-create-new-folder'>\n        ### 1. Create a new console application:\n        *   In Visual Studio, go to `File` > `New` > `Project`. Select `Console App` and give it a name (e.g., `SoundFlowExample`).\n        *   Or, use the .NET CLI:\n\n        ```bash\n        dotnet new console -o SoundFlowExample\n        cd SoundFlowExample\n        ```\n    </Step>\n\n    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\n        ### 2. Install the SoundFlow NuGet package: (If you haven't already)\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n    </Step>\n\n    <Step title=\"Write Code\" description=\"Implement the audio player\" icon='ph:code-bold'>\n        ### 3. Replace the contents of `Program.cs` with the following code:\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Abstracts.Devices;\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n\n        namespace SoundFlowExample;\n\n        internal static class Program\n        {\n            private static void Main(string[] args)\n            {\n                // 1. Initialize the engine context. It no longer starts a device directly.\n                using var engine = new MiniAudioEngine();\n\n                // 2. Define the audio format for playback.\n                var format = new AudioFormat\n                {\n                    SampleRate = 48000,\n                    Channels = 2,\n                    Format = SampleFormat.F32\n                };\n                \n                // 3. Initialize a specific playback device. Passing `null` will use the system default too.\n                // The `using` statement ensures the device is properly disposed.\n                var defaultDevice = engine.PlaybackDevices.FirstOrDefault(x => x.IsDefault);\n                using var playbackDevice = engine.InitializePlaybackDevice(defaultDevice, format);\n\n                // 4. Create a SoundPlayer, passing the engine and format context.\n                // Make sure you replace \"path/to/your/audiofile.wav\" with the actual path.\n                using var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"path/to/your/audiofile.wav\"));\n                var player = new SoundPlayer(engine, format, dataProvider);\n\n                // 5. Add the player to the device's master mixer.\n                playbackDevice.MasterMixer.AddComponent(player);\n\n                // 6. Start the device to begin the audio stream.\n                playbackDevice.Start();\n                \n                // 7. Start the player.\n                player.Play();\n\n                Console.WriteLine($\"Playing audio on '{playbackDevice.Info?.Name}'... Press any key to stop.\");\n                Console.ReadKey();\n\n                // 8. Stop the device, which also stops the audio stream.\n                playbackDevice.Stop();\n            }\n        }\n        ```\n        ***Replace `path/to/your/audiofile.wav` with the actual path to an audio file on your computer.***\n    </Step>\n\n    <Step title=\"Run\" description=\"Build and run the app\" icon='lucide:audio-lines'>\n        ### 4. Build and run the application:\n        *   In Visual Studio, press `F5` or go to `Debug` > `Start Debugging`.\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        *  use the .NET CLI:\n\n        ```bash\n        dotnet run\n        ```\n    </Step>\n</Steps>\n\nYou should now hear the audio file playing through your default audio output device.\n\n**Code Explanation:**\n\n*   `using SoundFlow...`: These lines import the necessary namespaces from the SoundFlow library.\n*   `using var engine = new MiniAudioEngine()`: This creates an instance of the `AudioEngine`. In the new architecture, the engine acts as a central **context** or **factory** for creating and managing audio devices. It no longer represents a single device itself.\n*   `var format = new AudioFormat { ... }`: We define the desired audio format using the `AudioFormat` struct. This bundles the sample rate, channel count, and sample format, and is passed to devices and components to ensure they operate in the correct context.\n*   `var defaultDevice = engine.PlaybackDevices.FirstOrDefault(...);` Getting the first device from the available playback devices.\n*   `using var playbackDevice = engine.InitializePlaybackDevice(defaultDevice, format)`: This is the new way to initialize a playback device. We ask the engine to initialize one for us, passing `null` to select the system's default device. The device itself is `IDisposable` and manages its own lifecycle.\n*   `using var dataProvider = ...`, `var player = new SoundPlayer(...)`: Components like `ISoundDataProvider` and `SoundPlayer` now require the `engine` and `format` contexts in their constructors.\n*   `playbackDevice.MasterMixer.AddComponent(player)`: This adds the `SoundPlayer` to the device-specific `Master` mixer, Each `AudioPlaybackDevice` has its own `MasterMixer` property. The `Master` mixer is the root of the audio graph and represents the final output of the audio device solely.\n*   `playbackDevice.Start()`: You must now explicitly start the device to begin its audio processing thread.\n*   `player.Play()`: This starts the player's internal logic, causing it to generate audio when its `Process` method is called by the device's mixer.\n*   `playbackDevice.Stop()`: This stops the device's audio thread, gracefully halting all audio output from it.\n\n## Troubleshooting\n\n*   **\"Could not load file or assembly 'SoundFlow'...\"**: Make sure you have installed the SoundFlow NuGet package or added a reference to the SoundFlow library if you built it from source.\n*   **No audio output**:\n    *   Verify that your audio device is properly configured and selected as the default output device in your operating system's sound settings.\n    *   Check the volume levels in your operating system and in the SoundFlow application.\n    *   Ensure that the audio file you are trying to play is in a supported format and is not corrupted.\n*   **Errors during installation**: If you encounter errors while installing the NuGet package, try clearing your NuGet cache (`dotnet nuget locals all --clear`) and try again.\n\nIf you encounter any other issues, please open an issue on the [GitHub repository](https://github.com/LSXPrime/SoundFlow).\n\n## Next Steps\n\nNow that you have successfully set up SoundFlow and played your first audio file, you can explore the more advanced features and concepts:\n\n*   [Core Concepts](./core-concepts): Learn more about the fundamental building blocks of SoundFlow.\n*   [Device Management](./device-management): Understand the new device-centric architecture, including multi-device playback, capture, and device switching.\n*   [Editing Engine & Persistence](./editing-engine): Discover the powerful non-destructive editing and project saving capabilities.\n*   [API Reference](./api-reference): Dive into the detailed documentation for each class and interface.\n*   [Tutorials and Examples](./tutorials-and-examples): Get hands-on experience with various SoundFlow features.\n\nHappy coding!"
  },
  {
    "id": 4,
    "slug": "editing-engine",
    "version": "1.2.0",
    "title": "Editing Engine & Persistence",
    "description": "Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.",
    "navOrder": 4,
    "category": "Core",
    "content": "---\r\nid: 4\r\ntitle: Editing Engine & Persistence\r\ndescription: Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.\r\nnavOrder: 4\r\ncategory: Core\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\nimport { Card, CardHeader, CardBody } from \"@heroui/react\";\r\n\r\n\r\n# SoundFlow Editing Engine & Persistence\r\n\r\nSoundFlow features a comprehensive, non-destructive audio editing engine and a robust project persistence system. This allows developers to programmatically build, manipulate, and save complex audio timelines, complete with effects, advanced timing controls, and media management.\r\n\r\n## Core Editing Concepts\r\n\r\nThe editing engine revolves around a few key classes:\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Composition\" description=\"The top-level project container\" icon='ph:stack-bold'>\r\n        ### `Composition` (`SoundFlow.Editing.Composition`)\r\n\r\n        The `Composition` is the top-level container for an audio project. Think of it as the main \"session\" or \"project file\" in a Digital Audio Workstation (DAW).\r\n\r\n        *   **Holds Tracks:** A `Composition` contains one or more `Track` objects.\r\n        *   **Master Settings:** It has master volume control (`MasterVolume`) and can have master effects (modifiers and analyzers) applied to the final mix.\r\n        *   **Renderable:** A `Composition` itself implements `ISoundDataProvider`, meaning the entire composed project can be played back directly using a `SoundPlayer` or rendered to an audio file.\r\n        *   **Project Properties:** Stores overall project settings like `Name`, `TargetSampleRate`, and `TargetChannels`.\r\n        *   **Dirty Flag:** Tracks unsaved changes via an `IsDirty` property.\r\n        *   **IDisposable:** Manages the disposal of resources within its scope.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Abstracts.Devices;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using SoundFlow.Structs;\r\n\r\n        // Define the format for the composition\r\n        var compositionFormat = new AudioFormat\r\n        {\r\n            SampleRate = 48000,\r\n            Channels = 2,\r\n            Format = SampleFormat.F32\r\n        };\r\n\r\n        // Create a new composition with the specified format\r\n        var composition = new Composition(compositionFormat, \"My Awesome Project\")\r\n        {\r\n            MasterVolume = 0.9f\r\n        };\r\n\r\n        // Add master effects (optional)\r\n        // composition.AddModifier(new SomeMasterReverb(compositionFormat));\r\n\r\n        // ... (add tracks and segments) ...\r\n\r\n        // To play the composition:\r\n        // 1. Create an engine context\r\n        using var engine = new MiniAudioEngine();\r\n\r\n        // 2. Initialize a playback device using the composition's format\r\n        using var playbackDevice = engine.InitializePlaybackDevice(null, composition.Format);\r\n\r\n        // 3. Create a player for the composition. The composition itself is the data provider.\r\n        var player = new SoundPlayer(engine, composition.Format, composition);\r\n\r\n        // 4. Add the player to the device's MasterMixer\r\n        playbackDevice.MasterMixer.AddComponent(player);\r\n\r\n        // 5. Start the device and the player\r\n        playbackDevice.Start();\r\n        player.Play();\r\n\r\n        // To render the composition to a float array:\r\n        // float[] renderedAudio = composition.Render(TimeSpan.Zero, composition.CalculateTotalDuration());\r\n        ```\r\n    </Step>\r\n    <Step title=\"Track\" description=\"A single audio timeline\" icon='lucide:audio-lines'>\r\n        ### `Track` (`SoundFlow.Editing.Track`)\r\n\r\n        A `Track` represents a single audio track within a `Composition`, similar to a track in a DAW.\r\n\r\n        *   **Holds Segments:** A `Track` contains a list of `AudioSegment` objects, which are the actual audio clips placed on the track's timeline.\r\n        *   **Track-Level Settings (`TrackSettings`):** Each track has its own settings:\r\n            *   `Volume`, `Pan`\r\n            *   `IsMuted`, `IsSoloed`, `IsEnabled`\r\n            *   Track-specific `Modifiers` and `Analyzers`.\r\n        *   **Timeline Management:** Tracks manage the arrangement of their segments.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n\r\n        // Assuming 'composition' from previous example\r\n        var track1 = new Track(\"Lead Vocals\");\r\n        track1.Settings.Volume = 0.8f;\r\n        track1.Settings.Pan = -0.1f; // Slightly to the left\r\n\r\n        var track2 = new Track(\"Background Music\");\r\n        track2.Settings.Volume = 0.5f;\r\n        track2.Settings.IsMuted = true; // Mute this track for now\r\n\r\n        composition.AddTrack(track1);\r\n        composition.AddTrack(track2);\r\n        ```\r\n    </Step>\r\n    <Step title=\"AudioSegment\" description=\"The fundamental audio clip\" icon='mdi:content-cut'>\r\n        ### `AudioSegment` (`SoundFlow.Editing.AudioSegment`)\r\n\r\n        The `AudioSegment` is the fundamental building block for audio content on a `Track`. It represents a specific portion of an audio source placed at a particular time on the track's timeline.\r\n\r\n        *   **Source Reference:** Points to an `ISoundDataProvider` for its audio data.\r\n        *   **Timeline Placement:**\r\n            *   `SourceStartTime`: The time offset within the `ISoundDataProvider` from which this segment begins.\r\n            *   `SourceDuration`: The duration of audio to use from the `ISoundDataProvider`.\r\n            *   `TimelineStartTime`: The time at which this segment starts on the parent `Track`'s timeline.\r\n        *   **Segment-Level Settings (`AudioSegmentSettings`):** Each segment has incredibly granular control:\r\n            *   `Volume`, `Pan`\r\n            *   `IsEnabled`\r\n            *   `IsReversed`: Play the segment's audio backward.\r\n            *   `Loop` (`LoopSettings`): Control repetitions or loop to fill a target duration.\r\n            *   `FadeInDuration`, `FadeInCurve`, `FadeOutDuration`, `FadeOutCurve`: Apply various fade shapes (`Linear`, `Logarithmic`, `S-Curve`).\r\n            *   `SpeedFactor`: Classic varispeed, affects pitch and tempo.\r\n        *   **Pitch-Preserved Time Stretching:**\r\n            *   `TimeStretchFactor`: Lengthen or shorten the segment without changing pitch (e.g., 0.5 for half duration, 2.0 for double duration).\r\n            *   `TargetStretchDuration`: Stretch the segment to fit a specific duration, preserving pitch.\r\n        *   Segment-specific `Modifiers` and `Analyzers`.\r\n        *   **Non-Destructive:** All operations (trimming, fades, stretching) are applied at runtime and do not alter the original audio source.\r\n        *   **IDisposable:** Can own and dispose its `ISoundDataProvider` if specified.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Providers;\r\n        using SoundFlow.Structs;\r\n        using System.IO;\r\n\r\n        // Assuming 'engine', 'track1' and 'compositionFormat' from previous examples\r\n        // And an audio file \"vocals.wav\" exists.\r\n        using var vocalProvider = new StreamDataProvider(engine, compositionFormat, File.OpenRead(\"vocals.wav\"));\r\n\r\n        // Create a segment: use 10 seconds of \"vocals.wav\" starting from 5s into the file,\r\n        // and place it at 2 seconds on track1's timeline.\r\n        var vocalSegment = new AudioSegment(\r\n            format: compositionFormat,\r\n            sourceDataProvider: vocalProvider,\r\n            sourceStartTime: TimeSpan.FromSeconds(5),\r\n            sourceDuration: TimeSpan.FromSeconds(10),\r\n            timelineStartTime: TimeSpan.FromSeconds(2),\r\n            name: \"Verse 1 Vocals\",\r\n            ownsDataProvider: false // vocalProvider is managed by 'using' here\r\n        );\r\n\r\n        // Apply settings\r\n        vocalSegment.Settings.Volume = 0.95f;\r\n        vocalSegment.Settings.FadeInDuration = TimeSpan.FromMilliseconds(200);\r\n        vocalSegment.Settings.FadeInCurve = FadeCurveType.SCurve;\r\n        vocalSegment.Settings.TimeStretchFactor = 1.1f; // Make it 10% longer without pitch change\r\n\r\n        track1.AddSegment(vocalSegment);\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n### Duration Calculations\r\n\r\n*   `AudioSegment.StretchedSourceDuration`: The duration of the segment's content *after* pitch-preserved time stretching is applied (but before `SpeedFactor`).\r\n*   `AudioSegment.EffectiveDurationOnTimeline`: The duration a single instance of the segment takes on the timeline, considering both `StretchedSourceDuration` and `SpeedFactor`.\r\n*   `AudioSegment.GetTotalLoopedDurationOnTimeline()`: The total duration the segment occupies on the timeline, including all loops.\r\n*   `AudioSegment.TimelineEndTime`: `TimelineStartTime + GetTotalLoopedDurationOnTimeline()`.\r\n*   `Track.CalculateDuration()`: The time of the latest `TimelineEndTime` among all its segments.\r\n*   `Composition.CalculateTotalDuration()`: The time of the latest `TimelineEndTime` among all its tracks.\r\n\r\n## Time Manipulation\r\n\r\nSoundFlow's editing engine offers sophisticated time manipulation capabilities for `AudioSegment`s:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Time manipulation options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"time-stretch\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:timelapse-outline' />\r\n                <span>Pitch-Preserved Time Stretching</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                This feature allows you to change the duration of an audio segment without affecting its pitch. It's ideal for:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>Fitting dialogue or music to a specific time slot.</li>\r\n                    <li>Creative sound design by drastically stretching or compressing audio.</li>\r\n                </ul>\r\n\r\n                It's controlled by two properties in `AudioSegmentSettings`:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>\r\n                        <strong><code>TimeStretchFactor</code> (float):</strong>\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li><code>1.0</code>: No stretching.</li>\r\n                            <li><code>&gt; 1.0</code>: Makes the segment longer (e.g., <code>2.0</code> doubles the duration).</li>\r\n                            <li><code>&lt; 1.0</code> and <code>&gt; 0.0</code>: Makes the segment shorter (e.g., <code>0.5</code> halves the duration).</li>\r\n                        </ul>\r\n                    </li>\r\n                    <li>\r\n                        <strong><code>TargetStretchDuration</code> (TimeSpan?):</strong>\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li>If set, this overrides `TimeStretchFactor`. The segment will be stretched or compressed to match this exact duration.</li>\r\n                            <li>Set to `null` to use `TimeStretchFactor` instead.</li>\r\n                        </ul>\r\n                    </li>\r\n                </ul>\r\n                <p className=\"mt-2\">Internally, SoundFlow uses a high-quality <strong>WSOLA (Waveform Similarity Overlap-Add)</strong> algorithm implemented in the <code>WsolaTimeStretcher</code> class.</p>\r\n                ```csharp\r\n                // Make a segment 50% shorter while preserving pitch\r\n                mySegment.Settings.TimeStretchFactor = 0.5f;\r\n\r\n                // Make a segment exactly 3.75 seconds long, preserving pitch\r\n                mySegment.Settings.TargetStretchDuration = TimeSpan.FromSeconds(3.75);\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"varispeed\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:speedometer' />\r\n                <span>Classic Speed Control (Varispeed)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The <code>SpeedFactor</code> property in `AudioSegmentSettings` provides traditional speed control, affecting both the tempo and the pitch of the audio, similar to changing the playback speed of a tape machine.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>\r\n                        <strong><code>SpeedFactor</code> (float):</strong>\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li><code>1.0</code>: Normal speed and pitch.</li>\r\n                            <li><code>&gt; 1.0</code>: Faster playback, higher pitch.</li>\r\n                            <li><code>&lt; 1.0</code> and <code>&gt; 0.0</code>: Slower playback, lower pitch.</li>\r\n                        </ul>\r\n                    </li>\r\n                </ul>\r\n                <p className=\"mt-2\"><strong>Interaction:</strong> Time stretching is applied to the source audio <em>first</em>, and then the <code>SpeedFactor</code> is applied to the time-stretched result.</p>\r\n                ```csharp\r\n                // Play segment at double speed (and an octave higher)\r\n                mySegment.Settings.SpeedFactor = 2.0f;\r\n\r\n                // Play segment at half speed (and an octave lower)\r\n                mySegment.Settings.SpeedFactor = 0.5f;\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Project Persistence (`SoundFlow.Editing.Persistence`)\r\n\r\nThe `CompositionProjectManager` class provides static methods for saving and loading your `Composition` objects. Projects are saved in a JSON-based format with the `.sfproj` extension.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Project persistence options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"save\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:save-outline' />\r\n                <span>Saving a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                ### Saving a Project\r\n\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using System.Threading.Tasks;\r\n\r\n                public async Task SaveMyProject(AudioEngine engine, Composition composition, string filePath)\r\n                {\r\n                    await CompositionProjectManager.SaveProjectAsync(\r\n                        engine,                 // The engine is now required for consolidation\r\n                        composition,\r\n                        filePath,\r\n                        consolidateMedia: true,  // Recommended for portability\r\n                        embedSmallMedia: true   // Embeds small audio files directly\r\n                    );\r\n                    Console.WriteLine($\"Project saved to {filePath}\");\r\n                }\r\n                ```\r\n\r\n                **Saving Options:**\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-2\">\r\n                    <li>\r\n                        <strong><code>consolidateMedia</code> (bool):</strong> If <code>true</code> (default), SoundFlow will attempt to copy all unique external audio files referenced by segments into an <code>Assets</code> subfolder next to your <code>.sfproj</code> file. This makes the project self-contained and portable. In-memory <code>ISoundDataProvider</code>s (like <code>RawDataProvider</code> from generated audio) will also be saved as WAV files in the <code>Assets</code> folder if <code>consolidateMedia</code> is true. The project file will then store relative paths to these consolidated assets.\r\n                    </li>\r\n                    <li>\r\n                        <strong><code>embedSmallMedia</code> (bool):</strong> If <code>true</code> (default), audio sources smaller than a certain threshold (currently 1MB) will be embedded directly into the <code>.sfproj</code> file as Base64-encoded strings. This is useful for short sound effects or jingles, avoiding the need for separate files. Embedding takes precedence over consolidation for small files.\r\n                    </li>\r\n                </ul>\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"load\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:folder-open-outline' />\r\n                <span>Loading a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                ### Loading a Project\r\n\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using SoundFlow.Structs;\r\n                using System.Threading.Tasks;\r\n                using System.Linq;\r\n                using System.Collections.Generic; \r\n\r\n                public async Task<(Composition?, List<ProjectSourceReference>)> LoadMyProject(AudioEngine engine, string filePath)\r\n                {\r\n                    if (!File.Exists(filePath))\r\n                    {\r\n                        Console.WriteLine($\"Project file not found: {filePath}\");\r\n                        return (null, new List<ProjectSourceReference>());\r\n                    }\r\n\r\n                    // NOTE: The loaded composition will have its own format. For playback, you'll\r\n                    // need to initialize a device with that specific format. Here we'll just\r\n                    // use a default format for the load operation itself.\r\n                    var loadingFormat = AudioFormat.DvdHq;\r\n\r\n                    var (loadedComposition, unresolvedSources) = await CompositionProjectManager.LoadProjectAsync(engine, loadingFormat, filePath);\r\n\r\n                    if (unresolvedSources.Any())\r\n                    {\r\n                        Console.WriteLine(\"Warning: Some media sources could not be found:\");\r\n                        foreach (var missing in unresolvedSources)\r\n                        {\r\n                            Console.WriteLine($\" - Missing ID: {missing.Id}, Original Path: {missing.OriginalAbsolutePath ?? \"N/A\"}\");\r\n                            // Here you could trigger a UI for relinking\r\n                        }\r\n                    }\r\n\r\n                    Console.WriteLine($\"Project '{loadedComposition.Name}' loaded successfully!\");\r\n                    return (loadedComposition, unresolvedSources);\r\n                }\r\n                ```\r\n\r\n                When loading, `LoadProjectAsync` returns a tuple:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>The loaded <code>Composition</code> object.</li>\r\n                    <li>A <code>List&lt;ProjectSourceReference&gt;</code> detailing any audio sources that could not be found (based on embedded data, consolidated paths, or original absolute paths).</li>\r\n                </ul>\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"relink\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='ph:link-bold' />\r\n                <span>Media Management & Relinking</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                ### Media Management & Relinking\r\n\r\n                SoundFlow's persistence system attempts to locate media in this order:\r\n                1.  **Embedded Data:** If the `ProjectSourceReference` indicates embedded data, it's decoded.\r\n                2.  **Consolidated Relative Path:** If not embedded, it looks for the file in the `Assets` folder relative to the project file.\r\n                3.  **Original Absolute Path:** If still not found, it tries the original absolute path stored during the save.\r\n\r\n                If a source is still missing, it's added to the `unresolvedSources` list. You can then use `CompositionProjectManager.RelinkMissingMediaAsync` to update the project with the new location of a missing file:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using SoundFlow.Structs;\r\n                using System.Threading.Tasks;\r\n\r\n                public async Task AttemptRelink(AudioEngine engine, AudioFormat format, ProjectSourceReference missingSource, string newFilePath, string projectDirectory)\r\n                {\r\n                    bool success = CompositionProjectManager.RelinkMissingMediaAsync(\r\n                        engine,\r\n                        format,\r\n                        missingSource,\r\n                        newFilePath,\r\n                        projectDirectory\r\n                    );\r\n\r\n                    if (success)\r\n                    {\r\n                        Console.WriteLine($\"Successfully relinked '{missingSource.Id}' to '{newFilePath}'.\");\r\n                        // You might need to re-resolve or update segments in your loaded composition\r\n                        // that use this missingSourceReference. One way is to reload the project:\r\n                        // (var reloadedComposition, var newMissing) = await CompositionProjectManager.LoadProjectAsync(engine, format, projectFilePath);\r\n                        // Or, manually update ISoundDataProvider instances in affected AudioSegments.\r\n                    }\r\n                    else\r\n                    {\r\n                        Console.WriteLine($\"Failed to relink '{missingSource.Id}'. File at new path might be invalid or inaccessible.\");\r\n                    }\r\n                }\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n<Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 mt-6\">\r\n    <CardHeader>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:lightbulb\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n            <h4 className=\"font-semibold text-lg\">Note on <code>ownsDataProvider</code></h4>\r\n        </div>\r\n    </CardHeader>\r\n    <CardBody className=\"pt-0\">\r\n        <ul className=\"list-disc pl-5 space-y-2 text-sm\">\r\n            <li>\r\n                When you create <code>AudioSegment</code>s manually for a new composition, you manage the lifecycle of their <code>ISoundDataProvider</code>s. If you pass <code>ownsDataProvider: true</code>, the segment will dispose of the provider when the segment itself (or its parent <code>Composition</code>) is disposed.\r\n            </li>\r\n            <li>\r\n                When a <code>Composition</code> is loaded from a project file, the <code>AudioSegment</code>s created during loading will typically have <code>ownsDataProvider: true</code> set for the <code>ISoundDataProvider</code>s that were resolved (from file, embedded, or consolidated assets), as the loading process instantiates these providers.\r\n            </li>\r\n        </ul>\r\n    </CardBody>\r\n</Card>\r\n\r\n## Dirty Flag (`IsDirty`)\r\n\r\n`Composition`, `Track`, and `AudioSegment` (via its `Settings`) have an `IsDirty` property.\r\n*   This flag is automatically set to `true` when any significant property that affects playback or persistence is changed.\r\n*   `CompositionProjectManager.SaveProjectAsync` calls `composition.ClearDirtyFlag()` internally upon successful save.\r\n*   You can use this flag to prompt users to save changes before closing an application, for example.\r\n\r\n## Basic Composition Example\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Enums;\r\nusing SoundFlow.Providers;\r\nusing SoundFlow.Editing; // New namespace\r\nusing System;\r\nusing System.IO;\r\nusing System.Threading.Tasks;\r\n\r\nnamespace BasicComposition;\r\n\r\ninternal static class Program\r\n{\r\n    private static async Task Main(string[] args)\r\n    {\r\n        // Initialize the audio engine.\r\n        // It's good practice to set format and channels to match your composition's target.\r\n        using var audioEngine = new MiniAudioEngine(48000, Capability.Playback, channels: 1);\r\n\r\n        // Create a composition\r\n        var composition = new Composition(\"My First Song\") { SampleRate = 48000, TargetChannels = 1 };\r\n\r\n        // Create a track\r\n        var track1 = new Track(\"Vocals\");\r\n        composition.AddTrack(track1);\r\n\r\n        // Load an audio file for a segment\r\n        // IMPORTANT: Replace with an actual path to a WAV file on your system\r\n        string audioPath = \"path/to/your/audio.wav\";\r\n        if (!File.Exists(audioPath))\r\n        {\r\n            Console.WriteLine($\"Audio file not found: {audioPath}\");\r\n            Console.WriteLine(\"Please update 'audioPath' in the example code to a valid WAV file on your system.\");\r\n            return;\r\n        }\r\n\r\n        // Create a StreamDataProvider from the audio file.\r\n        // The compositionFormat (or a compatible format) is needed for the provider.\r\n        var provider = new StreamDataProvider(audioEngine, composition.Format, File.OpenRead(audioPath));\r\n\r\n        // Create an audio segment\r\n        // Play from 0s of source, for 5s duration, place at 1s on timeline\r\n        var segment1 = new AudioSegment(\r\n            format: composition.Format, // Use the composition's format for the segment\r\n            sourceDataProvider: provider,\r\n            sourceStartTime: TimeSpan.Zero,\r\n            sourceDuration: TimeSpan.FromSeconds(5),\r\n            timelineStartTime: TimeSpan.FromSeconds(1),\r\n            name: \"Intro Vocal Clip\",\r\n            ownsDataProvider: true // The segment will dispose of the provider when the segment/composition is disposed\r\n        );\r\n\r\n        // Optionally, modify segment settings\r\n        segment1.Settings.Volume = 0.9f;\r\n        segment1.Settings.FadeInDuration = TimeSpan.FromMilliseconds(500);\r\n\r\n        track1.AddSegment(segment1);\r\n\r\n        // Create a SoundPlayer for the composition\r\n        // The composition itself acts as an ISoundDataProvider\r\n        var compositionPlayer = new SoundPlayer(audioEngine, composition.Format, composition);\r\n        audioEngine.MasterMixer.AddComponent(compositionPlayer); // Add player to the engine's master mixer\r\n        compositionPlayer.Play();\r\n\r\n        Console.WriteLine($\"Playing composition '{composition.Name}' for {composition.CalculateTotalDuration().TotalSeconds:F1}s... Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        compositionPlayer.Stop();\r\n        audioEngine.MasterMixer.RemoveComponent(compositionPlayer);\r\n\r\n        // Dispose the composition and engine to release resources\r\n        composition.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n## Examples in Action\r\n\r\nThe `SoundFlow.Samples.EditingMixer` project in the SoundFlow GitHub repository provides extensive, runnable examples demonstrating:\r\n*   Building compositions with dialogue and generated audio.\r\n*   Using various `AudioSegmentSettings` like fades, loops, reverse, speed, and time stretching.\r\n*   Saving projects with different media handling strategies (consolidation, embedding).\r\n*   Loading projects and handling missing media by relinking.\r\n\r\nExploring this sample project is highly recommended to see these concepts applied in practical scenarios."
  },
  {
    "id": 3,
    "slug": "device-management",
    "version": "1.2.0",
    "title": "Device Management",
    "description": "Learn how to manage audio devices in SoundFlow, including initialization, switching, and advanced configuration.",
    "navOrder": 3,
    "category": "Core",
    "content": "---\r\nid: 3\r\ntitle: Device Management\r\ndescription: Learn how to manage audio devices in SoundFlow, including initialization, switching, and advanced configuration.\r\nnavOrder: 3\r\ncategory: Core\r\n---\r\n\r\nimport { Card, CardBody, Tabs, Tab, Chip } from \"@heroui/react\";\r\nimport { Icon } from \"@iconify/react\";\r\n\r\n# Device Management\r\n\r\nSoundFlow v1.2 introduced a powerful, device-centric architecture. This guide covers how to discover, initialize, and manage audio devices for playback, capture, and more complex scenarios.\r\n\r\n## An Overview\r\n\r\nThe most significant change in v1.2 is the separation of the **`AudioEngine`** from the **`AudioDevice`**.\r\n\r\n*   **`AudioEngine`**: Acts as a central **context** or **factory**. It is responsible for interacting with the low-level audio backend (e.g., MiniAudio), discovering available hardware, and creating device instances.\r\n*   **`AudioDevice`**: Represents an active audio I/O stream to a specific piece of hardware. Each device has its own lifecycle (`Start`, `Stop`, `Dispose`), its own audio format (`AudioFormat`), and, in the case of playback devices, its own independent audio graph (`MasterMixer`).\r\n\r\nThis model allows for robust, multi-device applications where you can, for example, play audio to a speaker and a headphone jack simultaneously, each with different effects.\r\n\r\n## Listing Available Devices\r\n\r\nBefore initializing a device, you need to know what hardware is available. The `AudioEngine` provides this information.\r\n\r\n<Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 mt-4\">\r\n    <CardBody>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n            <p className=\"text-sm\">\r\n                You must call `engine.UpdateDevicesInfo()` before accessing the device lists to ensure they are up-to-date.\r\n            </p>\r\n        </div>\r\n    </CardBody>\r\n</Card>\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\n\r\n// 1. Initialize the engine context.\r\nusing var engine = new MiniAudioEngine();\r\n\r\n// 2. Refresh the list of available devices.\r\nengine.UpdateDevicesInfo();\r\n\r\n// 3. List available playback devices.\r\nConsole.WriteLine(\"--- Playback Devices ---\");\r\nif (engine.PlaybackDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No playback devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.PlaybackDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\r\n    }\r\n}\r\n\r\n\r\n// 4. List available capture devices.\r\nConsole.WriteLine(\"\\n--- Capture Devices ---\");\r\nif (engine.CaptureDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No capture devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.CaptureDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\r\n    }\r\n}\r\n```\r\n\r\n## Initializing an Audio Device\r\n\r\nOnce you have a `DeviceInfo` struct (or if you want to use the default device), you can ask the engine to initialize it.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Device initialization options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"playback\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='lucide:audio-lines' />\r\n                <span>Playback Device</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                An `AudioPlaybackDevice` is used for sending audio out to speakers or headphones.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>It has its own `MasterMixer`, which is the root of its audio graph.</li>\r\n                    <li>It must be started with `Start()` to begin processing audio.</li>\r\n                </ul>\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Abstracts.Devices;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Components;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n\r\n                    // Define the audio format for this device.\r\n                    var format = AudioFormat.DvdHq; // 48kHz, 2-channel, 32-bit float\r\n\r\n                    // To use the default device, pass `null` or `engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault)`\r\n                    using var playbackDevice = engine.InitializePlaybackDevice(null, format);\r\n                    Console.WriteLine($\"Initialized playback on: {playbackDevice.Info?.Name}\");\r\n\r\n                    // Create an oscillator component with the same format.\r\n                    var oscillator = new Oscillator(engine, format) { Frequency = 440f, Volume = 0.5f };\r\n\r\n                    // Add the oscillator to the device's MasterMixer.\r\n                    playbackDevice.MasterMixer.AddComponent(oscillator);\r\n\r\n                    // Start the device's audio stream.\r\n                    playbackDevice.Start();\r\n\r\n                    Console.WriteLine(\"Playing a 440 Hz tone. Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Stop the device.\r\n                    playbackDevice.Stop();\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"capture\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='lucide:mic' />\r\n                <span>Capture Device</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                An `AudioCaptureDevice` is used for receiving audio from a microphone or other input.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>Its primary interaction point is the `OnAudioProcessed` event.</li>\r\n                    <li>It must be started with `Start()` to begin capturing audio.</li>\r\n                </ul>\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Abstracts.Devices;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Enums;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n                    var format = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n\r\n                    // To use the default device, pass `null` or `engine.CaptureDevices.FirstOrDefault(d => d.IsDefault)`\r\n                    using var captureDevice = engine.InitializeCaptureDevice(null, format);\r\n                    Console.WriteLine($\"Initialized capture on: {captureDevice.Info?.Name}\");\r\n\r\n                    // Subscribe to the audio data event.\r\n                    captureDevice.OnAudioProcessed += (samples, capability) =>\r\n                    {\r\n                        // This code block will execute on the audio thread every time a buffer is ready.\r\n                        // Be efficient here to avoid audio dropouts.\r\n                        Console.Write(\".\"); // Print a dot to show that data is flowing.\r\n                    };\r\n\r\n                    // Start capturing.\r\n                    captureDevice.Start();\r\n\r\n                    Console.WriteLine(\"Capturing audio... Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Stop capturing.\r\n                    captureDevice.Stop();\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## High-Level Device Wrappers\r\n\r\nSoundFlow provides convenience wrappers for common and complex scenarios.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Device wrapper options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"fullduplex\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='carbon:arrows-horizontal' />\r\n                <span>Full-Duplex (Simultaneous I/O)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `FullDuplexDevice` simplifies scenarios like live effects processing or VoIP by managing a paired playback and capture device.\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Components;\r\n                    using SoundFlow.Providers;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n                    var format = AudioFormat.DvdHq;\r\n\r\n                    // Getting the default devices\r\n                    var playbackDevice = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                    var captureDevice = engine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n\r\n                    // Initialize a duplex device using the selected playback and capture hardware, or null for default.\r\n                    using var duplexDevice = engine.InitializeFullDuplexDevice(playbackDevice, captureDevice, format);\r\n\r\n                    Console.WriteLine($\"Passthrough active. Input: {duplexDevice.CaptureDevice.Info?.Name}, Output: {duplexDevice.PlaybackDevice.Info?.Name}\");\r\n\r\n                    // Create a provider that reads from the capture device's audio stream.\r\n                    using var micProvider = new MicrophoneDataProvider(duplexDevice); // MicrophoneDataProvider works with both Capture and Duplex devices\r\n\r\n                    // Create a player that plays the audio from the microphone provider.\r\n                    using var player = new SoundPlayer(engine, format, micProvider);\r\n\r\n                    // Add the player to the duplex device's playback mixer.\r\n                    duplexDevice.MasterMixer.AddComponent(player);\r\n\r\n                    // Start the duplex device, provider, and player.\r\n                    duplexDevice.Start();\r\n                    micProvider.StartCapture();\r\n                    player.Play();\r\n\r\n                    Console.WriteLine(\"Live microphone passthrough is active. Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Clean up\r\n                    duplexDevice.Stop();\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"loopback\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:record-rec' />\r\n                <span>Loopback Recording</span>\r\n                <Chip color=\"warning\" variant=\"flat\" size=\"sm\">Windows Only</Chip>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                Loopback allows you to capture the audio that your computer is currently playing. This is useful for recording game audio, system sounds, or audio from other applications.\r\n                <Card className=\"bg-warning-50/50 dark:bg-warning-500/10 border-1 border-warning-200/50 dark:border-warning-500/20 my-4\">\r\n                    <CardBody>\r\n                        <div className=\"flex items-center gap-3\">\r\n                            <Icon icon=\"lucide:alert-triangle\" className=\"text-warning text-2xl flex-shrink-0\" />\r\n                            <p className=\"text-sm\">\r\n                                Loopback recording is currently only supported on <strong>Windows</strong> via the WASAPI backend.\r\n                            </p>\r\n                        </div>\r\n                    </CardBody>\r\n                </Card>\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Components;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n                    var format = AudioFormat.DvdHq;\r\n\r\n                    // Initialize a loopback device. It internally finds the default playback\r\n                    // device and sets it up for capture.\r\n                    using var loopbackDevice = engine.InitializeLoopbackDevice(format);\r\n                    Console.WriteLine($\"Initialized loopback capture on: {loopbackDevice.Info?.Name}\");\r\n\r\n                    // Record the loopback audio to a file.\r\n                    using var fileStream = new FileStream(\"system_audio.wav\", FileMode.Create);\r\n                    using var recorder = new Recorder(loopbackDevice, fileStream);\r\n\r\n                    // Start capture and recording.\r\n                    loopbackDevice.Start();\r\n                    recorder.StartRecording();\r\n\r\n                    Console.WriteLine(\"Recording system audio... Play some sound! Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Stop and clean up.\r\n                    recorder.StopRecording();\r\n                    loopbackDevice.Stop();\r\n\r\n                    Console.WriteLine(\"Recording stopped. Saved to system_audio.wav\");\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Runtime Device Switching\r\n\r\nA powerful feature of SoundFlow is the ability to switch the underlying hardware device at runtime without interrupting the audio graph. The `Engine.SwitchDevice(...)` method handles this seamlessly.\r\n*   For **playback**, it moves all `SoundComponent`s from the old device's mixer to the new one.\r\n*   For **capture**, it moves all event subscribers from the old device's `OnAudioProcessed` event to the new one.\r\n*   It automatically disposes the old device instance.\r\n\r\n    ```csharp\r\n    // ... (Setup from the Playback Device example) ...\r\n    using var engine = new MiniAudioEngine();\r\n    var format = AudioFormat.DvdHq;\r\n\r\n    // 1. Initialize with the default device.\r\n    var playbackDevice = engine.InitializePlaybackDevice(null, format);\r\n    var oscillator = new Oscillator(engine, format) { Frequency = 440f, Volume = 0.5f };\r\n    playbackDevice.MasterMixer.AddComponent(oscillator);\r\n    playbackDevice.Start();\r\n    Console.WriteLine($\"Playing tone on: {playbackDevice.Info?.Name}\");\r\n\r\n    // 2. Loop to allow switching.\r\n    while(true)\r\n    {\r\n        Console.WriteLine(\"\\nPress 's' to switch device, or 'q' to quit.\");\r\n        if (Console.ReadKey(true).Key == ConsoleKey.Q) break;\r\n\r\n        // Prompt user to select a new device from the list.\r\n        engine.UpdateDevicesInfo();\r\n        Console.WriteLine(\"Select new playback device:\");\r\n        for (int i = 0; i < engine.PlaybackDevices.Length; i++)\r\n        Console.WriteLine($\"{i}: {engine.PlaybackDevices[i].Name}\");\r\n\r\n        if (int.TryParse(Console.ReadLine(), out var index) && index >= 0 && index < engine.PlaybackDevices.Length)\r\n    {\r\n        var newDeviceInfo = engine.PlaybackDevices[index];\r\n        Console.WriteLine($\"Switching playback to: {newDeviceInfo.Name}...\");\r\n\r\n        // 3. THE SWITCH:\r\n        // The old device is disposed, a new one is returned, and the oscillator is moved automatically.\r\n        playbackDevice = engine.SwitchDevice(playbackDevice, newDeviceInfo);\r\n\r\n        Console.WriteLine($\"Successfully switched. Now playing on: {playbackDevice.Info?.Name}\");\r\n    }\r\n    }\r\n\r\n    playbackDevice.Dispose();\r\n    ```\r\n\r\n## Advanced Device Configuration\r\n\r\nFor fine-grained control over latency and backend-specific features, you can use the `MiniAudioDeviceConfig` class when initializing a device. This allows you to tune parameters like buffer sizes, sharing modes, and platform-specific settings.\r\n\r\nThis is just a glimpse of the available options. For a full list of configurable parameters, please refer to the <strong>API Reference</strong> for <code>MiniAudioDeviceConfig</code> and its nested settings classes.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Abstracts.Devices;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Backends.MiniAudio.Devices;\r\nusing SoundFlow.Backends.MiniAudio.Enums;\r\nusing SoundFlow.Structs;\r\n\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\n\r\n// Create a detailed configuration object.\r\nvar customConfig = new MiniAudioDeviceConfig\r\n{\r\n// Request a specific buffer size. 960 frames at 48kHz stereo = 10ms latency.\r\nPeriodSizeInFrames = 960,\r\n\r\n// Use shared mode for better compatibility with other applications.\r\n// For lowest latency, you could try ShareMode.Exclusive.\r\nPlayback = new DeviceSubConfig { ShareMode = ShareMode.Shared },\r\nCapture = new DeviceSubConfig { ShareMode = ShareMode.Shared },\r\n\r\n// Platform-specific settings. For Windows, use WASAPI in ProAudio mode.\r\nWasapi = new WasapiSettings{ Usage = WasapiUsage.ProAudio }\r\n};\r\n\r\n// Pass the config when initializing the device.\r\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format, customConfig);\r\n\r\nConsole.WriteLine($\"Initialized device '{playbackDevice.Info?.Name}' with custom configuration.\");\r\n// ... rest of your playback logic ...\r\n```"
  },
  {
    "id": 2,
    "slug": "core-concepts",
    "version": "1.2.0",
    "title": "Core Concepts",
    "description": "Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.",
    "navOrder": 2,
    "category": "Core",
    "content": "---\nid: 2\ntitle: Core Concepts\ndescription: Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.\nnavOrder: 2\ncategory: Core\n---\n\n# Core Concepts\n\nThis section explains the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline. Understanding these core concepts is essential for effectively using and extending the SoundFlow framework.\n\nThe v1.2.0 architecture is built around a clear separation of concerns:\n1.  The **`AudioEngine`** acts as a central context and device factory.\n2.  **`AudioDevice`** instances represent the actual hardware I/O streams.\n3.  **`SoundComponent`** instances form the audio processing graph for each device.\n\n## Audio Engine (`AudioEngine`)\n\nThe `AudioEngine` is the top-level object in SoundFlow. It's a central context responsible for:\n\n*   **Initializing and managing the audio backend:** SoundFlow supports multiple audio backends (e.g., `MiniAudio`), which handle the low-level interaction with the operating system's audio API. The `AudioEngine` abstracts away the backend details.\n*   **Discovering and Enumerating Audio Devices:** The engine can list all available playback and capture devices and allows switching between them during runtime.\n*   **Acting as a Device Factory:** The primary role of the engine is to initialize `AudioDevice` instances (e.g., `AudioPlaybackDevice`, `AudioCaptureDevice`) which you will use for I/O.\n*   **Managing Global State:** It handles global features like the soloing system (`SoloComponent`/`UnsoloComponent`).\n\n> **Key Change in v1.2:** The `AudioEngine` is no longer a device. You initialize it once without parameters like sample rate or channels. These are now defined per-device.\n\n**Key Properties:**\n\n*   `PlaybackDevices`, `CaptureDevices`: Lists of available audio devices. Must be refreshed with `UpdateDevicesInfo()`.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n\n**Key Methods:**\n\n*   `InitializePlaybackDevice(...)`: Creates and returns a new `AudioPlaybackDevice` for audio output.\n*   `InitializeCaptureDevice(...)`: Creates and returns a new `AudioCaptureDevice` for audio input.\n*   `InitializeFullDuplexDevice(...)`: A convenience method to create a paired playback and capture device for live monitoring or VoIP.\n*   `InitializeLoopbackDevice(...)`: Creates a capture device to record system audio output (Windows only).\n*   `SwitchDevice(...)`: Switches an active device to a new physical device while preserving its state.\n*   `CreateEncoder(...)`, `CreateDecoder(...)`: Creates instances of backend-specific audio encoders and decoders.\n*   `UpdateDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.\n*   `Dispose()`: Releases the engine and all associated devices.\n\n**Example:**\n\n```csharp\n// 1. Initialize the engine context. It doesn't start any devices.\nusing var engine = new MiniAudioEngine();\n\n// 2. List available playback devices.\nengine.UpdateDevicesInfo();\nConsole.WriteLine(\"Available Playback Devices:\");\nforeach(var device in engine.PlaybackDevices)\n{\n    Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\n}\n```\n\n## Audio Devices (`AudioDevice`)\n\nNew in v1.2, the `AudioDevice` and its derivatives (`AudioPlaybackDevice`, `AudioCaptureDevice`) represent an active audio stream to a physical hardware device. Each device is an independent entity with its own audio format, lifecycle, and processing graph.\n\n### `AudioPlaybackDevice` (Output)\nAn `AudioPlaybackDevice` manages an audio output stream.\n\n*   **Owns a `MasterMixer`:** Each playback device has its own `MasterMixer` property. This is the root of the audio graph for that specific device. All components you want to hear on this device must be added to its mixer.\n*   **Independent `AudioFormat`:** Can be initialized with a specific sample rate, channel count, and sample format.\n*   **Lifecycle:** Must be explicitly started with `Start()` and stopped with `Stop()`. It is `IDisposable` and should be managed with a `using` statement.\n\n### `AudioCaptureDevice` (Input)\nAn `AudioCaptureDevice` manages an audio input stream.\n\n*   **`OnAudioProcessed` Event:** This event is raised whenever a new buffer of audio is captured from the hardware. You can subscribe to this event to process live microphone data.\n*   **Lifecycle:** Also has its own `Start()`, `Stop()`, and `Dispose()` methods.\n\n> For a deep dive into creating, managing, and switching devices, see the **[Device Management](./device-management)** documentation.\n\n## Sound Components (`SoundComponent`)\n\n`SoundComponent` remains the abstract base class for all audio processing units in SoundFlow (oscillators, players, filters, mixers). Each component represents a node in a directed acyclic graph (DAG), known as the **audio graph**.\n\n**Key Changes in v1.2:**\n\n*   **Explicit Context:** The constructor now requires an `AudioEngine` and an `AudioFormat`. This makes the component aware of its operating context without relying on a global static instance.\n*   **Graph per Device:** Audio graphs are now built *per device*. A component is typically part of a single device's graph via its `MasterMixer`.\n\n**Key Features:**\n\n*   **Modular Processing:** Components encapsulate specific audio processing logic, making the system modular and extensible.\n*   **Input and Output Connections:** Components can have zero or more input and output connections, allowing data to flow between them.\n*   **`GenerateAudio(Span<float> buffer, int channels)`:** The core processing method that derived classes must implement.\n    *   **Generate new audio samples:** For source components like oscillators or file players.\n    *   **Modify existing audio samples:** For effects, filters, or analyzers.\n*   **Properties:**\n    *   `Name`: A descriptive name for the component.\n    *   `Volume`: Controls the output gain.\n    *   `Pan`: Controls the stereo panning (0.0 for full left, 0.5 for center, 1.0 for full right).\n    *   `Enabled`: Enables or disables the component's processing.\n    *   `Solo`: Isolates the component for debugging.\n    *   `Mute`: Silences the component's output.\n    *   `Parent`: The `Mixer` to which this component belongs (if any).\n*   **Methods:**\n    *   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n    *   `DisconnectInput(SoundComponent input)`: Disconnects an input connection.\n    *   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an `AudioAnalyzer` to this component.\n    *   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an `AudioAnalyzer` from this component.\n    *   `AddModifier(SoundModifier modifier)`: Adds a `SoundModifier` to this component.\n    *   `RemoveModifier(SoundModifier modifier)`: Removes a `SoundModifier` from this component.\n\n**Example:**\n\n```csharp\n// A SineWaveGenerator component aware of its format.\npublic class SineWaveGenerator : SoundComponent\n{\n    public float Frequency { get; set; } = 440f;\n    private float _phase;\n\n    // The constructor now takes the engine and format context.\n    public SineWaveGenerator(AudioEngine engine, AudioFormat format) : base(engine, format) { }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels)\n    {\n        // Now uses the component's own Format property\n        var sampleRate = this.Format.SampleRate; \n        for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] = MathF.Sin(_phase);\n            _phase += 2 * MathF.PI * Frequency / sampleRate;\n            if (_phase > 2 * MathF.PI) _phase -= 2 * MathF.PI;\n        }\n    }\n}\n```\n\n## Mixer (`Mixer`)\n\nThe `Mixer` is a specialized `SoundComponent` that combines the output of multiple `SoundComponent` instances into a single audio stream.\n\n> **Breaking Change in v1.2:** The static `Mixer.Master` property has been **removed**. This is the most significant breaking change for users of previous versions.\n\n**Key Features:**\n\n*   **Device-Specific `MasterMixer`:** Each `AudioPlaybackDevice` now exposes its own `MasterMixer` property. This is the root mixer for that device. All audio that you wish to play on a device must ultimately be routed to its `MasterMixer`.\n*   **Creating Sub-Mixers:** You can still create your own `Mixer` instances (`new Mixer(engine, format)`) to group components before connecting them to a master mixer.\n*   **Adding and Removing Components:**\n    *   `AddComponent(SoundComponent component)`: Adds a component to the mixer's inputs.\n    *   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n\t\n**Example:**\n\n```csharp\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format);\n\n// Create a SoundPlayer and an Oscillator, providing the engine and format\nusing var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(engine, format, dataProvider);\nvar oscillator = new Oscillator(engine, format) { Frequency = 220, Type = Oscillator.WaveformType.Square };\n\n// Add both components to the Device'S MasterMixer\nplaybackDevice.MasterMixer.AddComponent(player);\nplaybackDevice.MasterMixer.AddComponent(oscillator);\n\n// Start the device to enable its audio stream\nplaybackDevice.Start();\nplayer.Play();\noscillator.Play();\n// ...\n```\n\n## Sound Modifiers (`SoundModifier`)\n\n`SoundModifier` is an abstract base class for creating audio effects that modify the audio stream. Modifiers are applied to `SoundComponent` instances or to `AudioSegment`, `Track`, `Composition` and process the audio data.\n\n**Key Features:**\n\n*   **`ProcessSample(float sample, int channel)`:** This is the core method that derived classes can implement to process audio on a sample-by-sample basis.\n*   **`Process(Span<float> buffer, int channels)`:** This method can be overridden for buffer-based processing, which is often more efficient for complex effects. By default, it calls `ProcessSample` for each sample.\n*   **`Enabled` Property:** Allows dynamically enabling or disabling the modifier's effect.\n*   **Chaining:** Modifiers can be chained together on `SoundComponent` instances (or `AudioSegment`, `Track`, `Composition`) to create complex effect pipelines.\n\n**Built-in Modifiers:**\n\nSoundFlow provides a variety of built-in modifiers, including:\n*   Algorithmic Reverb Modifier: Simulates reverberation.\n*   Ambient Reverb Modifier: Creates a sense of spaciousness.\n*   Bass Boost Modifier: Enhances low frequencies.\n*   Chorus Modifier: Creates a chorus effect.\n*   Compressor Modifier: Reduces dynamic range.\n*   Delay Modifier: Applies a delay effect.\n*   Frequency Band Modifier: Boosts or cuts frequency bands.\n*   Noise Reduction Modifier: Reduces noise.\n*   Parametric Equalizer: Provides precise EQ control.\n*   Stereo Chorus Modifier: Creates a stereo chorus.\n*   Treble Boost Modifier: Enhances high frequencies.\n*   And potentially external modifiers like `WebRtcApmModifier` via extensions.\n\n\n**Example:**\n\n```csharp\n// Create engine, format, and device\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format);\n\n// Create a SoundPlayer\nusing var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(engine, format, dataProvider);\n\n// Creating a modifier now requires the format context\nvar reverb = new AlgorithmicReverbModifier(format) { RoomSize = 0.8f, Wet = 0.2f };\n\n// Add the reverb modifier to the player\nplayer.AddModifier(reverb);\n\n// Add the player to the device's MasterMixer\nplaybackDevice.MasterMixer.AddComponent(player);\n// ...\n```\n\n## Sound Players (`SoundPlayerBase`, `SoundPlayer`, `SurroundPlayer`)\n\nThese classes provide the logic for playing audio from a data source. Their initialization now requires the `engine` and `format` context.\n\n*   **`SoundPlayerBase`:** The abstract base class that provides common functionality for all sound playback components. It implements `ISoundPlayer` and handles:\n    *   Core playback logic: reading from an `ISoundDataProvider`, managing playback state (Play, Pause, Stop).\n    *   Playback speed adjustment via the `PlaybackSpeed` property.\n    *   Looping with `IsLooping`, `LoopStartSamples`/`Seconds`, and `LoopEndSamples`/`Seconds`.\n    *   Seeking capabilities via `Seek` methods (accepting `TimeSpan`, `float` seconds, or `int` sample offset).\n    *   Volume control (inherited from `SoundComponent`).\n    *   A `PlaybackEnded` event.\n\n*   **`SoundPlayer`:** The standard concrete implementation of `SoundPlayerBase` for typical mono or stereo audio playback.\n\n*   **`SurroundPlayer`:**\n    *   All features from `SoundPlayerBase` and `ISoundPlayer`.\n    *   `SpeakerConfiguration`: Allows you to define the speaker setup (e.g., Stereo, Quad, 5.1, 7.1, or a custom configuration).\n    *   `PanningMethod`: Selects the panning algorithm to use (Linear, EqualPower, or VBAP).\n    *   `ListenerPosition`: Sets the listener's position relative to the speakers.\n    *   `VbapParameters`: Provides fine-grained control over VBAP (Vector Base Amplitude Panning) settings.\n\n\n## Audio Recording (`Recorder`)\n\nThe `Recorder` captures audio from an `AudioCaptureDevice`, directing it to a `Stream` for file storage or a `ProcessCallback` for real-time processing. It integrates `SoundModifier` and `AudioAnalyzer` components for on-the-fly audio manipulation and analysis, and implements `IDisposable` for resource management.\n\n**Key Features:**\n*   **Dual Recording Modes:** Records to an output `Stream` (e.g., file) or processes raw samples via a `ProcessCallback`.\n*   **`StartRecording()`**: Begins audio capture.\n*   **`PauseRecording()`**: Pauses recording; no data processed during this state.\n*   **`ResumeRecording()`**: Resumes a paused recording.\n*   **`StopRecording()`**: Stops recording, finalizes output, and releases resources.\n*   **`State`**: Current recording state (`Playing`, `Paused`, `Stopped`).\n*   **`SampleFormat`**: Sample format for raw audio (e.g., `Float32`), inherited from the capture device.\n*   **`EncodingFormat`**: Encoding format for stream output (e.g., `Wav`). Limited backend support.\n*   **`SampleRate`**: Audio sample rate (e.g., 44100 Hz), inherited from the capture device.\n*   **`Channels`**: Number of audio channels (e.g., 1 for mono, 2 for stereo), inherited from the capture device.\n*   **`Stream`**: Output `Stream` for encoded audio when recording to file. `Stream.Null` if using `ProcessCallback`.\n*   **`ProcessCallback`**: `AudioProcessCallback` delegate invoked with raw audio samples for real-time custom processing. `null` if using `Stream` output.\n*   **`Modifiers`**: Read-only collection of `SoundModifier` components.\n    *   **`AddModifier()`**: Adds a `SoundModifier` to the pipeline.\n    *   **`RemoveModifier()`**: Removes a `SoundModifier` from the pipeline.\n*   **`Analyzers`**: Read-only collection of `AudioAnalyzer` components.\n    *   **`AddAnalyzer()`**: Adds an `AudioAnalyzer` to the pipeline.\n    *   **`RemoveAnalyzer()`**: Removes an `AudioAnalyzer` from the pipeline.\n*   **Resource Management**: Implements `IDisposable` for proper resource cleanup.\n\n\n## Audio Providers (`ISoundDataProvider`)\n\n`ISoundDataProvider` is an interface that defines a standard way to access audio data, regardless of its source. This interface is stable, but its implementations now require an `engine` and `format` context in their constructors to correctly create internal decoders.\n\n**Key Features:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Can be `-1` for live streams.\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio.\n*   `ReadBytes(Span<float> buffer)`: Reads a chunk of audio data into the provided buffer.\n*   `Seek(int offset)`: Moves the read position to a specific offset (in samples).\n*   `EndOfStreamReached`: An event that is raised when the end of the audio data is reached.\n*   `PositionChanged`: An event that is raised when the read position changes.\n*   `Dispose()`: Implementations should release underlying resources (e.g., file streams).\n\n**Built-in Providers:**\n\n*   `AssetDataProvider`: Loads audio data from a byte array or `Stream` entirely into memory.\n*   `StreamDataProvider`: Reads audio data from a `Stream`, decoding on the fly.\n*   `MicrophoneDataProvider`: Provides a live stream from an `AudioCaptureDevice`.\n*   `ChunkedDataProvider`: Efficiently reads large files or streams in chunks.\n*   `NetworkDataProvider`: Reads audio data from a network source (URL, HLS).\n*   `QueueDataProvider`: A thread-safe queue for scenarios where one part of your application generates audio and another part consumes it.\n*   `RawDataProvider`: Reads audio data from a raw PCM stream or various raw array types (`float[]`, `byte[]`, `int[]`, `short[]`).\n\nIt's good practice to dispose of `ISoundDataProvider` instances when they are no longer needed, for example, using a `using` statement.\n\n```csharp\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\n\n// The provider needs the engine and format to create an internal decoder.\nusing var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\n// Use dataProvider\n```\n\n## Audio Encoding/Decoding (`ISoundEncoder`, `ISoundDecoder`)\n\n`ISoundEncoder` and `ISoundDecoder` are interfaces for encoding and decoding audio data to and from different formats. Both are `IDisposable`.\n\n*   **`ISoundEncoder`:** Encodes raw audio samples into a specific format (e.g., WAV, FLAC, MP3). Currently only WAV supported by miniaudio backend.\n*   **`ISoundDecoder`:** Decodes audio data from a specific format into raw audio samples.\n\n**`MiniAudio` Backend:**\n\nThe `MiniAudio` backend provides implementations of these interfaces using the `miniaudio` library:\n\n*   `MiniAudioEncoder`\n*   `MiniAudioDecoder`\n\n\n## Audio Analysis (`AudioAnalyzer`)\n\n`AudioAnalyzer` is an abstract base class for creating components that analyze audio data. Analyzers typically extract information from the audio stream without modifying it.\n\n**Key Features:**\n\n*   **Constructor:** Initialized with an `AudioFormat` and an optional `IVisualizer` to send data to.\n*   `Analyze(Span<float> buffer, int channels)`: An abstract method that derived classes must implement to perform their specific analysis.\n*   `Enabled`: If false, the `Analyze` step might be skipped by the `SoundComponent` it's attached to.\n*   **Integration with Visualizers:** Analyzers are often used in conjunction with `IVisualizer` implementations to display the analysis results visually.\n\n**Built-in Analyzers:**\n\n*   `LevelMeterAnalyzer`: Measures the RMS (root-mean-square) and peak levels of an audio signal.\n*   `SpectrumAnalyzer`: Computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).\n*   `VoiceActivityDetector`: Detects the presence of human voice in an audio stream.\n\n\n## Audio Visualization (`IVisualizer`)\n\n`IVisualizer` is an interface for creating components that visualize audio data. Visualizers typically don't modify the audio stream but instead render a graphical representation of the data. It implements `IDisposable`.\n\n**Key Features:**\n\n*   `Name`: A descriptive name for the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: This method is called by the audio engine to provide the visualizer with a chunk of audio data to process.\n*   `Render(IVisualizationContext context)`: This method is called to render the visualization. It receives an `IVisualizationContext` instance, which provides drawing methods.\n*   `VisualizationUpdated`: An event that is raised when the visualization needs to be redrawn (e.g., when new audio data has been processed).\n*   `Dispose()`: Releases resources held by the visualizer.\n\n## Visualization Context (`IVisualizationContext`):\n\nThis interface provides a set of drawing methods for rendering the visualization. The specific implementation of `IVisualizationContext` will depend on the UI framework you are using.\n\n**Built-in Visualizers:**\n\n*   `LevelMeterVisualizer`: Displays a level meter that shows the current RMS or peak level of the audio.\n*   `SpectrumVisualizer`: Renders a bar graph representing the frequency spectrum of the audio.\n*   `WaveformVisualizer`: Draws the waveform of the audio signal.\n\n## Editing Engine & Persistence (`SoundFlow.Editing`, `SoundFlow.Editing.Persistence`)\n\nSoundFlow features a powerful non-destructive audio editing engine. For a detailed guide, please see the [Editing Engine & Persistence](./editing-engine) documentation.\n\n**Key Concepts:**\n\n*   **`Composition`**: The main container for an audio project, holding multiple `Track`s. It can be treated as an `ISoundDataProvider` and can be rendered or played back.\n*   **`Track`**: Represents a single audio track within a `Composition`. Contains `AudioSegment`s and has its own settings (volume, pan, mute, solo, effects).\n*   **`AudioSegment`**: A clip of audio placed on a `Track`'s timeline. It references a portion of an `ISoundDataProvider` and has its own extensive settings.\n    *   **`AudioSegmentSettings`**: Controls volume, pan, fades (with `FadeCurveType`), looping (`LoopSettings`), reverse playback, speed, and **pitch-preserved time stretching** (via `TimeStretchFactor` or `TargetStretchDuration`, powered by `WsolaTimeStretcher`).\n    *   Supports segment-level modifiers and analyzers.\n*   **Non-Destructive:** Edits do not alter the original audio source files. All operations are applied at runtime during playback or rendering.\n*   **Project Persistence (`CompositionProjectManager`)**:\n    *   Save and load entire compositions as `.sfproj` files.\n    *   **Media Consolidation**: Option to copy all external audio files into an `Assets` folder within the project.\n    *   **Embed Small Media**: Option to embed small audio files (e.g., SFX) directly into the project file.\n    *   **Relink Missing Media**: If an audio file is moved, the project can be relinked to its new location.\n\nThis new engine allows for programmatic creation and manipulation of complex audio timelines, effects processing at multiple levels (segment, track, master), and robust project management."
  },
  {
    "id": 5,
    "slug": "api-reference",
    "version": "1.2.0",
    "title": "API Reference",
    "description": "A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.",
    "navOrder": 5,
    "category": "Core",
    "content": "---\nid: 5\ntitle: API Reference\ndescription: A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.\nnavOrder: 5\ncategory: Core\n---\n\n# API Reference\n\nThis section provides a detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members. It serves as a comprehensive reference for developers working with the SoundFlow library.\n\n## Namespaces\n\nSoundFlow is organized into the following namespaces:\n\n*   **`SoundFlow.Abstracts`:** Contains abstract classes and interfaces that define the core framework of SoundFlow. This includes base classes for the audio engine, audio devices, sound components, modifiers, and analyzers.\n*   **`SoundFlow.Backends`:** Provides backend-specific implementations for audio input/output. The primary backend currently supported is `SoundFlow.Backends.MiniAudio`, which uses the `miniaudio` library.\n*   **`SoundFlow.Components`:** Contains concrete `SoundComponent` classes that provide various audio processing functionalities, including playback, mixing, synthesis, and analysis. It also includes standalone components like the `Recorder`.\n*   **`SoundFlow.Editing`:** Contains classes for non-destructive audio editing, including `Composition`, `Track`, `AudioSegment`, `AudioSegmentSettings`, `LoopSettings`, and `FadeCurveType`.\n*   **`SoundFlow.Editing.Persistence`:** Contains classes for saving and loading audio compositions, such as `CompositionProjectManager` and various project data DTOs.\n*   **`SoundFlow.Enums`:** Contains enumerations used throughout the SoundFlow library to represent different states, options, and capabilities.\n*   **`SoundFlow.Exceptions`:** Contains custom exception classes used for error handling within SoundFlow.\n*   **`SoundFlow.Interfaces`:** Contains interfaces that define contracts for various functionalities, such as audio data providers, encoders, decoders, and visualizers.\n*   **`SoundFlow.Modifiers`:** Contains concrete `SoundModifier` classes that implement various audio effects.\n*   **`SoundFlow.Providers`:** Contains classes that implement the `ISoundDataProvider` interface, providing ways to load audio data from different sources.\n*   **`SoundFlow.Structs`:** Contains custom struct types used within SoundFlow, often for interop or specific data representation.\n*   **`SoundFlow.Utils`:** Contains utility classes and extension methods that provide helpful functionalities for working with audio data and performing common operations.\n*   **`SoundFlow.Visualization`:** Contains classes related to audio visualization, including analyzers and visualizers.\n*   **`SoundFlow.Extensions`:** Namespace for official extensions.\n    *   **`SoundFlow.Extensions.WebRtc.Apm`:** Provides integration with the WebRTC Audio Processing Module for features like echo cancellation, noise suppression, and automatic gain control.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Components`:** Contains components utilizing the WebRTC APM, like `NoiseSuppressor`.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Modifiers`:** Contains modifiers utilizing the WebRTC APM, like `WebRtcApmModifier`.\n\n## Key Classes and Interfaces\n\nBelow is a summary of the key classes and interfaces in SoundFlow.\n\n\n### Abstracts\n\n| Class/Interface                                         | Description                                                                                                                                                                           |\n|---------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`AudioAnalyzer`](#abstracts-audioanalyzer)             | Abstract base class for audio analysis components. Inherits from `SoundComponent`.                                                                                                    |\n| [`AudioEngine`](#abstracts-audioengine)                 | Abstract base class for an audio engine. Manages audio device lifecycle, provides encoding/decoding, and is the root context.                                                         |\n| [`AudioDevice`](#abstracts-audiodevice)                 | Abstract base class for an initialized audio device (playback or capture).                                                                                                            |\n| [`AudioPlaybackDevice`](#abstracts-audioplaybackdevice) | Abstract class representing an initialized output/playback device. Contains a `MasterMixer`.                                                                                          |\n| [`AudioCaptureDevice`](#abstracts-audiocapturedevice)   | Abstract class representing an initialized input/capture device. Exposes an `OnAudioProcessed` event.                                                                                 |\n| [`FullDuplexDevice`](#abstracts-fullduplexdevice)       | A high-level abstraction managing a paired playback and capture device for simultaneous I/O.                                                                                          |\n| [`DeviceConfig`](#abstracts-deviceconfig)               | Abstract base class for backend-specific device configuration objects.                                                                                                                |\n| [`SoundComponent`](#abstracts-soundcomponent)           | Abstract base class for all audio processing units in SoundFlow. Represents a node in the audio graph.                                                                                |\n| [`SoundModifier`](#abstracts-soundmodifier)             | Abstract base class for audio effects that modify audio samples.                                                                                                                      |\n| [`SoundPlayerBase`](#abstracts-soundplayerbase)         | Abstract base class providing common functionality for sound playback components. Inherits from `SoundComponent` and implements `ISoundPlayer`. Includes support for time-stretching. |\n\n### Backends.MiniAudio\n\n| Class/Interface                                                     | Description                                                                                                                  |\n|---------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n| [`MiniAudioEngine`](#backendsminiaudio-miniaudioengine)             | `AudioEngine` implementation that uses the `miniaudio` library for audio I/O.                                                |\n| [`MiniAudioDeviceConfig`](#backendsminiaudio-miniaudiodeviceconfig) | `DeviceConfig` implementation for MiniAudio, providing detailed, backend-specific settings for WASAPI, CoreAudio, ALSA, etc. |\n| [`MiniAudioDecoder`](#backendsminiaudio-miniaudiodecoder)           | `ISoundDecoder` implementation using the `miniaudio` library.                                                                |\n| [`MiniAudioEncoder`](#backendsminiaudio-miniaudiodecoder)           | `ISoundEncoder` implementation using the `miniaudio` library.                                                                |\n\n### Components\n\n| Class/Interface                                                | Description                                                                                                                                            |\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`EnvelopeGenerator`](#components-envelopegenerator)           | `SoundComponent` that generates an ADSR (Attack, Decay, Sustain, Release) envelope signal.                                                             |\n| [`Filter`](#components-filter)                                 | `SoundComponent` that applies a digital filter (low-pass, high-pass, band-pass, notch) to the audio signal.                                            |\n| [`LowFrequencyOscillator`](#components-lowfrequencyoscillator) | `SoundComponent` that generates a low-frequency oscillator (LFO) signal with various waveforms.                                                        |\n| [`Mixer`](#components-mixer)                                   | `SoundComponent` that mixes multiple audio streams together. The `Mixer.Master` property provides access to the default root mixer.                    |\n| [`Oscillator`](#components-oscillator)                         | `SoundComponent` that generates various waveforms (sine, square, sawtooth, triangle, noise, pulse).                                                    |\n| [`Recorder`](#components-recorder)                             | `SoundComponent` that captures audio input from a recording device and allows saving it to a stream or processing it via a callback.                     |\n| [`SoundPlayer`](#components-soundplayer)                       | `SoundPlayerBase` implementation that plays audio from an `ISoundDataProvider`.                                                                        |\n| [`SurroundPlayer`](#components-surroundplayer)                 | `SoundPlayerBase` implementation that extends `SoundPlayer` to support surround sound configurations with customizable speaker positions, delays, and panning methods. |\n| [`VoiceActivityDetector`](#components-voiceactivitydetector)   | An `AudioAnalyzer` that detects human voice in an audio stream, featuring configurable activation and hangover times to prevent rapid state changes.   |\n| [`WsolaTimeStretcher`](#components-wsolatimestretcher)\t | Implements WSOLA algorithm for real-time, pitch-preserved time stretching. Used internally by `AudioSegment`. |\n\n### Editing\n\n| Class/Interface                                       | Description                                                                                                                                            |\n| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`Composition`](#editing-composition)                 | Top-level container for audio tracks, representing a complete project. Implements `ISoundDataProvider` for rendering. `IDisposable`.                 |\n| [`Track`](#editing-track)                             | Represents a single audio track within a `Composition`, containing `AudioSegment`s and track-level settings.                                         |\n| [`AudioSegment`](#editing-audiosegment)               | Represents a single audio clip on a `Track`'s timeline, referencing a portion of an audio source and applying playback settings. `IDisposable`.      |\n| [`AudioSegmentSettings`](#editing-audiosegmentsettings) | Configurable settings for an `AudioSegment` (volume, pan, fades, loop, reverse, speed, time stretch, modifiers, analyzers).                          |\n| [`TrackSettings`](#editing-tracksettings)             | Configurable settings for a `Track` (volume, pan, mute, solo, enabled, modifiers, analyzers).                                                          |\n| [`LoopSettings`](#editing-loopsettings)               | (struct) Defines looping behavior for an `AudioSegment` (repetitions, target duration).                                                                |\n| [`FadeCurveType`](#editing-fadecurvetype)             | (enum) Defines curve types for fade effects (Linear, Logarithmic, SCurve).                                                                             |\n\n### Editing.Persistence\n\n| Class/Interface                                                        | Description                                                                                                                            |\n| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| [`CompositionProjectManager`](#editingpersistence-compositionprojectmanager) | Static class for saving and loading `Composition` projects to/from `.sfproj` files. Handles media consolidation and relinking.       |\n| [`ProjectData`](#editingpersistence-projectdata)                         | DTO representing the root of a saved project file.                                                                                     |\n| [`ProjectTrack`](#editingpersistence-projecttrack)                       | DTO for a `Track` within a saved project.                                                                                              |\n| [`ProjectSegment`](#editingpersistence-projectsegment)                   | DTO for an `AudioSegment` within a saved project.                                                                                      |\n| [`ProjectAudioSegmentSettings`](#editingpersistence-projectaudiosegmentsettings) | DTO for `AudioSegmentSettings` within a saved project.                                                                                 |\n| [`ProjectTrackSettings`](#editingpersistence-projecttracksettings)       | DTO for `TrackSettings` within a saved project.                                                                                        |\n| [`ProjectSourceReference`](#editingpersistence-projectsourcereference)   | DTO representing how an audio source is referenced in a project (file path, embedded data, consolidation).                             |\n| [`ProjectEffectData`](#editingpersistence-projecteffectdata)             | DTO for serializing `SoundModifier` or `AudioAnalyzer` instances (type name, parameters).                                              |\n\n### Enums\n\n| Enum                                             | Description                                                                                                                                 |\n| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Capability`](#enums-capability)                | Specifies the capabilities of an audio device (Playback, Record, Mixed, Loopback).                                                          |\n| [`DeviceType`](#enums-devicetype)                | Specifies the type of audio device (Playback, Capture).                                                                                     |\n| [`EncodingFormat`](#enums-encodingformat)        | Specifies the audio encoding format to use (e.g., WAV, FLAC, MP3, Vorbis). *Note: MiniAudio backend currently only supports WAV for encoding.* |\n| [`PlaybackState`](#enums-playbackstate)          | Specifies the current playback state of a player or recorder (Stopped, Playing, Paused).                                                      |\n| [`Result`](#enums-result)                        | Represents the result of a native operation, including success and various error codes.                                                     |\n| [`SampleFormat`](#enums-sampleformat)            | Specifies the format of audio samples (e.g., U8, S16, S24, S32, F32).                                                                     |\n| [`FilterType`](#enums-filtertype)                | Specifies the type of filter to use in the `Filter` and `ParametricEqualizer` components (Peaking, LowShelf, HighShelf, BandPass, Notch, LowPass, HighPass) |\n| [`EnvelopeGenerator.EnvelopeState`](#enums-envelopegenerator-envelopestate) | Specifies the current state of the envelope generator (Idle, Attack, Decay, Sustain, Release) |\n| [`EnvelopeGenerator.TriggerMode`](#enums-envelopegenerator-triggermode) | Specifies how the envelope generator is triggered (NoteOn, Gate, Trigger) |\n| [`LowFrequencyOscillator.WaveformType`](#enums-lowfrequencyoscillator-waveformtype) | Specifies the waveform type for the low-frequency oscillator (Sine, Square, Triangle, Sawtooth, ReverseSawtooth, Random, SampleAndHold) |\n| [`LowFrequencyOscillator.TriggerMode`](#enums-lowfrequencyoscillator-triggermode) | Specifies how the LFO is triggered (FreeRunning, NoteTrigger) |\n| [`Oscillator.WaveformType`](#enums-oscillator-waveformtype) | Specifies the waveform type for the oscillator (Sine, Square, Sawtooth, Triangle, Noise, Pulse) |\n| [`SurroundPlayer.SpeakerConfiguration`](#enums-surroundplayer-speakerconfiguration) | Specifies the speaker configuration for the surround player (Stereo, Quad, Surround51, Surround71, Custom) |\n| [`SurroundPlayer.PanningMethod`](#enums-surroundplayer-panningmethod) | Specifies the panning method for the surround player (Linear, EqualPower, Vbap) |\n| [`FadeCurveType`](#editing-fadecurvetype)\t   | Specifies curve types for fade effects (Linear, Logarithmic, SCurve). |\n| **`SoundFlow.Extensions.WebRtc.Apm` Enums**          |                                                                                                                                             |\n| [`ApmError`](#extensions-webrtc-apm-apmerror)    | Error codes returned by the WebRTC Audio Processing Module.                                                                                |\n| [`NoiseSuppressionLevel`](#extensions-webrtc-apm-noisesuppressionlevel) | Specifies noise suppression levels (Low, Moderate, High, VeryHigh).                                                                  |\n| [`GainControlMode`](#extensions-webrtc-apm-gaincontrolmode) | Specifies gain controller modes (AdaptiveAnalog, AdaptiveDigital, FixedDigital).                                                        |\n| [`DownmixMethod`](#extensions-webrtc-apm-downmixmethod) | Specifies methods for downmixing audio channels (AverageChannels, UseFirstChannel).                                                    |\n| [`RuntimeSettingType`](#extensions-webrtc-apm-runtimesettingtype) | Specifies types of runtime settings for the WebRTC APM.                                                                        |\n\n### Exceptions\n\n| Class                                           | Description                                                                                   |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [`BackendException`](#exceptions-backendexception) | Thrown when an error occurs in a specific audio backend.                                     |\n\n### Extensions.WebRtc.Apm\n\n| Class/Interface                                                                 | Description                                                                                                                                       |\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AudioProcessingModule`](#extensions-webrtc-apm-audioprocessingmodule)         | Provides access to the native WebRTC Audio Processing Module for advanced audio processing like AEC, NS, AGC.                                     |\n| [`ApmConfig`](#extensions-webrtc-apm-apmconfig)                                 | Represents a configuration for the `AudioProcessingModule`, allowing enabling/disabling and setting parameters for various APM features.            |\n| [`StreamConfig`](#extensions-webrtc-apm-streamconfig)                           | Represents a stream configuration (sample rate, channels) for audio processing within the APM.                                                    |\n| [`ProcessingConfig`](#extensions-webrtc-apm-processingconfig)                   | Holds multiple `StreamConfig` instances for input, output, and reverse streams for the APM.                                                       |\n| **Components Namespace**                                                        |                                                                                                                                                   |\n| [`NoiseSuppressor`](#extensions-webrtc-apm-components-noisesuppressor)          | A component for offline/batch noise suppression using WebRTC APM, processing audio from an `ISoundDataProvider`.                                  |\n| **Modifiers Namespace**                                                         |                                                                                                                                                   |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier)       | A `SoundModifier` that applies WebRTC APM features (AEC, NS, AGC, etc.) in real-time to an audio stream within the SoundFlow graph. Configurable. |\n\n### Interfaces\n\n| Interface                                           | Description                                                                                                                                  |\n| --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ISoundDataProvider`](#interfaces-isounddataprovider) | Defines a standard way to access audio data from various sources. Implements `IDisposable`.                                                |\n| [`ISoundDecoder`](#interfaces-isounddecoder)         | Defines the contract for decoding audio data from a specific format into raw audio samples.                                                 |\n| [`ISoundEncoder`](#interfaces-isoundencoder)         | Defines the contract for encoding raw audio samples into a specific format.                                                                 |\n| [`ISoundPlayer`](#interfaces-isoundplayer)           | Defines the contract for controlling audio playback (Play, Pause, Stop, Seek, Looping, Speed, Volume).                                       |\n| [`IVisualizationContext`](#interfaces-ivisualizationcontext) | Provides drawing methods for rendering audio visualizations. The implementation depends on the specific UI framework used.               |\n| [`IVisualizer`](#interfaces-ivisualizer)             | Defines the contract for components that visualize audio data.                                                                            |\n\n### Modifiers\n\n| Class                                                               | Description                                                                                                                                                                                           |\n| ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AlgorithmicReverbModifier`](#modifiers-algorithmicreverbmodifier) | Simulates reverberation using a network of comb and all-pass filters. Now supports multi-channel processing.                                                                                          |\n| [`BassBoosterModifier`](#modifiers-bassboostmodifier)               | Enhances low-frequency content using a resonant low-pass filter.                                                                                                                                      |\n| [`ChorusModifier`](#modifiers-chorusmodifier)                       | Creates a chorus effect by mixing delayed and modulated copies of the signal.                                                                                                                         |\n| [`CompressorModifier`](#modifiers-compressormodifier)               | Reduces the dynamic range of the audio signal using a compressor algorithm.                                                                                                                           |\n| [`DelayModifier`](#modifiers-delaymodifier)                         | Applies a delay effect with feedback and optional low-pass filtering of the delayed signal.                                                                                                           |\n| [`FrequencyBandModifier`](#modifiers-frequencybandmodifier)         | Allows boosting or cutting specific frequency bands using a combination of low-pass and high-pass filters.                                                                                            |\n| [`ParametricEqualizer`](#modifiers-parametricequalizer)             | Provides precise control over the frequency spectrum with multiple configurable bands, each of which can be set as a peaking, low-shelf, high-shelf, band-pass, notch, low-pass, or high-pass filter. |\n| [`MultiChannelChorusModifier`](#modifiers-multichannelchorusmodifier) | Creates a chorus effect with independent processing for each channel, allowing for rich spatial effects.                                                                                              |\n| [`TrebleBoostModifier`](#modifiers-trebleboostmodifier)             | Enhances high-frequency content using a high-pass filter.                                                                                                                                             |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier) (from `SoundFlow.Extensions.WebRtc.Apm.Modifiers`) | Applies WebRTC APM features like echo cancellation, noise suppression, and AGC in real-time. |\n\n### Providers\n\n| Class                                                         | Description                                                                                                                                  |\n| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AssetDataProvider`](#providers-assetdataprovider)           | `ISoundDataProvider` implementation that reads audio data from a byte array (useful for in-memory assets). Implements `IDisposable`.            |\n| [`StreamDataProvider`](#providers-streamdataprovider)         | `ISoundDataProvider` that reads audio data from a generic `Stream` on-demand (supports seeking if the stream is seekable). Implements `IDisposable`. |\n| [`MicrophoneDataProvider`](#providers-microphonedataprovider) | `ISoundDataProvider` that captures and provides audio data from an `AudioCaptureDevice` in real-time. Implements `IDisposable`.                 |\n| [`ChunkedDataProvider`](#providers-chunkeddataprovider)       | `ISoundDataProvider` that reads and decodes audio data from a file or stream in chunks, improving efficiency for large files. Implements `IDisposable`. |\n| [`NetworkDataProvider`](#providers-networkdataprovider)       | `ISoundDataProvider` that provides audio data from a network source (direct URL or HLS playlist). Implements `IDisposable`.                    |\n| [`QueueDataProvider`](#providers-queuedataprovider)           | `ISoundDataProvider` fed by an external source in real-time, ideal for generated or procedural audio. Implements `IDisposable`.                |\n| [`RawDataProvider`](#providers-rawdataprovider)               | `ISoundDataProvider` for reading raw PCM audio data from a stream or byte/float/int/short array. Implements `IDisposable`.                     |\n\n### Structs\n\n| Struct                                       | Description                                                                                    |\n| -------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| [`AudioFormat`](#structs-audioformat)        | (record struct) Represents the format of an audio stream (sample format, channels, sample rate). |\n| [`DeviceInfo`](#structs-deviceinfo)          | Represents information about an audio device, including ID, name, and supported formats.         |\n| [`NativeDataFormat`](#structs-nativedataformat) | Represents a native data format supported by an audio device (format, channels, sample rate). |\n\n### Utils\n\n| Class                                       | Description                                                                                                                                                                                                 |\n| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Extensions`](#utils-extensions)           | Provides extension methods for working with audio data and other utility functions, including `ReadArray<T>` for reading structures from native memory.                                                                                                                             |\n| [`MathHelper`](#utils-mathhelper)           | Provides mathematical functions and algorithms used in audio processing, including optimized FFT, window functions, `Mod`, and `PrincipalAngle`. |\n\n### Visualization\n\n| Class/Interface                                         | Description                                                                                                                                                                                      |\n| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`LevelMeterAnalyzer`](#visualization-levelmeteranalyzer) | `AudioAnalyzer` that calculates the RMS (root mean square) and peak levels of an audio signal.                                                                                                  |\n| [`LevelMeterVisualizer`](#visualization-levelmetervisualizer) | `IVisualizer` that displays a level meter showing the current RMS or peak level of the audio.                                                                                                 |\n| [`SpectrumAnalyzer`](#visualization-spectrumanalyzer)    | `AudioAnalyzer` that computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).                                                                                         |\n| [`SpectrumVisualizer`](#visualization-spectrumvisualizer)  | `IVisualizer` that renders a bar graph representing the frequency spectrum of the audio.                                                                                                        |\n| [`WaveformVisualizer`](#visualization-waveformvisualizer)  | `IVisualizer` that draws the waveform of the audio signal.                                                                                                                                     |\n\n## Detailed Class and Interface Documentation\n\nThis section provides more in-depth information about some of the key classes and interfaces.\n\n### Abstracts `AudioAnalyzer`\n\n```csharp\npublic abstract class AudioAnalyzer\n{\n    protected AudioAnalyzer(AudioFormat format, IVisualizer? visualizer = null);\n\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n    public AudioFormat Format { get; }\n\n    public void Process(Span<float> buffer, int channels);\n    protected abstract void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Enabled`: Gets or sets whether the analyzer is active. If false, `Analyze` is skipped.\n*   `Format`: The audio format the analyzer is configured to process.\n\n**Methods:**\n\n*   `Process(Span<float> buffer, int channels)`: Processes the audio data, calling `Analyze` and then sending data to the attached visualizer.\n*   `Analyze(Span<float> buffer, int channels)`: Abstract method to be implemented by derived classes to perform audio analysis on the provided buffer.\n\n### Abstracts `AudioEngine`\n\n```csharp\npublic abstract class AudioEngine : IDisposable\n{\n    protected AudioEngine();\n\n    public DeviceInfo[] PlaybackDevices { get; protected set; }\n    public DeviceInfo[] CaptureDevices { get; protected set; }\n    public bool IsDisposed { get; private set; }\n\n    ~AudioEngine();\n\n    public void SoloComponent(SoundComponent component);\n    public void UnsoloComponent(SoundComponent component);\n    public SoundComponent? GetSoloedComponent();\n    public abstract ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format);\n    public abstract ISoundDecoder CreateDecoder(Stream stream, AudioFormat format);\n    public abstract AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public abstract FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);\n    public abstract void UpdateDevicesInfo();\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `PlaybackDevices`: An array of available playback devices.\n*   `CaptureDevices`: An array of available capture devices.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n\n**Methods:**\n\n*   `SoloComponent(SoundComponent component)`: Solos a specific component, muting all others.\n*   `UnsoloComponent(SoundComponent component)`: Removes the solo status from a component.\n*   `GetSoloedComponent()`: Returns the currently soloed component, if any.\n*   `CreateEncoder(...)`: Creates a backend-specific sound encoder.\n*   `CreateDecoder(...)`: Creates a backend-specific sound decoder.\n*   `InitializePlaybackDevice(...)`: Initializes and returns a new playback device.\n*   `InitializeCaptureDevice(...)`: Initializes and returns a new capture device.\n*   `InitializeFullDuplexDevice(...)`: Initializes a high-level full-duplex device for simultaneous I/O.\n*   `InitializeLoopbackDevice(...)`: Initializes a device for loopback recording of system audio.\n*   `SwitchDevice(...)`: Switches an active device to a new physical device, preserving its state (components or subscribers).\n*   `UpdateDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.\n*   `Dispose()`: Disposes the engine and all associated resources.\n\n### Abstracts `AudioDevice`\n\n```csharp\npublic abstract class AudioDevice : IDisposable\n{\n    public AudioEngine Engine { get; }\n    public DeviceInfo? Info { get; internal init; }\n    public DeviceConfig Config { get; internal init; }\n    public Capability Capability { get; internal init; }\n    public AudioFormat Format { get; }\n    public bool IsRunning { get; protected set; }\n    public bool IsDisposed { get; protected set; }\n\n    public event EventHandler? OnDisposed;\n\n    public abstract void Start();\n    public abstract void Stop();\n    public abstract void Dispose();\n}\n```\n\n**Description:** An abstract base class representing an initialized audio device, which is managed by an `AudioEngine`. This class encapsulates the common state and behavior for both playback and capture devices, serving as the foundation for all device interactions within the framework.\n\n**Properties:**\n\n*   `Engine`: Gets the parent `AudioEngine` that manages this device instance.\n*   `Info`: Gets the informational struct (`DeviceInfo`) for the physical device being used. This contains details like the device name and ID. `null` if a default device was used without specific info.\n*   `Config`: Gets the `DeviceConfig` object used to initialize the device. This holds any backend-specific configuration settings.\n*   `Capability`: Gets the capability of this device (e.g., `Playback`, `Record`, `Loopback`).\n*   `Format`: Gets the `AudioFormat` (sample rate, channels, bit depth) that the device was initialized with.\n*   `IsRunning`: Gets a value indicating whether the device is currently started and processing audio.\n*   `IsDisposed`: Gets a value indicating whether this device instance has been disposed and can no longer be used.\n\n**Events:**\n\n*   `OnDisposed`: This event is raised when the device's `Dispose()` method is called, signaling that it is being shut down and its resources are being released.\n\n**Methods:**\n\n*   `Start()`: Abstract method that must be implemented by derived classes to start the audio stream for the device.\n*   `Stop()`: Abstract method that must be implemented by derived classes to stop the audio stream for the device.\n*   `Dispose()`: Abstract method to release all resources used by the audio device.\n\n### Abstracts `AudioPlaybackDevice`\n\n```csharp\npublic abstract class AudioPlaybackDevice : AudioDevice\n{\n    public Mixer MasterMixer { get; }\n}\n```\n\n**Description:** An abstract class that represents an initialized playback (output) audio device. It inherits from `AudioDevice` and extends it with functionality specific to audio output.\n\n**Properties:**\n\n*   `MasterMixer`: Gets the master `Mixer` for this device. All audio to be played on this device must be routed through this mixer. You can connect various `SoundComponent`s (like `SoundPlayer`, `Oscillator`, or other `Mixer`s) to this master mixer to combine them into a final output signal.\n\n### Abstracts `AudioCaptureDevice`\n\n```csharp\npublic abstract class AudioCaptureDevice : AudioDevice\n{\n    public event AudioProcessCallback? OnAudioProcessed;\n    internal Delegate[] GetEventSubscribers();\n}\n```\n\n**Description:** An abstract class that represents an initialized capture (input) audio device. It inherits from `AudioDevice` and provides the core mechanism for receiving audio data from an input source like a microphone.\n\n**Events:**\n\n*   `OnAudioProcessed`: This event is the primary way to receive audio data from the capture device. It is raised by the backend whenever a new chunk of audio samples has been captured. Subscribe to this event to process live audio input. The event delegate is `AudioProcessCallback(Span<float> samples, Capability capability)`.\n\n**Methods:**\n\n*   `GetEventSubscribers()`: (Internal) An internal method used by the `AudioEngine` to retrieve the list of subscribers to the `OnAudioProcessed` event. This is crucial for the device switching functionality, allowing event subscriptions to be preserved when moving from one physical device to another.\n\n### Abstracts `FullDuplexDevice`\n\n```csharp\npublic sealed class FullDuplexDevice : AudioDevice, IDisposable\n{\n    public AudioPlaybackDevice PlaybackDevice { get; }\n    public AudioCaptureDevice CaptureDevice { get; }\n    public Mixer MasterMixer => PlaybackDevice.MasterMixer;\n\n    public event AudioProcessCallback? OnAudioProcessed;\n\n    public override void Start();\n    public override void Stop();\n    public override void Dispose();\n}\n```\n\n**Description:** A high-level, sealed class that simplifies full-duplex (simultaneous input and output) audio operations. It internally manages a paired `AudioPlaybackDevice` and `AudioCaptureDevice`, making it ideal for applications like live effects processing, VoIP, or real-time instrument monitoring where you need to listen to an input while producing an output.\n\n**Properties:**\n\n*   `PlaybackDevice`: Gets the underlying `AudioPlaybackDevice` instance used for audio output.\n*   `CaptureDevice`: Gets the underlying `AudioCaptureDevice` instance used for audio input.\n*   `MasterMixer`: A convenient shortcut to access the `MasterMixer` of the underlying `PlaybackDevice`. This is where you should route all audio you want to play out.\n\n**Events:**\n\n*   `OnAudioProcessed`: An event that is raised when audio data is captured from the input device. This event is a direct pass-through to the `OnAudioProcessed` event of the underlying `CaptureDevice`.\n\n**Methods:**\n\n*   `Start()`: Starts both the underlying capture and playback devices simultaneously.\n*   `Stop()`: Stops both the underlying capture and playback devices simultaneously.\n*   `Dispose()`: Stops and disposes of all resources, including the underlying `PlaybackDevice` and `CaptureDevice`.\n\n### Abstracts `DeviceConfig`\n\n```csharp\npublic abstract class DeviceConfig;\n```\n**Description:** A marker base class for creating backend-specific device configuration objects. This allows passing detailed, implementation-specific settings during device initialization. See `MiniAudioDeviceConfig` for an example.\n\n### Abstracts `SoundComponent`\n\n```csharp\npublic abstract class SoundComponent : IDisposable\n{\n    protected SoundComponent(AudioEngine engine, AudioFormat format);\n\n    public AudioEngine Engine { get; }\n    public AudioFormat Format { get; }\n    public virtual string Name { get; set; }\n    public Mixer? Parent { get; set; }\n    public virtual float Volume { get; set; }\n    public virtual float Pan { get; set; } // Range 0.0 (Left) to 1.0 (Right)\n    public virtual bool Enabled { get; set; }\n    public virtual bool Solo { get; set; }\n    public virtual bool Mute { get; set; }\n    public bool IsDisposed { get; private set; }\n\n    public IReadOnlyList<SoundComponent> Inputs { get; }\n    public IReadOnlyList<SoundModifier> Modifiers { get; }\n    public IReadOnlyList<AudioAnalyzer> Analyzers { get; }\n\n    public void ConnectInput(SoundComponent input);\n    public void DisconnectInput(SoundComponent input);\n    public void AddModifier(SoundModifier modifier);\n    public void RemoveModifier(SoundModifier modifier);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    internal void Process(Span<float> outputBuffer, int channels);\n    protected abstract void GenerateAudio(Span<float> buffer, int channels);\n    public virtual void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Engine`: The engine context this component belongs to.\n*   `Format`: The audio format of this component.\n*   `Name`: The name of the component.\n*   `Parent`: The parent mixer of this component.\n*   `Inputs`: Read-only list of connected input components.\n*   `Modifiers`: Read-only list of applied modifiers.\n*   `Analyzers`: Read-only list of attached audio analyzers.\n*   `Volume`: The volume of the component's output.\n*   `Pan`: The panning of the component's output (0.0 for full left, 0.5 for center, 1.0 for full right).\n*   `Enabled`: Whether the component is enabled.\n*   `Solo`: Whether the component is soloed.\n*   `Mute`: Whether the component is muted.\n*   `IsDisposed`: Indicates whether the component has been disposed.\n\n**Methods:**\n\n*   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n*   `DisconnectInput(SoundComponent input)`: Disconnects an input from this component.\n*   `AddModifier(SoundModifier modifier)`: Adds a sound modifier to the component.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a sound modifier from the component.\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an audio analyzer to the component.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an audio analyzer from the component.\n*   `Process(Span<float> outputBuffer, int channels)`: Processes the component's audio, applying modifiers and handling input/output connections.\n*   `GenerateAudio(Span<float> buffer, int channels)`: Abstract method that derived classes must implement to generate or modify audio data.\n*   `Dispose()`: Disposes the component and disconnects it from the audio graph.\n\n### Abstracts `SoundModifier`\n\n```csharp\npublic abstract class SoundModifier\n{\n    public SoundModifier();\n\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    public abstract float ProcessSample(float sample, int channel);\n    public virtual void Process(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n*   `Enabled`: Gets or sets whether the modifier is active and should process audio. Defaults to true.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Abstract method to be implemented by derived classes to process a single audio sample.\n*   `Process(Span<float> buffer)`: Processes a buffer of audio data. Applies `ProcessSample` to each sample if not overridden.\n\n### Abstracts `SoundPlayerBase`\n\n```csharp\npublic abstract class SoundPlayerBase : SoundComponent, ISoundPlayer\n{\n    protected SoundPlayerBase(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n\n    public float PlaybackSpeed { get; set; }\n    public PlaybackState State { get; private set; }\n    public bool IsLooping { get; set; }\n    public float Time { get; }\n    public float Duration { get; }\n    public int LoopStartSamples { get; }\n    public int LoopEndSamples { get; }\n    public float LoopStartSeconds { get; }\n    public float LoopEndSeconds { get; }\n    // Volume is inherited from SoundComponent\n\n    public event EventHandler<EventArgs>? PlaybackEnded;\n\n    protected override void GenerateAudio(Span<float> output, int channels);\n    protected virtual void HandleEndOfStream(Span<float> remainingOutputBuffer, int channels);\n    protected virtual void OnPlaybackEnded();\n\n    public void Play();\n    public void Pause();\n    public void Stop();\n    public bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    public bool Seek(float time);\n    public bool Seek(int sampleOffset);\n    public void SetLoopPoints(float startTime, float? endTime = -1f);\n    public void SetLoopPoints(int startSample, int endSample = -1);\n    public void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `PlaybackSpeed`: Gets or sets the playback speed (1.0 is normal). Values other than 1.0 use a WSOLA time stretcher for pitch preservation.\n*   `State`: Gets the current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Gets or sets whether looping is enabled.\n*   `Time`: Gets the current playback position in seconds, affected by `PlaybackSpeed`.\n*   `Duration`: Gets the total duration of the audio in seconds.\n*   `LoopStartSamples`: Gets the loop start point in samples.\n*   `LoopEndSamples`: Gets the loop end point in samples (-1 for end of audio).\n*   `LoopStartSeconds`: Gets the loop start point in seconds.\n*   `LoopEndSeconds`: Gets the loop end point in seconds (-1 for end of audio).\n*   `Volume`: (Inherited from `SoundComponent`) Gets or sets the volume of the player.\n\n**Events:**\n\n*   `PlaybackEnded`: Occurs when playback reaches the end of the audio (not raised during looping).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> output)`: (Protected Override) Core audio generation logic, handles reading from data provider, resampling for playback speed, and looping.\n*   `HandleEndOfStream(Span<float> remainingOutputBuffer)`: (Protected Virtual) Handles logic when the data provider reaches its end (looping or stopping).\n*   `OnPlaybackEnded()`: (Protected Virtual) Invokes the `PlaybackEnded` event.\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to a specific time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to a specific time in seconds. Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to a specific sample offset. Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures loop points using start/end times in seconds.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures loop points using start/end sample indices.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures loop points using `TimeSpan`.\n*   `Dispose()`: Disposes the player and its underlying data provider.\n\n### Abstracts `WsolaTimeStretcher`\n```csharp\npublic class WsolaTimeStretcher\n{\n    public WsolaTimeStretcher(int initialChannels = 2, float initialSpeed = 1.0f);\n\n    public void SetChannels(int channels);\n    public void SetSpeed(float speed);\n    public int MinInputSamplesToProcess { get; }\n    public void Reset();\n    public float GetTargetSpeed();\n    public int Process(ReadOnlySpan<float> input, Span<float> output, out int samplesConsumedFromInputBuffer, out int sourceSamplesRepresentedByOutput);\n    public int Flush(Span<float> output);\n}\n```\n**Description:** Implements the WSOLA (Waveform Similarity Overlap-Add) algorithm for real-time, pitch-preserved time stretching of audio. Allows changing playback speed without altering pitch. Used internally by `SoundPlayerBase` and `AudioSegment`.\n\n\n### Backends.MiniAudio `MiniAudioDecoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioDecoder : ISoundDecoder\n{\n    internal MiniAudioDecoder(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n    public int Length { get; private set; } // Length can be updated after initial check\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int Decode(Span<float> samples);\n    public void Dispose();\n    public bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n* `IsDisposed`: Indicates whether the decoder has been disposed.\n* `Length`: The total length of the decoded audio data in samples. *Note: Can be updated after initial checks if the stream length was not immediately available.*\n* `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n* `EndOfStreamReached`: Occurs when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n* `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer. Internally synchronized.\n* `Dispose()`: Releases the resources used by the decoder.\n* `Seek(int offset)`: Seeks to the specified offset within the audio stream (in samples). Internally synchronized.\n\n### Backends.MiniAudio `MiniAudioEncoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioEncoder : ISoundEncoder\n{\n    public MiniAudioEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n\n    public void Dispose();\n    public int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the encoder.\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples and writes them to the output file or stream.\n\n### Backends.MiniAudio `MiniAudioEngine`\n\n```csharp\npublic sealed class MiniAudioEngine : AudioEngine\n{\n    public MiniAudioEngine();\n\n    // Inherits all public methods from AudioEngine\n    public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);\n    public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format);\n    public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format);\n    public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);\n    public override void UpdateDevicesInfo();\n}\n```\n\n**Description:** This is the concrete implementation of the abstract `AudioEngine` class using the powerful `miniaudio` C library as its backend. It is responsible for all low-level audio I/O operations, including device discovery, initialization, and data processing callbacks. It manages the native `miniaudio` context and serves as the factory for creating `MiniAudioDecoder` and `MiniAudioEncoder` instances. Because it handles the native interop, this class ensures that SoundFlow is cross-platform, supporting Windows, macOS, Linux, Android, and iOS.\n\n**Methods:**\n\n*   `InitializePlaybackDevice(...)`: Creates and returns a `MiniAudioPlaybackDevice`, which represents an active connection to a physical sound output device.\n*   `InitializeCaptureDevice(...)`: Creates and returns a `MiniAudioCaptureDevice` for a physical sound input device.\n*   `InitializeFullDuplexDevice(...)`: Creates and returns a `FullDuplexDevice` that manages both a playback and capture stream simultaneously.\n*   `InitializeLoopbackDevice(...)`: Creates a special capture device that records the system's audio output. On Windows, this uses the WASAPI loopback feature.\n*   `CreateEncoder(...)`: Returns a `MiniAudioEncoder` instance, currently supporting `.wav` file encoding.\n*   `CreateDecoder(...)`: Returns a `MiniAudioDecoder` instance, capable of decoding various audio formats like `.wav`, `.mp3`, and `.flac`.\n*   `SwitchDevice(...)`: Implements the logic to seamlessly switch between physical audio devices at runtime, preserving the state of the audio graph.\n*   `UpdateDevicesInfo()`: Communicates with the `miniaudio` backend to refresh the lists of available playback and capture devices.\n\n---\n\n### Backends.MiniAudio `MiniAudioDeviceConfig`\n\n```csharp\npublic class MiniAudioDeviceConfig : DeviceConfig\n{\n    public uint PeriodSizeInFrames { get; set; }\n    public uint PeriodSizeInMilliseconds { get; set; }\n    public uint Periods { get; set; }\n    public bool NoPreSilencedOutputBuffer { get; set; }\n    public bool NoClip { get; set; }\n    public bool NoDisableDenormals { get; set; }\n    public bool NoFixedSizedCallback { get; set; }\n    public DeviceSubConfig Playback { get; set; }\n    public DeviceSubConfig Capture { get; set; }\n    public WasapiSettings? Wasapi { get; set; }\n    public CoreAudioSettings? CoreAudio { get; set; }\n    public AlsaSettings? Alsa { get; set; }\n    public PulseSettings? Pulse { get; set; }\n    public OpenSlSettings? OpenSL { get; set; }\n    public AAudioSettings? AAudio { get; set; }\n}\n```\n\n**Description:** A detailed configuration object that inherits from `DeviceConfig` and is specifically designed for initializing a MiniAudio device. It provides fine-grained control over buffer sizes, performance flags, and exposes nested configuration classes for OS-specific audio backends like WASAPI (Windows), CoreAudio (macOS), ALSA (Linux), and others. This allows developers to tune performance and behavior for specific platforms.\n\n**General Properties:**\n\n*   `PeriodSizeInFrames`: Gets or sets the desired size of the internal processing buffer in frames (a frame is one sample for each channel). This gives precise, sample-level control over buffer latency. Takes precedence over `PeriodSizeInMilliseconds`. Default is 0 (backend default).\n*   `PeriodSizeInMilliseconds`: Gets or sets the desired size of the internal processing buffer in milliseconds. A more intuitive way to control latency. Default is 0 (backend default).\n*   `Periods`: Gets or sets the number of periods to use for the device's buffer. Default is 0 (backend default).\n*   `NoPreSilencedOutputBuffer`: If `true`, the output buffer passed to the audio callback will contain undefined data instead of being cleared to silence. This can be a minor performance optimization if you are always filling the entire buffer. Default is `false`.\n*   `NoClip`: If `true`, the backend will not clip F32 sample values that are outside the [-1.0, 1.0] range. Default is `false`.\n*   `NoDisableDenormals`: If `true`, the backend will not attempt to disable denormal floating-point numbers, which can slightly improve precision at the cost of performance on some CPUs. Default is `false`.\n*   `NoFixedSizedCallback`: If `true`, the backend is not required to provide buffers of a fixed size in every callback. This can be an optimization if your processing logic is flexible. Default is `false`.\n\n**Sub-Configurations:**\n\n*   `Playback`: A `DeviceSubConfig` object for playback-specific settings.\n*   `Capture`: A `DeviceSubConfig` object for capture-specific settings.\n*   `Wasapi`: A `WasapiSettings` object for Windows-specific settings. Only used on Windows.\n*   `CoreAudio`: A `CoreAudioSettings` object for macOS/iOS-specific settings. Only used on Apple platforms.\n*   `Alsa`: An `AlsaSettings` object for Linux-specific settings. Only used on Linux with ALSA.\n*   `Pulse`: A `PulseSettings` object for Linux-specific settings. Only used on Linux with PulseAudio.\n*   `OpenSL`: An `OpenSlSettings` object for Android-specific settings.\n*   `AAudio`: An `AAudioSettings` object for modern Android-specific settings.\n\n---\n\n#### `DeviceSubConfig` (Nested Class)\n\n```csharp\npublic class DeviceSubConfig\n{\n    public ShareMode ShareMode { get; set; } = ShareMode.Shared;\n    internal bool IsLoopback { get; set; }\n}\n```\n\n**Description:** Contains settings for a specific direction (playback or capture).\n\n**Properties:**\n\n*   `ShareMode`: Specifies how the device is opened. `ShareMode.Shared` (default) allows multiple applications to use the device. `ShareMode.Exclusive` attempts to gain exclusive control for the lowest possible latency, but may not be supported by all devices.\n*   `IsLoopback`: (Internal) A flag used to indicate that a capture device should be initialized in loopback mode.\n\n---\n\n#### `WasapiSettings` (Nested Class)\n\n```csharp\npublic class WasapiSettings\n{\n    public WasapiUsage Usage { get; set; } = WasapiUsage.Default;\n    public bool NoAutoConvertSRC { get; set; }\n    public bool NoDefaultQualitySRC { get; set; }\n    public bool NoAutoStreamRouting { get; set; }\n    public bool NoHardwareOffloading { get; set; }\n}\n```\n**Description:** Contains settings specific to the WASAPI audio backend on Windows.\n\n**Properties:**\n\n*   `Usage`: Hints to the OS about the stream's purpose (`Default`, `Games`, `ProAudio`), which can affect system-level audio processing and prioritization.\n*   `NoAutoConvertSRC`: If `true`, disables automatic sample rate conversion by WASAPI, letting MiniAudio handle it instead.\n*   `NoDefaultQualitySRC`: If `true`, prevents WASAPI from using its default quality for sample rate conversion.\n*   `NoAutoStreamRouting`: If `true`, disables automatic stream routing by the OS.\n*   `NoHardwareOffloading`: If `true`, disables WASAPI's hardware offloading feature.\n\n---\n\n#### `CoreAudioSettings` (Nested Class)\n\n```csharp\npublic class CoreAudioSettings\n{\n    public bool AllowNominalSampleRateChange { get; set; }\n}\n```\n**Description:** Contains settings specific to the CoreAudio backend on macOS and iOS.\n\n**Properties:**\n\n*   `AllowNominalSampleRateChange`: If `true`, allows the OS to change the device's sample rate to match the stream. Typically used on desktop macOS.\n\n---\n\n#### `AlsaSettings` (Nested Class)\n\n```csharp\npublic class AlsaSettings\n{\n    public bool NoMMap { get; set; }\n    public bool NoAutoFormat { get; set; }\n    public bool NoAutoChannels { get; set; }\n    public bool NoAutoResample { get; set; }\n}\n```\n**Description:** Contains settings specific to the ALSA audio backend on Linux.\n\n**Properties:**\n\n*   `NoMMap`: If `true`, disables memory-mapped (MMap) mode for ALSA.\n*   `NoAutoFormat`: If `true`, prevents ALSA from performing automatic format conversion.\n*   `NoAutoChannels`: If `true`, prevents ALSA from performing automatic channel count conversion.\n*   `NoAutoResample`: If `true`, prevents ALSA from performing automatic resampling.\n\n---\n\n#### `PulseSettings` (Nested Class)\n\n```csharp\npublic class PulseSettings\n{\n    public string? StreamNamePlayback { get; set; }\n    public string? StreamNameCapture { get; set; }\n}\n```\n**Description:** Contains settings specific to the PulseAudio backend on Linux.\n\n**Properties:**\n\n*   `StreamNamePlayback`: Sets a custom name for the playback stream as it appears in PulseAudio volume controls.\n*   `StreamNameCapture`: Sets a custom name for the capture stream.\n\n---\n\n#### `OpenSlSettings` (Nested Class)\n\n```csharp\npublic class OpenSlSettings\n{\n    public OpenSlStreamType StreamType { get; set; }\n    public OpenSlRecordingPreset RecordingPreset { get; set; }\n}\n```\n**Description:** Contains settings specific to the OpenSL ES backend on Android.\n\n**Properties:**\n\n*   `StreamType`: Specifies the type of audio stream (e.g., `Voice`, `Media`, `Alarm`) to help Android manage audio focus and routing.\n*   `RecordingPreset`: Optimizes the microphone input for a specific scenario (e.g., `VoiceCommunication`, `Camcorder`).\n\n---\n\n#### `AAudioSettings` (Nested Class)\n\n```csharp\npublic class AAudioSettings\n{\n    public AAudioUsage Usage { get; set; }\n    public AAudioContentType ContentType { get; set; }\n    public AAudioInputPreset InputPreset { get; set; }\n    public AAudioAllowedCapturePolicy AllowedCapturePolicy { get; set; }\n}\n```\n**Description:** Contains settings specific to the modern AAudio backend on Android.\n\n**Properties:**\n\n*   `Usage`: Hints to the system about the stream's purpose (e.g., `Media`, `Game`, `Assistant`) for optimized routing and resource management.\n*   `ContentType`: Describes the type of content being played (e.g., `Music`, `Speech`, `Sonification`).\n*   `InputPreset`: Specifies a configuration for the audio input, optimizing it for scenarios like `VoiceRecognition` or `Camcorder`.\n*   `AllowedCapturePolicy`: Controls whether other applications are allowed to capture the audio from this stream.\n\n### Components `EnvelopeGenerator`\n\n```csharp\npublic class EnvelopeGenerator : SoundComponent\n{\n    public EnvelopeGenerator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float AttackTime { get; set; }\n    public float DecayTime { get; set; }\n    public override string Name { get; set; }\n    public float ReleaseTime { get; set; }\n    public bool Retrigger { get; set; }\n    public float SustainLevel { get; set; }\n    public TriggerMode Trigger { get; set; }\n\n    public event Action<float>? LevelChanged;\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public void TriggerOff();\n    public void TriggerOn();\n}\n```\n\n**Properties:**\n\n*   `AttackTime`: The attack time of the envelope (in seconds).\n*   `DecayTime`: The decay time of the envelope (in seconds).\n*   `Name`: The name of the envelope generator.\n*   `ReleaseTime`: The release time of the envelope (in seconds).\n*   `Retrigger`: Whether to retrigger the envelope on each new trigger.\n*   `SustainLevel`: The sustain level of the envelope.\n*   `Trigger`: The trigger mode (`NoteOn`, `Gate`, `Trigger`).\n\n**Events:**\n\n*   `LevelChanged`: Occurs when the envelope level changes.\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the envelope signal.\n*   `TriggerOff()`: Triggers the release stage of the envelope (if in `Gate` mode).\n*   `TriggerOn()`: Triggers the attack stage of the envelope.\n\n### Components `Filter`\n\n```csharp\npublic class Filter : SoundComponent\n{\n    public Filter(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float CutoffFrequency { get; set; }\n    public override string Name { get; set; }\n    public float Resonance { get; set; }\n    public FilterType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency of the filter.\n*   `Name`: The name of the filter.\n*   `Resonance`: The resonance of the filter.\n*   `Type`: The filter type (`LowPass`, `HighPass`, `BandPass`, `Notch`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Applies the filter to the audio buffer.\n\n### Components `LowFrequencyOscillator`\n\n```csharp\npublic class LowFrequencyOscillator : SoundComponent\n{\n    public LowFrequencyOscillator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float Depth { get; set; }\n    public TriggerMode Mode { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float Rate { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public float GetLastOutput();\n    public void Trigger();\n}\n```\n\n**Properties:**\n\n*   `Depth`: The depth of the LFO's modulation.\n*   `Mode`: The trigger mode (`FreeRunning`, `NoteTrigger`).\n*   `Name`: The name of the LFO.\n*   `Phase`: The initial phase of the LFO.\n*   `Rate`: The rate (frequency) of the LFO.\n*   `Type`: The waveform type (`Sine`, `Square`, `Triangle`, `Sawtooth`, `ReverseSawtooth`, `Random`, `SampleAndHold`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the LFO signal.\n*   `GetLastOutput()`: Returns the last generated output sample.\n*   `Trigger()`: Triggers the LFO (if in `NoteTrigger` mode).\n\n### Components `Mixer`\n\n```csharp\npublic sealed class Mixer : SoundComponent\n{\n    public Mixer(AudioEngine engine, AudioFormat format, bool isMasterMixer = false);\n\n    public IReadOnlyCollection<SoundComponent> Components { get; }\n    public AudioPlaybackDevice? ParentDevice { get; internal set; }\n    public bool IsMasterMixer { get; }\n    public override string Name { get; set; }\n\n    public void AddComponent(SoundComponent component);\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public void RemoveComponent(SoundComponent component);\n    public override void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Components`: A read-only collection of sound components mixed by this mixer.\n*   `ParentDevice`: The playback device this mixer is the master for, if any.\n*   `IsMasterMixer`: A value indicating whether this is a master mixer for a device.\n*   `Name`: The name of the mixer.\n\n**Methods:**\n\n*   `AddComponent(SoundComponent component)`: Adds a component to the mixer.\n*   `GenerateAudio(Span<float> buffer, int channels)`: Mixes the audio from all connected components.\n*   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n*   `Dispose()`: Disposes the mixer and all components within it.\n\n### Components `Oscillator`\n\n```csharp\npublic class Oscillator : SoundComponent\n{\n    public Oscillator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float Amplitude { get; set; }\n    public float Frequency { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float PulseWidth { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Amplitude`: The amplitude of the oscillator.\n*   `Frequency`: The frequency of the oscillator.\n*   `Name`: The name of the oscillator.\n*   `Phase`: The initial phase of the oscillator.\n*   `PulseWidth`: The pulse width (for pulse waveforms).\n*   `Type`: The waveform type (`Sine`, `Square`, `Sawtooth`, `Triangle`, `Noise`, `Pulse`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the oscillator's output.\n\n### Components `Recorder`\n\n```csharp\npublic class Recorder : IDisposable\n{\n    public Recorder(AudioCaptureDevice captureDevice, Stream stream, EncodingFormat encodingFormat = EncodingFormat.Wav);\n    public Recorder(AudioCaptureDevice captureDevice, AudioProcessCallback callback);\n\n    public PlaybackState State { get; private set; }\n    public readonly SampleFormat SampleFormat;\n    public readonly EncodingFormat EncodingFormat;\n    public readonly int SampleRate;\n    public readonly int Channels;\n    public readonly Stream Stream;\n    public AudioProcessCallback? ProcessCallback;\n    public ReadOnlyCollection<SoundModifier> Modifiers { get; }\n    public ReadOnlyCollection<AudioAnalyzer> Analyzers { get; }\n\n    public void StartRecording();\n    public void ResumeRecording();\n    public void PauseRecording();\n    public void StopRecording();\n    public void AddModifier(SoundModifier modifier);\n    public void RemoveModifier(SoundModifier modifier);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Analyzers`: Gets a read-only collection of <see cref=\"AudioAnalyzer\"/> components applied to the recorder. Analyzers are used to process and extract data from the audio stream during recording.\n*   `Modifiers`: Gets a read-only collection of <see cref=\"SoundModifier\"/> components applied to the recorder. Modifiers are applied to the audio data before encoding or processing via callback, allowing for real-time audio effects during recording.\n*   `Channels`: The number of channels to record.\n*   `EncodingFormat`: The encoding format for the recorded audio.\n*   `Stream`: The stream to write encoded recorded audio to.\n*   `ProcessCallback`: A callback for processing recorded audio in real time.\n*   `SampleRate`: The sample rate for recording.\n*   `State`: The current recording state (`Stopped`, `Playing`, `Paused`).\n*   `SampleFormat`: The sample format for recording.\n\n**Methods:**\n\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an <see cref=\"AudioAnalyzer\"/> to the recording pipeline. Analyzers process audio data during recording, enabling real-time analysis.\n*   `AddModifier(SoundModifier modifier)`: Adds a <see cref=\"SoundModifier\"/> to the recording pipeline. Modifiers apply effects to the audio data in real-time as it's being recorded.\n*   `Dispose()`: Releases resources used by the recorder.\n*   `PauseRecording()`: Pauses the recording.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes a specific <see cref=\"AudioAnalyzer\"/> from the recording pipeline.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a specific <see cref=\"SoundModifier\"/> from the recording pipeline.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `StartRecording()`: Starts the recording.\n*   `StopRecording()`: Stops the recording.\n*   `Dispose()`: Stops recording and releases all resources.\n\n### Components `SoundPlayer`\n\n```csharp\npublic sealed class SoundPlayer : SoundPlayerBase\n{\n    public SoundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n}\n```\nInherits all playback functionality, properties, and events from `SoundPlayerBase`.\n\n**Properties:**\n* `Name`: The name of the sound player component (default: \"Sound Player\").\n\n### Components `SurroundPlayer`\n\n```csharp\npublic sealed class SurroundPlayer : SoundPlayerBase\n{\n    public SurroundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n    public Vector2 ListenerPosition { get; set; }\n    public PanningMethod Panning { get; set; }\n    public VbapParameters VbapParameters { get; set; }\n    public SurroundConfiguration SurroundConfig { get; set; }\n    public SpeakerConfiguration SpeakerConfig { get; set; }\n\n    protected override void GenerateAudio(Span<float> output, int channels);\n    public void SetSpeakerConfiguration(SpeakerConfiguration config);\n    // Inherits Play, Pause, Stop, Seek, IsLooping, etc. from SoundPlayerBase\n}\n```\nInherits base playback functionality from `SoundPlayerBase` and adds surround-specific features.\n\n**Properties:**\n*   `Name`: The name of the surround player component (default: \"Surround Player\").\n*   `ListenerPosition`: The position of the listener in the surround sound field (Vector2).\n*   `Panning`: Gets or sets the panning method to use for surround sound (`Linear`, `EqualPower`, `VBAP`).\n*   `SpeakerConfig`: Gets or sets the speaker configuration (`Stereo`, `Quad`, `Surround51`, `Surround71`, `Custom`).\n*   `VbapParameters`: Gets or sets parameters for Vector Base Amplitude Panning (VBAP).\n*   `SurroundConfig`: Gets or sets the custom surround configuration when `SpeakerConfig` is set to `Custom`.\n\n**Methods:**\n*   `GenerateAudio(Span<float> output, int channels)`: (Overrides `SoundPlayerBase`) Reads audio data, applies resampling, then applies surround processing and looping if enabled.\n*   `SetSpeakerConfiguration(SpeakerConfiguration config)`: Sets the speaker configuration for surround sound playback.\n*   `Seek(int sampleOffset)`: (Overrides `SoundPlayerBase`) Seeks and re-initializes delay lines for surround processing.\n\n\n### Components `VoiceActivityDetector`\n\n```csharp\npublic class VoiceActivityDetector : AudioAnalyzer\n{\n    public VoiceActivityDetector(AudioFormat format, int fftSize = 1024, float energyThreshold = 5f, IVisualizer? visualizer = null);\n\n    public bool IsVoiceActive { get; private set; }\n    public float EnergyThreshold { get; set; }\n    public float ActivationTimeMs { get; set; }\n    public float HangoverTimeMs { get; set; }\n    public int SpeechLowBand { get; set; }\n    public int SpeechHighBand { get; set; }\n\n    public override string Name { get; set; }\n\n    public event Action<bool>? SpeechDetected;\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: Gets or sets the name of the voice activity detector component, useful for identification and debugging.\n*   `IsVoiceActive`: Read-only property indicating if voice is currently detected.\n*   `EnergyThreshold`: The energy threshold for detection.\n*   `ActivationTimeMs`: Time in milliseconds the signal must be considered speech before activation. Helps prevent short noise bursts from triggering.\n*   `HangoverTimeMs`: Time in milliseconds to keep the VAD active after the last speech frame. Prevents deactivation during short pauses.\n*   `SpeechLowBand`/`SpeechHighBand`: The frequency range (in Hz) to analyze for speech.\n\n**Events:**\n\n*   `SpeechDetected`: An event that is raised whenever the voice activity state changes (i.e., when speech is detected or ceases to be detected). Listeners can subscribe to this event to respond in real-time to changes in voice activity.\n\n**Methods:**\n\n*   `VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null)`: Constructor for the VoiceActivityDetector class. Initializes a new instance of the voice activity detector with configurable FFT size, detection threshold and optional visualizer for audio analysis visualization.\n    *   `fftSize`: `int` – The size of the FFT (Fast Fourier Transform) window used for spectral analysis. Must be a power of two. Larger FFT sizes provide finer frequency resolution but may increase processing latency. Default is 1024.\n    *   `energyThreshold`: `float` – The sensitivity threshold for voice detection. A lower value increases sensitivity. Default is 0.01.\n    *   `visualizer`: `IVisualizer?` – An optional visualizer instance that can be attached to the analyzer for visualizing audio processing data, useful for debugging and tuning. Default is `null`.\n*   `Analyze(Span<float> buffer, int channels)`: перевіряє audio buffer та оновлює `IsVoiceActive` property на основі алгоритму детекції.\n    *   `buffer`: `Span<float>` – The audio buffer to analyze for voice activity. The audio data in this buffer is processed to determine if voice is present.\n\n**Remarks:**\n\n*   **Frequency Range:** The `SpeechLowBand` and `SpeechHighBand` properties allow you to customize the frequency range that the VAD focuses on for speech detection. Speech typically falls within the 300Hz to 3400Hz range, but you may need to adjust these values depending on the characteristics of your audio and the type of speech you are detecting.\n*   **Threshold Sensitivity:** The `Threshold` property is crucial for controlling the sensitivity of the voice activity detection. Adjusting this threshold may be necessary to achieve optimal performance in different environments and with varying audio input levels.\n*   **FFT Size:** The `fftSize` parameter in the constructor determines the FFT window size. A larger FFT size provides better frequency resolution, which can be beneficial in noisy environments or when detecting subtle voice activity. However, it also increases the computational cost and latency. Ensure that the FFT size is always a power of 2 for optimal performance and compatibility with FFT algorithms.\n*   **Performance Tuning:** For optimal performance, especially in real-time applications, carefully tune the `fftSize` and `Threshold` parameters. Larger FFT sizes are more computationally intensive but offer better frequency resolution. Adjust the `Threshold` based on the ambient noise level and the desired sensitivity of voice detection.\n*   **Environment Considerations:** The ideal settings for `fftSize`, `Threshold`, `SpeechLowBand`, and `SpeechHighBand` may vary depending on the environment in which the voice activity detector is used. In noisy environments, you might need to increase the `fftSize` and adjust the `Threshold` to minimize false positives.\n*   **Visualizer for Debugging:** The optional `visualizer` parameter in the constructor is highly useful for debugging and tuning the voice activity detector. By attaching a visualizer, you can visually inspect the audio data and the detector's response, which can help in understanding and adjusting the detector's parameters for optimal performance in your specific use case.\n\n### Enums `Capability`\n\n```csharp\n[Flags]\npublic enum Capability\n{\n    Playback = 1,\n    Record = 2,\n    Mixed = Playback | Record,\n    Loopback = 4\n}\n```\n\n**Values:**\n\n*   `Playback`: Indicates playback capability.\n*   `Record`: Indicates recording capability.\n*   `Mixed`: Indicates both playback and recording capability.\n*   `Loopback`: Indicates loopback capability (recording system audio output).\n\n### Enums `DeviceType`\n\n```csharp\npublic enum DeviceType\n{\n    Playback,\n    Capture\n}\n```\n**Values:**\n*   `Playback`: Device used for audio playback.\n*   `Capture`: Device used for audio capture.\n\n### Enums `EncodingFormat`\n\n```csharp\npublic enum EncodingFormat\n{\n    Unknown = 0,\n    Wav,\n    Flac,\n    Mp3,\n    Vorbis\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown encoding format.\n*   `Wav`: Waveform Audio File Format (WAV).\n*   `Flac`: Free Lossless Audio Codec (FLAC).\n*   `Mp3`: MPEG-1 Audio Layer III (MP3).\n*   `Vorbis`: Ogg Vorbis.\n\n### Enums `PlaybackState`\n\n```csharp\npublic enum PlaybackState\n{\n    Stopped,\n    Playing,\n    Paused\n}\n```\n\n**Values:**\n\n*   `Stopped`: Playback is stopped.\n*   `Playing`: Playback is currently in progress.\n*   `Paused`: Playback is paused.\n\n### Enums `Result`\n\n```csharp\npublic enum Result\n{\n    Success = 0,\n    Error = -1,\n    // ... (other error codes)\n    CrcMismatch = -100,\n    FormatNotSupported = -200,\n    // ... (other backend-specific error codes)\n    DeviceNotInitialized = -300,\n    // ... (other device-related error codes)\n    FailedToInitBackend = -400\n    // ... (other backend initialization error codes)\n}\n```\n\n**Values:**\n\n*   `Success`: The operation was successful.\n*   `Error`: A generic error occurred.\n*   `CrcMismatch`: CRC checksum mismatch.\n*   `FormatNotSupported`: The requested audio format is not supported.\n*   `DeviceNotInitialized`: The audio device is not initialized.\n*   `FailedToInitBackend`: Failed to initialize the audio backend.\n*   **(Many other error codes representing various error conditions)**\n\n### Enums `SampleFormat`\n\n```csharp\npublic enum SampleFormat\n{\n    Unknown = 0,\n    U8 = 1,\n    S16 = 2,\n    S24 = 3,\n    S32 = 4,\n    F32 = 5\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown sample format.\n*   `U8`: Unsigned 8-bit integer.\n*   `S16`: Signed 16-bit integer.\n*   `S24`: Signed 24-bit integer packed in 3 bytes.\n*   `S32`: Signed 32-bit integer.\n*   `F32`: 32-bit floating-point.\n\n### Enums `FilterType`\n\n```csharp\npublic enum FilterType\n{\n    Peaking,\n    LowShelf,\n    HighShelf,\n    BandPass,\n    Notch,\n    LowPass,\n    HighPass\n}\n```\n\n**Values:**\n\n*   `Peaking`: Peaking filter.\n*   `LowShelf`: Low-shelf filter.\n*   `HighShelf`: High-shelf filter.\n*   `BandPass`: Band-pass filter.\n*   `Notch`: Notch filter.\n*   `LowPass`: Low-pass filter.\n*   `HighPass`: High-pass filter.\n\n### Enums `EnvelopeGenerator.EnvelopeState`\n\n```csharp\npublic enum EnvelopeState\n{\n    Idle,\n    Attack,\n    Decay,\n    Sustain,\n    Release\n}\n```\n\n**Values:**\n\n*   `Idle`: The envelope is inactive.\n*   `Attack`: The attack stage of the envelope.\n*   `Decay`: The decay stage of the envelope.\n*   `Sustain`: The sustain stage of the envelope.\n*   `Release`: The release stage of the envelope.\n\n### Enums `EnvelopeGenerator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    NoteOn,\n    Gate,\n    Trigger\n}\n```\n\n**Values:**\n\n* `NoteOn`: The envelope will go directly from attack to sustain, without a decay stage.\n* `Gate`: The envelope will progress normally, and will only enter release stage when trigger is off.\n* `Trigger`: The envelope will always progress to the end, including the release stage.\n\n### Enums `LowFrequencyOscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Triangle,\n    Sawtooth,\n    ReverseSawtooth,\n    Random,\n    SampleAndHold\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Triangle`: Triangle wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `ReverseSawtooth`: Reverse sawtooth wave.\n*   `Random`: Random values.\n*   `SampleAndHold`: Sample and hold random values.\n\n### Enums `LowFrequencyOscillator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    FreeRunning,\n    NoteTrigger\n}\n```\n\n**Values:**\n\n* `FreeRunning`: The LFO will run continuously without needing a trigger.\n* `NoteTrigger`: The LFO will only start when triggered.\n\n### Enums `Oscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Sawtooth,\n    Triangle,\n    Noise,\n    Pulse\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `Triangle`: Triangle wave.\n*   `Noise`: White noise.\n*   `Pulse`: Pulse wave.\n\n### Enums `SurroundPlayer.SpeakerConfiguration`\n\n```csharp\npublic enum SpeakerConfiguration\n{\n    Stereo,\n    Quad,\n    Surround51,\n    Surround71,\n    Custom\n}\n```\n\n**Values:**\n\n*   `Stereo`: Standard stereo configuration (2 speakers).\n*   `Quad`: Quadraphonic configuration (4 speakers).\n*   `Surround51`: 5.1 surround sound configuration (6 speakers).\n*   `Surround71`: 7.1 surround sound configuration (8 speakers).\n* *   `Custom`: A custom speaker configuration defined by the user.\n\n### Enums `SurroundPlayer.PanningMethod`\n\n```csharp\npublic enum PanningMethod\n{\n    Linear,\n    EqualPower,\n    Vbap\n}\n```\n\n**Values:**\n\n*   `Linear`: Linear panning.\n*   `EqualPower`: Equal power panning.\n*   `Vbap`: Vector Base Amplitude Panning (VBAP).\n\n### Editing `Composition`\nSee [Editing and Persistence Guide](./editing-engine.mdx#composition) for details.\n```csharp\npublic class Composition : ISoundDataProvider, IDisposable\n{\n    public Composition(string name = \"Composition\", int? targetChannels = null);\n\n    public string Name { get; set; }\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public List<Track> Tracks { get; }\n    public float MasterVolume { get; set; }\n    public bool IsDirty { get; }\n    public int SampleRate { get; set; } // Target sample rate for rendering\n    public int TargetChannels { get; set; }\n\n    // ISoundDataProvider implementation\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public bool IsDisposed { get; private set; }\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    \n    public void AddTrack(Track track);\n    public bool RemoveTrack(Track track);\n    public TimeSpan CalculateTotalDuration();\n    public float[] Render(TimeSpan startTime, TimeSpan duration);\n    public int Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer);\n    public void MarkDirty();\n    internal void ClearDirtyFlag();\n    public void Dispose();\n    // ... other methods like ReplaceSegment, RemoveSegment, SilenceSegment, InsertSegment ...\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `Track`\nSee [Editing and Persistence Guide](./editing-engine.mdx#track) for details.\n```csharp\npublic class Track\n{\n    public Track(string name = \"Track\", TrackSettings? settings = null);\n\n    public string Name { get; set; }\n    public List<AudioSegment> Segments { get; }\n    public TrackSettings Settings { get; set; }\n    internal Composition? ParentComposition { get; set; }\n\n    public void MarkDirty();\n    public void AddSegment(AudioSegment segment);\n    public bool RemoveSegment(AudioSegment segment, bool shiftSubsequent = false);\n    public void InsertSegmentAt(AudioSegment segmentToInsert, TimeSpan insertionTime, bool shiftSubsequent = true);\n    public TimeSpan CalculateDuration();\n    public int Render(TimeSpan overallStartTime, TimeSpan durationToRender, Span<float> outputBuffer, int targetSampleRate, int targetChannels);\n}\n```\n\n### Editing `AudioSegment`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegment) for details.\n```csharp\npublic class AudioSegment : IDisposable\n{\n    public AudioSegment(\n        ISoundDataProvider sourceDataProvider,\n        TimeSpan sourceStartTime,\n        TimeSpan sourceDuration,\n        TimeSpan timelineStartTime,\n        string name = \"Segment\",\n        AudioSegmentSettings? settings = null,\n        bool ownsDataProvider = false);\n\n    public string Name { get; set; }\n    public ISoundDataProvider SourceDataProvider { get; private set; }\n    public TimeSpan SourceStartTime { get; set; }\n    public TimeSpan SourceDuration { get; set; }\n    public TimeSpan TimelineStartTime { get; set; }\n    public AudioSegmentSettings Settings { get; set; }\n    internal Track? ParentTrack { get; set; }\n\n    public TimeSpan StretchedSourceDuration { get; }\n    public TimeSpan EffectiveDurationOnTimeline { get; }\n    public TimeSpan TimelineEndTime { get; }\n    public TimeSpan GetTotalLoopedDurationOnTimeline();\n    public AudioSegment Clone(TimeSpan? newTimelineStartTime = null);\n    internal void ReplaceSource(ISoundDataProvider newSource, TimeSpan newSourceStartTime, TimeSpan newSourceDuration);\n    public int ReadProcessedSamples(TimeSpan segmentTimelineOffset, TimeSpan durationToRead, Span<float> outputBuffer, int outputBufferOffset, int targetSampleRate, int targetChannels);\n    internal void FullResetState();\n    public void Dispose();\n    public void MarkDirty();\n}\n```\n\n### Editing `AudioSegmentSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegmentsettings) for details.\n```csharp\npublic class AudioSegmentSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public TimeSpan FadeInDuration { get; set; }\n    public FadeCurveType FadeInCurve { get; set; }\n    public TimeSpan FadeOutDuration { get; set; }\n    public FadeCurveType FadeOutCurve { get; set; }\n    public bool IsReversed { get; set; }\n    public LoopSettings Loop { get; set; }\n    public float SpeedFactor { get; set; }\n    public float TimeStretchFactor { get; set; } // Overridden by TargetStretchDuration if set\n    public TimeSpan? TargetStretchDuration { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public AudioSegmentSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `TrackSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#tracksettings) for details.\n```csharp\npublic class TrackSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public bool IsMuted { get; set; }\n    public bool IsSoloed { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public TrackSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `LoopSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#loopsettings) for details.\n```csharp\npublic record struct LoopSettings\n{\n    public int Repetitions { get; }\n    public TimeSpan? TargetDuration { get; }\n    public LoopSettings(int repetitions = 0, TimeSpan? targetDuration = null);\n    public static LoopSettings PlayOnce { get; }\n}\n```\n\n### Editing `FadeCurveType`\nSee [Editing and Persistence Guide](./editing-engine.mdx#fadecurvetype) for details.\n```csharp\npublic enum FadeCurveType\n{\n    Linear,\n    Logarithmic,\n    SCurve\n}\n```\n\n### Editing.Persistence\nThese are Data Transfer Objects (DTOs) for serialization. See [Editing and Persistence Guide](./editing-engine.mdx#project-persistence) for their purpose.\n*   `CompositionProjectManager` (static class): `SaveProjectAsync`, `LoadProjectAsync`, `RelinkMissingMediaAsync`.\n*   `ProjectData`\n*   `ProjectTrack`\n*   `ProjectSegment`\n*   `ProjectAudioSegmentSettings`\n*   `ProjectTrackSettings`\n*   `ProjectSourceReference`\n*   `ProjectEffectData`\n\n\n### Extensions.WebRtc.Apm\n\n#### `AudioProcessingModule` (Class)\n```csharp\npublic class AudioProcessingModule : IDisposable\n{\n    public AudioProcessingModule();\n    public ApmError ApplyConfig(ApmConfig config);\n    public ApmError Initialize();\n    public ApmError ProcessStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    public ApmError ProcessReverseStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    // ... other methods for setting delays, levels, runtime settings, getting info, AEC dump ...\n    public static int GetFrameSize(int sampleRateHz);\n    public void Dispose();\n}\n```\n**Description:** Provides low-level access to the WebRTC Audio Processing Module. It's responsible for initializing the APM, applying configurations, and processing audio frames. Generally used internally by `WebRtcApmModifier` and `NoiseSuppressor`.\n\n#### `ApmConfig` (Class)\n```csharp\npublic class ApmConfig : IDisposable\n{\n    public ApmConfig();\n    public void SetEchoCanceller(bool enabled, bool mobileMode);\n    public void SetNoiseSuppression(bool enabled, NoiseSuppressionLevel level);\n    public void SetGainController1(bool enabled, GainControlMode mode, int targetLevelDbfs, int compressionGainDb, bool enableLimiter);\n    public void SetGainController2(bool enabled);\n    public void SetHighPassFilter(bool enabled);\n    public void SetPreAmplifier(bool enabled, float fixedGainFactor);\n    public void SetPipeline(int maxInternalRate, bool multiChannelRender, bool multiChannelCapture, DownmixMethod downmixMethod);\n    public void Dispose();\n}\n```\n**Description:** Used to configure the features of the `AudioProcessingModule` such as echo cancellation, noise suppression, gain control, etc.\n\n#### `StreamConfig` (Class)\n```csharp\npublic class StreamConfig : IDisposable\n{\n    public StreamConfig(int sampleRateHz, int numChannels);\n    public int SampleRateHz { get; }\n    public int NumChannels { get; }\n    public void Dispose();\n}\n```\n**Description:** Defines the properties (sample rate, number of channels) of an audio stream being processed by the APM.\n\n#### `ProcessingConfig` (Class)\nThis class holds multiple `StreamConfig` instances for different parts of the APM pipeline (input, output, reverse input, reverse output).\n\n#### `NoiseSuppressor` (Component - `SoundFlow.Extensions.WebRtc.Apm.Components`)\n```csharp\npublic class NoiseSuppressor : IDisposable\n{\n    public NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High, bool useMultichannelProcessing = false);\n    public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;\n    public float[] ProcessAll();\n    public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null);\n    public void Dispose();\n}\n```\n**Description:** A component for offline/batch noise suppression using WebRTC APM. It takes an `ISoundDataProvider`, processes its audio, and outputs the cleaned audio either as a whole or in chunks.\n**Key Members:**\n*   `NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, ...)`: Constructor.\n*   `OnAudioChunkProcessed` (event): Raised when a chunk of audio is processed.\n*   `ProcessAll()`: Processes the entire audio stream and returns it.\n*   `ProcessChunks()`: Processes audio in chunks, raising `OnAudioChunkProcessed`.\n\n#### `WebRtcApmModifier` (Modifier - `SoundFlow.Extensions.WebRtc.Apm.Modifiers`)\n```csharp\npublic sealed class WebRtcApmModifier : SoundModifier, IDisposable\n{\n    public WebRtcApmModifier(\n        bool aecEnabled = false, bool aecMobileMode = false, int aecLatencyMs = 40,\n        bool nsEnabled = false, NoiseSuppressionLevel nsLevel = NoiseSuppressionLevel.High,\n        // ... other AGC, HPF, PreAmp, Pipeline settings ...\n    );\n\n    public override string Name { get; set; }\n    public EchoCancellationSettings EchoCancellation { get; }\n    public NoiseSuppressionSettings NoiseSuppression { get; }\n    public AutomaticGainControlSettings AutomaticGainControl { get; }\n    public ProcessingPipelineSettings ProcessingPipeline { get; }\n    public bool HighPassFilterEnabled { get; set; }\n    public bool PreAmplifierEnabled { get; set; }\n    public float PreAmplifierGainFactor { get; set; }\n    public float PostProcessGain { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n    public void Dispose();\n}\n```\n**Description:** A `SoundModifier` that applies various WebRTC APM features (AEC, NS, AGC, HPF, PreAmp) to an audio stream in real-time.\n**Key Members:**\n*   Constructor with detailed initial settings.\n*   Properties for configuring each APM feature (`EchoCancellation`, `NoiseSuppression`, `AutomaticGainControl`, `ProcessingPipeline`, `HighPassFilterEnabled`, etc.).\n*   `Process(Span<float> buffer)`: Core processing logic.\n*   `Dispose()`: Releases native APM resources.\n\n#### Enums for WebRTC APM\n*   `ApmError`: Error codes.\n*   `NoiseSuppressionLevel`: Low, Moderate, High, VeryHigh.\n*   `GainControlMode`: AdaptiveAnalog, AdaptiveDigital, FixedDigital.\n*   `DownmixMethod`: AverageChannels, UseFirstChannel.\n*   `RuntimeSettingType`: Types for runtime APM settings.\n\n### Interfaces `ISoundDataProvider`\n\n```csharp\npublic interface ISoundDataProvider : IDisposable\n{\n    int Position { get; }\n    int Length { get; }\n    bool CanSeek { get; }\n    SampleFormat SampleFormat { get; }\n    int SampleRate { get; }\n    bool IsDisposed { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n    event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    int ReadBytes(Span<float> buffer);\n    void Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples).\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio data.\n*   `Dispose()`: Releases resources held by the data provider.\n\n### Interfaces `ISoundDecoder`\n\n```csharp\npublic interface ISoundDecoder : IDisposable\n{\n    bool IsDisposed { get; }\n    int Length { get; }\n    SampleFormat SampleFormat { get; }\n    int Channels { get; }\n    int SampleRate { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n\n    int Decode(Span<float> samples);\n    bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the decoder has been disposed.\n*   `Length`: The total length of the decoded audio data (in samples).\n*   `SampleFormat`: The sample format of the decoded audio data.\n*   `Channels`: The number of channels in the decoded audio data.\n*   `SampleRate`: The sample rate of the decoded audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n*   `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio stream.\n*   `Dispose()`: Releases the resources used by the decoder.\n\n### Interfaces `ISoundEncoder`\n\n```csharp\npublic interface ISoundEncoder : IDisposable\n{\n    bool IsDisposed { get; }\n\n    int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples.\n*   `Dispose()`: Releases the resources used by the encoder.\n\n\n### Interfaces `ISoundPlayer`\n\n```csharp\npublic interface ISoundPlayer\n{\n    PlaybackState State { get; }\n    bool IsLooping { get; set; }\n    float PlaybackSpeed { get; set; }\n    float Volume { get; set; }\n    float Time { get; }\n    float Duration { get; }\n    float LoopStartSeconds { get; }\n    float LoopEndSeconds { get; }\n    int LoopStartSamples { get; }\n    int LoopEndSamples { get; }\n\n    void Play();\n    void Pause();\n    void Stop();\n    bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    bool Seek(float time);\n    bool Seek(int sampleOffset);\n    void SetLoopPoints(float startTime, float? endTime = -1f);\n    void SetLoopPoints(int startSample, int endSample = -1);\n    void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `State`: The current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Whether looping is enabled or disabled (`get`, `set`).\n*   `PlaybackSpeed`: Gets or sets the playback speed. 1.0 is normal speed.\n*   `Volume`: Gets or sets the volume of the sound player (0.0 to 1.0 or higher for gain).\n*   `Time`: The current playback position (in seconds).\n*   `Duration`: The total duration of the audio (in seconds).\n*   `LoopStartSeconds`: Gets the configured loop start point in seconds.\n*   `LoopEndSeconds`: Gets the configured loop end point in seconds.  -1 indicates looping to the natural end.\n*   `LoopStartSamples`: Gets the configured loop start point in samples.\n*   `LoopEndSamples`: Gets the configured loop end point in samples. -1 indicates looping to the natural end.\n\n**Methods:**\n\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to the specified time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to the specified time (in seconds). Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset (in samples). Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures custom loop points using start and end times in seconds. `endTime` is optional;  use -1 or `null` to loop to the natural end.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures custom loop points using start and end sample indices. `endSample` is optional; use -1 to loop to the natural end.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures custom loop points using `TimeSpan`.\n\n### Interfaces `IVisualizationContext`\n\n```csharp\npublic interface IVisualizationContext\n{\n    void Clear();\n    void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f);\n    void DrawRectangle(float x, float y, float width, float height, Color color);\n}\n```\n\n**Methods:**\n\n*   `Clear()`: Clears the drawing surface.\n*   `DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)`: Draws a line from (`x1`, `y1`) to (`x2`, `y2`) with the specified color and thickness.\n*   `DrawRectangle(float x, float y, float width, float height, Color color)`: Draws a rectangle with the specified position, dimensions, and color.\n\n### Interfaces `IVisualizer`\n\n```csharp\npublic interface IVisualizer : IDisposable\n{\n    string Name { get; }\n\n    event EventHandler VisualizationUpdated;\n\n    void ProcessOnAudioData(Span<float> audioData);\n    void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes a chunk of audio data for visualization.\n*   `Render(IVisualizationContext context)`: Renders the visualization using the provided `IVisualizationContext`.\n*   `Dispose()`: Releases the resources used by the visualizer.\n\n### Modifiers `AlgorithmicReverbModifier`\n\n```csharp\npublic sealed class AlgorithmicReverbModifier : SoundModifier\n{\n    public AlgorithmicReverbModifier(AudioFormat format);\n\n    public override string Name { get; set; }\n    public float Wet { get; set; }\n    public float RoomSize { get; set; }\n    public float Damp { get; set; }\n    public float Width { get; set; }\n    public float PreDelay { get; set; }\n    public float Mix { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Description:** An algorithmic reverb modifier based on the Freeverb algorithm. Now supports multi-channel audio, applying distinct delay lines and modulation to create a wider spatial effect.\n\n**Constructor:** `AlgorithmicReverbModifier(AudioFormat format)`: Initializes the modifier for the specified audio format.\n\n**Properties:**\n\n*   `Damp`: The damping factor of the reverb.\n*   `Name`: The name of the modifier.\n*   `PreDelay`: The pre-delay time (in milliseconds).\n*   `RoomSize`: The simulated room size.\n*   `Wet`: The wet/dry mix of the reverb (0 = dry, 1 = wet).\n*   `Width`: The stereo width of the reverb.\n*   `Mix`: The mix level of the reverb.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the reverb effect.\n\n### Modifiers `BassBoostModifier`\n\n```csharp\npublic class BassBoostModifier : SoundModifier\n{\n    public BassBoostModifier(AudioFormat format, float cutoff, float boostGain);\n\n    public float Cutoff { get; set; }\n    public float BoostGain { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Cutoff`: The cutoff frequency below which the bass boost is applied.\n*   `BoostGain`: The gain applied to the bass boost in dB.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the bass boost effect.\n\n### Modifiers `ChorusModifier`\n\n```csharp\npublic sealed class ChorusModifier : SoundModifier\n{\n    public ChorusModifier(AudioFormat format, float depthMs = 2f, float rateHz = 0.5f, float feedback = 0.7f, float wetDryMix = 0.5f, float maxDelayMs = 50f);\n\n    public float DepthMs { get; set; }\n    public float RateHz { get; set; }\n    public float Feedback { get; } // Read-only\n    public float WetDryMix { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `DepthMs`: The depth of the chorus effect.\n*   `RateHz`: The rate of the chorus effect.\n*   `Feedback`: The feedback amount of the chorus effect.\n*   `WetDryMix`: The wet/dry mix of the chorus effect.\n*   `maxDelayMs`: The maximum delay length (in samples) used by the chorus effect.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the chorus effect.\n\n### Modifiers `CompressorModifier`\n\n```csharp\npublic class CompressorModifier : SoundModifier\n{\n    public CompressorModifier(AudioFormat format, float thresholdDb, float ratio, float attackMs, float releaseMs, float kneeDb = 0, float makeupGainDb = 0);\n\n    public float ThresholdDb { get; set; }\n    public float Ratio { get; set; }\n    public float AttackMs { get; set; }\n    public float ReleaseMs { get; set; }\n    public float KneeDb { get; set; }\n    public float MakeupGainDb { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `ThresholdDb`: The threshold (in dB) above which compression is applied.\n*   `Ratio`: The compression ratio.\n*   `AttackMs`: The attack time (in milliseconds).\n*   `ReleaseMs`: The release time (in milliseconds).\n*   `KneeDb`: The knee width (in dB).\n*   `MakeupGainDb`: The amount of makeup gain to apply after compression.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the compression effect.\n\n### Modifiers `DelayModifier`\n\n```csharp\npublic class DelayModifier : SoundModifier\n{\n    public DelayModifier(int delayLength, float feedback, float wetMix, float cutoffFrequency);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `delayLength`: The delay length (in samples).\n*   `feedback`: The feedback amount of the delay.\n*   `wetMix`: The wet/dry mix of the delay (0 = dry, 1 = wet).\n*   `cutoffFrequency`: The cutoff frequency for the low-pass filter applied to the delayed signal.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the delay effect.\n\n### Modifiers `FrequencyBandModifier`\n\n```csharp\npublic class FrequencyBandModifier : SoundModifier\n{\n    public FrequencyBandModifier(float lowCutoffFrequency, float highCutoffFrequency);\n\n    public float HighCutoffFrequency { get; set; }\n    public float LowCutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `HighCutoffFrequency`: The high cutoff frequency of the frequency band.\n*   `LowCutoffFrequency`: The low cutoff frequency of the frequency band.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the frequency band modification.\n\n### Modifiers `NoiseReductionModifier`\n\n```csharp\npublic class NoiseReductionModifier : SoundModifier\n{\n    public NoiseReductionModifier(int fftSize = 2048, float alpha = 3f, float beta = 0.001f, float smoothingFactor = 0.9f, float gain = 1.5f, int noiseFrames = 5, VoiceActivityDetector? vad = null);\n\n    public override string Name { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying noise reduction.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `NoiseReductionModifier` operates on buffers, not individual samples.\n\n### Modifiers `ParametricEqualizer`\n\n```csharp\npublic class ParametricEqualizer : SoundModifier\n{\n    public ParametricEqualizer(int channels);\n\n    public override string Name { get; set; }\n    \n    public void AddBand(EqualizerBand band);\n    public void AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth);\n    public void AddBands(IEnumerable<EqualizerBand> bands);\n    public void ClearBands();\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n    public void RemoveBand(EqualizerBand band);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `AddBand(EqualizerBand band)`: Adds an `EqualizerBand` to the equalizer.\n*   `AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth)`: Adds an equalizer band with the specified parameters.\n*   `AddBands(IEnumerable<EqualizerBand> bands)`: Adds multiple equalizer bands.\n*   `ClearBands()`: Removes all equalizer bands.\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying equalization.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `ParametricEqualizer` operates on buffers, not individual samples.\n*   `RemoveBand(EqualizerBand band)`: Removes a specific equalizer band.\n\n\n### Modifiers `MultiChannelChorusModifier`\n\n```csharp\npublic class MultiChannelChorusModifier : SoundModifier\n{\n    public MultiChannelChorusModifier(\n        AudioFormat format,\n        float wetMix,\n        int maxDelay,\n        params (float depth, float rate, float feedback)[] channelParameters);\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel); // Throws NotImplementedException\n}\n```\n**Description:** A sound modifier that implements a multi-channel chorus effect, allowing for different chorus parameters (depth, rate, feedback) for each audio channel. This is useful for creating rich, spatial chorus effects.\n\n### Modifiers `TrebleBoostModifier`\n\n```csharp\npublic class TrebleBoostModifier : SoundModifier\n{\n    public TrebleBoostModifier(float cutoffFrequency);\n\n    public float CutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency above which the treble boost is applied.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the treble boost effect.\n\n### Providers `AssetDataProvider`\n\n```csharp\npublic sealed class AssetDataProvider : ISoundDataProvider\n{\n    public AssetDataProvider(AudioEngine engine, AudioFormat format, Stream stream);\n    public AssetDataProvider(AudioEngine engine, AudioFormat format, byte[] data);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; }\n    public int SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; private set; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public void Dispose();\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (always true for `AssetDataProvider`).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the provider.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio data.\n\n### Providers `StreamDataProvider`\n\n```csharp\npublic sealed class StreamDataProvider : ISoundDataProvider\n{\n    public StreamDataProvider(AudioEngine engine, AudioFormat format, Stream stream);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; private set; }\n    public int SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the stream into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio stream (if supported).\n*   `Dispose()`: Releases resources used by the provider.\n\n### Providers `MicrophoneDataProvider`\n\n```csharp\npublic class MicrophoneDataProvider : ISoundDataProvider\n{\n    public MicrophoneDataProvider(AudioDevice captureDevice, int bufferSize = 8);\n\n    public int Position { get; private set; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void StartCapture();\n    public void StopCapture();    \n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the captured audio data (in samples).\n*   `Length`: Returns -1, indicating an unknown length for the live microphone stream.\n*   `CanSeek`: Returns `false` because seeking is not supported for live microphone input.\n*   `SampleFormat`: The sample format of the captured audio data, which matches the `AudioEngine`'s sample format.\n*   `SampleRate`: The sample rate of the captured audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when `StopCapture()` is called, signaling the end of the microphone input stream.\n*   `PositionChanged`: Raised after reading data, indicating that the read position has changed.\n\n**Methods:**\n\n*   `MicrophoneDataProvider(int bufferSize = 8, int? sampleRate = null)`: Constructor that initializes the `MicrophoneDataProvider`. It sets the buffer queue size (default is 8), the sample rate (defaults to the `AudioEngine`'s sample rate), and subscribes to the `AudioEngine.OnAudioProcessed` event to capture audio data.\n    * `bufferSize`: The number of audio sample arrays to hold in internal queue. Higher values will lead to higher latency but will be more resilient to performance spikes.\n    * `sampleRate`: The sample rate of the microphone, will use the audioEngine sample rate if not set.\n*   `StartCapture()`: Starts capturing audio data from the microphone.\n*   `StopCapture()`: Stops capturing audio data and raises the `EndOfStreamReached` event.\n*   `ReadBytes(Span<float> buffer)`: Reads captured audio samples into the provided buffer. If not enough data is available in the queue it will fill the rest of the buffer with silence.\n*   `Seek(int offset)`: Throws `NotSupportedException` because seeking is not supported for live microphone input.\n*   `Dispose()`: Releases resources used by the `MicrophoneDataProvider`, unsubscribes from the `AudioEngine.OnAudioProcessed` event, and clears the internal buffer queue.\n\n### Providers `ChunkedDataProvider`\n\n```csharp\npublic sealed class ChunkedDataProvider : ISoundDataProvider\n{\n    public ChunkedDataProvider(AudioEngine engine, AudioFormat format, Stream stream, int chunkSize = DefaultChunkSize);\n    public ChunkedDataProvider(AudioEngine engine, AudioFormat format, string filePath, int chunkSize = DefaultChunkSize);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data in samples.\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a `Stream`. It creates an `ISoundDecoder` to decode the stream, sets the default chunk size (220500 samples per channel, which is 10 seconds at 44.1 kHz), and starts prefetching data.\n*   `ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a file path. It opens a `FileStream` and calls the other constructor.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is prefilled with decoded audio. If the buffer runs out, it decodes another chunk from the stream.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported by the stream and decoder). It disposes of the current decoder, creates a new one, and seeks within the stream. Then, it clears the internal buffer and refills it from the new position.\n*   `Dispose()`: Releases the resources used by the `ChunkedDataProvider`, including the decoder and the stream.\n\n**Remarks:**\n\nThe `ChunkedDataProvider` is designed to handle large audio files efficiently by reading and decoding them in chunks. This prevents the entire file from being loaded into memory at once. The default chunk size is set to 10 seconds of audio at 44.1 kHz, but you can adjust this value in the constructor. The class uses an internal buffer (`Queue<float>`) to store decoded audio samples and prefetches data in the background.\n\n### Providers `QueueDataProvider`\n\n```csharp\npublic enum QueueFullBehavior\n{\n    /// <summary>\n    ///     Throw an <see cref=\"InvalidOperationException\"/> when the queue is full. This is the default behavior.\n    /// </summary>\n    Throw,\n\n    /// <summary>\n    ///     Block the calling thread until space becomes available in the queue.\n    /// </summary>\n    Block,\n\n    /// <summary>\n    ///     Silently drop the incoming samples and return immediately.\n    /// </summary>\n    Drop\n}\n\npublic class QueueDataProvider : ISoundDataProvider\n{\n    public QueueDataProvider(AudioFormat format, int? maxSamples = null, QueueFullBehavior fullBehavior = QueueFullBehavior.Throw);\n\n    public int SamplesAvailable { get; }\n    public long TotalSamplesEnqueued { get; }\n    public bool IsDisposed { get; }\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void AddSamples(ReadOnlySpan<float> samples);\n    public void Reset();\n    public void CompleteAdding();\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from an in-memory queue that is fed samples externally. This provider is ideal for scenarios where audio data is generated or received in chunks. It supports configurable behavior for when the queue becomes full, specified by the `QueueFullBehavior` enum.\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: Always returns -1, indicating the total length is unknown as it's a dynamic queue.\n*   `CanSeek`: Always returns `false`, as seeking is not supported by this provider.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n*   `SamplesAvailable`: Gets the number of samples currently available in the queue.\n*   `TotalSamplesEnqueued`: Gets the total number of samples that have been enqueued into the provider since its creation or last reset.\n*   `IsDisposed`: Gets a value indicating whether the provider has been disposed.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached, which occurs when `CompleteAdding()` has been called and all samples have been read from the queue.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `AddSamples(ReadOnlySpan<float> samples)`: Adds audio samples to the internal queue. The behavior when the queue is full is determined by the `QueueFullBehavior` set during construction:\n*   If `QueueFullBehavior.Throw` (default), an `InvalidOperationException` is thrown if adding samples would exceed the maximum size.\n*   If `QueueFullBehavior.Block`, the calling thread will block until space becomes available in the queue for the entire block of samples.\n*   If `QueueFullBehavior.Drop`, the incoming samples are silently discarded.\n*   Throws `InvalidOperationException` if called after `CompleteAdding()` has been invoked.\n*   `Reset()`: Resets the provider to its initial state, clearing the sample queue, resetting the position, and allowing samples to be added again. Any threads previously blocked in `AddSamples` (if `QueueFullBehavior.Block` was active) will be unblocked.\n*   `CompleteAdding()`: Marks that no more samples will be added to the queue. Once called, subsequent calls to `AddSamples` will throw an `InvalidOperationException`. The `EndOfStreamReached` event will be raised when all remaining samples in the queue have been read.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the internal queue into the provided buffer. Returns the number of samples read. If the queue is empty, it returns 0. If `CompleteAdding()` has been called and the queue becomes empty, the `EndOfStreamReached` event will be fired.\n*   `Seek(int sampleOffset)`: Throws an `InvalidOperationException` because seeking is not supported by this queue-based provider.\n*   `Dispose()`: Releases the resources used by the `QueueDataProvider`, clears the sample queue, and unblocks any waiting threads.\n\n### Providers `NetworkDataProvider`\n\n```csharp\npublic sealed class NetworkDataProvider : ISoundDataProvider\n{\n    public NetworkDataProvider(AudioEngine engine, AudioFormat format, string url);\n\n    public int Position { get; }\n    public int Length { get; private set; }\n    public bool CanSeek { get; private set; }\n    public SampleFormat SampleFormat { get; private set; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Returns -1 for HLS streams without an `#EXT-X-ENDLIST` tag, indicating an unknown or continuously growing length.\n*   `CanSeek`: Indicates whether seeking is supported. It's `true` for direct audio URLs if the server supports range requests and for HLS streams with an `#EXT-X-ENDLIST` tag; otherwise, it's `false`.\n*   `SampleFormat`: The format of the audio samples. Determined after the initial connection to the stream.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `NetworkDataProvider(string url, int? sampleRate = null)`: Constructor that initializes the `NetworkDataProvider` with a network URL. It determines whether the URL points to a direct audio file or an HLS playlist.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is filled asynchronously.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported). The behavior differs for direct URLs and HLS streams:\n    *   **Direct URLs:** Performs an HTTP range request to fetch data starting from the desired offset.\n    *   **HLS Streams:** Locates the HLS segment containing the desired time offset and starts downloading from that segment.\n*   `Dispose()`: Releases the resources used by the `NetworkDataProvider`, including the `HttpClient`, decoder, and stream.\n\n**Remarks:**\n\nThe `NetworkDataProvider` can handle both direct audio URLs and HLS (HTTP Live Streaming) playlists. It automatically detects the stream type and behaves accordingly.\n\n**Direct Audio URLs:**\n\n*   It uses `HttpClient` to make requests to the URL.\n*   It supports seeking if the server responds with an \"Accept-Ranges: bytes\" header.\n*   It creates an `ISoundDecoder` to decode the audio stream.\n*   It buffers audio data asynchronously in a background thread.\n\n**HLS Playlists:**\n\n*   It downloads and parses the M3U(8) playlist file.\n*   It identifies the individual media segments (e.g., `.ts` files).\n*   It downloads and decodes segments sequentially.\n*   It refreshes the playlist periodically for live streams.\n*   It supports seeking by selecting the appropriate segment based on the desired time offset.\n*   It determines whether the playlist has a defined end by checking for the `#EXT-X-ENDLIST` tag, which affects whether `Length` is known and `CanSeek` is true.\n\nThe class uses an internal `Queue<float>` to buffer audio samples. The `ReadBytes` method waits for data to become available in the buffer if it's empty.\n\n### Providers `RawDataProvider`\n\n```csharp\npublic sealed class RawDataProvider : ISoundDataProvider, IDisposable\n{\n    public RawDataProvider(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from a stream containing raw PCM audio data.\n**Properties:**\n*   `Position`: The current read position in samples.\n*   `Length`: The total length of the stream in samples.\n*   `CanSeek`: Indicates if the underlying stream is seekable.\n*   `SampleFormat`: The sample format of the raw audio data.\n*   `SampleRate`: The sample rate of the raw audio data.\n    **Events:**\n*   `EndOfStreamReached`: Raised when the end of the stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n    **Methods:**\n*   `ReadBytes(Span<float> buffer)`: Reads raw PCM data from the stream and converts it to `float` if necessary.\n*   `Seek(int offset)`: Seeks to the specified offset in the underlying stream if `CanSeek` is true.\n*   `Dispose()`: Disposes the underlying stream.\n\n\n### Structs\n\n### `AudioFormat`\n```csharp\npublic record struct AudioFormat\n{\n    public SampleFormat Format;\n    public int Channels;\n    public int SampleRate;\n    public float InverseSampleRate { get; }\n\n    public static readonly AudioFormat Cd;\n    public static readonly AudioFormat Dvd;\n    public static readonly AudioFormat DvdHq;\n    public static readonly AudioFormat Studio;\n    public static readonly AudioFormat StudioHq;\n    public static readonly AudioFormat Broadcast;\n    public static readonly AudioFormat Telephony;\n}\n```\n**Description:** A record struct representing the format of an audio stream, including sample format, channel count, and sample rate. It is a value type and provides several common presets.\n\n**Fields:**\n\n*   `Format`: The sample format (e.g., `SampleFormat.S16`, `SampleFormat.F32`).\n*   `Channels`: The number of audio channels (e.g., 1 for mono, 2 for stereo).\n*   `SampleRate`: The sample rate in Hertz (e.g., 44100, 48000).\n\n**Properties:**\n\n*   `InverseSampleRate`: Gets the inverse of the sample rate, useful for calculations involving sample duration.\n\n**Static Presets:**\n\n*   `Cd`: Standard Compact Disc (CD) audio format (S16, 2 Channels, 44100 Hz).\n*   `Dvd`: Standard DVD-Video audio format (S16, 2 Channels, 48000 Hz).\n*   `DvdHq`: High-quality DVD-Video audio format (F32, 2 Channels, 48000 Hz).\n*   `Studio`: Common studio recording format (S24, 2 Channels, 96000 Hz).\n*   `StudioHq`: High-quality studio recording format (F32, 2 Channels, 96000 Hz).\n*   `Broadcast`: Standard broadcast audio format (S16, 1 Channel, 48000 Hz).\n*   `Telephony`: Telephony and VoIP audio format (U8, 1 Channel, 8000 Hz).\n\n#### `DeviceInfo`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct DeviceInfo\n{\n    public IntPtr Id;\n    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 256)]\n    public string Name;\n    [MarshalAs(UnmanagedType.U1)]\n    public bool IsDefault;\n    public uint NativeDataFormatCount;\n    public IntPtr NativeDataFormats; // Pointer to an array of NativeDataFormat\n}\n```\n**Description:** Represents information about an audio device, including its native ID, name, whether it's the default system device, and a count/pointer to its supported native data formats.\n\n#### `NativeDataFormat`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct NativeDataFormat\n{\n    public SampleFormat Format;\n    public uint Channels;\n    public uint SampleRate;\n    public uint Flags;\n}\n```\n**Description:** Represents a specific native data format (sample format, channels, sample rate) supported by an audio device. Accessed via the `NativeDataFormats` pointer in `DeviceInfo`.\n\n### Utils `Extensions`\n\n```csharp\npublic static class Extensions\n{\n    public static int GetBytesPerSample(this SampleFormat sampleFormat);\n    public static unsafe Span<T> GetSpan<T>(nint ptr, int length) where T : unmanaged;\n    public static T[] ReadArray<T>(this nint pointer, int count) where T : struct;\n}\n```\n**Methods:**\n*   `GetBytesPerSample(this SampleFormat sampleFormat)`: Returns the number of bytes per sample for a given sample format.\n*   `GetSpan<T>(nint ptr, int length)`: Creates a span of type `T` from a native memory pointer.\n*   `ReadArray<T>(this nint pointer, int count) where T : struct`: Reads an array of structures of type `T` from a native memory pointer.\n\n### `MathHelper`\n```csharp\npublic static class MathHelper\n{\n    public static bool EnableAvx { get; set; }\n    public static bool EnableSse { get; set; }\n\n    public static void InverseFft(Complex[] data);\n    public static void Fft(Complex[] data);\n\n    public static float[] HammingWindow(int size);\n    public static float[] HanningWindow(int size);\n\n    public static float Lerp(float a, float b, float t);\n    public static bool IsPowerOfTwo(long n);\n    public static double Mod(this double x, double y);\n    public static float PrincipalAngle(float angle);\n}\n```\n**Description:** Provides static helper methods for common mathematical operations, with a particular focus on signal processing functions like Fast Fourier Transforms (FFT/IFFT) and windowing functions. These methods leverage SIMD (Single Instruction, Multiple Data) instructions (AVX and SSE) for optimized performance on compatible hardware, with fallbacks to scalar implementations.\n\n**Properties:**\n\n*   `EnableAvx`: Gets or sets a value indicating whether to use AVX (Advanced Vector Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of AVX, forcing a fallback to SSE or scalar code.\n*   `EnableSse`: Gets or sets a value indicating whether to use SSE (Streaming SIMD Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of SSE, forcing a fallback to scalar code. This also affects AVX routines that may utilize SSE for specific operations.\n\n**Methods:**\n\n*   `InverseFft(Complex[] data)`: Computes the Inverse Fast Fourier Transform (IFFT) of a complex array. It leverages the `Fft` method internally after conjugating the input and then scales and conjugates the output.\n*   `Fft(Complex[] data)`: Computes the Fast Fourier Transform (FFT) of a complex array. The length of the `data` array must be a power of two. This method automatically selects the most optimized implementation available:\n*   Prioritizes AVX (Advanced Vector Extensions) if `EnableAvx` is `true` and hardware supports it.\n*   Falls back to SSE (Streaming SIMD Extensions) if AVX is not used, `EnableSse` is `true`, and hardware supports SSE3.\n*   Uses a scalar (non-SIMD) implementation if neither AVX nor SSE is available or enabled.\n*   **Throws:** `ArgumentException` if the `data` array length is not a power of two.\n*   `HammingWindow(int size)`: Generates a Hamming window of the specified `size`. Utilizes SIMD (AVX or SSE) acceleration for performance if `EnableAvx` or `EnableSse` is `true` and hardware supports the necessary instructions (SSE4.1 for `Floor` intrinsic), otherwise falls back to a scalar implementation.\n*   `HanningWindow(int size)`: Generates a Hanning window of the specified `size`. Similar to `HammingWindow`, it uses SIMD acceleration (AVX or SSE) if enabled and supported, otherwise falls back to a scalar implementation.\n*   `Lerp(float a, float b, float t)`: Performs linear interpolation between two float values `a` and `b`. The interpolation factor `t` is clamped between 0 and 1, ensuring the result is always between `a` and `b`.\n*   `IsPowerOfTwo(long n)`: Checks if a given long integer `n` is a power of two (e.g., 2, 4, 8, 16, etc.). Returns `true` if `n` is positive and is a power of two, `false` otherwise.\n*   `Mod(this double x, double y)`: An extension method for `double` that returns the remainder of the division of `x` by `y`, ensuring the result is always in the range `[0, y)`. This differs from the standard `%` operator for negative `x`.\n*   `PrincipalAngle(float angle)`: Calculates the principal angle for a given `angle` in radians, ensuring the result lies within the range `[-PI, PI)`.\n\n### Visualization `LevelMeterAnalyzer`\n\n```csharp\npublic class LevelMeterAnalyzer : AudioAnalyzer\n{\n    public LevelMeterAnalyzer(AudioFormat format, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public float Peak { get; }\n    public float Rms { get; }\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Peak`: The peak level of the audio signal.\n*   `Rms`: The RMS (root mean square) level of the audio signal.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to calculate the RMS and peak levels.\n\n### Visualization `LevelMeterVisualizer`\n\n```csharp\npublic class LevelMeterVisualizer : IVisualizer\n{\n    public LevelMeterVisualizer(LevelMeterAnalyzer levelMeterAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public Color PeakHoldColor { get; set; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the level meter bar.\n*   `Name`: The name of the visualizer.\n*   `PeakHoldColor`: The color of the peak hold indicator.\n*   `Size`: The size of the level meter.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the level meter.\n*   `Render(IVisualizationContext context)`: Renders the level meter visualization.\n\n### Visualization `SpectrumAnalyzer`\n\n```csharp\npublic class SpectrumAnalyzer : AudioAnalyzer\n{\n    public SpectrumAnalyzer(AudioFormat format, int fftSize, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public ReadOnlySpan<float> SpectrumData { get; }\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `SpectrumData`: The calculated frequency spectrum data.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to compute the frequency spectrum using an FFT.\n\n### Visualization `SpectrumVisualizer`\n\n```csharp\npublic class SpectrumVisualizer : IVisualizer\n{\n    public SpectrumVisualizer(SpectrumAnalyzer spectrumAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the spectrum bars.\n*   `Name`: The name of the visualizer.\n*   `Size`: The size of the spectrum visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the spectrum.\n*   `Render(IVisualizationContext context)`: Renders the spectrum visualization.\n\n### Visualization `WaveformVisualizer`\n\n```csharp\npublic class WaveformVisualizer : IVisualizer\n{\n    public WaveformVisualizer();\n\n    public string Name { get; }\n    public List<float> Waveform { get; }\n    public Color WaveformColor { get; set; }\n    public Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n*   `Waveform`: The waveform data.\n*   `WaveformColor`: The color of the waveform.\n*   `Size`: The size of the waveform visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the waveform.\n*   `Render(IVisualizationContext context)`: Renders the waveform visualization."
  },
  {
    "id": 6,
    "slug": "advanced-topics",
    "version": "1.2.0",
    "title": "Advanced Topics",
    "description": "Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.",
    "navOrder": 6,
    "category": "Core",
    "content": "---\nid: 6\ntitle: Advanced Topics\ndescription: Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.\nnavOrder: 6\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\nThis section delves into more advanced topics related to SoundFlow, including extending the engine with custom components, optimizing performance, and understanding threading considerations.\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Advanced Topics\">\n    <Tab\n        key=\"extending\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ph:puzzle-piece-bold' />\n                <span>Extending SoundFlow</span>\n            </div>\n        }\n    >\n        ## Extending SoundFlow\n\n        One of SoundFlow's key strengths is its extensibility. You can tailor the engine to your specific needs by creating custom:\n\n        *   **Sound Components (`SoundComponent`)**\n        *   **Sound Modifiers (`SoundModifier`)**\n        *   **Audio Analyzers (`AudioAnalyzer`)** and **Visualizers (`IVisualizer`)**\n        *   **Audio Backends (`AudioEngine`)**\n        *   **Sound Data Providers (`ISoundDataProvider`)**\n        *   **Extensions (e.g., for specific DSP libraries)**: SoundFlow supports integration with external audio processing libraries. For instance, the `SoundFlow.Extensions.WebRtc.Apm` package provides features like noise suppression and echo cancellation by wrapping the WebRTC Audio Processing Module. You can create similar extensions for other libraries.\n\n        ### Custom Sound Components\n\n        Creating custom `SoundComponent` classes allows you to implement unique audio processing logic and integrate it seamlessly into the SoundFlow audio graph.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundComponent\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundComponent` class. Your constructor must accept `AudioEngine` and `AudioFormat` parameters and pass them to the base constructor.\n            </Step>\n            <Step title=\"Implement GenerateAudio\" icon='lucide:audio-lines'>\n                Override the `GenerateAudio(Span<float> buffer, int channels)` method. This is where you'll write the core audio processing code for your component. The `buffer` passed to this method already contains the mixed output from all of the component's inputs.\n                *   If your component **generates** new audio (e.g., an oscillator), it should add its generated samples to the `buffer`.\n                *   If your component **modifies** incoming audio, it should process the samples within the `buffer` in-place.\n            </Step>\n            <Step title=\"Override other methods (optional)\" icon='icon-park-outline:switch-one'>\n                You can override methods like `ConnectInput`, `AddAnalyzer`, `AddModifier`, etc., to customize how your component interacts with the audio graph.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your component to expose configurable parameters that users can adjust.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Structs;\n        using System;\n\n        public class CustomGainComponent : SoundComponent\n        {\n            public float Gain { get; set; } = 1.0f; // Default gain\n\n            public override string Name { get; set; } = \"Custom Gain\";\n            \n            // The constructor must match the base class requirements.\n            public CustomGainComponent(AudioEngine engine, AudioFormat format) : base(engine, format) { }\n\n            protected override void GenerateAudio(Span<float> buffer, int channels)\n            {\n                // The buffer already contains mixed audio from any connected inputs.\n                // We simply modify it in-place.\n                for (int i = 0; i < buffer.Length; i++)\n                {\n                    buffer[i] *= Gain;\n                }\n            }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System.Linq;\n        \n        // 1. Create an instance of an audio engine.\n        using var engine = new MiniAudioEngine();\n\n        // 2. Define the desired audio format.\n        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\n\n        // 3. Get the default playback device info from the engine.\n        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\n\n        // 4. Initialize a playback device.\n        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);\n\n        // 5. Create the player and your custom component, passing the engine and format.\n        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(engine, audioFormat, dataProvider);\n        var gainComponent = new CustomGainComponent(engine, audioFormat) { Gain = 0.5f };\n\n        // 6. Connect the player as an input to the gain component.\n        gainComponent.ConnectInput(player);\n        \n        // 7. Add the final component in the chain to the device's master mixer.\n        device.MasterMixer.AddComponent(gainComponent);\n        \n        // 8. Start the device and play the sound.\n        device.Start();\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Sound Modifiers\n\n        Custom `SoundModifier` classes allow you to implement your own audio effects.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundModifier\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundModifier` class.\n            </Step>\n            <Step title=\"Implement ProcessSample\" icon='icon-park-outline:sound-wave'>\n                Implement `ProcessSample(float sample, int channel)` (or override `Process` for buffer-level):\n                *   `ProcessSample(float sample, int channel)`: This method takes a single audio sample and the channel index as input and returns the modified sample.\n                *   `Process(Span<float> buffer, int channels)`: Override this for more complex effects that operate on entire buffers (e.g., FFT-based effects). By default, it calls `ProcessSample` for each sample.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your modifier to expose configurable parameters.\n            </Step>\n            <Step title=\"Use 'Enabled' property\" icon='material-symbols:toggle-on-outline'>\n                Your modifier will have an `Enabled` property (defaulting to `true`) to allow toggling its effect.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using System;\n\n        public class CustomDistortionModifier : SoundModifier\n        {\n            public float Threshold { get; set; } = 0.5f;\n\n            public override string Name { get; set; } = \"Custom Distortion\";\n\n            public override float ProcessSample(float sample, int channel)\n            {\n                // Simple hard clipping distortion\n                if (sample > Threshold)\n                {\n                    return Threshold;\n                }\n                else if (sample < -Threshold)\n                {\n                    return -Threshold;\n                }\n                else\n                {\n                    return sample;\n                }\n            }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System.Linq;\n\n        // 1. Create an engine and define the audio format.\n        using var engine = new MiniAudioEngine();\n        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\n        \n        // 2. Get device info and initialize a playback device.\n        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\n        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);\n        \n        // 3. Create the data provider and player.\n        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(engine, audioFormat, dataProvider);\n\n        // 4. Create an instance of your custom modifier.\n        var distortion = new CustomDistortionModifier { Threshold = 0.7f };\n        // distortion.Enabled = false; // To disable it\n\n        // 5. Add the modifier to a SoundComponent (like a player).\n        player.AddModifier(distortion);\n        \n        // 6. Add the player to the device's master mixer.\n        device.MasterMixer.AddComponent(player);\n        \n        // 7. Start the device and play.\n        device.Start();\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Visualizers\n\n        Custom `IVisualizer` classes allow you to create unique visual representations of audio data.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement IVisualizer\" icon='ph:plugs-connected-bold'>\n                Create a new class that implements the `IVisualizer` interface.\n            </Step>\n            <Step title=\"Implement ProcessOnAudioData\" icon='carbon:data-vis-4'>\n                This method receives a `Span<float>` containing audio data. You should process this data and store the relevant information needed for rendering.\n            </Step>\n            <Step title=\"Implement Render\" icon='material-symbols:draw-outline'>\n                This method receives an `IVisualizationContext`. Use the drawing methods provided by the context (e.g., `DrawLine`, `DrawRectangle`) to render your visualization.\n            </Step>\n            <Step title=\"Raise VisualizationUpdated\" icon='mdi:bell-ring-outline'>\n                When the visualization data changes (e.g., after processing new audio data), raise the `VisualizationUpdated` event to notify the UI to update the display.\n            </Step>\n            <Step title=\"Implement Dispose\" icon='material-symbols:delete-outline'>\n                Release any unmanaged resources or unsubscribe from events.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Interfaces;\n        using System;\n        using System.Numerics;\n\n        public class CustomBarGraphVisualizer : IVisualizer\n        {\n            private float _level;\n\n            public string Name => \"Custom Bar Graph\";\n\n            public event EventHandler? VisualizationUpdated;\n\n            public void ProcessOnAudioData(Span<float> audioData)\n            {\n                if (audioData.IsEmpty) return;\n                // Calculate the average level (simplified for this example)\n                float sum = 0;\n                for (int i = 0; i < audioData.Length; i++)\n                {\n                    sum += Math.Abs(audioData[i]);\n                }\n                _level = sum / audioData.Length;\n\n                // Notify that the visualization needs to be updated\n                VisualizationUpdated?.Invoke(this, EventArgs.Empty);\n            }\n\n            public void Render(IVisualizationContext context)\n            {\n                // Clear the drawing area\n                context.Clear();\n\n                // Draw a simple bar graph based on the calculated level\n                float barHeight = _level * 200; // Scale the level for visualization\n                context.DrawRectangle(10, 200 - barHeight, 30, barHeight, new Color(0, 1, 0));\n            }\n\n            public void Dispose()\n            {\n                // Unsubscribe from events, release resources if any\n                VisualizationUpdated = null;\n            }\n        }\n        ```\n\n        ### Adding Audio Backends\n\n        SoundFlow is designed to support multiple audio backends. Currently, it includes a `MiniAudio` backend. You can add support for other audio APIs (e.g., WASAPI, ASIO, CoreAudio) by creating a new backend.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from AudioEngine\" icon='ph:engine-bold'>\n                Create a new class that inherits from the abstract `AudioEngine`. This class will manage the entire lifecycle of your custom backend.\n            </Step>\n            <Step title=\"Implement Abstract Methods\" icon='material-symbols:function'>\n                Implement all the abstract methods from `AudioEngine`:\n                *   `InitializeBackend()` and `CleanupBackend()`: Handle global setup/teardown of your native backend's context.\n                *   `InitializePlaybackDevice()`, `InitializeCaptureDevice()`, `InitializeFullDuplexDevice()`, `InitializeLoopbackDevice()`: These methods are the core of device management. You will implement the logic to initialize a device using your backend's API and return an instance of a class that inherits from the appropriate abstract device class.\n                *   `SwitchDevice(...)` (3 overloads): Implement logic to tear down an old device and initialize a new one while preserving the state (audio graph, event listeners).\n                *   `CreateEncoder(...)` and `CreateDecoder(...)`: Return your backend-specific implementations of the `ISoundEncoder` and `ISoundDecoder` interfaces.\n                *   `UpdateDevicesInfo()`: Implement logic to enumerate playback and capture devices using the backend's API, populating `PlaybackDevices` and `CaptureDevices`.\n            </Step>\n            <Step title=\"Create Device Wrappers\" icon='ph:package-bold'>\n                Create concrete classes inheriting from `AudioPlaybackDevice` and `AudioCaptureDevice`. These will wrap the native device handles and logic specific to your backend, including the audio callback that drives the processing graph.\n            </Step>\n            <Step title=\"Implement Codec Interfaces\" icon='mdi:file-code-outline'>\n                Create classes that implement `ISoundEncoder` and `ISoundDecoder` to handle audio encoding and decoding for your chosen backend.\n            </Step>\n        </Steps>\n\n        **Example (Skeleton):**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Abstracts.Devices;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Enums;\n        using SoundFlow.Structs;\n        using System;\n        using System.IO;\n        \n        // Custom Backend Engine\n        public class MyNewAudioEngine : AudioEngine\n        {\n            private nint _context; // Example native context handle\n            \n            // The parameterless constructor calls InitializeBackend() automatically.\n            public MyNewAudioEngine() { }\n\n            protected override void InitializeBackend()\n            {\n                // _context = NativeApi.InitContext();\n                // UpdateDevicesInfo(); // Initial device enumeration\n                Console.WriteLine(\"MyNewAudioEngine: Backend initialized.\");\n            }\n\n            protected override void CleanupBackend()\n            {\n                // NativeApi.UninitContext(_context);\n                Console.WriteLine(\"MyNewAudioEngine: Backend cleaned up.\");\n            }\n            \n            public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)\n            {\n                // Create and return your backend-specific playback device wrapper\n                return new MyNewPlaybackDevice(this, _context, deviceInfo, format, config);\n            }\n\n            public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)\n            {\n                // Similar logic for capture devices...\n                throw new NotImplementedException();\n            }\n\n            // Implement other abstract methods...\n            public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format) => throw new NotImplementedException();\n            public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format) => throw new NotImplementedException();\n            public override void UpdateDevicesInfo() { /* NativeApi.GetDevices(...); */ }\n            public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n        }\n\n        // Custom Playback Device Wrapper\n        public class MyNewPlaybackDevice : AudioPlaybackDevice\n        {\n            private nint _deviceHandle;\n            \n            public MyNewPlaybackDevice(AudioEngine engine, nint context, DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config)\n                : base(engine, format, config ?? new MyDeviceConfig())\n            {\n                // _deviceHandle = NativeApi.InitDevice(context, deviceInfo?.Id, OnAudioCallback);\n                // this.Info = ... // Populate DeviceInfo from native API\n                this.Capability = Capability.Playback;\n            }\n\n            public override void Start() { /* NativeApi.StartDevice(_deviceHandle); */ IsRunning = true; }\n            public override void Stop() { /* NativeApi.StopDevice(_deviceHandle); */ IsRunning = false; }\n            public override void Dispose() \n            {\n                if (IsDisposed) return;\n                Stop();\n                // NativeApi.UninitDevice(_deviceHandle);\n                OnDisposedHandler();\n                IsDisposed = true;\n            }\n            \n            // This callback is invoked by the native backend on the audio thread.\n            private void OnAudioCallback(Span<float> buffer)\n            {\n                // 1. Clear the buffer\n                buffer.Clear();\n                // 2. Process the master mixer or a soloed component\n                var soloed = Engine.GetSoloedComponent();\n                if (soloed != null)\n                    soloed.Process(buffer, Format.Channels);\n                else\n                    MasterMixer.Process(buffer, Format.Channels);\n            }\n        }\n        \n        // Custom Device Configuration (Optional)\n        public class MyDeviceConfig : DeviceConfig { /* ... */ }\n        ```\n    </Tab>\n\n    <Tab\n        key=\"performance\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ic:round-speed' />\n                <span>Performance Optimization</span>\n            </div>\n        }\n    >\n        ## Performance Optimization\n\n        Here are some tips for optimizing the performance of your SoundFlow applications:\n\n        *   **Buffer Sizes:** Choose appropriate buffer sizes for your use case. Smaller buffers reduce latency but increase CPU overhead. Larger buffers can improve efficiency but may introduce latency. With the `MiniAudio` backend, you can specify this via `MiniAudioDeviceConfig` when initializing a device.\n        *   **SIMD:** SoundFlow uses SIMD instructions (when available) in critical paths like the `Mixer`, `SoundComponent` (for volume and panning), `MathHelper` (for FFTs and windowing), and in the `DeviceBufferHelper` for audio format conversions. Ensure your target platform supports SIMD for the best performance.\n        *   **Profiling:** Use a profiler (like the one built into Visual Studio) to identify performance bottlenecks in your audio processing pipeline.\n        *   **Asynchronous Operations:** For long-running operations (e.g., loading large files, network requests in `NetworkDataProvider`, project saving/loading), use asynchronous programming (`async` and `await`) to avoid blocking the main thread or the audio thread.\n        *   **Avoid Allocations:** Minimize memory allocations within the `GenerateAudio` method of `SoundComponent` and the `ProcessSample` or `Process` method of `SoundModifier`. SoundFlow internally uses `ArrayPool<T>.Shared` for many temporary buffers to reduce GC pressure.\n        *   **Efficient Algorithms:** Use efficient algorithms for audio processing, especially in performance-critical sections.\n        *   **Modifier Overhead:** Each `SoundModifier` added to a component or to the editing hierarchy (`AudioSegment`, `Track`, `Composition`) introduces some overhead. For very simple operations, integrating them directly into a custom `SoundComponent` might be more performant than using many tiny modifiers. However, modifiers offer better reusability and modularity.\n        *   **Effect Toggling:** Use the `Enabled` property on `SoundModifier`, `AudioAnalyzer`, `SoundComponent`, `AudioSegmentSettings`, and `TrackSettings` to non-destructively disable effects or entire processing paths instead of removing and re-adding them, which is more efficient.\n    </Tab>\n\n    <Tab\n        key=\"threading\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='carbon:thread' />\n                <span>Threading Considerations</span>\n            </div>\n        }\n    >\n        ## Threading Considerations\n\n        SoundFlow uses a dedicated, high-priority thread (managed by the audio backend, e.g., MiniAudio) for audio processing. This ensures that audio is processed in real time and minimizes the risk of glitches or dropouts.\n\n        **Key Considerations:**\n\n        *   **Audio Thread:** The audio processing logic is executed on a dedicated audio thread. This thread is responsible for calling the audio callback provided to the backend. In SoundFlow's `MiniAudio` implementation, this callback triggers the `Process` method on the appropriate `MiniAudioPlaybackDevice` or `MiniAudioCaptureDevice`, which in turn traverses the `SoundComponent` graph (e.g., calling `MasterMixer.Process(...)`). Therefore, all code within `GenerateAudio` (for `SoundComponent`) and `Process`/`ProcessSample` (for `SoundModifier`) runs on this critical audio thread. Avoid performing any long-running or blocking operations (like I/O, complex non-audio computations, or direct UI updates) on this thread.\n        *   **UI Thread:** Never perform audio processing directly on the UI thread. This can lead to unresponsiveness and glitches. Use the `AudioEngine`'s audio thread for all audio-related operations. For UI updates based on audio events (e.g., from an `IVisualizer`'s `VisualizationUpdated` event), you must marshal the calls to the UI thread (e.g., using `Dispatcher.Invoke` in WPF/Avalonia, or `Control.Invoke` in WinForms).\n        *   **Thread Safety:** If you need to access or modify shared data from both the audio thread and another thread (e.g., the UI thread updating a `SoundModifier`'s property), use appropriate synchronization mechanisms (like `lock`, `Monitor`, or thread-safe collections) to ensure data integrity and prevent race conditions. Many properties on `SoundComponent` and `SoundModifier` are internally locked for thread-safe access from different threads.\n    </Tab>\n</Tabs>"
  },
  {
    "id": 11,
    "slug": "webrtc-apm",
    "version": "1.1.2",
    "title": "WebRTC Audio Processing Module (APM) Extension",
    "description": "Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.",
    "navOrder": 7,
    "category": "Extensions",
    "content": "﻿---\r\ntitle: WebRTC Audio Processing Module (APM) Extension\r\ndescription: Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.\r\nnavOrder: 7\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# WebRTC Audio Processing Module (APM) Extension for SoundFlow\r\n\r\nThe `SoundFlow.Extensions.WebRtc.Apm` package integrates a native library based on the high-quality WebRTC Audio Processing Module (APM) into the SoundFlow audio engine. This extension provides advanced voice processing features essential for real-time communication and audio enhancement.\r\n\r\n## Features\r\n\r\nThe WebRTC APM extension offers several key audio processing features:\r\n\r\n*   **Acoustic Echo Cancellation (AEC):** Reduces or eliminates echoes that occur when audio played through speakers is picked up by the microphone.\r\n*   **Noise Suppression (NS):** Attenuates steady-state background noise (e.g., fans, hums) to improve speech clarity. Multiple suppression levels are available.\r\n*   **Automatic Gain Control (AGC):** Dynamically adjusts the microphone input volume to maintain a consistent audio level, preventing clipping or overly quiet audio. Supports different modes and target levels.\r\n*   **High Pass Filter (HPF):** Removes low-frequency components (typically below 80Hz) to reduce rumble and DC offset.\r\n*   **Pre-Amplifier:** Applies a configurable fixed gain to the audio signal before other APM processing steps.\r\n*   **Multi-channel Processing Configuration:** Allows specifying how multi-channel audio is handled and downmixed.\r\n\r\nThese features can be configured and applied primarily through the `WebRtcApmModifier` for real-time processing within the SoundFlow audio graph, or using the `NoiseSuppressor` component for offline batch processing.\r\n\r\n**Important Note on Sample Rates:** The WebRTC APM native library primarily supports specific sample rates: **8000 Hz, 16000 Hz, 32000 Hz, and 48000 Hz**. Ensure your SoundFlow `AudioEngine` is initialized with one of these sample rates when using this extension for optimal performance and compatibility.\r\n\r\n## Installation\r\n\r\nTo use this extension, you need to have the core `SoundFlow` library installed. Then, add the `SoundFlow.Extensions.WebRtc.Apm` package to your project:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\nThis package includes the necessary C# wrapper and the native WebRTC APM binaries for supported platforms.\r\n\r\n## Usage\r\n\r\n### Real-time Processing with `WebRtcApmModifier`\r\n\r\nThe `WebRtcApmModifier` is a `SoundModifier` that can be added to any `SoundComponent` to process its audio output in real-time. This is ideal for applications like voice chat, live audio input processing, etc.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Use a supported sample rate\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        Make sure to use a supported sample rate (e.g., 48000 Hz).\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n\r\n        // Initialize with a WebRTC APM compatible sample rate, e.g., 48kHz\r\n        // And enable mixed capability if you plan to use microphone input and playback for AEC.\r\n        var audioEngine = new MiniAudioEngine(48000, Capability.Mixed, channels: 1); // Mono for typical voice\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create SoundComponent\" description=\"Create a player for your audio source\" icon='ph:speaker-high-bold'>\r\n        ### 2. Create your `SoundComponent`\r\n        This could be a `SoundPlayer` playing microphone input, or any other component whose output you want to process.\r\n\r\n        ```csharp\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Providers;\r\n\r\n        // Example: Using microphone input\r\n        var microphoneDataProvider = new MicrophoneDataProvider();\r\n        var micPlayer = new SoundPlayer(microphoneDataProvider);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Configure APM Modifier\" description=\"Instantiate and set up the modifier\" icon='material-symbols:settings-outline'>\r\n        ### 3. Instantiate and Configure `WebRtcApmModifier`\r\n        The modifier's constructor allows setting initial states for all features. You can also adjust them dynamically via its public properties.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Modifiers;\r\n\r\n        var apmModifier = new WebRtcApmModifier(\r\n        // Echo Cancellation (AEC) settings\r\n        aecEnabled: true,\r\n        aecMobileMode: false, // Desktop mode is generally more robust\r\n        aecLatencyMs: 40,     // Estimated system latency for AEC (tune this)\r\n\r\n        // Noise Suppression (NS) settings\r\n        nsEnabled: true,\r\n        nsLevel: NoiseSuppressionLevel.High,\r\n\r\n        // Automatic Gain Control (AGC) - Version 1 (legacy)\r\n        agc1Enabled: true,\r\n        agcMode: GainControlMode.AdaptiveDigital,\r\n        agcTargetLevel: -3,   // Target level in dBFS (0 is max, typical is -3 to -18)\r\n        agcCompressionGain: 9, // Only for FixedDigital mode\r\n        agcLimiter: true,\r\n\r\n        // Automatic Gain Control (AGC) - Version 2 (newer, often preferred)\r\n        agc2Enabled: false, // Set to true to use AGC2, potentially disable AGC1\r\n\r\n        // High Pass Filter (HPF)\r\n        hpfEnabled: true,\r\n\r\n        // Pre-Amplifier\r\n        preAmpEnabled: false,\r\n        preAmpGain: 1.0f,\r\n\r\n        // Pipeline settings for multi-channel audio (if numChannels > 1)\r\n        useMultichannelCapture: false, // Process capture (mic) as mono/stereo as configured by AudioEngine\r\n        useMultichannelRender: false,  // Process render (playback for AEC) as mono/stereo\r\n        downmixMethod: DownmixMethod.AverageChannels // Method if downmixing is needed\r\n        );\r\n\r\n        // Example of changing a setting dynamically:\r\n        // apmModifier.NoiseSuppression.Level = NoiseSuppressionLevel.VeryHigh;\r\n        ```\r\n    </Step>\r\n    <Step title=\"Add Modifier\" description=\"Attach the modifier to the component\" icon='ic:baseline-plus'>\r\n        ### 4. Add the Modifier to your `SoundComponent`\r\n\r\n        ```csharp\r\n        micPlayer.AddModifier(apmModifier);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Start Processing\" description=\"Add to mixer and start playback\" icon='mdi:play-box-outline'>\r\n        ### 5. Add the `SoundComponent` to the `Mixer` and start processing/playback\r\n\r\n        ```csharp\r\n        Mixer.Master.AddComponent(micPlayer);\r\n        microphoneDataProvider.StartCapture(); // If using microphone\r\n        micPlayer.Play(); // Start processing the microphone input\r\n\r\n        Console.WriteLine(\"WebRTC APM processing microphone input. Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        // Cleanup\r\n        microphoneDataProvider.StopCapture();\r\n        micPlayer.Stop();\r\n        Mixer.Master.RemoveComponent(micPlayer);\r\n        apmModifier.Dispose(); // Important to release native resources\r\n        microphoneDataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n**AEC Far-End (Playback) Signal:** For Acoustic Echo Cancellation to work effectively, the `WebRtcApmModifier` automatically listens to `AudioEngine.OnAudioProcessed` events for audio being played back (capability `Playback`). This playback audio is fed as the \"far-end\" or \"render\" signal to the AEC. Ensure your `AudioEngine` is initialized with `Capability.Mixed` if you're using AEC with live microphone input and simultaneous playback.\r\n\r\n### Offline Processing with `NoiseSuppressor`\r\n\r\nThe `NoiseSuppressor` component is designed for batch processing of audio from an `ISoundDataProvider` (e.g., an audio file). It applies only the WebRTC Noise Suppression feature.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Required for encoding/decoding\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        (Required for `ISoundDataProvider` decoding and `ISoundEncoder` encoding, even if not playing back). Use a supported sample rate.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n\r\n        var audioEngine = new MiniAudioEngine(48000, Capability.Playback); // Or Record, if only encoding\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create Data Provider\" description=\"Load your noisy audio file\" icon='mdi:file-music-outline'>\r\n        ### 2. Create an `ISoundDataProvider` for your noisy audio file\r\n\r\n        ```csharp\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Ensure this file's sample rate and channel count match what NoiseSuppressor expects\r\n        string noisyFilePath = \"path/to/your/noisy_audio.wav\";\r\n        var dataProvider = new StreamDataProvider(File.OpenRead(noisyFilePath));\r\n        ```\r\n    </Step>\r\n    <Step title=\"Instantiate NoiseSuppressor\" description=\"Set up the offline processor\" icon='icon-park-outline:sound-wave'>\r\n        ### 3. Instantiate `NoiseSuppressor`\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Components;\r\n\r\n        // Parameters for NoiseSuppressor: dataProvider, sampleRate, numChannels, suppressionLevel\r\n        // These MUST match the actual properties of the audio from dataProvider.\r\n        var noiseSuppressor = new NoiseSuppressor(dataProvider, 48000, 1, NoiseSuppressionLevel.VeryHigh);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Process the Audio\" description=\"Process all at once or in chunks\" icon='carbon:cics-transaction-server-zos'>\r\n        ### 4. Process the audio\r\n        You can process all audio at once (for smaller files) or chunk by chunk.\r\n\r\n        **Option A: Process All (returns `float[]`)**\r\n        ```csharp\r\n        float[] cleanedAudio = noiseSuppressor.ProcessAll();\r\n        // Now 'cleanedAudio' contains the noise-suppressed audio data.\r\n        // You can save it using an ISoundEncoder:\r\n        var fileStream = new FileStream(\"cleaned_audio.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        var encoder = audioEngine.CreateEncoder(fileStream, EncodingFormat.Wav, SampleFormat.F32, 1, 48000);\r\n        encoder.Encode(cleanedAudio.AsSpan());\r\n        encoder.Dispose();\r\n        fileStream.Dispose();\r\n        ```\r\n\r\n        **Option B: Process Chunks (via event or direct handler)**\r\n        ```csharp\r\n        var chunkFileStream = new FileStream(\"cleaned_audio_chunked.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        var chunkEncoder = audioEngine.CreateEncoder(chunkFileStream, EncodingFormat.Wav, SampleFormat.F32, 1, 48000);\r\n\r\n        noiseSuppressor.OnAudioChunkProcessed += (processedChunk) =>\r\n        {\r\n            if (!chunkEncoder.IsDisposed)\r\n        {\r\n            chunkEncoder.Encode(processedChunk.ToArray());\r\n        }\r\n        };\r\n\r\n        // ProcessChunks is a blocking call until the entire provider is processed.\r\n        noiseSuppressor.ProcessChunks();\r\n        chunkEncoder.Dispose(); // Finalize and save the encoded file\r\n        chunkFileStream.Dispose();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Dispose Resources\" description=\"Clean up all IDisposable objects\" icon='material-symbols:delete-outline'>\r\n        ### 5. Dispose resources\r\n\r\n        ```csharp\r\n        noiseSuppressor.Dispose();\r\n        dataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n## Configuration Details\r\n\r\n### `WebRtcApmModifier` Properties:\r\n\r\n*   **`Enabled` (bool):** Enables/disables the entire APM modifier.\r\n*   **`EchoCancellation` (`EchoCancellationSettings`):**\r\n*   `Enabled` (bool): Enables/disables AEC.\r\n*   `MobileMode` (bool): Toggles between desktop (false) and mobile (true) AEC modes.\r\n*   `LatencyMs` (int): Estimated system audio latency in milliseconds. Crucial for AEC performance. Tune this value for your setup.\r\n*   **`NoiseSuppression` (`NoiseSuppressionSettings`):**\r\n*   `Enabled` (bool): Enables/disables NS.\r\n*   `Level` (`NoiseSuppressionLevel`): Sets the aggressiveness (Low, Moderate, High, VeryHigh).\r\n*   **`AutomaticGainControl` (`AutomaticGainControlSettings`):**\r\n*   `Agc1Enabled` (bool): Enables/disables the legacy AGC1.\r\n*   `Mode` (`GainControlMode`): Sets the mode for AGC1 (AdaptiveAnalog, AdaptiveDigital, FixedDigital).\r\n*   `TargetLevelDbfs` (int): Target level for AGC1 AdaptiveDigital mode (-31 to 0 dBFS).\r\n*   `CompressionGainDb` (int): Gain for AGC1 FixedDigital mode (0 to 90 dB).\r\n*   `LimiterEnabled` (bool): Enables/disables the limiter for AGC1.\r\n*   `Agc2Enabled` (bool): Enables/disables the newer AGC2.\r\n*   **`HighPassFilterEnabled` (bool):** Enables/disables the HPF.\r\n*   **`PreAmplifierEnabled` (bool):** Enables/disables the pre-amplifier.\r\n*   **`PreAmplifierGainFactor` (float):** Gain factor for the pre-amplifier (e.g., 1.0 is no change, 2.0 is +6dB).\r\n*   **`ProcessingPipeline` (`ProcessingPipelineSettings`):**\r\n*   `UseMultichannelCapture` (bool): If true and input is multi-channel, APM processes it as such. Otherwise, it might downmix.\r\n*   `UseMultichannelRender` (bool): Similar to capture, but for the far-end/render signal for AEC.\r\n*   `DownmixMethod` (`DownmixMethod`): Specifies how to downmix if multi-channel processing is disabled for a stream (AverageChannels, UseFirstChannel).\r\n*   **`PostProcessGain` (float):** A final gain applied after all APM processing (default 1.0f).\r\n\r\n### `NoiseSuppressor` Constructor:\r\n\r\n*   `dataProvider` (`ISoundDataProvider`): The audio source.\r\n*   `sampleRate` (int): Sample rate of the source audio (must be 8k, 16k, 32k, or 48k).\r\n*   `numChannels` (int): Number of channels in the source audio.\r\n*   `suppressionLevel` (`NoiseSuppressionLevel`): Desired noise suppression level.\r\n*   `useMultichannelProcessing` (bool): If true and `numChannels > 1`, attempts to process channels independently.\r\n\r\n## Licensing\r\n\r\n*   The C# code (`SoundFlow.Extensions.WebRtc.Apm` wrapper and components) is licensed under the **MIT License**.\r\n*   The native `webrtc-apm` library used by this extension is based on the WebRTC Audio Processing Module, which is typically licensed under the **BSD 3-Clause \"New\" or \"Revised\" License**. The specific version included is derived from the [PulseAudio project's extraction](https://gitlab.freedesktop.org/pulseaudio/webrtc-audio-processing).\r\n\r\n**Users must comply with the terms of both licenses.** This generally involves including the copyright notice and license text of the WebRTC code if distributing applications using this extension. Please consult the native library's specific distribution for exact requirements.\r\n\r\n## Troubleshooting\r\n\r\n*   **No effect or poor quality:**\r\n*   Verify the `AudioEngine` sample rate matches one supported by WebRTC APM (8k, 16k, 32k, 48k Hz).\r\n*   For AEC, ensure `aecLatencyMs` is tuned appropriately for your system. Too low or too high values can degrade performance.\r\n*   Ensure the far-end signal is correctly being captured if AEC is enabled (usually handled automatically by the modifier via `AudioEngine.OnAudioProcessed`).\r\n*   **Errors during initialization:** Check the console output for any specific error messages from the native APM library. Ensure the native binaries are correctly deployed with your application.\r\n*   **Performance issues:** While WebRTC APM is optimized, processing many channels or enabling all features at very high settings can be CPU intensive. Monitor performance and adjust settings if needed."
  },
  {
    "id": 12,
    "slug": "tutorials-and-examples",
    "version": "1.1.2",
    "title": "Tutorials and Examples",
    "description": "A collection of tutorials and examples to help you learn how to use SoundFlow for various audio processing tasks.",
    "navOrder": 5,
    "category": "Core",
    "content": "---\r\ntitle: Tutorials and Examples\r\ndescription: A collection of tutorials and examples to help you learn how to use SoundFlow for various audio processing tasks.\r\nnavOrder: 5\r\ncategory: Core\r\n---\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Tutorials and Examples\r\n\r\nThis section provides a collection of tutorials and examples to help you learn how to use SoundFlow for various audio processing tasks. Each tutorial provides step-by-step instructions and explanations, while the examples offer ready-to-run code snippets that demonstrate specific features.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Tutorial categories\">\r\n    <Tab key=\"playback\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:play-circle\"/><span>Playback</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Playback tutorials\">\r\n            <Tab key=\"basic-playback\" title=\"Basic Playback\">\r\n                This tutorial demonstrates how to play an audio file from disk using `SoundPlayer` and\r\n                `StreamDataProvider`.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application:\r\n                        ```bash\r\n                        dotnet new console -o BasicPlayback\r\n                        cd BasicPlayback\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                        ### 2. Install the SoundFlow NuGet package:\r\n                        ```bash\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the basic player\" icon='ph:code-bold'>\r\n                        ### 3. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace BasicPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine with the MiniAudio backend.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                // Replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until playback finishes or the user presses a key.\r\n                                Console.WriteLine(\"Playing audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                // dataProvider is disposed automatically due to 'using' statement.\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file on your\r\n                        computer.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 4. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `StreamDataProvider` (which is `IDisposable` and\r\n                managed by a `using` statement) to load an audio file, creates a `SoundPlayer` with this provider, adds\r\n                the player to the `Master` mixer, and starts playback. The console application then waits for the user\r\n                to press a key before stopping playback and cleaning up.\r\n            </Tab>\r\n\r\n            <Tab key=\"web-playback\" title=\"Web Playback\">\r\n                This tutorial demonstrates how to play an audio stream from a URL using `SoundPlayer` and\r\n                `NetworkDataProvider`.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application:\r\n                        ```bash\r\n                        dotnet new console -o WebPlayback\r\n                        cd WebPlayback\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                        ### 2. Install the SoundFlow NuGet package:\r\n                        ```bash\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the network player\" icon='ph:code-bold'>\r\n                        ### 3. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n                        using System.IO;\r\n                        using System.Threading.Tasks;\r\n\r\n                        namespace WebPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static async Task Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a NetworkDataProvider. Replace \"your-audio-stream-url\"\r\n                                // with the actual URL (direct audio file or HLS .m3u8 playlist).\r\n                                // NetworkDataProvider is IDisposable.\r\n                                using var dataProvider = new NetworkDataProvider(\"your-audio-stream-url\");\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing stream... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                // dataProvider is disposed automatically.\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"your-audio-stream-url\"` with the actual URL of an audio stream (e.g., direct\r\n                        MP3/WAV or an HLS .m3u8 playlist).***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 4. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `NetworkDataProvider` for the given URL (which\r\n                handles direct files or HLS playlists), creates a `SoundPlayer`, adds it to the `Master` mixer, and\r\n                starts playback. `NetworkDataProvider` is `IDisposable` and managed with a `using` statement.\r\n            </Tab>\r\n\r\n            <Tab key=\"playback-control\" title=\"Playback Control\">\r\n                This tutorial demonstrates how to control audio playback using `Play`, `Pause`, `Stop`, `Seek`, and\r\n                `PlaybackSpeed`.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o PlaybackControl\r\n                        cd PlaybackControl\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the interactive player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace PlaybackControl;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider) {Volume = 0.8f}; // Example: set initial volume\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n                                Console.WriteLine(\"Playing audio... (p: pause/play, s: seek, +/-: speed, v/m: volume, any other: stop)\");\r\n\r\n                                // Handle user input for playback control.\r\n                                while (player.State != PlaybackState.Stopped)\r\n                                {\r\n                                    var keyInfo = Console.ReadKey(true);\r\n                                    switch (keyInfo.Key)\r\n                                    {\r\n                                    case ConsoleKey.P:\r\n                                        if (player.State == PlaybackState.Playing)\r\n                                            player.Pause();\r\n                                        else\r\n                                            player.Play();\r\n                                        Console.WriteLine(player.State == PlaybackState.Paused ? \"Paused\" : \"Playing\");\r\n                                        break;\r\n                                    case ConsoleKey.S:\r\n                                        Console.Write(\"Enter seek time (in seconds, e.g., 10.5): \");\r\n                                        if (float.TryParse(Console.ReadLine(), out var seekTimeSeconds))\r\n                                        {\r\n                                            if (player.Seek(TimeSpan.FromSeconds(seekTimeSeconds)))\r\n                                                Console.WriteLine($\"Seeked to {seekTimeSeconds:F1}s. Current time: {player.Time:F1}s\");\r\n                                            else\r\n                                                Console.WriteLine(\"Seek failed.\");\r\n                                        }\r\n                                        else\r\n                                            Console.WriteLine(\"Invalid seek time.\");\r\n                                    break;\r\n                                    case ConsoleKey.OemPlus:\r\n                                    case ConsoleKey.Add:\r\n                                        player.PlaybackSpeed = Math.Min(2.0f, player.PlaybackSpeed + 0.1f);\r\n                                        Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                    case ConsoleKey.OemMinus:\r\n                                    case ConsoleKey.Subtract:\r\n                                        player.PlaybackSpeed = Math.Max(0.1f, player.PlaybackSpeed - 0.1f);\r\n                                        Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                    case ConsoleKey.V:\r\n                                        player.Volume = Math.Min(1.5f, player.Volume + 0.1f); // Allow gain up to 150%\r\n                                        Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                    case ConsoleKey.M:\r\n                                        player.Volume = Math.Max(0.0f, player.Volume - 0.1f);\r\n                                        Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                    default:\r\n                                        player.Stop();\r\n                                        Console.WriteLine(\"Stopped\");\r\n                                    break;\r\n                                }\r\n                                }\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, adds it to the `Master` mixer, and\r\n                starts playback. It then enters a loop that handles user input for playback control:\r\n\r\n                * `P`: Pauses or resumes playback.\r\n                * `S`: Prompts for a seek time (in seconds) and seeks using `TimeSpan.FromSeconds()`. The `Seek` method\r\n                now returns a boolean indicating success.\r\n                * `+`/`-`: Adjusts `PlaybackSpeed`.\r\n                * `V`/`M`: Adjusts `player.Volume`.\r\n                * Any other key: Stops playback.\r\n            </Tab>\r\n\r\n            <Tab key=\"looping\" title=\"Looping\">\r\n                This tutorial demonstrates how to enable looping for a `SoundPlayer` and how to set custom loop points.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o LoopingPlayback\r\n                        cd LoopingPlayback\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the looping player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace LoopingPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Enable looping.\r\n                                player.IsLooping = true;\r\n\r\n                                // **Optional: Set custom loop points**\r\n\r\n                                // Example 1: Loop from 2.5 seconds to 7.0 seconds (using float seconds)\r\n                                // player.SetLoopPoints(2.5f, 7.0f);\r\n\r\n                                // Example 2: Loop from sample 110250 to sample 308700 (using samples)\r\n                                // player.SetLoopPoints(110250, 308700); // Assuming 44.1kHz stereo, these are example values\r\n\r\n                                // Example 3: Loop from 1.5 seconds to the natural end of the audio (using TimeSpan, end point is optional)\r\n                                player.SetLoopPoints(TimeSpan.FromSeconds(1.5));\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio in a loop... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code builds upon the basic playback example and introduces audio looping.\r\n                * **`player.IsLooping = true;`**: Enables looping.\r\n                * **`player.SetLoopPoints(...)`**: Configures the loop region.\r\n                * Overloads accept `float` seconds, `int` samples, or `TimeSpan`.\r\n                * If `endTime` (or `endSample`) is omitted or set to `-1f` (or `-1`), the loop goes to the natural end\r\n                of the audio.\r\n            </Tab>\r\n\r\n            <Tab key=\"surround-sound\" title=\"Surround Sound\">\r\n                This tutorial demonstrates how to use `SurroundPlayer` to play audio with surround sound configurations.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o SurroundPlayback\r\n                        cd SurroundPlayback\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the surround player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.Numerics;\r\n                        using System.IO;\r\n\r\n                        namespace SurroundPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine with appropriate channels for surround.\r\n                                // For 7.1, use 8 channels.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback, channels: 8);\r\n\r\n                                // Create a SurroundPlayer. Load a mono or stereo file for surround upmixing,\r\n                                // or a multi-channel file if your source is already surround.\r\n                                // The SurroundPlayer will attempt to pan mono/stereo to the configured speakers.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\")); // Can be mono/stereo\r\n                                var player = new SurroundPlayer(dataProvider);\r\n\r\n                                // Configure the SurroundPlayer for 7.1 surround sound.\r\n                                player.SpeakerConfig = SurroundPlayer.SpeakerConfiguration.Surround71;\r\n\r\n                                // Set the panning method (VBAP is often good for surround).\r\n                                player.Panning = SurroundPlayer.PanningMethod.Vbap;\r\n\r\n                                // Set the listener position (optional, (0,0) is center).\r\n                                player.ListenerPosition = new Vector2(0, 0);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until playback finishes or the user presses a key.\r\n                                Console.WriteLine(\"Playing surround sound audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file (mono, stereo,\r\n                        or multi-channel).***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes `AudioEngine` with 8 channels for 7.1. A `SurroundPlayer` is created. If the input\r\n                `audiofile.wav` is mono or stereo, the `SurroundPlayer` will pan it across the configured 7.1 speaker\r\n                layout. `SpeakerConfig` and `Panning` method are set. The `ListenerPosition` can also be adjusted.\r\n            </Tab>\r\n\r\n            <Tab key=\"chunked-data\" title=\"Chunked Data\">\r\n                This tutorial demonstrates how to use the `ChunkedDataProvider` for efficient playback of large audio\r\n                files.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o ChunkedPlayback\r\n                        cd ChunkedPlayback\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the chunked data player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace ChunkedPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a ChunkedDataProvider and load a large audio file.\r\n                                // Replace \"path/to/your/large/audiofile.wav\" with the actual path.\r\n                                using var dataProvider = new ChunkedDataProvider(\"path/to/your/large/audiofile.wav\");\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                Mixer.Master.AddComponent(player);\r\n                                player.Play();\r\n\r\n                                Console.WriteLine(\"Playing audio with ChunkedDataProvider... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/large/audiofile.wav\"` with the path to a large audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                The `ChunkedDataProvider` reads and decodes audio in chunks, suitable for large files. It's\r\n                `IDisposable` and managed with `using`.\r\n            </Tab>\r\n\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"recording\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:mic\"/><span>Recording</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Recording tutorials\">\r\n            <Tab key=\"basic-recording\" title=\"Basic Recording\">\r\n                This tutorial demonstrates how to record audio from the default recording device and save it to a WAV\r\n                file.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o BasicRecording\r\n                        cd BasicRecording\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the basic recorder\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs`:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace BasicRecording;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize for recording, e.g., 48kHz.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Record);\r\n\r\n                                string outputFilePath = Path.Combine(Directory.GetCurrentDirectory(), \"output.wav\");\r\n                                using var fileStream = new FileStream(outputFilePath, FileMode.Create, FileAccess.Write, FileShare.None);\r\n                                using var recorder = new Recorder(fileStream, sampleRate: 48000, encodingFormat: EncodingFormat.Wav);\r\n\r\n                                Console.WriteLine(\"Recording... Press any key to stop.\");\r\n                                recorder.StartRecording();\r\n                                Console.ReadKey();\r\n                                recorder.StopRecording();\r\n\r\n                                Console.WriteLine($\"Recording stopped. Saved to {outputFilePath}\");\r\n                            }\r\n                        }\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                Initializes `AudioEngine` for recording, creates a `Recorder` to save to \"output.wav\", starts recording,\r\n                waits for a key, then stops.\r\n            </Tab>\r\n\r\n            <Tab key=\"custom-processing\" title=\"Custom Processing\">\r\n                This tutorial demonstrates using a callback to process recorded audio in real-time.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow.\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement real-time processing\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs`:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace CustomProcessing;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Record);\r\n                                using var recorder = new Recorder(ProcessAudio, sampleRate: 48000);\r\n\r\n                                Console.WriteLine(\"Recording with custom processing... Press any key to stop.\");\r\n                                recorder.StartRecording();\r\n                                Console.ReadKey();\r\n                                recorder.StopRecording();\r\n                                Console.WriteLine(\"Recording stopped.\");\r\n                            }\r\n\r\n                                // This method will be called for each chunk of recorded audio.\r\n                            private static void ProcessAudio(Span<float> samples)\r\n                            {\r\n                                // Perform custom processing on the audio samples.\r\n                                // For example, calculate the average level:\r\n                                float sum = 0;\r\n                                for (int i = 0; i < samples.Length; i++)\r\n                                {\r\n                                    sum += Math.Abs(samples[i]);\r\n                                }\r\n                                float averageLevel = sum / samples.Length;\r\n\r\n                                Console.WriteLine($\"Average level: {averageLevel:F4}\");\r\n                            }\r\n                        }\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application.\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                A `Recorder` is created with a `ProcessAudio` callback that gets called with chunks of recorded audio.\r\n            </Tab>\r\n\r\n            <Tab key=\"mic-playback\" title=\"Mic Playback (Monitor)\">\r\n                This tutorial demonstrates capturing microphone audio and playing it back in real-time.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow.\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement microphone loopback\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs`:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n\r\n                        namespace MicrophonePlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Mixed capability for simultaneous record & playback.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Mixed);\r\n                                using var microphoneDataProvider = new MicrophoneDataProvider();\r\n\r\n                                // Create a SoundPlayer and connect the MicrophoneDataProvider.\r\n                                var player = new SoundPlayer(microphoneDataProvider);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start capturing audio from the microphone.\r\n                                microphoneDataProvider.StartCapture();\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                Console.WriteLine(\"Playing live microphone audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and capture.\r\n                                player.Stop();\r\n                                microphoneDataProvider.StopCapture(); // Stop capture before provider is disposed by 'using'\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application.\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                Uses `MicrophoneDataProvider` as a source for a `SoundPlayer` to achieve real-time microphone\r\n                monitoring. `AudioEngine` needs `Capability.Mixed`.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"effects\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:wand-2\"/><span>Effects</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Effects tutorials\">\r\n            <Tab key=\"reverb\" title=\"Reverb\">\r\n                Demonstrates how to apply a reverb effect using the `AlgorithmicReverbModifier`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o ReverbEffect\r\n                        cd ReverbEffect\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with reverb\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace ReverbEffect;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create an AlgorithmicReverbModifier.\r\n                                var reverb = new AlgorithmicReverbModifier\r\n                                {\r\n                                    RoomSize = 0.8f,\r\n                                    Damp = 0.5f,\r\n                                    Wet = 0.3f,\r\n                                    Dry = 0.7f,\r\n                                    Width = 1f\r\n                                };\r\n\r\n                                // Add the reverb modifier to the player.\r\n                                player.AddModifier(reverb);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with reverb... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates an\r\n                `AlgorithmicReverbModifier` with custom settings, adds the modifier to the player, adds the player to\r\n                the `Master` mixer, and starts playback. You will hear the audio with the reverb effect applied.\r\n                Experiment with different values for `RoomSize`, `Damp`, `Wet`, `Dry`, and `Width` to change the\r\n                characteristics of the reverb.\r\n            </Tab>\r\n\r\n            <Tab key=\"equalization\" title=\"Equalization\">\r\n                Demonstrates how to use the `ParametricEqualizer` to adjust frequency balance.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o Equalization\r\n                        cd Equalization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with EQ\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace Equalization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a ParametricEqualizer.\r\n                                var equalizer = new ParametricEqualizer(AudioEngine.Channels);\r\n\r\n                                // Add some equalizer bands:\r\n                                // Boost low frequencies (bass)\r\n                                equalizer.AddBand(FilterType.LowShelf, 100, 6, 0.7f, 0);\r\n                                // Cut mid frequencies\r\n                                equalizer.AddBand(FilterType.Peaking, 1000, -4, 2, 0);\r\n                                // Boost high frequencies (treble)\r\n                                equalizer.AddBand(FilterType.HighShelf, 10000, 5, 0.7f, 0);\r\n\r\n                                // Add the equalizer to the player.\r\n                                player.AddModifier(equalizer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with equalization... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust bands\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `ParametricEqualizer`, adds three equalizer bands (low-shelf boost, peaking cut, high-shelf boost), adds\r\n                the equalizer to the player, adds the player to the `Master` mixer, and starts playback. You will hear\r\n                the audio with the equalization applied. Experiment with different filter types (`FilterType`),\r\n                frequencies, gain values, and Q values to shape the sound to your liking.\r\n            </Tab>\r\n\r\n            <Tab key=\"chorus-delay\" title=\"Chorus & Delay\">\r\n                Demonstrates how to apply chorus and delay effects using `ChorusModifier` and `DelayModifier`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o ChorusDelay\r\n                        cd ChorusDelay\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with effects\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace ChorusDelay;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a ChorusModifier.\r\n                                var chorus = new ChorusModifier(\r\n                                depth: 25f,           // Depth (in milliseconds)\r\n                                rate: 0.8f,          // Rate (in Hz)\r\n                                feedback: 0.5f,      // Feedback amount\r\n                                wetDryMix: 0.5f,     // Wet/dry mix (0 = dry, 1 = wet)\r\n                                maxDelayLength: 500  // Maximum delay length (in milliseconds)\r\n                                );\r\n\r\n                                // Create a DelayModifier.\r\n                                var delay = new DelayModifier(\r\n                                delayLength: 500,   // Delay length (in milliseconds)\r\n                                feedback: 0.6f,      // Feedback amount\r\n                                wetMix: 0.4f,       // Wet/dry mix\r\n                                cutoffFrequency: 4000 // Cutoff frequency for the low-pass filter\r\n                                );\r\n\r\n                                // Add the chorus and delay modifiers to the player.\r\n                                player.AddModifier(chorus);\r\n                                player.AddModifier(delay);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with chorus and delay... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust effects\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates\r\n                `ChorusModifier` and `DelayModifier` instances with custom settings, adds both modifiers to the player\r\n                (they will be applied in the order they are added), adds the player to the `Master` mixer, and starts\r\n                playback. You will hear the audio with both chorus and delay effects applied. Experiment with different\r\n                parameter values for the chorus and delay modifiers to create a wide range of sonic textures.\r\n            </Tab>\r\n\r\n            <Tab key=\"compression\" title=\"Compression\">\r\n                Demonstrates how to use the `CompressorModifier` to reduce the dynamic range of an audio stream.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o Compression\r\n                        cd Compression\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with compressor\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace Compression;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a CompressorModifier.\r\n                                var compressor = new CompressorModifier(\r\n                                threshold: -20f, // Threshold (in dB)\r\n                                ratio: 4f,        // Compression ratio\r\n                                attack: 10f,      // Attack time (in milliseconds)\r\n                                release: 100f,    // Release time (in milliseconds)\r\n                                knee: 5f,         // Knee width (in dB)\r\n                                makeupGain: 6f   // Makeup gain (in dB)\r\n                                );\r\n\r\n                                // Add the compressor to the player.\r\n                                player.AddModifier(compressor);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with compression... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `CompressorModifier` with specific settings, adds the compressor to the player, adds the player to the\r\n                `Master` mixer, and starts playback. You will hear the audio with the compression effect applied,\r\n                resulting in a more consistent volume level. Experiment with different values for `threshold`, `ratio`,\r\n                `attack`, `release`, `knee`, and `makeupGain` to understand how they affect the compression.\r\n            </Tab>\r\n\r\n            <Tab key=\"noise-reduction\" title=\"Noise Reduction\">\r\n                Demonstrates how to use the `NoiseReductionModifier` to reduce noise in an audio stream.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o NoiseReduction\r\n                        cd NoiseReduction\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with noise reduction\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace NoiseReduction;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load a noisy audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/noisy/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a NoiseReductionModifier.\r\n                                var noiseReducer = new NoiseReductionModifier(\r\n                                fftSize: 2048,       // FFT size (power of 2)\r\n                                alpha: 3f,          // Smoothing factor for noise estimation\r\n                                beta: 0.001f,        // Minimum gain for noise reduction\r\n                                smoothingFactor: 0.8f, // Smoothing factor for gain\r\n                                gain: 1.0f,         // Post-processing gain\r\n                                noiseFrames: 10     // Number of initial frames to use for noise estimation\r\n                                );\r\n\r\n                                // Add the noise reducer to the player.\r\n                                player.AddModifier(noiseReducer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with noise reduction... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/noisy/audiofile.wav\"` with the actual path to a noisy audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and fine-tune parameters\"\r\n                          icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads a noisy audio file, creates a\r\n                `NoiseReductionModifier` with specific settings, adds the noise reducer to the player, adds the player\r\n                to the `Master` mixer, and starts playback. You should hear a reduction in the noise level of the audio.\r\n                Experiment with different values for `fftSize`, `alpha`, `beta`, `smoothingFactor`, `gain`, and\r\n                `noiseFrames` to fine-tune the noise reduction.\r\n            </Tab>\r\n\r\n            <Tab key=\"mixing\" title=\"Mixing\">\r\n                Demonstrates how to use the `Mixer` to combine multiple audio sources.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o Mixing\r\n                        cd Mixing\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Combine multiple audio sources\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace Mixing;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create two SoundPlayer instances and load different audio files.\r\n                                using var dataProvider1 = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile1.wav\"));\r\n                                var player1 = new SoundPlayer(dataProvider1);\r\n\r\n                                using var dataProvider2 = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile2.wav\"));\r\n                                var player2 = new SoundPlayer(dataProvider2);\r\n\r\n                                // Create an Oscillator that generates a sine wave.\r\n                                var oscillator = new Oscillator\r\n                                {\r\n                                    Frequency = 440, // 440 Hz (A4 note)\r\n                                    Amplitude = 0.5f,\r\n                                    Type = Oscillator.WaveformType.Sine\r\n                                };\r\n\r\n                                // Add the players and the oscillator to the master mixer.\r\n                                Mixer.Master.AddComponent(player1);\r\n                                Mixer.Master.AddComponent(player2);\r\n                                Mixer.Master.AddComponent(oscillator);\r\n\r\n                                // Start playback for both players.\r\n                                player1.Play();\r\n                                player2.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing mixed audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback for both players.\r\n                                player1.Stop();\r\n                                player2.Stop();\r\n\r\n                                // Remove the components from the mixer.\r\n                                Mixer.Master.RemoveComponent(player1);\r\n                                Mixer.Master.RemoveComponent(player2);\r\n                                Mixer.Master.RemoveComponent(oscillator);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile1.wav\"` and `\"path/to/your/audiofile2.wav\"` with the actual\r\n                        paths to two different audio files.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust levels\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates two `SoundPlayer` instances, loads two different audio\r\n                files, creates an `Oscillator` that generates a sine wave, adds all three components to the `Master`\r\n                mixer, and starts playback for the players. You will hear the two audio files and the sine wave mixed\r\n                together. Experiment with adding more sound sources to the mixer and adjusting their individual volumes\r\n                and panning using the `Volume` and `Pan` properties of each `SoundComponent`.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"advanced-processing\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"mdi:webrtc\"/><span>WebRTC APM</span></div>}>\r\n        This tutorial shows how to use the `WebRtcApmModifier` for real-time noise suppression and echo cancellation on\r\n        microphone input. For offline file processing, refer to the [WebRTC APM Extension\r\n        documentation](./extensions/webrtc-apm).\r\n\r\n        **Prerequisites:**\r\n        * SoundFlow core package.\r\n        * `SoundFlow.Extensions.WebRtc.Apm` NuGet package.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & both packages\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a console app and install both SoundFlow packages:\r\n                ```bash\r\n                dotnet new console -o WebRtcApmDemo\r\n                cd WebRtcApmDemo\r\n                dotnet add package SoundFlow\r\n                dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement microphone processing\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs`:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Extensions.WebRtc.Apm; // For enums like NoiseSuppressionLevel\r\n                using SoundFlow.Extensions.WebRtc.Apm.Modifiers; // For WebRtcApmModifier\r\n                using System;\r\n\r\n                namespace WebRtcApmDemo;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize AudioEngine. 48kHz is good for WebRTC APM.\r\n                        // Capability.Mixed is needed for AEC (to get playback audio for far-end).\r\n                        using var audioEngine = new MiniAudioEngine(48000, Capability.Mixed, channels: 1); // Mono for voice\r\n\r\n                        // Setup microphone input\r\n                        using var micProvider = new MicrophoneDataProvider();\r\n                        var micPlayer = new SoundPlayer(micProvider) {Name = \"MicrophoneInput\"};\r\n\r\n                        // Instantiate and configure WebRtcApmModifier\r\n                        var apmModifier = new WebRtcApmModifier(\r\n                        aecEnabled: true,       // Enable Acoustic Echo Cancellation\r\n                        aecMobileMode: false,\r\n                        aecLatencyMs: 40,       // Adjust based on your system's latency\r\n\r\n                        nsEnabled: true,        // Enable Noise Suppression\r\n                        nsLevel: NoiseSuppressionLevel.High,\r\n\r\n                        agc1Enabled: true,      // Enable Automatic Gain Control (AGC1)\r\n                        agcMode: GainControlMode.AdaptiveDigital,\r\n                        agcTargetLevel: -6,     // Target level in dBFS\r\n                        agcLimiter: true,\r\n\r\n                        hpfEnabled: true        // Enable High Pass Filter\r\n                        );\r\n                        micPlayer.AddModifier(apmModifier);\r\n\r\n                        // To test AEC, you might want to play some audio simultaneously\r\n                        // For simplicity, this example focuses on mic input processing.\r\n                        // If you play audio through Mixer.Master, AEC will use it as far-end.\r\n\r\n                        Mixer.Master.AddComponent(micPlayer);\r\n\r\n                        micProvider.StartCapture();\r\n                        micPlayer.Play();\r\n\r\n                        Console.WriteLine(\"Processing microphone with WebRTC APM (AEC, NS, AGC, HPF)...\");\r\n                        Console.WriteLine(\"Speak into your microphone. Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        micPlayer.Stop();\r\n                        micProvider.StopCapture();\r\n                        Mixer.Master.RemoveComponent(micPlayer);\r\n                        apmModifier.Dispose(); // Important!\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run & Test\" description=\"Speak into your microphone\" icon='lucide:play'>\r\n                ### 3. Build and run. Speak into your microphone to hear the processing.\r\n                **Explanation:**\r\n                This setup processes microphone audio in real-time.\r\n                * `AudioEngine` is set to `Capability.Mixed` and a compatible sample rate (48kHz).\r\n                * `WebRtcApmModifier` is configured with AEC, NS, AGC, and HPF enabled.\r\n                * AEC requires a far-end signal. The modifier automatically listens to `AudioEngine.OnAudioProcessed`\r\n                for `Playback` capability audio to use as the far-end reference. If you were playing music through\r\n                another `SoundPlayer` added to `Mixer.Master`, that would be the far-end signal.\r\n                * ***Remember to `Dispose()` the `WebRtcApmModifier` to release native resources.***\r\n            </Step>\r\n        </Steps>\r\n    </Tab>\r\n\r\n    <Tab key=\"analysis\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:activity\"/><span>Analysis</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Analysis tutorials\">\r\n            <Tab key=\"level-metering\" title=\"Level Metering\">\r\n                This tutorial demonstrates how to use the `LevelMeterAnalyzer` to measure the RMS and peak levels of an\r\n                audio stream.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o LevelMetering\r\n                        cd LevelMetering\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System.IO;\r\n\r\n                        namespace LevelMetering;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a LevelMeterAnalyzer.\r\n                                var levelMeter = new LevelMeterAnalyzer();\r\n\r\n                                // Connect the player's output to the level meter's input.\r\n                                player.AddAnalyzer(levelMeter);\r\n\r\n                                // Add the player to the master mixer (the level meter doesn't produce output).\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Create a timer to periodically display the RMS and peak levels.\r\n                                var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    Console.WriteLine($\"RMS Level: {levelMeter.Rms:F4}, Peak Level: {levelMeter.Peak:F4}\");\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `LevelMeterAnalyzer`, connects the player's output to the analyzer's input, adds the player to the\r\n                `Master` mixer, and starts playback. It then creates a timer that fires every 100 milliseconds, printing\r\n                the current RMS and peak levels to the console.\r\n            </Tab>\r\n\r\n            <Tab key=\"spectrum-analysis\" title=\"Spectrum Analysis\">\r\n                This tutorial demonstrates how to use the `SpectrumAnalyzer` to analyze frequency content using FFT.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o SpectrumAnalysis\r\n                        cd SpectrumAnalysis\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace SpectrumAnalysis;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                                var spectrumAnalyzer = new SpectrumAnalyzer(fftSize: 2048);\r\n\r\n                                // Connect the player's output to the spectrum analyzer's input.\r\n                                player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                                // Add the player to the master mixer (the spectrum analyzer doesn't produce output).\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Create a timer to periodically display the spectrum data.\r\n                                var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    // Get the spectrum data from the analyzer.\r\n                                    var spectrumData = spectrumAnalyzer.SpectrumData;\r\n\r\n                                    // Print the magnitude of the first few frequency bins.\r\n                                    if (spectrumData.Length > 0)\r\n                                    {\r\n                                        Console.Write(\"Spectrum: \");\r\n                                        for (int i = 0; i < Math.Min(10, spectrumData.Length); i++)\r\n                                        {\r\n                                            Console.Write($\"{spectrumData[i]:F2} \");\r\n                                        }\r\n                                        Console.WriteLine();\r\n                                    }\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying spectrum data... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `SpectrumAnalyzer` with an FFT size of 2048, connects the player's output to the analyzer's input, adds\r\n                the player to the `Master` mixer, and starts playback. It then creates a timer that fires every 100\r\n                milliseconds, printing the magnitude of the first 10 frequency bins of the spectrum data to the console.\r\n            </Tab>\r\n\r\n            <Tab key=\"vad\" title=\"Voice Activity Detection\">\r\n                This tutorial demonstrates how to use the `VoiceActivityDetector` to detect the presence of human voice.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o VoiceActivityDetection\r\n                        cd VoiceActivityDetection\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement VAD on a source\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace VoiceActivityDetection;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine (either for playback or recording).\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback); // Or Capability.Record for microphone input\r\n\r\n                                // Create a SoundPlayer and load an audio file (if using playback).\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a VoiceActivityDetector.\r\n                                var vad = new VoiceActivityDetector();\r\n\r\n                                // Connect the VAD as an analyzer to the player's output (or microphone input).\r\n                                player.AddAnalyzer(vad);\r\n\r\n                                // Subscribe to the SpeechDetected event.\r\n                                vad.SpeechDetected += isDetected => Console.WriteLine($\"Speech detected: {isDetected}\");\r\n\r\n                                // Add the player to the master mixer (if using playback).\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback (if using playback).\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Analyzing audio for voice activity... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback (if using playback) and clean up.\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine` (either for playback or recording), creates a `SoundPlayer` and\r\n                loads an audio file (if using playback), creates a `VoiceActivityDetector`, connects the player's output\r\n                (or microphone input) to the VAD, subscribes to the `SpeechDetected` event to print messages to the\r\n                console when speech is detected or not detected, adds the player to the `Master` mixer (if using\r\n                playback), and starts playback.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"visualization\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:bar-chart-3\"/><span>Visualization</span>\r\n         </div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Visualization tutorials\">\r\n            <Tab key=\"level-meter-viz\" title=\"Level Meter\">\r\n                Demonstrates creating a console-based level meter using the `LevelMeterAnalyzer` and\r\n                `LevelMeterVisualizer`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o LevelMeterVisualization\r\n                        cd LevelMeterVisualization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with visualizer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System.Diagnostics;\r\n                        using System.IO;\r\n\r\n                        namespace LevelMeterVisualization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a LevelMeterAnalyzer.\r\n                                var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                                // Create a LevelMeterVisualizer.\r\n                                var levelMeterVisualizer = new LevelMeterVisualizer(levelMeterAnalyzer);\r\n\r\n                                // Connect the player's output to the level meter analyzer's input.\r\n                                player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                                levelMeterVisualizer.VisualizationUpdated += (sender, e) =>\r\n                                {\r\n                                    DrawLevelMeter(levelMeterAnalyzer.Rms, levelMeterAnalyzer.Peak);\r\n                                };\r\n\r\n                                // Start a timer to update the visualization.\r\n                                var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    levelMeterVisualizer.ProcessOnAudioData(System.Array.Empty<float>());\r\n                                    levelMeterVisualizer.Render(new ConsoleVisualizationContext());\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                levelMeterVisualizer.Dispose();\r\n                            }\r\n\r\n                            // Helper method to draw a simple console-based level meter.\r\n                            private static void DrawLevelMeter(float rms, float peak)\r\n                            {\r\n                                int barLength = (int)(rms * 40); // Scale the RMS value to a bar length\r\n                                int peakBarLength = (int)(peak * 40); // Scale the peak value to a bar length\r\n\r\n                                Console.SetCursorPosition(0, 0);\r\n                                Console.Write(\"RMS:  \");\r\n                                Console.Write(new string('#', barLength));\r\n                                Console.Write(new string(' ', 40 - barLength));\r\n                                Console.Write(\"|\\n\");\r\n\r\n                                Console.SetCursorPosition(0, 1);\r\n                                Console.Write(\"Peak: \");\r\n                                Console.Write(new string('#', peakBarLength));\r\n                                Console.Write(new string(' ', 40 - peakBarLength));\r\n                                Console.Write(\"|\");\r\n\r\n                                Console.SetCursorPosition(0, 3);\r\n                            }\r\n                        }\r\n\r\n                        // Simple IVisualizationContext implementation for console output.\r\n                        public class ConsoleVisualizationContext : IVisualizationContext\r\n                        {\r\n                            public void Clear() {}\r\n                            public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1) {}\r\n                            public void DrawRectangle(float x, float y, float width, float height, Color color) {}\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `LevelMeterAnalyzer` and a `LevelMeterVisualizer`, connects the player's output to the analyzer, adds\r\n                the player to the `Master` mixer, and starts playback. It then subscribes to the `VisualizationUpdated`\r\n                event of the visualizer to redraw the level meter when the data changes. Finally, it starts a timer that\r\n                calls `ProcessOnAudioData` and `Render` on the visualizer approximately 60 times per second. The\r\n                `DrawLevelMeter` method is a helper function that draws a simple console-based level meter using `#`\r\n                characters.\r\n            </Tab>\r\n\r\n            <Tab key=\"waveform-viz\" title=\"Waveform\">\r\n                Demonstrates using `WaveformVisualizer` to display an audio waveform.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o WaveformVisualization\r\n                        cd WaveformVisualization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement waveform visualizer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System;\r\n                        using System.Collections.Generic;\r\n                        using System.IO;\r\n\r\n                        namespace WaveformVisualization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a LevelMeterAnalyzer or any analyzer you want.\r\n                                var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                                // Create a WaveformVisualizer.\r\n                                var waveformVisualizer = new WaveformVisualizer();\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                                waveformVisualizer.VisualizationUpdated += (sender, e) =>\r\n                                {\r\n                                    DrawWaveform(waveformVisualizer.Waveform);\r\n                                };\r\n\r\n                                // Connect the player's output to the level meter analyzer's input.\r\n                                player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Start a timer to update the visualization.\r\n                                var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    waveformVisualizer.Render(new ConsoleVisualizationContext()); // ConsoleVisualizationContext is just a placeholder\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying waveform... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                waveformVisualizer.Dispose();\r\n                            }\r\n\r\n                            // Helper method to draw a simple console-based waveform.\r\n                            private static void DrawWaveform(List<float> waveform)\r\n                            {\r\n                                Console.Clear();\r\n                                int consoleWidth = Console.WindowWidth;\r\n                                int consoleHeight = Console.WindowHeight;\r\n\r\n                                if (waveform.Count == 0) return;\r\n\r\n                                for (int i = 0; i < consoleWidth; i++)\r\n                                {\r\n                                    int waveformIndex = (int)(i * (waveform.Count / (float)consoleWidth));\r\n                                    waveformIndex = Math.Clamp(waveformIndex, 0, waveform.Count - 1);\r\n\r\n                                    float sampleValue = waveform[waveformIndex];\r\n                                    int consoleY = (int)((sampleValue + 1) * 0.5 * consoleHeight);\r\n                                    consoleY = Math.Clamp(consoleY, 0, consoleHeight - 1);\r\n\r\n                                    if (i < Console.WindowWidth && (consoleHeight - consoleY - 1) < Console.WindowHeight)\r\n                                    {\r\n                                        Console.SetCursorPosition(i, consoleHeight - consoleY - 1);\r\n                                        Console.Write(\"*\");\r\n                                    }\r\n                                }\r\n                                Console.SetCursorPosition(0, consoleHeight - 1);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `WaveformVisualizer`, adds the player to the `Master` mixer, and starts playback. It subscribes to the\r\n                `VisualizationUpdated` event of the visualizer to redraw the waveform when the data changes. The\r\n                `DrawWaveform` method is a helper function that draws a simple console-based waveform using `*`\r\n                characters. The `AudioEngine.OnAudioProcessed` is used to send chunks of processed audio data to the\r\n                `WaveformVisualizer`.\r\n            </Tab>\r\n\r\n            <Tab key=\"spectrum-viz\" title=\"Spectrum Analyzer\">\r\n                Demonstrates creating a console-based spectrum analyzer using the `SpectrumAnalyzer` and\r\n                `SpectrumVisualizer`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o SpectrumAnalyzerVisualization\r\n                        cd SpectrumAnalyzerVisualization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement spectrum visualizer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace SpectrumAnalyzerVisualization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                                var spectrumAnalyzer = new SpectrumAnalyzer(fftSize: 2048);\r\n\r\n                                // Create a SpectrumVisualizer.\r\n                                var spectrumVisualizer = new SpectrumVisualizer(spectrumAnalyzer);\r\n\r\n                                // Connect the player's output to the spectrum analyzer's input.\r\n                                player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                                spectrumVisualizer.VisualizationUpdated += (sender, e) =>\r\n                                {\r\n                                    DrawSpectrum(spectrumAnalyzer.SpectrumData);\r\n                                };\r\n\r\n                                // Start a timer to update the visualization.\r\n                                var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    spectrumVisualizer.ProcessOnAudioData(Array.Empty<float>());\r\n                                    spectrumVisualizer.Render(new ConsoleVisualizationContext());\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying spectrum analyzer... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                spectrumVisualizer.Dispose();\r\n                            }\r\n\r\n                            // Helper method to draw a simple console-based spectrum analyzer.\r\n                            private static void DrawSpectrum(ReadOnlySpan<float> spectrumData)\r\n                            {\r\n                                Console.Clear();\r\n                                int consoleWidth = Console.WindowWidth;\r\n                                int consoleHeight = Console.WindowHeight;\r\n\r\n                                if (spectrumData.IsEmpty) return;\r\n\r\n                                int barWidth = Math.Max(1, consoleWidth / spectrumData.Length);\r\n\r\n                                for (int i = 0; i < spectrumData.Length; i++)\r\n                                {\r\n                                    float magnitude = spectrumData[i];\r\n                                    int barHeight = (int)(magnitude * consoleHeight / 2);\r\n                                    barHeight = Math.Clamp(barHeight, 0, consoleHeight - 1);\r\n\r\n                                    for (int j = 0; j < barHeight; j++)\r\n                                    {\r\n                                        for (int w = 0; w < barWidth; w++)\r\n                                        {\r\n                                            if ((i * barWidth + w) < consoleWidth - 1)\r\n                                            {\r\n                                                Console.SetCursorPosition(i * barWidth + w, consoleHeight - 1 - j);\r\n                                                Console.Write(\"*\");\r\n                                            }\r\n                                        }\r\n                                    }\r\n                                }\r\n                                Console.SetCursorPosition(0, consoleHeight - 1);\r\n                            }\r\n                        }\r\n\r\n                        // Simple IVisualizationContext implementation for console output.\r\n                        public class ConsoleVisualizationContext : IVisualizationContext\r\n                        {\r\n                            public void Clear() {}\r\n                            public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f) {}\r\n                            public void DrawRectangle(float x, float y, float width, float height, Color color) {}\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `SpectrumAnalyzer` and a `SpectrumVisualizer`, connects the player's output to the analyzer, adds the\r\n                player to the `Master` mixer, and starts playback. It subscribes to the `VisualizationUpdated` event of\r\n                the visualizer to redraw the spectrum when the data changes. The `DrawSpectrum` method is a helper\r\n                function that draws a simple console-based spectrum analyzer using `*` characters. The height of each\r\n                bar represents the magnitude of the corresponding frequency bin.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"ui-integration\" title={<div className=\"flex items-center gap-2\"><Icon\r\n        icon=\"lucide:layout-template\"/><span>UI Integration</span></div>}>\r\n        These examples use basic console output for simplicity. To integrate SoundFlow's visualizers with a GUI\r\n        framework (like WPF, WinForms, Avalonia, or MAUI), you'll need to:\r\n        <Steps layout='vertical'>\r\n            <Step title=\"Implement IVisualizationContext\" description=\"Wrap your UI framework's drawing primitives\"\r\n                  icon='material-symbols:draw-outline'>\r\n                This class will wrap the drawing primitives of your chosen UI framework. For example, in WPF, you might\r\n                use `DrawingContext` methods to draw shapes on a `Canvas`.\r\n            </Step>\r\n            <Step title=\"Update UI from Event\" description=\"Trigger a redraw on the UI thread\" icon='mdi:update'>\r\n                In the `VisualizationUpdated` event handler, trigger a redraw of your UI element that hosts the\r\n                visualization. Make sure to marshal the update to the UI thread using `Dispatcher.Invoke` or a similar\r\n                mechanism if the event is raised from a different thread.\r\n            </Step>\r\n            <Step title=\"Call Render Method\" description=\"Pass your context to the visualizer\" icon='lucide:render'>\r\n                In your UI's rendering logic, call the `Render` method of the visualizer, passing your\r\n                `IVisualizationContext` implementation.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Example (Conceptual WPF):**\r\n\r\n        ```csharp\r\n        // In your XAML:\r\n        // <Canvas x:Name=\"VisualizationCanvas\"/>\r\n\r\n        // In your code-behind:\r\n        public partial class MainWindow : Window\r\n        {\r\n            private readonly WaveformVisualizer _visualizer;\r\n\r\n            public MainWindow()\r\n            {\r\n                InitializeComponent();\r\n\r\n                // ... Initialize AudioEngine, SoundPlayer, etc. ...\r\n\r\n                _visualizer = new WaveformVisualizer();\r\n                _visualizer.VisualizationUpdated += OnVisualizationUpdated;\r\n\r\n                // ...\r\n            }\r\n\r\n            private void OnVisualizationUpdated(object? sender, EventArgs e)\r\n            {\r\n                // Marshal the update to the UI thread\r\n                Dispatcher.Invoke(() =>\r\n                {\r\n                    VisualizationCanvas.Children.Clear(); // Clear previous drawing\r\n\r\n                    // Create a custom IVisualizationContext that wraps the Canvas\r\n                    var context = new WpfVisualizationContext(VisualizationCanvas);\r\n\r\n                    // Render the visualization\r\n                    _visualizer.Render(context);\r\n                });\r\n            }\r\n\r\n            // ...\r\n        }\r\n\r\n        // IVisualizationContext implementation for WPF\r\n        public class WpfVisualizationContext : IVisualizationContext\r\n        {\r\n            private readonly Canvas _canvas;\r\n\r\n            public WpfVisualizationContext(Canvas canvas)\r\n            {\r\n                _canvas = canvas;\r\n            }\r\n\r\n            public void Clear()\r\n            {\r\n                _canvas.Children.Clear();\r\n            }\r\n\r\n            public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)\r\n            {\r\n                var line = new Line\r\n                {\r\n                    X1 = x1,\r\n                    Y1 = y1,\r\n                    X2 = x2,\r\n                    Y2 = y2,\r\n                    Stroke = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255))),\r\n                    StrokeThickness = thickness\r\n                };\r\n                _canvas.Children.Add(line);\r\n            }\r\n\r\n            public void DrawRectangle(float x, float y, float width, float height, Color color)\r\n            {\r\n                var rect = new Rectangle\r\n                {\r\n                    Width = width,\r\n                    Height = height,\r\n                    Fill = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255)))\r\n                };\r\n                Canvas.SetLeft(rect, x);\r\n                Canvas.SetTop(rect, y);\r\n                _canvas.Children.Add(rect);\r\n            }\r\n        }\r\n        ```\r\n\r\n        Remember to adapt this conceptual example to your specific UI framework and project structure.\r\n    </Tab>\r\n\r\n    <Tab key=\"device-management\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:audio-lines\"/><span>Device Management</span>\r\n         </div>}>\r\n        This tutorial demonstrates how to list available audio devices and switch the playback device.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow.\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the device switcher\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs`:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace DeviceSwitcher;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize engine (MiniAudioEngine used here)\r\n                        using var engine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                        void PrintDevices()\r\n                        {\r\n                            engine.UpdateDevicesInfo();\r\n                            Console.WriteLine(\"\\nAvailable Playback Devices:\");\r\n                            for (int i = 0; i < engine.PlaybackDeviceCount; i++)\r\n                            {\r\n                                Console.WriteLine($\"{i}: {engine.PlaybackDevices[i].Name} {(engine.PlaybackDevices[i].IsDefault ? \"(Default)\" : \"\")}\");\r\n                            }\r\n                                Console.WriteLine($\"Current Playback Device: {engine.CurrentPlaybackDevice?.Name ?? \"None selected\"}\");\r\n                        }\r\n\r\n                        PrintDevices();\r\n\r\n                        // Simple audio playback setup\r\n                        Console.Write(\"Enter path to an audio file to play: \");\r\n                        string? filePath = Console.ReadLine()?.Trim('\"');\r\n                        if (string.IsNullOrEmpty(filePath) || !File.Exists(filePath))\r\n                        {\r\n                            Console.WriteLine(\"Invalid file path. Exiting.\");\r\n                            return;\r\n                        }\r\n\r\n                        using var dataProvider = new StreamDataProvider(File.OpenRead(filePath));\r\n                        var player = new SoundPlayer(dataProvider);\r\n                        Mixer.Master.AddComponent(player);\r\n                        player.Play();\r\n                        Console.WriteLine($\"Playing on {engine.CurrentPlaybackDevice?.Name ?? \"default device\"}.\");\r\n\r\n                        while (true)\r\n                        {\r\n                            Console.Write(\"Enter device number to switch to, 'r' to refresh list, or 'q' to quit: \");\r\n                            string? input = Console.ReadLine();\r\n\r\n                            if (input?.ToLower() == \"q\") break;\r\n                            if (input?.ToLower() == \"r\")\r\n                            {\r\n                                PrintDevices();\r\n                                continue;\r\n                            }\r\n\r\n                            if (int.TryParse(input, out int deviceIndex) && deviceIndex >= 0 && deviceIndex < engine.PlaybackDeviceCount)\r\n                            {\r\n                                try\r\n                                {\r\n                                    Console.WriteLine($\"Switching to {engine.PlaybackDevices[deviceIndex].Name}...\");\r\n                                    engine.SwitchDevice(engine.PlaybackDevices[deviceIndex], DeviceType.Playback);\r\n                                    Console.WriteLine($\"Successfully switched to {engine.CurrentPlaybackDevice?.Name}.\");\r\n                                }\r\n                                catch (Exception ex)\r\n                                {\r\n                                    Console.WriteLine($\"Error switching device: {ex.Message}\");\r\n                                }\r\n                            }\r\n                            else\r\n                            {\r\n                                Console.WriteLine(\"Invalid input.\");\r\n                            }\r\n                        }\r\n\r\n                        player.Stop();\r\n                        Mixer.Master.RemoveComponent(player);\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run & Test\" description=\"Switch devices during playback\" icon='lucide:play'>\r\n                ### 3. Build and run.\r\n                You'll see a list of playback devices. Enter the number of the device you want to switch to while audio\r\n                is playing.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        The `AudioEngine` (here `MiniAudioEngine`) provides `UpdateDevicesInfo()` to get device lists\r\n        (`PlaybackDevices`, `CaptureDevices`). `SwitchDevice()` or `SwitchDevices()` can then be used to change the\r\n        active audio output/input.\r\n    </Tab>\r\n\r\n    <Tab key=\"editing\" title={<div className=\"flex items-center gap-2\"><Icon icon=\"mdi:content-cut\"/><span>Editing & Persistence</span>\r\n    </div>}>\r\n        This new section covers the powerful non-destructive editing engine introduced in SoundFlow v1.1.0. See the\r\n        dedicated [Editing Engine & Persistence Guide](./editing-engine) for comprehensive details and examples.\r\n\r\n        **Key features demonstrated in the guide:**\r\n        * Creating `Composition`s, `Track`s, and `AudioSegment`s.\r\n        * Manipulating segment properties: `SourceStartTime`, `SourceDuration`, `TimelineStartTime`.\r\n        * Using `AudioSegmentSettings`: volume, pan, reverse, looping, fades (`FadeCurveType`).\r\n\r\n        A simple example of creating a composition and adding a segment:\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Providers;\r\n        using SoundFlow.Editing; // New namespace\r\n        using System;\r\n        using System.IO;\r\n        using System.Threading.Tasks;\r\n\r\n        namespace BasicComposition;\r\n\r\n        internal static class Program\r\n        {\r\n            private static async Task Main(string[] args)\r\n            {\r\n                // Initialize the audio engine.\r\n                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback, channels: 1);\r\n\r\n                // Create a composition\r\n                var composition = new Composition(\"My First Song\") {SampleRate = 48000, TargetChannels = 1};\r\n\r\n                // Create a track\r\n                var track1 = new Track(\"Vocals\");\r\n                composition.AddTrack(track1);\r\n\r\n                // Load an audio file for a segment\r\n                string audioPath = \"path/to/your/audio.wav\";\r\n                if (!File.Exists(audioPath))\r\n                {\r\n                    Console.WriteLine($\"Audio file not found: {audioPath}\");\r\n                    return;\r\n                }\r\n                var provider = new StreamDataProvider(File.OpenRead(audioPath));\r\n\r\n                // Create an audio segment\r\n                // Play from 0s of source, for 5s duration, place at 1s on timeline\r\n                var segment1 = new AudioSegment(provider,\r\n                TimeSpan.Zero,\r\n                TimeSpan.FromSeconds(5),\r\n                TimeSpan.FromSeconds(1),\r\n                \"Intro\",\r\n                ownsDataProvider: true);\r\n\r\n                // Optionally, modify segment settings\r\n                segment1.Settings.Volume = 0.9f;\r\n                segment1.Settings.FadeInDuration = TimeSpan.FromMilliseconds(500);\r\n\r\n                track1.AddSegment(segment1);\r\n\r\n                // Create a SoundPlayer for the composition\r\n                var compositionPlayer = new SoundPlayer(composition); // Composition itself is an ISoundDataProvider\r\n                Mixer.Master.AddComponent(compositionPlayer);\r\n                compositionPlayer.Play();\r\n\r\n                Console.WriteLine($\"Playing composition '{composition.Name}' for {composition.CalculateTotalDuration().TotalSeconds:F1}s... Press any key to stop.\");\r\n                Console.ReadKey();\r\n\r\n                compositionPlayer.Stop();\r\n                Mixer.Master.RemoveComponent(compositionPlayer);\r\n\r\n                composition.Dispose();\r\n            }\r\n        }\r\n        ```\r\n        *For this example to run, you'd need an audio file. The new `SoundFlow.Samples.EditingMixer` project contains\r\n        sample audio files and more complex editing examples.*\r\n    </Tab>\r\n</Tabs>\r\n\r\n---\r\n\r\nThese tutorials and examples provide a starting point for using SoundFlow in your own audio applications.Explore the different components, modifiers, analyzers, and visualizers to create a wide range of audio processing and visualization solutions.Refer to the ** Core Concepts ** and ** API Reference ** sections of the Wiki for more detailed information about each class and interface."
  },
  {
    "id": 13,
    "slug": "getting-started",
    "version": "1.1.2",
    "title": "Getting Started with SoundFlow",
    "description": "Learn how to install the library, set up your development environment, and write your first SoundFlow application.",
    "navOrder": 1,
    "category": "Core",
    "content": "---\ntitle: Getting Started with SoundFlow\ndescription: Learn how to install the library, set up your development environment, and write your first SoundFlow application.\nnavOrder: 1\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\n# Getting Started with SoundFlow\n\nThis guide will help you get up and running with SoundFlow quickly. You'll learn how to install the library, set up your development environment, and write your first SoundFlow application.\n\n## Prerequisites\n\nBefore you begin, make sure you have the following installed:\n\n*   **[.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0) or later:** SoundFlow is built on .NET 8.0, so you'll need the corresponding SDK to build and run SoundFlow projects.\n*   **An IDE or code editor:** You can use any IDE or code editor that supports .NET development. Popular choices include:\n    *   [Visual Studio](https://visualstudio.microsoft.com/) (Recommended for Windows)\n    *   [Visual Studio Code](https://code.visualstudio.com/) (Cross-platform)\n    *   [JetBrains Rider](https://www.jetbrains.com/rider/) (Cross-platform)\n*   **Basic knowledge of C# and .NET:** Familiarity with C# programming and .NET concepts will be helpful.\n\n**Supported Operating Systems:**\n\n*   Windows\n*   macOS\n*   Linux\n*   Android\n*   iOS\n*   FreeBSD\n\n## Installation\n\nYou can install SoundFlow in several ways:\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\n    <Tab\n        key=\"nuget\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:nuget' />\n                <span>NuGet Package Manager</span>\n            </div>\n        }\n    >\n        ### Option 1: Using the NuGet Package Manager (Recommended)\n\n        This is the easiest and recommended way to add SoundFlow to your .NET projects.\n\n        1. **Open the NuGet Package Manager Console:** In Visual Studio, go to `Tools` > `NuGet Package Manager` > `Package Manager Console`.\n        2. **Run the installation command:**\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        This command will download and install the latest version of SoundFlow and its dependencies into your current project.\n    </Tab>\n\n    <Tab\n        key=\"cli\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:dotnet' />\n                <span>.NET CLI</span>\n            </div>\n        }\n    >\n        ### Option 2: Using the .NET CLI\n\n        1. **Open your terminal or command prompt.**\n        2. **Navigate to your project directory.**\n        3. **Run the following command:**\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n\n        This command will add a reference to the SoundFlow package in your project file (`.csproj`).\n    </Tab>\n\n    <Tab\n        key=\"source\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:git' />\n                <span>Building from Source</span>\n            </div>\n        }\n    >\n        ### Option 3: Building from Source\n\n        If you want to use the latest development version of SoundFlow or contribute to the project, you can build it from source:\n\n        1. **Clone the SoundFlow repository:**\n\n        ```bash\n        git clone https://github.com/LSXPrime/SoundFlow.git\n        ```\n\n        2. **Navigate to the cloned directory:**\n\n        ```bash\n        cd SoundFlow\n        ```\n\n        3. **Build the project using the .NET CLI:**\n\n        ```bash\n        dotnet build\n        ```\n    </Tab>\n</Tabs>\n\n## Basic Usage Example\n\nLet's create a simple console application that plays an audio file using SoundFlow.\n\n<Steps layout='horizontal' variant='glow' nextLabel='Got it, Next' finishLabel='All Done!' resetLabel='Start Again' summaryMessage=\"Congratulations! You've built and run your first audio application with SoundFlow. Feel free to experiment with the code or start over.\">\n    <Step title=\"Create Project\" description=\"Use VS or .NET CLI\" icon='ic:outline-create-new-folder'>\n        ### 1. Create a new console application:\n        *   In Visual Studio, go to `File` > `New` > `Project`. Select `Console App` and give it a name (e.g., `SoundFlowExample`).\n        *   Or, use the .NET CLI:\n\n        ```bash\n        dotnet new console -o SoundFlowExample\n        cd SoundFlowExample\n        ```\n    </Step>\n\n    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\n        ### 2. Install the SoundFlow NuGet package: (If you haven't already)\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n    </Step>\n\n    <Step title=\"Write Code\" description=\"Implement the audio player\" icon='ph:code-bold'>\n        ### 3. Replace the contents of `Program.cs` with the following code:\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Providers;\n\n        namespace SoundFlowExample;\n\n        internal static class Program\n        {\n            private static void Main(string[] args)\n            {\n                // Initialize the audio engine with the MiniAudio backend, 44.1kHz sample rate, and playback capability.\n                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\n\n                // Create a SoundPlayer and load an audio file.\n                // Make sure you replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\n                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\n                var player = new SoundPlayer(dataProvider);\n\n                // Add the player to the master mixer. This connects the player's output to the audio engine's output.\n                Mixer.Master.AddComponent(player);\n\n                // Start playback.\n                player.Play();\n\n                // Keep the console application running until playback finishes or the user presses a key.\n                Console.WriteLine(\"Playing audio... Press any key to stop.\");\n                Console.ReadKey();\n\n                // Stop playback.\n                player.Stop();\n\n                // Remove the player from the mixer.\n                Mixer.Master.RemoveComponent(player);\n            }\n        }\n        ```\n        ***Replace `path/to/your/audiofile.wav` with the actual path to an audio file on your computer.***\n    </Step>\n\n    <Step title=\"Run\" description=\"Build and run the app\" icon='lucide:audio-lines'>\n        ### 4. Build and run the application:\n        *   In Visual Studio, press `F5` or go to `Debug` > `Start Debugging`.\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        *  use the .NET CLI:\n\n        ```bash\n        dotnet run\n        ```\n    </Step>\n</Steps>\n\nYou should now hear the audio file playing through your default audio output device.\n\n**Code Explanation:**\n\n*   `using SoundFlow...`: These lines import the necessary namespaces from the SoundFlow library.\n*   `using var audioEngine = new MiniAudioEngine(...)`: This creates an instance of the `MiniAudioEngine`, which is the default audio backend for SoundFlow. It's initialized with a sample rate of 44100 Hz and playback capability. The `using` statement ensures that the engine is properly disposed of when it's no longer needed.\n*   `using var dataProvider = new StreamDataProvider(...)`: Creates a `StreamDataProvider` to load the audio file. `ISoundDataProvider` (which `StreamDataProvider` implements) is now `IDisposable`, so it's good practice to use a `using` statement.\n*   `var player = new SoundPlayer(dataProvider)`: This creates a `SoundPlayer` instance with the loaded audio data.\n*   `Mixer.Master.AddComponent(player)`: This adds the `SoundPlayer` to the `Master` mixer. The `Master` mixer is the root of the audio graph and represents the final output of the audio engine.\n*   `player.Play()`: This starts the playback of the audio file.\n*   `Console.WriteLine(...)` and `Console.ReadKey()`: These lines keep the console application running and wait for the user to press a key.\n*   `player.Stop()`: This stops the playback.\n*   `Mixer.Master.RemoveComponent(player)`: This removes the `SoundPlayer` from the `Master` mixer, disconnecting it from the audio graph.\n\n\n## Troubleshooting\n\n*   **\"Could not load file or assembly 'SoundFlow'...\"**: Make sure you have installed the SoundFlow NuGet package or added a reference to the SoundFlow library if you built it from source.\n*   **No audio output**:\n    *   Verify that your audio device is properly configured and selected as the default output device in your operating system's sound settings.\n    *   Check the volume levels in your operating system and in the SoundFlow application.\n    *   Ensure that the audio file you are trying to play is in a supported format and is not corrupted.\n*   **Errors during installation**: If you encounter errors while installing the NuGet package, try clearing your NuGet cache (`dotnet nuget locals all --clear`) and try again.\n\nIf you encounter any other issues, please open an issue on the [GitHub repository](https://github.com/LSXPrime/SoundFlow).\n\n## Next Steps\n\nNow that you have successfully set up SoundFlow and played your first audio file, you can explore the more advanced features and concepts covered in this Wiki:\n\n*   [Core Concepts](./core-concepts): Learn more about the fundamental building blocks of SoundFlow.\n*   [Editing Engine & Persistence](./editing-engine): Discover the powerful new non-destructive editing and project saving capabilities.\n*   [API Reference](./api-reference): Dive into the detailed documentation for each class and interface.\n*   [Tutorials and Examples](./tutorials-and-examples): Get hands-on experience with various SoundFlow features.\n*   [WebRTC APM Extension](./extensions/webrtc-apm): Explore advanced voice processing features like noise suppression and echo cancellation.\n\nHappy coding!"
  },
  {
    "id": 14,
    "slug": "editing-engine",
    "version": "1.1.2",
    "title": "Editing Engine & Persistence",
    "description": "Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.",
    "navOrder": 3,
    "category": "Core",
    "content": "---\r\ntitle: Editing Engine & Persistence\r\ndescription: Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.\r\nnavOrder: 3\r\ncategory: Core\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# SoundFlow Editing Engine & Persistence\r\n\r\nSoundFlow v1.1.0 introduces a comprehensive, non-destructive audio editing engine and a robust project persistence system. This allows developers to programmatically build, manipulate, and save complex audio timelines, complete with effects, advanced timing controls, and media management.\r\n\r\n## Core Editing Concepts\r\n\r\nThe editing engine revolves around a few key classes:\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Composition\" description=\"The top-level project container\" icon='ph:stack-bold'>\r\n        ### `Composition` (`SoundFlow.Editing.Composition`)\r\n\r\n        The `Composition` is the top-level container for an audio project. Think of it as the main \"session\" or \"project file\" in a Digital Audio Workstation (DAW).\r\n\r\n        *   **Holds Tracks:** A `Composition` contains one or more `Track` objects.\r\n        *   **Master Settings:** It has master volume control (`MasterVolume`) and can have master effects (modifiers and analyzers) applied to the final mix.\r\n        *   **Renderable:** A `Composition` itself implements `ISoundDataProvider`, meaning the entire composed project can be played back directly using a `SoundPlayer` or rendered to an audio file.\r\n        *   **Project Properties:** Stores overall project settings like `Name`, `TargetSampleRate`, and `TargetChannels`.\r\n        *   **Dirty Flag:** Tracks unsaved changes via an `IsDirty` property.\r\n        *   **IDisposable:** Manages the disposal of resources within its scope.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n\r\n        // Create a new composition\r\n        var composition = new Composition(\"My Awesome Project\")\r\n        {\r\n            TargetSampleRate = 48000,\r\n            TargetChannels = 2, // Stereo\r\n            MasterVolume = 0.9f\r\n        };\r\n\r\n        // Add master effects (optional)\r\n        // composition.AddModifier(new SomeMasterReverb());\r\n\r\n        // ... (add tracks and segments) ...\r\n\r\n        // To play the composition:\r\n        // using var audioEngine = new MiniAudioEngine(composition.TargetSampleRate, Capability.Playback, channels: composition.TargetChannels);\r\n        // var player = new SoundPlayer(composition);\r\n        // Mixer.Master.AddComponent(player);\r\n        // player.Play();\r\n\r\n        // To render the composition to a float array:\r\n        // float[] renderedAudio = composition.Render(TimeSpan.Zero, composition.CalculateTotalDuration());\r\n        ```\r\n    </Step>\r\n    <Step title=\"Track\" description=\"A single audio timeline\" icon='lucide:audio-lines'>\r\n        ### `Track` (`SoundFlow.Editing.Track`)\r\n\r\n        A `Track` represents a single audio track within a `Composition`, similar to a track in a DAW.\r\n\r\n        *   **Holds Segments:** A `Track` contains a list of `AudioSegment` objects, which are the actual audio clips placed on the track's timeline.\r\n        *   **Track-Level Settings (`TrackSettings`):** Each track has its own settings:\r\n        *   `Volume`, `Pan`\r\n        *   `IsMuted`, `IsSoloed`, `IsEnabled`\r\n        *   Track-specific `Modifiers` and `Analyzers`.\r\n        *   **Timeline Management:** Tracks manage the arrangement of their segments.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n\r\n        var track1 = new Track(\"Lead Vocals\");\r\n        track1.Settings.Volume = 0.8f;\r\n        track1.Settings.Pan = -0.1f; // Slightly to the left\r\n\r\n        var track2 = new Track(\"Background Music\");\r\n        track2.Settings.Volume = 0.5f;\r\n        track2.Settings.IsMuted = true; // Mute this track for now\r\n\r\n        composition.AddTrack(track1);\r\n        composition.AddTrack(track2);\r\n        ```\r\n    </Step>\r\n    <Step title=\"AudioSegment\" description=\"The fundamental audio clip\" icon='mdi:content-cut'>\r\n        ### `AudioSegment` (`SoundFlow.Editing.AudioSegment`)\r\n\r\n        The `AudioSegment` is the fundamental building block for audio content on a `Track`. It represents a specific portion of an audio source placed at a particular time on the track's timeline.\r\n\r\n        *   **Source Reference:** Points to an `ISoundDataProvider` for its audio data.\r\n        *   **Timeline Placement:**\r\n        *   `SourceStartTime`: The time offset within the `ISoundDataProvider` from which this segment begins.\r\n        *   `SourceDuration`: The duration of audio to use from the `ISoundDataProvider`.\r\n        *   `TimelineStartTime`: The time at which this segment starts on the parent `Track`'s timeline.\r\n        *   **Segment-Level Settings (`AudioSegmentSettings`):** Each segment has incredibly granular control:\r\n        *   `Volume`, `Pan`\r\n        *   `IsEnabled`\r\n        *   `IsReversed`: Play the segment's audio backward.\r\n        *   `Loop` (`LoopSettings`): Control repetitions or loop to fill a target duration.\r\n        *   `FadeInDuration`, `FadeInCurve`, `FadeOutDuration`, `FadeOutCurve`: Apply various fade shapes (`Linear`, `Logarithmic`, `S-Curve`).\r\n        *   `SpeedFactor`: Classic varispeed, affects pitch and tempo.\r\n        *   **Pitch-Preserved Time Stretching:**\r\n        *   `TimeStretchFactor`: Lengthen or shorten the segment without changing pitch (e.g., 0.5 for half duration, 2.0 for double duration).\r\n        *   `TargetStretchDuration`: Stretch the segment to fit a specific duration, preserving pitch.\r\n        *   Segment-specific `Modifiers` and `Analyzers`.\r\n        *   **Non-Destructive:** All operations (trimming, fades, stretching) are applied at runtime and do not alter the original audio source.\r\n        *   **IDisposable:** Can own and dispose its `ISoundDataProvider` if specified.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Assuming 'track1' and 'composition' from previous examples\r\n        // And an audio file \"vocals.wav\" exists.\r\n        using var vocalProvider = new StreamDataProvider(File.OpenRead(\"vocals.wav\"));\r\n\r\n        // Create a segment: use 10 seconds of \"vocals.wav\" starting from 5s into the file,\r\n        // and place it at 2 seconds on track1's timeline.\r\n        var vocalSegment = new AudioSegment(\r\n        sourceDataProvider: vocalProvider,\r\n        sourceStartTime: TimeSpan.FromSeconds(5),\r\n        sourceDuration: TimeSpan.FromSeconds(10),\r\n        timelineStartTime: TimeSpan.FromSeconds(2),\r\n        name: \"Verse 1 Vocals\",\r\n        ownsDataProvider: false // vocalProvider is managed by 'using' here\r\n        );\r\n\r\n        // Apply settings\r\n        vocalSegment.Settings.Volume = 0.95f;\r\n        vocalSegment.Settings.FadeInDuration = TimeSpan.FromMilliseconds(200);\r\n        vocalSegment.Settings.FadeInCurve = FadeCurveType.SCurve;\r\n        vocalSegment.Settings.TimeStretchFactor = 1.1f; // Make it 10% longer without pitch change\r\n\r\n        track1.AddSegment(vocalSegment);\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n### Duration Calculations\r\n\r\n*   `AudioSegment.StretchedSourceDuration`: The duration of the segment's content *after* pitch-preserved time stretching is applied (but before `SpeedFactor`).\r\n*   `AudioSegment.EffectiveDurationOnTimeline`: The duration a single instance of the segment takes on the timeline, considering both `StretchedSourceDuration` and `SpeedFactor`.\r\n*   `AudioSegment.GetTotalLoopedDurationOnTimeline()`: The total duration the segment occupies on the timeline, including all loops.\r\n*   `AudioSegment.TimelineEndTime`: `TimelineStartTime + GetTotalLoopedDurationOnTimeline()`.\r\n*   `Track.CalculateDuration()`: The time of the latest `TimelineEndTime` among all its segments.\r\n*   `Composition.CalculateTotalDuration()`: The time of the latest `TimelineEndTime` among all its tracks.\r\n\r\n## Time Manipulation\r\n\r\nSoundFlow's editing engine offers sophisticated time manipulation capabilities for `AudioSegment`s:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Time manipulation options\">\r\n    <Tab\r\n        key=\"time-stretch\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:timelapse-outline' />\r\n                <span>Pitch-Preserved Time Stretching</span>\r\n            </div>\r\n        }\r\n    >\r\n        This feature allows you to change the duration of an audio segment without affecting its pitch. It's ideal for:\r\n        *   Fitting dialogue or music to a specific time slot.\r\n        *   Creative sound design by drastically stretching or compressing audio.\r\n\r\n        It's controlled by two properties in `AudioSegmentSettings`:\r\n\r\n        *   **`TimeStretchFactor` (float):**\r\n        *   `1.0`: No stretching.\r\n        *   `> 1.0`: Makes the segment longer (e.g., `2.0` doubles the duration).\r\n        *   `< 1.0` and `> 0.0`: Makes the segment shorter (e.g., `0.5` halves the duration).\r\n        *   **`TargetStretchDuration` (TimeSpan?):**\r\n        *   If set, this overrides `TimeStretchFactor`. The segment will be stretched or compressed to match this exact duration.\r\n        *   Set to `null` to use `TimeStretchFactor` instead.\r\n\r\n        Internally, SoundFlow uses a high-quality **WSOLA (Waveform Similarity Overlap-Add)** algorithm implemented in the `WsolaTimeStretcher` class.\r\n\r\n        ```csharp\r\n        // Make a segment 50% shorter while preserving pitch\r\n        mySegment.Settings.TimeStretchFactor = 0.5f;\r\n\r\n        // Make a segment exactly 3.75 seconds long, preserving pitch\r\n        mySegment.Settings.TargetStretchDuration = TimeSpan.FromSeconds(3.75);\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"varispeed\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:speedometer' />\r\n                <span>Classic Speed Control (Varispeed)</span>\r\n            </div>\r\n        }\r\n    >\r\n        The `SpeedFactor` property in `AudioSegmentSettings` provides traditional speed control, affecting both the tempo and the pitch of the audio, similar to changing the playback speed of a tape machine.\r\n\r\n        *   **`SpeedFactor` (float):**\r\n        *   `1.0`: Normal speed and pitch.\r\n        *   `> 1.0`: Faster playback, higher pitch.\r\n        *   `< 1.0` and `> 0.0`: Slower playback, lower pitch.\r\n\r\n        ```csharp\r\n        // Play segment at double speed (and an octave higher)\r\n        mySegment.Settings.SpeedFactor = 2.0f;\r\n\r\n        // Play segment at half speed (and an octave lower)\r\n        mySegment.Settings.SpeedFactor = 0.5f;\r\n        ```\r\n\r\n        **Interaction:** Time stretching is applied to the source audio *first*, and then the `SpeedFactor` is applied to the time-stretched result.\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Project Persistence (`SoundFlow.Editing.Persistence`)\r\n\r\nThe `CompositionProjectManager` class provides static methods for saving and loading your `Composition` objects. Projects are saved in a JSON-based format with the `.sfproj` extension.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Project persistence options\">\r\n    <Tab\r\n        key=\"save\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:save-outline' />\r\n                <span>Saving a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        ### Saving a Project\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Editing.Persistence;\r\n        using System.Threading.Tasks;\r\n\r\n        public async Task SaveMyProject(Composition composition, string filePath)\r\n        {\r\n            await CompositionProjectManager.SaveProjectAsync(\r\n                composition,\r\n                filePath,\r\n                consolidateMedia: true,  // Recommended for portability\r\n            embedSmallMedia: true   // Embeds small audio files directly\r\n            );\r\n            Console.WriteLine($\"Project saved to {filePath}\");\r\n        }\r\n        ```\r\n\r\n        **Saving Options:**\r\n\r\n        *   **`consolidateMedia` (bool):**\r\n        *   If `true` (default), SoundFlow will attempt to copy all unique external audio files referenced by segments into an `Assets` subfolder next to your `.sfproj` file. This makes the project self-contained and portable.\r\n        *   In-memory `ISoundDataProvider`s (like `RawDataProvider` from generated audio) will also be saved as WAV files in the `Assets` folder if `consolidateMedia` is true.\r\n        *   The project file will then store relative paths to these consolidated assets.\r\n        *   **`embedSmallMedia` (bool):**\r\n        *   If `true` (default), audio sources smaller than a certain threshold (currently 1MB) will be embedded directly into the `.sfproj` file as Base64-encoded strings. This is useful for short sound effects or jingles, avoiding the need for separate files.\r\n        *   Embedding takes precedence over consolidation for small files.\r\n    </Tab>\r\n    <Tab\r\n        key=\"load\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:folder-open-outline' />\r\n                <span>Loading a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        ### Loading a Project\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Editing.Persistence;\r\n        using System.Threading.Tasks;\r\n        using System.Collections.Generic; // For List\r\n\r\n        public async Task<(Composition?, List<ProjectSourceReference>)> LoadMyProject(string filePath)\r\n        {\r\n            if (!File.Exists(filePath))\r\n        {\r\n            Console.WriteLine($\"Project file not found: {filePath}\");\r\n            return (null, new List<ProjectSourceReference>());\r\n        }\r\n\r\n            var (loadedComposition, unresolvedSources) = await CompositionProjectManager.LoadProjectAsync(filePath);\r\n\r\n            if (unresolvedSources.Any())\r\n        {\r\n            Console.WriteLine(\"Warning: Some media sources could not be found:\");\r\n            foreach (var missing in unresolvedSources)\r\n        {\r\n            Console.WriteLine($\" - Missing ID: {missing.Id}, Original Path: {missing.OriginalAbsolutePath ?? \"N/A\"}\");\r\n            // Here you could trigger a UI for relinking\r\n        }\r\n        }\r\n\r\n            Console.WriteLine($\"Project '{loadedComposition.Name}' loaded successfully!\");\r\n            return (loadedComposition, unresolvedSources);\r\n        }\r\n        ```\r\n\r\n        When loading, `LoadProjectAsync` returns a tuple:\r\n        *   The loaded `Composition` object.\r\n        *   A `List<ProjectSourceReference>` detailing any audio sources that could not be found (based on embedded data, consolidated paths, or original absolute paths).\r\n    </Tab>\r\n    <Tab\r\n        key=\"relink\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='ph:link-bold' />\r\n                <span>Media Management & Relinking</span>\r\n            </div>\r\n        }\r\n    >\r\n        ### Media Management & Relinking\r\n\r\n        SoundFlow's persistence system attempts to locate media in this order:\r\n        1.  **Embedded Data:** If the `ProjectSourceReference` indicates embedded data, it's decoded.\r\n        2.  **Consolidated Relative Path:** If not embedded, it looks for the file in the `Assets` folder relative to the project file.\r\n        3.  **Original Absolute Path:** If still not found, it tries the original absolute path stored during the save.\r\n\r\n        If a source is still missing, it's added to the `unresolvedSources` list. You can then use `CompositionProjectManager.RelinkMissingMediaAsync` to update the project with the new location of a missing file:\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Editing.Persistence;\r\n        using System.Threading.Tasks;\r\n\r\n        public async Task AttemptRelink(ProjectSourceReference missingSource, string newFilePath, string projectDirectory)\r\n        {\r\n            bool success = CompositionProjectManager.RelinkMissingMediaAsync(\r\n            missingSource,\r\n            newFilePath,\r\n            projectDirectory\r\n            );\r\n\r\n            if (success)\r\n        {\r\n            Console.WriteLine($\"Successfully relinked '{missingSource.Id}' to '{newFilePath}'.\");\r\n            // You might need to re-resolve or update segments in your loaded composition\r\n            // that use this missingSourceReference. One way is to reload the project:\r\n            // (var reloadedComposition, var newMissing) = await CompositionProjectManager.LoadProjectAsync(projectFilePath);\r\n            // Or, manually update ISoundDataProvider instances in affected AudioSegments.\r\n        }\r\n            else\r\n        {\r\n            Console.WriteLine($\"Failed to relink '{missingSource.Id}'. File at new path might be invalid or inaccessible.\");\r\n        }\r\n        }\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\n**Note on `ownsDataProvider` in `AudioSegment`:**\r\n*   When you create `AudioSegment`s manually for a new composition, you manage the lifecycle of their `ISoundDataProvider`s. If you pass `ownsDataProvider: true`, the segment will dispose of the provider when the segment itself (or its parent `Composition`) is disposed.\r\n*   When a `Composition` is loaded from a project file, the `AudioSegment`s created during loading will typically have `ownsDataProvider: true` set for the `ISoundDataProvider`s that were resolved (from file, embedded, or consolidated assets), as the loading process instantiates these providers.\r\n\r\n## Dirty Flag (`IsDirty`)\r\n\r\n`Composition`, `Track`, and `AudioSegment` (via its `Settings`) have an `IsDirty` property.\r\n*   This flag is automatically set to `true` when any significant property that affects playback or persistence is changed.\r\n*   `CompositionProjectManager.SaveProjectAsync` calls `composition.ClearDirtyFlag()` internally upon successful save.\r\n*   You can use this flag to prompt users to save changes before closing an application, for example.\r\n\r\n## Examples in Action\r\n\r\nThe `SoundFlow.Samples.EditingMixer` project in the SoundFlow GitHub repository provides extensive, runnable examples demonstrating:\r\n*   Building compositions with dialogue and generated audio.\r\n*   Using various `AudioSegmentSettings` like fades, loops, reverse, speed, and time stretching.\r\n*   Saving projects with different media handling strategies (consolidation, embedding).\r\n*   Loading projects and handling missing media by relinking.\r\n\r\nExploring this sample project is highly recommended to see these concepts applied in practical scenarios."
  },
  {
    "id": 15,
    "slug": "core-concepts",
    "version": "1.1.2",
    "title": "Core Concepts",
    "description": "Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.",
    "navOrder": 2,
    "category": "Core",
    "content": "---\ntitle: Core Concepts\ndescription: Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.\nnavOrder: 2\ncategory: Core\n---\n\n# Core Concepts\n\nThis section explains the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline. Understanding these core concepts is essential for effectively using and extending the SoundFlow audio engine.\n\n## Audio Engine (`AudioEngine`)\n\nThe `AudioEngine` is the heart of SoundFlow. It's responsible for:\n\n*   **Initializing and managing the audio backend:** SoundFlow supports multiple audio backends (e.g., `MiniAudio`), which handle the low-level interaction with the operating system's audio API. The `AudioEngine` abstracts away the backend details, providing a consistent interface for higher-level components.\n*   **Enumerating and Managing Audio Devices:** The engine can list available playback and capture devices and allows switching between them during runtime.\n*   **Controlling audio device settings:** The engine allows you to configure parameters like sample rate, channel count, and buffer size.\n*   **Driving the audio processing loop:** The `AudioEngine` runs a dedicated, high-priority thread (or uses backend-driven callbacks) that continuously processes audio data. This ensures real-time performance and minimizes latency.\n*   **Providing the root of the audio graph:** The engine hosts the `Master` mixer, which is the starting point for building complex audio processing pipelines.\n\n**Key Properties:**\n\n*   `SampleRate`: The audio sample rate (e.g., 44100 Hz, 48000 Hz).\n*   `Channels`: The number of audio channels (e.g., 2 for stereo).\n*   `Capability`:  Indicates whether the engine is configured for `Playback`, `Recording`, `Mixed` (both), or `Loopback`.\n*   `SampleFormat`: The format of audio samples (e.g., `F32` for 32-bit floating-point).\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n*   `CurrentPlaybackDevice`, `CurrentCaptureDevice`: Information about the currently active audio devices.\n*   `PlaybackDevices`, `CaptureDevices`: Lists of available audio devices.\n\n**Key Methods:**\n\n*   `CreateEncoder(...)`: Creates an instance of an `ISoundEncoder` for the current backend.\n*   `CreateDecoder(...)`: Creates an instance of an `ISoundDecoder` for the current backend.\n*   `UpdateDevicesInfo()`: Refreshes the list of available audio devices.\n*   `SwitchDevice(...)`, `SwitchDevices(...)`: Changes the active playback and/or capture device.\n*   `SoloComponent(...)`: Isolates a specific `SoundComponent` in the audio graph for debugging or monitoring.\n*   `UnsoloComponent(...)`: Removes a component from the soloed state.\n*   `Dispose()`: Releases the resources used by the `AudioEngine`.\n\n**Example:**\n\n```csharp\n// Initialize a MiniAudioEngine with a 48kHz sample rate, stereo output, and 32-bit float samples.\n// 48kHz is a common rate and compatible with extensions like WebRTC APM.\nusing var audioEngine = new MiniAudioEngine(48000, Capability.Playback, SampleFormat.F32, 2);\n\n// List playback devices\naudioEngine.UpdateDevicesInfo();\nforeach(var device in audioEngine.PlaybackDevices)\n{\n    Console.WriteLine($\"Device: {device.Name}, Default: {device.IsDefault}\");\n}\n```\n\n\n## Sound Components (`SoundComponent`)\n\n`SoundComponent` is the abstract base class for all audio processing units in SoundFlow. Each component represents a node in a directed acyclic graph (DAG), known as the **audio graph**.\n\n**Key Features:**\n\n*   **Modular Processing:** Components encapsulate specific audio processing logic, making the system modular and extensible.\n*   **Input and Output Connections:** Components can have zero or more input and output connections, allowing data to flow between them.\n*   **`GenerateAudio(Span<float> buffer)`:** This is the core method that derived classes must implement. It's called repeatedly by the audio engine to either:\n    *   **Generate new audio samples:** For source components like oscillators or file players.\n    *   **Modify existing audio samples:** For effects, filters, or analyzers.\n*   **Properties:**\n    *   `Name`: A descriptive name for the component.\n    *   `Volume`: Controls the output gain.\n    *   `Pan`: Controls the stereo panning (0.0 for full left, 0.5 for center, 1.0 for full right).\n    *   `Enabled`: Enables or disables the component's processing.\n    *   `Solo`: Isolates the component for debugging.\n    *   `Mute`: Silences the component's output.\n    *   `Parent`: The `Mixer` to which this component belongs (if any).\n*   **Methods:**\n    *   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n    *   `DisconnectInput(SoundComponent input)`: Disconnects an input connection.\n    *   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an `AudioAnalyzer` to this component.\n    *   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an `AudioAnalyzer` from this component.\n    *   `AddModifier(SoundModifier modifier)`: Adds a `SoundModifier` to this component.\n    *   `RemoveModifier(SoundModifier modifier)`: Removes a `SoundModifier` from this component.\n\n**Example:**\n\n```csharp\n// A simple custom SoundComponent that generates a sine wave\npublic class SineWaveGenerator : SoundComponent\n{\n    public float Frequency { get; set; } = 440f; // Frequency in Hz\n    private float _phase;\n\n    protected override void GenerateAudio(Span<float> buffer)\n    {\n        var sampleRate = AudioEngine.Instance.SampleRate;\n        for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] = MathF.Sin(_phase); // Assumes mono output or fills one channel\n            _phase += 2 * MathF.PI * Frequency / sampleRate;\n            if (_phase > 2 * MathF.PI) _phase -= 2 * MathF.PI;\n        }\n    }\n}\n```\n\n\n## Mixer (`Mixer`)\n\nThe `Mixer` is a specialized `SoundComponent` that combines the output of multiple `SoundComponent` instances into a single audio stream.\n\n**Key Features:**\n\n*   **`Master` Mixer:** The `Mixer.Master` static property provides access to the default root mixer, which is automatically created by the `AudioEngine`. All audio ultimately flows through the `Master` mixer before reaching the output device.\n*   **Adding and Removing Components:**\n    *   `AddComponent(SoundComponent component)`: Adds a component to the mixer's inputs.\n    *   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n*   **Efficient Mixing:** The `Mixer` uses SIMD instructions (when available) to perform mixing operations very efficiently.\n\n**Example:**\n\n```csharp\n// Create a SoundPlayer and an Oscillator\nusing var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(dataProvider);\nvar oscillator = new Oscillator { Frequency = 220, Type = Oscillator.WaveformType.Square };\n\n// Add both to the Master mixer\nMixer.Master.AddComponent(player);\nMixer.Master.AddComponent(oscillator);\n// ...\n// Don't forget to dispose dataProvider when done if not using 'using' on player scope\n```\n\n\n## Sound Modifiers (`SoundModifier`)\n\n`SoundModifier` is an abstract base class for creating audio effects that modify the audio stream. Modifiers are applied to `SoundComponent` instances or to `AudioSegment`, `Track`, `Composition` and process the audio data.\n\n**Key Features:**\n\n*   **`ProcessSample(float sample, int channel)`:** This is the core method that derived classes can implement to process audio on a sample-by-sample basis.\n*   **`Process(Span<float> buffer)`:** This method can be overridden for buffer-based processing, which is often more efficient for complex effects. By default, it calls `ProcessSample` for each sample.\n*   **`Enabled` Property:** Allows dynamically enabling or disabling the modifier's effect.\n*   **Chaining:** Modifiers can be chained together on a `SoundComponent` to create complex effect pipelines.\n\n**Built-in Modifiers:**\n\nSoundFlow provides a variety of built-in modifiers, including:\n*   Algorithmic Reverb Modifier: Simulates reverberation.\n*   Ambient Reverb Modifier: Creates a sense of spaciousness.\n*   Bass Boost Modifier: Enhances low frequencies.\n*   Chorus Modifier: Creates a chorus effect.\n*   Compressor Modifier: Reduces dynamic range.\n*   Delay Modifier: Applies a delay effect.\n*   Frequency Band Modifier: Boosts or cuts frequency bands.\n*   Noise Reduction Modifier: Reduces noise.\n*   Parametric Equalizer: Provides precise EQ control.\n*   Stereo Chorus Modifier: Creates a stereo chorus.\n*   Treble Boost Modifier: Enhances high frequencies.\n*   And potentially external modifiers like `WebRtcApmModifier` via extensions.\n\n\n**Example:**\n\n```csharp\n// Create a SoundPlayer and a Reverb modifier\nusing var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(dataProvider);\nvar reverb = new AlgorithmicReverbModifier { RoomSize = 0.8f, Wet = 0.2f };\n\n// Add the reverb modifier to the player\nplayer.AddModifier(reverb);\n\n// Add the player to the Master mixer\nMixer.Master.AddComponent(player);\n// ...\n```\n\n## Sound Player Base (`SoundPlayerBase`)\n\n`SoundPlayerBase` is a new abstract class that provides common functionality for sound playback components like `SoundPlayer` and `SurroundPlayer`.\n\n**Key Features (inherited by `SoundPlayer` and `SurroundPlayer`):**\n\n*   Implements `ISoundPlayer`.\n*   Handles core playback logic: reading from an `ISoundDataProvider`, managing playback state (Play, Pause, Stop).\n*   Supports playback speed adjustment via `PlaybackSpeed` property.\n*   Manages looping with `IsLooping`, `LoopStartSamples`/`Seconds`, `LoopEndSamples`/`Seconds`.\n*   Provides seeking capabilities via `Seek` methods (accepting time in seconds, sample offset, or `TimeSpan`).\n*   `Volume` control (inherited from `SoundComponent`).\n*   `PlaybackEnded` event.\n\n## Audio Playback (`SoundPlayer`, `SurroundPlayer`)\n\nSoundFlow provides concrete classes for audio playback, deriving from `SoundPlayerBase`:\n\n*   **`SoundPlayer`:** A `SoundPlayerBase` implementation for standard mono or stereo audio playback from an `ISoundDataProvider`.\n*   **`SurroundPlayer`:** An extended `SoundPlayerBase` implementation that supports advanced surround sound configurations.\n\n**Key Features (`SoundPlayer`):**\n*   All features from `SoundPlayerBase` and `ISoundPlayer`.\n\n**Key Features (`SurroundPlayer`):**\n*   All features from `SoundPlayerBase` and `ISoundPlayer`.\n*   `SpeakerConfiguration`: Allows you to define the speaker setup (e.g., Stereo, Quad, 5.1, 7.1, or a custom configuration).\n*   `PanningMethod`: Selects the panning algorithm to use (Linear, EqualPower, or VBAP).\n*   `ListenerPosition`: Sets the listener's position relative to the speakers.\n*   `VbapParameters`: Provides fine-grained control over VBAP (Vector Base Amplitude Panning) settings.\n\n\n## Audio Recording (`Recorder`)\n\nThe `Recorder` class allows you to capture audio input from a recording device.\n\n**Key Features:**\n*   `StartRecording()`: Begins the recording process.\n*   `PauseRecording()`: Pauses the recording.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `StopRecording()`: Stops the recording and finalizes the output.\n*   `State`: Indicates the current recording state (Playing, Paused, Stopped).\n*   `SampleFormat`: The sample format to use for recording.\n*   `EncodingFormat`: The encoding format to use when saving to a file (e.g., WAV, FLAC), Currently only WAV supported by miniaudio backend.\n*   `SampleRate`: The sample rate for recording.\n*   `Channels`: The number of channels to record.\n*   `Stream`: The output `Stream` where recorded audio is written.\n*   `ProcessCallback`: A delegate that can be used to process recorded audio samples in real time.\n\n\n## Audio Providers (`ISoundDataProvider`)\n\n`ISoundDataProvider` is an interface that defines a standard way to access audio data, regardless of its source.\n\n**Key Features:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples).\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio.\n*   `ReadBytes(Span<float> buffer)`: Reads a chunk of audio data into the provided buffer.\n*   `Seek(int offset)`: Moves the read position to a specific offset (in samples).\n*   `EndOfStreamReached`: An event that is raised when the end of the audio data is reached.\n*   `PositionChanged`: An event that is raised when the read position changes.\n*   `Dispose()`: Implementations should release underlying resources (e.g., file streams).\n\n**Built-in Providers:**\n\n*   `AssetDataProvider`: Loads audio data from a byte array or `Stream`.\n*   `StreamDataProvider`: Reads audio data from a `Stream`.\n*   `MicrophoneDataProvider`: Captures audio data from the microphone in real-time.\n*   `ChunkedDataProvider`: Reads audio data from a file or stream in chunks.\n*   `NetworkDataProvider`: Reads audio data from a network source (URL, HLS).\n*   `RawDataProvider`: Reads audio data from a raw PCM stream or various raw array types (`float[]`, `byte[]`, `int[]`, `short[]`).\n\nIt's good practice to dispose of `ISoundDataProvider` instances when they are no longer needed, for example, using a `using` statement.\n\n```csharp\nusing var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\n// Use dataProvider\n```\n\n## Audio Encoding/Decoding (`ISoundEncoder`, `ISoundDecoder`)\n\n`ISoundEncoder` and `ISoundDecoder` are interfaces for encoding and decoding audio data to and from different formats. Both are `IDisposable`.\n\n*   **`ISoundEncoder`:** Encodes raw audio samples into a specific format (e.g., WAV, FLAC, MP3). Currently only WAV supported by miniaudio backend.\n*   **`ISoundDecoder`:** Decodes audio data from a specific format into raw audio samples.\n\n**`MiniAudio` Backend:**\n\nThe `MiniAudio` backend provides implementations of these interfaces using the `miniaudio` library:\n\n*   `MiniAudioEncoder`\n*   `MiniAudioDecoder`\n\n\n## Audio Analysis (`AudioAnalyzer`)\n\n`AudioAnalyzer` is an abstract base class for creating components that analyze audio data. Analyzers typically extract information from the audio stream without modifying it.\n\n**Key Features:**\n\n*   `Analyze(Span<float> buffer)`: An abstract method that derived classes must implement to perform their specific analysis.\n*   `Enabled`: If false, the `Analyze` step might be skipped by the `SoundComponent` it's attached to.\n*   **Integration with Visualizers:** Analyzers are often used in conjunction with `IVisualizer` implementations to display the analysis results visually.\n\n**Built-in Analyzers:**\n\n*   Level Meter Analyzer: Measures the RMS (root mean square) and peak levels of an audio signal.\n*   Spectrum Analyzer: Computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).\n*   Voice Activity Detector: Detects the presence of human voice in an audio stream.\n\n\n## Audio Visualization (`IVisualizer`)\n\n`IVisualizer` is an interface for creating components that visualize audio data. Visualizers typically don't modify the audio stream but instead render a graphical representation of the data. It implements `IDisposable`.\n\n**Key Features:**\n\n*   `Name`: A descriptive name for the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: This method is called by the audio engine to provide the visualizer with a chunk of audio data to process.\n*   `Render(IVisualizationContext context)`: This method is called to render the visualization. It receives an `IVisualizationContext` instance, which provides drawing methods.\n*   `VisualizationUpdated`: An event that is raised when the visualization needs to be redrawn (e.g., when new audio data has been processed).\n*   `Dispose()`: Releases resources held by the visualizer.\n\n## Visualization Context (`IVisualizationContext`):\n\nThis interface provides a set of drawing methods for rendering the visualization. The specific implementation of `IVisualizationContext` will depend on the UI framework you are using.\n\n**Built-in Visualizers:**\n\n*   Level Meter Visualizer: Displays a level meter that shows the current RMS or peak level of the audio.\n*   Spectrum Visualizer: Renders a bar graph representing the frequency spectrum of the audio.\n*   Waveform Visualizer: Draws the waveform of the audio signal.\n\n## Editing Engine & Persistence (`SoundFlow.Editing`, `SoundFlow.Editing.Persistence`)\n\nSoundFlow v1.1.0 introduces a powerful non-destructive audio editing engine. For a detailed guide, please see the [Editing Engine & Persistence](./editing-engine.mdx) documentation.\n\n**Key Concepts:**\n\n*   **`Composition`**: The main container for an audio project, holding multiple `Track`s. It can be rendered or played back.\n*   **`Track`**: Represents a single audio track within a `Composition`. Contains `AudioSegment`s and has its own settings (volume, pan, mute, solo, effects).\n*   **`AudioSegment`**: A clip of audio placed on a `Track`'s timeline. It references a portion of an `ISoundDataProvider` and has its own extensive settings.\n    *   **`AudioSegmentSettings`**: Controls volume, pan, fades (with `FadeCurveType`), looping (`LoopSettings`), reverse playback, speed, and **pitch-preserved time stretching** (via `TimeStretchFactor` or `TargetStretchDuration`, powered by `WsolaTimeStretcher`).\n    *   Supports segment-level modifiers and analyzers.\n*   **Non-Destructive:** Edits do not alter the original audio source files. All operations are applied at runtime during playback or rendering.\n*   **Project Persistence (`CompositionProjectManager`)**:\n    *   Save and load entire compositions as `.sfproj` files.\n    *   **Media Consolidation**: Option to copy all external audio files into an `Assets` folder within the project.\n    *   **Embed Small Media**: Option to embed small audio files (e.g., SFX) directly into the project file.\n    *   **Relink Missing Media**: If an audio file is moved, the project can be relinked to its new location.\n\nThis new engine allows for programmatic creation and manipulation of complex audio timelines, effects processing at multiple levels (segment, track, master), and robust project management."
  },
  {
    "id": 16,
    "slug": "api-reference",
    "version": "1.1.2",
    "title": "API Reference",
    "description": "A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.",
    "navOrder": 4,
    "category": "Core",
    "content": "---\ntitle: API Reference\ndescription: A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.\nnavOrder: 4\ncategory: Core\n---\n\n# API Reference\n\nThis section provides a detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members. It serves as a comprehensive reference for developers working with the SoundFlow library.\n\n## Namespaces\n\nSoundFlow is organized into the following namespaces:\n\n*   **`SoundFlow.Abstracts`:** Contains abstract classes and interfaces that define the core framework of SoundFlow. These classes provide the foundation for building custom components, modifiers, and other extensions.\n*   **`SoundFlow.Backends`:** Provides backend-specific implementations for audio input/output. The primary backend currently supported is `SoundFlow.Backends.MiniAudio`, which uses the `miniaudio` library.\n*   **`SoundFlow.Components`:** Contains concrete `SoundComponent` classes that provide various audio processing functionalities, including playback, recording, mixing, and analysis.\n*   **`SoundFlow.Editing`:** Contains classes for non-destructive audio editing, including `Composition`, `Track`, `AudioSegment`, `AudioSegmentSettings`, `LoopSettings`, and `FadeCurveType`.\n*   **`SoundFlow.Editing.Persistence`:** Contains classes for saving and loading audio compositions, such as `CompositionProjectManager` and various project data DTOs.\n*   **`SoundFlow.Enums`:** Contains enumerations used throughout the SoundFlow library to represent different states, options, and capabilities.\n*   **`SoundFlow.Exceptions`:** Contains custom exception classes used for error handling within SoundFlow.\n*   **`SoundFlow.Extensions`:** Namespace for official extensions.\n    *   **`SoundFlow.Extensions.WebRtc.Apm`:** Provides integration with the WebRTC Audio Processing Module for features like echo cancellation, noise suppression, and automatic gain control.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Components`:** Contains components utilizing the WebRTC APM, like `NoiseSuppressor`.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Modifiers`:** Contains modifiers utilizing the WebRTC APM, like `WebRtcApmModifier`.\n*   **`SoundFlow.Interfaces`:** Contains interfaces that define contracts for various functionalities, such as audio data providers, encoders, decoders, and visualizers.\n*   **`SoundFlow.Modifiers`:** Contains concrete `SoundModifier` classes that implement various audio effects.\n*   **`SoundFlow.Providers`:** Contains classes that implement the `ISoundDataProvider` interface, providing ways to load audio data from different sources.\n*   **`SoundFlow.Structs`:** Contains custom struct types used within SoundFlow, often for interop or specific data representation.\n*   **`SoundFlow.Utils`:** Contains utility classes and extension methods that provide helpful functionalities for working with audio data and performing common operations.\n*   **`SoundFlow.Visualization`:** Contains classes related to audio visualization, including analyzers and visualizers.\n\n## Key Classes and Interfaces\n\nBelow is a summary of the key classes and interfaces in SoundFlow.\n\n\n### Abstracts\n\n| Class/Interface                               | Description                                                                                                     |\n| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n| [`AudioAnalyzer`](#abstracts-audioanalyzer)   | Abstract base class for audio analysis components. Inherits from `SoundComponent`.                              |\n| [`AudioEngine`](#abstracts-audioengine)       | Abstract base class for audio engine implementations. Manages audio device, processing thread, and audio graph. |\n| [`SoundComponent`](#abstracts-soundcomponent) | Abstract base class for all audio processing units in SoundFlow. Represents a node in the audio graph.          |\n| [`SoundModifier`](#abstracts-soundmodifier)   | Abstract base class for audio effects that modify audio samples.                                                |\n| [`SoundPlayerBase`](#abstracts-soundplayerbase) | Abstract base class providing common functionality for sound playback components. Inherits from `SoundComponent` and implements `ISoundPlayer`. Includes support for time-stretching. |\n\n### Backends.MiniAudio\n\n| Class/Interface                                           | Description                                                                                                                                |\n| --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`MiniAudioDecoder`](#backendsminiaudio-miniaudiodecoder) | `ISoundDecoder` implementation using the `miniaudio` library.                                                                              |\n| [`MiniAudioEncoder`](#backendsminiaudio-miniaudiodecoder) | `ISoundEncoder` implementation using the `miniaudio` library.                                                                              |\n| [`MiniAudioEngine`](#backendsminiaudio-miniaudioengine)   | `AudioEngine` implementation that uses the `miniaudio` library for audio I/O. Provides concrete implementations for encoding and decoding. |\n\n### Components\n\n| Class/Interface                                                | Description                                                                                                                                            |\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`EnvelopeGenerator`](#components-envelopegenerator)           | `SoundComponent` that generates an ADSR (Attack, Decay, Sustain, Release) envelope signal.                                                             |\n| [`Filter`](#components-filter)                                 | `SoundComponent` that applies a digital filter (low-pass, high-pass, band-pass, notch) to the audio signal.                                            |\n| [`LowFrequencyOscillator`](#components-lowfrequencyoscillator) | `SoundComponent` that generates a low-frequency oscillator (LFO) signal with various waveforms.                                                        |\n| [`Mixer`](#components-mixer)                                   | `SoundComponent` that mixes multiple audio streams together. The `Mixer.Master` property provides access to the default root mixer.                    |\n| [`Oscillator`](#components-oscillator)                         | `SoundComponent` that generates various waveforms (sine, square, sawtooth, triangle, noise, pulse).                                                    |\n| [`Recorder`](#components-recorder)                             | `SoundComponent` that captures audio input from a recording device and allows saving it to a stream or processing it via a callback.                     |\n| [`SoundPlayer`](#components-soundplayer)                       | `SoundPlayerBase` implementation that plays audio from an `ISoundDataProvider`.                                                                        |\n| [`SurroundPlayer`](#components-surroundplayer)                 | `SoundPlayerBase` implementation that extends `SoundPlayer` to support surround sound configurations with customizable speaker positions, delays, and panning methods. |\n| [`VoiceActivityDetector`](#components-voiceactivitydetector)   | `SoundComponent` and `AudioAnalyzer` that detects the presence of human voice in an audio stream using spectral features and energy thresholds.        |\n| [`WsolaTimeStretcher`](#components-wsolatimestretcher)\t | Implements WSOLA algorithm for real-time, pitch-preserved time stretching. Used internally by `AudioSegment`. |\n\n### Editing\n\n| Class/Interface                                       | Description                                                                                                                                            |\n| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`Composition`](#editing-composition)                 | Top-level container for audio tracks, representing a complete project. Implements `ISoundDataProvider` for rendering. `IDisposable`.                 |\n| [`Track`](#editing-track)                             | Represents a single audio track within a `Composition`, containing `AudioSegment`s and track-level settings.                                         |\n| [`AudioSegment`](#editing-audiosegment)               | Represents a single audio clip on a `Track`'s timeline, referencing a portion of an audio source and applying playback settings. `IDisposable`.      |\n| [`AudioSegmentSettings`](#editing-audiosegmentsettings) | Configurable settings for an `AudioSegment` (volume, pan, fades, loop, reverse, speed, time stretch, modifiers, analyzers).                          |\n| [`TrackSettings`](#editing-tracksettings)             | Configurable settings for a `Track` (volume, pan, mute, solo, enabled, modifiers, analyzers).                                                          |\n| [`LoopSettings`](#editing-loopsettings)               | (struct) Defines looping behavior for an `AudioSegment` (repetitions, target duration).                                                                |\n| [`FadeCurveType`](#editing-fadecurvetype)             | (enum) Defines curve types for fade effects (Linear, Logarithmic, SCurve).                                                                             |\n\n### Editing.Persistence\n\n| Class/Interface                                                        | Description                                                                                                                            |\n| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| [`CompositionProjectManager`](#editingpersistence-compositionprojectmanager) | Static class for saving and loading `Composition` projects to/from `.sfproj` files. Handles media consolidation and relinking.       |\n| [`ProjectData`](#editingpersistence-projectdata)                         | DTO representing the root of a saved project file.                                                                                     |\n| [`ProjectTrack`](#editingpersistence-projecttrack)                       | DTO for a `Track` within a saved project.                                                                                              |\n| [`ProjectSegment`](#editingpersistence-projectsegment)                   | DTO for an `AudioSegment` within a saved project.                                                                                      |\n| [`ProjectAudioSegmentSettings`](#editingpersistence-projectaudiosegmentsettings) | DTO for `AudioSegmentSettings` within a saved project.                                                                                 |\n| [`ProjectTrackSettings`](#editingpersistence-projecttracksettings)       | DTO for `TrackSettings` within a saved project.                                                                                        |\n| [`ProjectSourceReference`](#editingpersistence-projectsourcereference)   | DTO representing how an audio source is referenced in a project (file path, embedded data, consolidation).                             |\n| [`ProjectEffectData`](#editingpersistence-projecteffectdata)             | DTO for serializing `SoundModifier` or `AudioAnalyzer` instances (type name, parameters).                                              |\n\n### Enums\n\n| Enum                                             | Description                                                                                                                                 |\n| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Capability`](#enums-capability)                | Specifies the capabilities of an `AudioEngine` instance (Playback, Recording, Mixed, Loopback).                                           |\n| [`DeviceType`](#enums-devicetype)                | Specifies the type of audio device (Playback, Capture).                                                                                     |\n| [`EncodingFormat`](#enums-encodingformat)        | Specifies the audio encoding format to use (e.g., WAV, FLAC, MP3, Vorbis). *Note: MiniAudio backend currently only supports WAV for encoding.* |\n| [`PlaybackState`](#enums-playbackstate)          | Specifies the current playback state of a `SoundPlayer` or `SurroundPlayer` (Stopped, Playing, Paused).                                    |\n| [`Result`](#enums-result)                        | Represents the result of an operation, including success and various error codes.                                                           |\n| [`SampleFormat`](#enums-sampleformat)            | Specifies the format of audio samples (e.g., U8, S16, S24, S32, F32).                                                                     |\n| [`FilterType`](#enums-filtertype)                | Specifies the type of filter to use in the `Filter` and `ParametricEqualizer` components (Peaking, LowShelf, HighShelf, BandPass, Notch, LowPass, HighPass) |\n| [`EnvelopeGenerator.EnvelopeState`](#enums-envelopegenerator-envelopestate) | Specifies the current state of the envelope generator (Idle, Attack, Decay, Sustain, Release) |\n| [`EnvelopeGenerator.TriggerMode`](#enums-envelopegenerator-triggermode) | Specifies how the envelope generator is triggered (NoteOn, Gate, Trigger) |\n| [`LowFrequencyOscillator.WaveformType`](#enums-lowfrequencyoscillator-waveformtype) | Specifies the waveform type for the low-frequency oscillator (Sine, Square, Triangle, Sawtooth, ReverseSawtooth, Random, SampleAndHold) |\n| [`LowFrequencyOscillator.TriggerMode`](#enums-lowfrequencyoscillator-triggermode) | Specifies how the LFO is triggered (FreeRunning, NoteTrigger) |\n| [`Oscillator.WaveformType`](#enums-oscillator-waveformtype) | Specifies the waveform type for the oscillator (Sine, Square, Sawtooth, Triangle, Noise, Pulse) |\n| [`SurroundPlayer.SpeakerConfiguration`](#enums-surroundplayer-speakerconfiguration) | Specifies the speaker configuration for the surround player (Stereo, Quad, Surround51, Surround71, Custom) |\n| [`SurroundPlayer.PanningMethod`](#enums-surroundplayer-panningmethod) | Specifies the panning method for the surround player (Linear, EqualPower, Vbap) |\n| [`FadeCurveType`](#editing-fadecurvetype)\t   | Specifies curve types for fade effects (Linear, Logarithmic, SCurve). |\n| **`SoundFlow.Extensions.WebRtc.Apm` Enums**          |                                                                                                                                             |\n| [`ApmError`](#extensions-webrtc-apm-apmerror)    | Error codes returned by the WebRTC Audio Processing Module.                                                                                |\n| [`NoiseSuppressionLevel`](#extensions-webrtc-apm-noisesuppressionlevel) | Specifies noise suppression levels (Low, Moderate, High, VeryHigh).                                                                  |\n| [`GainControlMode`](#extensions-webrtc-apm-gaincontrolmode) | Specifies gain controller modes (AdaptiveAnalog, AdaptiveDigital, FixedDigital).                                                        |\n| [`DownmixMethod`](#extensions-webrtc-apm-downmixmethod) | Specifies methods for downmixing audio channels (AverageChannels, UseFirstChannel).                                                    |\n| [`RuntimeSettingType`](#extensions-webrtc-apm-runtimesettingtype) | Specifies types of runtime settings for the WebRTC APM.                                                                        |\n\n### Exceptions\n\n| Class                                           | Description                                                                                   |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [`BackendException`](#exceptions-backendexception) | Thrown when an error occurs in a specific audio backend.                                     |\n\n### Extensions.WebRtc.Apm\n\n| Class/Interface                                                                 | Description                                                                                                                                       |\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AudioProcessingModule`](#extensions-webrtc-apm-audioprocessingmodule)         | Provides access to the native WebRTC Audio Processing Module for advanced audio processing like AEC, NS, AGC.                                     |\n| [`ApmConfig`](#extensions-webrtc-apm-apmconfig)                                 | Represents a configuration for the `AudioProcessingModule`, allowing enabling/disabling and setting parameters for various APM features.            |\n| [`StreamConfig`](#extensions-webrtc-apm-streamconfig)                           | Represents a stream configuration (sample rate, channels) for audio processing within the APM.                                                    |\n| [`ProcessingConfig`](#extensions-webrtc-apm-processingconfig)                   | Holds multiple `StreamConfig` instances for input, output, and reverse streams for the APM.                                                       |\n| **Components Namespace**                                                        |                                                                                                                                                   |\n| [`NoiseSuppressor`](#extensions-webrtc-apm-components-noisesuppressor)          | A component for offline/batch noise suppression using WebRTC APM, processing audio from an `ISoundDataProvider`.                                  |\n| **Modifiers Namespace**                                                         |                                                                                                                                                   |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier)       | A `SoundModifier` that applies WebRTC APM features (AEC, NS, AGC, etc.) in real-time to an audio stream within the SoundFlow graph. Configurable. |\n\n### Interfaces\n\n| Interface                                           | Description                                                                                                                                  |\n| --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ISoundDataProvider`](#interfaces-isounddataprovider) | Defines a standard way to access audio data from various sources. Implements `IDisposable`.                                                |\n| [`ISoundDecoder`](#interfaces-isounddecoder)         | Defines the contract for decoding audio data from a specific format into raw audio samples.                                                 |\n| [`ISoundEncoder`](#interfaces-isoundencoder)         | Defines the contract for encoding raw audio samples into a specific format.                                                                 |\n| [`ISoundPlayer`](#interfaces-isoundplayer)           | Defines the contract for controlling audio playback (Play, Pause, Stop, Seek, Looping, Speed, Volume).                                       |\n| [`IVisualizationContext`](#interfaces-ivisualizationcontext) | Provides drawing methods for rendering audio visualizations. The implementation depends on the specific UI framework used.               |\n| [`IVisualizer`](#interfaces-ivisualizer)             | Defines the contract for components that visualize audio data.                                                                            |\n\n### Modifiers\n\n| Class                                                               | Description                                                                                                                                                                                           |\n| ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AlgorithmicReverbModifier`](#modifiers-algorithmicreverbmodifier) | Simulates reverberation using a network of comb and all-pass filters.                                                                                                                                 |\n| [`AmbientReverbModifier`](#modifiers-ambientreverbmodifier)         | Creates a sense of spaciousness by simulating ambient reflections.                                                                                                                                    |\n| [`BassBoostModifier`](#modifiers-bassboostmodifier)                 | Enhances low-frequency content using a low-pass filter.                                                                                                                                               |\n| [`ChorusModifier`](#modifiers-chorusmodifier)                       | Creates a chorus effect by mixing delayed and modulated copies of the signal.                                                                                                                         |\n| [`CompressorModifier`](#modifiers-compressormodifier)               | Reduces the dynamic range of the audio signal using a compressor algorithm.                                                                                                                           |\n| [`DelayModifier`](#modifiers-delaymodifier)                         | Applies a delay effect with feedback and optional low-pass filtering of the delayed signal.                                                                                                           |\n| [`FrequencyBandModifier`](#modifiers-frequencybandmodifier)         | Allows boosting or cutting specific frequency bands using a combination of low-pass and high-pass filters.                                                                                            |\n| [`NoiseReductionModifier`](#modifiers-noisereductionmodifier)       | Reduces noise in an audio stream using spectral subtraction.                                                                                                                                          |\n| [`ParametricEqualizer`](#modifiers-parametricequalizer)             | Provides precise control over the frequency spectrum with multiple configurable bands, each of which can be set as a peaking, low-shelf, high-shelf, band-pass, notch, low-pass, or high-pass filter. |\n| [`StereoChorusModifier`](#modifiers-stereochorusmodifier)           | Creates a stereo chorus effect with independent processing for the left and right channels.                                                                                                           |\n| [`TrebleBoostModifier`](#modifiers-trebleboostmodifier)             | Enhances high-frequency content using a high-pass filter.                                                                                                                                             |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier) (from `SoundFlow.Extensions.WebRtc.Apm.Modifiers`) | Applies WebRTC APM features like echo cancellation, noise suppression, and AGC in real-time. |\n\n### Providers\n\n| Class                                                         | Description                                                                                                                                  |\n| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AssetDataProvider`](#providers-assetdataprovider)           | `ISoundDataProvider` implementation that reads audio data from a byte array (useful for in-memory assets). Implements `IDisposable`.            |\n| [`StreamDataProvider`](#providers-streamdataprovider)         | `ISoundDataProvider` implementation that reads audio data from a generic `Stream` (supports seeking if the stream is seekable). Implements `IDisposable`. |\n| [`MicrophoneDataProvider`](#providers-microphonedataprovider) | `ISoundDataProvider` implementation that captures and provides audio data from the microphone in real-time. Implements `IDisposable`.            |\n| [`ChunkedDataProvider`](#providers-chunkeddataprovider)       | `ISoundDataProvider` implementation that reads and decodes audio data from a file or stream in chunks, improving efficiency for large files. Implements `IDisposable`. |\n| [`NetworkDataProvider`](#providers-networkdataprovider)       | `ISoundDataProvider` implementation that provides audio data from a network source (direct URL or HLS playlist). Implements `IDisposable`.   |\n| [`RawDataProvider`](#providers-rawdataprovider)               | `ISoundDataProvider` implementation for reading raw PCM audio data from a stream. Implements `IDisposable`.                                   |\n\n### Structs\n\n| Struct                                       | Description                                                                                    |\n| -------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| [`DeviceInfo`](#structs-deviceinfo)          | Represents information about an audio device, including ID, name, and supported formats.         |\n| [`NativeDataFormat`](#structs-nativedataformat) | Represents a native data format supported by an audio device (format, channels, sample rate). |\n\n### Utils\n\n| Class                                       | Description                                                                                                                                                                                                 |\n| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Extensions`](#utils-extensions)           | Provides extension methods for working with audio data and other utility functions, including `ReadArray<T>` for reading structures from native memory.                                                                                                                             |\n| [`MathHelper`](#utils-mathhelper)           | Provides mathematical functions and algorithms used in audio processing, including optimized FFT, window functions, `Mod`, and `PrincipalAngle`. |\n\n### Visualization\n\n| Class/Interface                                         | Description                                                                                                                                                                                      |\n| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`LevelMeterAnalyzer`](#visualization-levelmeteranalyzer) | `AudioAnalyzer` that calculates the RMS (root mean square) and peak levels of an audio signal.                                                                                                  |\n| [`LevelMeterVisualizer`](#visualization-levelmetervisualizer) | `IVisualizer` that displays a level meter showing the current RMS or peak level of the audio.                                                                                                 |\n| [`SpectrumAnalyzer`](#visualization-spectrumanalyzer)    | `AudioAnalyzer` that computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).                                                                                         |\n| [`SpectrumVisualizer`](#visualization-spectrumvisualizer)  | `IVisualizer` that renders a bar graph representing the frequency spectrum of the audio.                                                                                                        |\n| [`WaveformVisualizer`](#visualization-waveformvisualizer)  | `IVisualizer` that draws the waveform of the audio signal.                                                                                                                                     |\n\n## Detailed Class and Interface Documentation\n\nThis section provides more in-depth information about some of the key classes and interfaces.\n\n### Abstracts `AudioAnalyzer`\n\n```csharp\npublic abstract class AudioAnalyzer : SoundComponent\n{\n    protected AudioAnalyzer(IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    protected abstract void Analyze(Span<float> buffer);\n    protected override void GenerateAudio(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Enabled`: Gets or sets whether the analyzer is active. If false, `Analyze` might be skipped.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer)`: Abstract method to be implemented by derived classes to perform audio analysis on the provided buffer.\n*   `GenerateAudio(Span<float> buffer)`: Overrides `SoundComponent`'s `GenerateAudio`, internally calls `Analyze` and then processes the visualizer if one is attached.\n\n### Abstracts `AudioEngine`\n\n```csharp\npublic abstract class AudioEngine : IDisposable\n{\n    protected AudioEngine(int sampleRate, Capability capability, SampleFormat sampleFormat, int channels);\n\n    public static int Channels { get; protected set; }\n    public Capability Capability { get; }\n    public static AudioEngine Instance { get; }\n    public bool IsDisposed { get; protected set; }\n    public float InverseSampleRate { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n    public DeviceInfo? CurrentPlaybackDevice { get; protected set; }\n    public DeviceInfo? CurrentCaptureDevice { get; protected set; }\n    public int CaptureDeviceCount { get; protected set; }\n    public int PlaybackDeviceCount { get; protected set; }\n    public DeviceInfo[] PlaybackDevices { get; protected set; }\n    public DeviceInfo[] CaptureDevices { get; protected set; }\n\n    public static event AudioProcessCallback? OnAudioProcessed;\n\n    ~AudioEngine();\n\n    protected abstract void CleanupAudioDevice();\n    public abstract ISoundDecoder CreateDecoder(Stream stream);\n    public abstract ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate);\n    protected virtual void Dispose(bool disposing);\n    protected abstract void InitializeAudioDevice();\n    protected abstract void ProcessAudioData();\n    protected void ProcessAudioInput(nint input, int length);\n    protected void ProcessGraph(nint output, int length);\n    public void SoloComponent(SoundComponent component);\n    public void UnsoloComponent(SoundComponent component);\n    public abstract void SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback);\n    public abstract void SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo);\n    public abstract void UpdateDevicesInfo();\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Channels`: The number of audio channels.\n*   `Capability`: The audio engine's capabilities (Playback, Recording, Mixed, Loopback).\n*   `Instance`: Gets the singleton instance of the `AudioEngine`.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n*   `InverseSampleRate`: The inverse of the sample rate (1 / `SampleRate`).\n*   `SampleFormat`: The audio sample format.\n*   `SampleRate`: The audio sample rate.\n*   `CurrentPlaybackDevice`: Gets the currently selected playback device.\n*   `CurrentCaptureDevice`: Gets the currently selected capture device.\n*   `CaptureDeviceCount`: Gets the number of available capture devices.\n*   `PlaybackDeviceCount`: Gets the number of available playback devices.\n*   `PlaybackDevices`: Gets an array of available playback devices.\n*   `CaptureDevices`: Gets an array of available capture devices.\n\n\n**Events:**\n\n*   `OnAudioProcessed`: Event that is raised after each audio processing cycle.\n\n**Methods:**\n\n*   `CleanupAudioDevice()`: Abstract method to be implemented by derived classes to clean up audio device resources.\n*   `CreateDecoder(Stream stream)`: Creates an `ISoundDecoder` for the specific backend.\n*   `CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate)`: Creates an `ISoundEncoder` for the specific backend.\n*   `Dispose(bool disposing)`: Releases resources used by the engine.\n*   `InitializeAudioDevice()`: Abstract method to be implemented by derived classes to initialize the audio device.\n*   `ProcessAudioData()`: Abstract method to be implemented by derived classes to perform the main audio processing loop.\n*   `ProcessAudioInput(nint input, int length)`: Processes the audio input buffer.\n*   `ProcessGraph(nint output, int length)`: Processes the audio graph and outputs to the specified buffer.\n*   `SoloComponent(SoundComponent component)`: Solos a component in the audio graph.\n*   `UnsoloComponent(SoundComponent component)`: Unsolos a component in the audio graph.\n*   `SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback)`: Switches the audio engine to use the specified device.\n*   `SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo)`: Switches playback and/or capture devices.\n*   `UpdateDevicesInfo()`: Retrieves and updates the list of available audio devices.\n*   `Dispose()`: Public method to dispose of the engine and its resources.\n\n### Abstracts `SoundComponent`\n\n```csharp\npublic abstract class SoundComponent\n{\n    protected SoundComponent();\n\n    public virtual float Pan { get; set; } // Range 0.0 (Left) to 1.0 (Right), 0.5 is Center\n    public virtual string Name { get; set; }\n    public Mixer? Parent { get; set; }\n    public virtual bool Solo { get; set; }\n    public virtual float Volume { get; set; }\n    public virtual bool Enabled { get; set; }\n    public virtual bool Mute { get; set; }\n\n    public IReadOnlyList<SoundComponent> Inputs { get; }\n    public IReadOnlyList<SoundModifier> Modifiers { get; }\n    public IReadOnlyList<AudioAnalyzer> Analyzers { get; }\n\n    public void AddModifier(SoundModifier modifier);\n    public void ConnectInput(SoundComponent input);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void DisconnectInput(SoundComponent input);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    protected abstract void GenerateAudio(Span<float> buffer);\n    internal void Process(Span<float> outputBuffer);\n    public void RemoveModifier(SoundModifier modifier);\n}\n```\n\n**Properties:**\n\n* `Pan`: The panning of the component's output (0.0 for full left, 0.5 for center, 1.0 for full right, using equal-power panning).\n* `Name`: The name of the component.\n* `Parent`: The parent mixer of this component.\n* `Solo`: Whether the component is soloed.\n* `Volume`: The volume of the component's output.\n* `Enabled`: Whether the component is enabled.\n* `Mute`: Whether the component is muted.\n* `Inputs`: Read-only list of connected input components.\n* `Modifiers`: Read-only list of applied modifiers.\n* `Analyzers`: Read-only list of attached audio analyzers.\n\n**Methods:**\n\n* `AddModifier(SoundModifier modifier)`: Adds a sound modifier to the component.\n* `RemoveModifier(SoundModifier modifier)`: Removes a sound modifier from the component.\n* `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n* `DisconnectInput(SoundComponent input)`: Disconnects an input from this component.\n* `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an audio analyzer to the component.\n* `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an audio analyzer from the component.\n* `GenerateAudio(Span<float> buffer)`: Abstract method that derived classes must implement to generate or modify audio data.\n* `Process(Span<float> outputBuffer)`: Processes the component's audio, including applying modifiers and handling input/output connections.\n\n### Abstracts `SoundModifier`\n\n```csharp\npublic abstract class SoundModifier\n{\n    public SoundModifier();\n\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    public abstract float ProcessSample(float sample, int channel);\n    public virtual void Process(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n*   `Enabled`: Gets or sets whether the modifier is active and should process audio. Defaults to true.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Abstract method to be implemented by derived classes to process a single audio sample.\n*   `Process(Span<float> buffer)`: Processes a buffer of audio data. Applies `ProcessSample` to each sample if not overridden.\n\n### Abstracts `SoundPlayerBase`\n\n```csharp\npublic abstract class SoundPlayerBase : SoundComponent, ISoundPlayer\n{\n    protected SoundPlayerBase(ISoundDataProvider dataProvider);\n\n    public float PlaybackSpeed { get; set; }\n    public PlaybackState State { get; private set; }\n    public bool IsLooping { get; set; }\n    public float Time { get; }\n    public float SourceTimeSeconds { get; } // Time in normal playback speed (1.0)\n    public float Duration { get; }\n    public int LoopStartSamples { get; }\n    public int LoopEndSamples { get; }\n    public float LoopStartSeconds { get; }\n    public float LoopEndSeconds { get; }\n    // Volume is inherited from SoundComponent\n\n    public event EventHandler<EventArgs>? PlaybackEnded;\n\n    protected override void GenerateAudio(Span<float> output);\n    protected virtual void HandleEndOfStream(Span<float> remainingOutputBuffer);\n    protected virtual void OnPlaybackEnded();\n\n    public void Play();\n    public void Pause();\n    public void Stop();\n    public bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    public bool Seek(float time);\n    public bool Seek(int sampleOffset);\n    public void SetLoopPoints(float startTime, float? endTime = -1f);\n    public void SetLoopPoints(int startSample, int endSample = -1);\n    public void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `PlaybackSpeed`: Gets or sets the playback speed (1.0 is normal).\n*   `State`: Gets the current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Gets or sets whether looping is enabled.\n*   `Time`: Gets the current playback position in seconds, affected by `PlaybackSpeed`.\n*   `SourceTimeSeconds`: Gets the current playback position in seconds as if `PlaybackSpeed` were 1.0.\n*   `Duration`: Gets the total duration of the audio in seconds.\n*   `LoopStartSamples`: Gets the loop start point in samples.\n*   `LoopEndSamples`: Gets the loop end point in samples (-1 for end of audio).\n*   `LoopStartSeconds`: Gets the loop start point in seconds.\n*   `LoopEndSeconds`: Gets the loop end point in seconds (-1 for end of audio).\n*   `Volume`: (Inherited from `SoundComponent`) Gets or sets the volume of the player.\n\n**Events:**\n\n*   `PlaybackEnded`: Occurs when playback reaches the end of the audio (not raised during looping).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> output)`: (Protected Override) Core audio generation logic, handles reading from data provider, resampling for playback speed, and looping.\n*   `HandleEndOfStream(Span<float> remainingOutputBuffer)`: (Protected Virtual) Handles logic when the data provider reaches its end (looping or stopping).\n*   `OnPlaybackEnded()`: (Protected Virtual) Invokes the `PlaybackEnded` event.\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to a specific time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to a specific time in seconds. Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to a specific sample offset. Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures loop points using start/end times in seconds.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures loop points using start/end sample indices.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures loop points using `TimeSpan`.\n\n### Abstracts `WsolaTimeStretcher`\n```csharp\npublic class WsolaTimeStretcher\n{\n    public WsolaTimeStretcher(int initialChannels = 2, float initialSpeed = 1.0f);\n\n    public void SetChannels(int channels);\n    public void SetSpeed(float speed);\n    public int MinInputSamplesToProcess { get; }\n    public void Reset();\n    public float GetTargetSpeed();\n    public int Process(ReadOnlySpan<float> input, Span<float> output, out int samplesConsumedFromInputBuffer, out int sourceSamplesRepresentedByOutput);\n    public int Flush(Span<float> output);\n}\n```\n**Description:** Implements the WSOLA (Waveform Similarity Overlap-Add) algorithm for real-time, pitch-preserved time stretching of audio. Allows changing playback speed without altering pitch. Primarily used internally by `AudioSegment`.\n\n\n### Backends.MiniAudio `MiniAudioDecoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioDecoder : ISoundDecoder\n{\n    internal MiniAudioDecoder(Stream stream);\n\n    public bool IsDisposed { get; private set; }\n    public int Length { get; private set; } // Length can be updated after initial check\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int Decode(Span<float> samples);\n    public void Dispose();\n    public bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n* `IsDisposed`: Indicates whether the decoder has been disposed.\n* `Length`: The total length of the decoded audio data in samples. *Note: Can be updated after initial checks if the stream length was not immediately available.*\n* `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n* `EndOfStreamReached`: Occurs when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n* `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer. Internally synchronized.\n* `Dispose()`: Releases the resources used by the decoder.\n* `Seek(int offset)`: Seeks to the specified offset within the audio stream (in samples). Internally synchronized.\n\n### Backends.MiniAudio `MiniAudioEncoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioEncoder : ISoundEncoder\n{\n    public MiniAudioEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n\n    public void Dispose();\n    public int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the encoder.\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples and writes them to the output file or stream.\n\n### Backends.MiniAudio `MiniAudioEngine`\n\n```csharp\npublic sealed class MiniAudioEngine : AudioEngine\n{\n    public MiniAudioEngine(int sampleRate, Capability capability, SampleFormat sampleFormat = SampleFormat.F32, int channels = 2);\n\n    protected override void CleanupAudioDevice();\n    public override ISoundDecoder CreateDecoder(Stream stream);\n    public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate); // Now public\n    protected override void InitializeAudioDevice();\n    protected override void ProcessAudioData();\n    public override void SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback);\n    public override void SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo);\n    public override void UpdateDevicesInfo();\n}\n```\n\n**Methods:**\n\n*   `CleanupAudioDevice()`: Cleans up the audio device resources.\n*   `CreateDecoder(Stream stream)`: Creates a `MiniAudioDecoder` instance.\n*   `CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate)`: Creates a `MiniAudioEncoder` instance.\n*   `InitializeAudioDevice()`: Initializes the audio device using `miniaudio`, including context initialization.\n*   `ProcessAudioData()`: Implements the main audio processing loop using `miniaudio` (typically via callbacks).\n*   `SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback)`: Switches the playback or capture device.\n*   `SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo)`: Switches both playback and capture devices if specified.\n*   `UpdateDevicesInfo()`: Retrieves and updates the list of available playback and capture devices from MiniAudio.\n\n### Components `EnvelopeGenerator`\n\n```csharp\npublic class EnvelopeGenerator : SoundComponent\n{\n    public EnvelopeGenerator();\n\n    public float AttackTime { get; set; }\n    public float DecayTime { get; set; }\n    public override string Name { get; set; }\n    public float ReleaseTime { get; set; }\n    public bool Retrigger { get; set; }\n    public float SustainLevel { get; set; }\n    public TriggerMode Trigger { get; set; }\n\n    public event Action<float>? LevelChanged;\n\n    protected override void GenerateAudio(Span<float> buffer);\n    public void TriggerOff();\n    public void TriggerOn();\n}\n```\n\n**Properties:**\n\n*   `AttackTime`: The attack time of the envelope (in seconds).\n*   `DecayTime`: The decay time of the envelope (in seconds).\n*   `Name`: The name of the envelope generator.\n*   `ReleaseTime`: The release time of the envelope (in seconds).\n*   `Retrigger`: Whether to retrigger the envelope on each new trigger.\n*   `SustainLevel`: The sustain level of the envelope.\n*   `Trigger`: The trigger mode (`NoteOn`, `Gate`, `Trigger`).\n\n**Events:**\n\n*   `LevelChanged`: Occurs when the envelope level changes.\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Generates the envelope signal.\n*   `TriggerOff()`: Triggers the release stage of the envelope (if in `Gate` mode).\n*   `TriggerOn()`: Triggers the attack stage of the envelope.\n\n### Components `Filter`\n\n```csharp\npublic class Filter : SoundComponent\n{\n    public Filter();\n\n    public float CutoffFrequency { get; set; }\n    public override string Name { get; set; }\n    public float Resonance { get; set; }\n    public FilterType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency of the filter.\n*   `Name`: The name of the filter.\n*   `Resonance`: The resonance of the filter.\n*   `Type`: The filter type (`LowPass`, `HighPass`, `BandPass`, `Notch`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Applies the filter to the audio buffer.\n\n### Components `LowFrequencyOscillator`\n\n```csharp\npublic class LowFrequencyOscillator : SoundComponent\n{\n    public LowFrequencyOscillator();\n\n    public float Depth { get; set; }\n    public TriggerMode Mode { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float Rate { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer);\n    public float GetLastOutput();\n    public void Trigger();\n}\n```\n\n**Properties:**\n\n*   `Depth`: The depth of the LFO's modulation.\n*   `Mode`: The trigger mode (`FreeRunning`, `NoteTrigger`).\n*   `Name`: The name of the LFO.\n*   `Phase`: The initial phase of the LFO.\n*   `Rate`: The rate (frequency) of the LFO.\n*   `Type`: The waveform type (`Sine`, `Square`, `Triangle`, `Sawtooth`, `ReverseSawtooth`, `Random`, `SampleAndHold`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Generates the LFO signal.\n*   `GetLastOutput()`: Returns the last generated output sample.\n*   `Trigger()`: Triggers the LFO (if in `NoteTrigger` mode).\n\n### Components `Mixer`\n\n```csharp\npublic sealed class Mixer : SoundComponent\n{\n    public Mixer();\n\n    public static Mixer Master { get; }\n    public override string Name { get; set; }\n\n    public void AddComponent(SoundComponent component);\n    protected override void GenerateAudio(Span<float> buffer);\n    public void RemoveComponent(SoundComponent component);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Master`: The static instance of the master mixer.\n*   `Name`: The name of the mixer.\n\n**Methods:**\n\n*   `AddComponent(SoundComponent component)`: Adds a component to the mixer.\n*   `GenerateAudio(Span<float> buffer)`: Mixes the audio from all connected components.\n*   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n\n### Components `Oscillator`\n\n```csharp\npublic class Oscillator : SoundComponent\n{\n    public Oscillator();\n\n    public float Amplitude { get; set; }\n    public float Frequency { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float PulseWidth { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Amplitude`: The amplitude of the oscillator.\n*   `Frequency`: The frequency of the oscillator.\n*   `Name`: The name of the oscillator.\n*   `Phase`: The initial phase of the oscillator.\n*   `PulseWidth`: The pulse width (for pulse waveforms).\n*   `Type`: The waveform type (`Sine`, `Square`, `Sawtooth`, `Triangle`, `Noise`, `Pulse`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Generates the oscillator's output.\n\n### Components `Recorder`\n\n```csharp\npublic class Recorder : IDisposable\n{\n    public Recorder(Stream stream, SampleFormat sampleFormat = SampleFormat.F32, EncodingFormat encodingFormat = EncodingFormat.Wav, int sampleRate = 44100, int channels = 2, VoiceActivityDetector? vad = null);\n    public Recorder(AudioProcessCallback callback, SampleFormat sampleFormat = SampleFormat.F32, EncodingFormat encodingFormat = EncodingFormat.Wav, int sampleRate = 44100, int channels = 2, VoiceActivityDetector? vad = null);\n\n    public ReadOnlyCollection<AudioAnalyzer> Analyzers { get; }\n    public int Channels { get; }\n    public EncodingFormat EncodingFormat { get; }\n    public Stream Stream { get; }\n    public ReadOnlyCollection<SoundModifier> Modifiers { get; }\n    public AudioProcessCallback? ProcessCallback { get; set; }\n    public int SampleRate { get; }\n    public PlaybackState State { get; }\n    public SampleFormat SampleFormat {get;}\n\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void AddModifier(SoundModifier modifier);\n    public void Dispose();\n    public void PauseRecording();\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveModifier(SoundModifier modifier);\n    public void ResumeRecording();\n    public void StartRecording();\n    public void StopRecording();\n}\n```\n\n**Properties:**\n\n*   `Analyzers`: Gets a read-only collection of <see cref=\"AudioAnalyzer\"/> components applied to the recorder. Analyzers are used to process and extract data from the audio stream during recording.\n*   `Channels`: The number of channels to record.\n*   `EncodingFormat`: The encoding format for the recorded audio.\n*   `Stream`: The stream to write encoded recorded audio to.\n*   `Modifiers`: Gets a read-only collection of <see cref=\"SoundModifier\"/> components applied to the recorder. Modifiers are applied to the audio data before encoding or processing via callback, allowing for real-time audio effects during recording.\n*   `ProcessCallback`: A callback for processing recorded audio in real time.\n*   `SampleRate`: The sample rate for recording.\n*   `State`: The current recording state (`Stopped`, `Playing`, `Paused`).\n*   `SampleFormat`: The sample format for recording.\n\n**Methods:**\n\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an <see cref=\"AudioAnalyzer\"/> to the recording pipeline. Analyzers process audio data during recording, enabling real-time analysis.\n*   `AddModifier(SoundModifier modifier)`: Adds a <see cref=\"SoundModifier\"/> to the recording pipeline. Modifiers apply effects to the audio data in real-time as it's being recorded.\n*   `Dispose()`: Releases resources used by the recorder.\n*   `PauseRecording()`: Pauses the recording.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes a specific <see cref=\"AudioAnalyzer\"/> from the recording pipeline.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a specific <see cref=\"SoundModifier\"/> from the recording pipeline.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `StartRecording()`: Starts the recording.\n*   `StopRecording()`: Stops the recording.\n### Components `SoundPlayer`\n\n```csharp\npublic sealed class SoundPlayer : SoundPlayerBase\n{\n    public SoundPlayer(ISoundDataProvider dataProvider);\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n}\n```\nInherits all playback functionality, properties, and events from `SoundPlayerBase`.\n\n**Properties:**\n* `Name`: The name of the sound player component (default: \"Sound Player\").\n\n### Components `SurroundPlayer`\n\n```csharp\npublic sealed class SurroundPlayer : SoundPlayerBase\n{\n    public SurroundPlayer(ISoundDataProvider dataProvider);\n\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n    public Vector2 ListenerPosition { get; set; }\n    public PanningMethod Panning { get; set; }\n    public VbapParameters VbapParameters { get; set; }\n    public SurroundConfiguration SurroundConfig { get; set; }\n    public SpeakerConfiguration SpeakerConfig { get; set; }\n\n    protected override void GenerateAudio(Span<float> output);\n    public void SetSpeakerConfiguration(SpeakerConfiguration config);\n    // Inherits Play, Pause, Stop, Seek, IsLooping, etc. from SoundPlayerBase\n}\n```\nInherits base playback functionality from `SoundPlayerBase` and adds surround-specific features.\n\n**Properties:**\n*   `Name`: The name of the surround player component (default: \"Surround Player\").\n*   `ListenerPosition`: The position of the listener in the surround sound field (Vector2).\n*   `Panning`: Gets or sets the panning method to use for surround sound (`Linear`, `EqualPower`, `VBAP`).\n*   `SpeakerConfig`: Gets or sets the speaker configuration (`Stereo`, `Quad`, `Surround51`, `Surround71`, `Custom`).\n*   `VbapParameters`: Gets or sets parameters for Vector Base Amplitude Panning (VBAP).\n*   `SurroundConfig`: Gets or sets the custom surround configuration when `SpeakerConfig` is set to `Custom`.\n\n**Methods:**\n*   `GenerateAudio(Span<float> output)`: (Overrides `SoundPlayerBase`) Reads audio data, applies resampling, then applies surround processing and looping if enabled.\n*   `SetSpeakerConfiguration(SpeakerConfiguration config)`: Sets the speaker configuration for surround sound playback.\n*   `Seek(int sampleOffset)`: (Overrides `SoundPlayerBase`) Seeks and re-initializes delay lines for surround processing.\n\n\n### Components `VoiceActivityDetector`\n\n```csharp\npublic class VoiceActivityDetector : AudioAnalyzer\n{\n    public VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null);\n\n    public bool IsVoiceActive { get; }\n    public int SpeechHighBand { get; set; }\n    public int SpeechLowBand { get; set; }\n    public double Threshold { get; set; }\n\n    public override string Name { get; set; }\n\n    public event Action<bool>? SpeechDetected;\n\n    protected override void Analyze(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `IsVoiceActive`: Indicates whether voice activity is currently detected. This is a read-only property that reflects the detector's current state.\n*   `Name`: Gets or sets the name of the voice activity detector component, useful for identification and debugging.\n*   `SpeechHighBand`: Gets or sets the upper bound of the frequency range (in Hz) that the detector uses to analyze for speech. Frequencies above this band are ignored in the voice activity detection process. Default is 3400 Hz.\n*   `SpeechLowBand`: Gets or sets the lower bound of the frequency range (in Hz) that the detector focuses on for speech detection. Frequencies below this band are not considered for voice activity. Default is 300 Hz.\n*   `Threshold`: Gets or sets the detection sensitivity threshold. This value determines how sensitive the detector is to voice activity. A lower threshold value increases sensitivity, making the detector more likely to identify quieter sounds as voice activity. Default is 0.01.\n\n**Events:**\n\n*   `SpeechDetected`: An event that is raised whenever the voice activity state changes (i.e., when speech is detected or ceases to be detected). Listeners can subscribe to this event to respond in real-time to changes in voice activity.\n\n**Methods:**\n\n*   `VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null)`: Constructor for the VoiceActivityDetector class. Initializes a new instance of the voice activity detector with configurable FFT size, detection threshold and optional visualizer for audio analysis visualization.\n    *   `fftSize`: `int` – The size of the FFT (Fast Fourier Transform) window used for spectral analysis. Must be a power of two. Larger FFT sizes provide finer frequency resolution but may increase processing latency. Default is 1024.\n    *   `threshold`: `float` – The sensitivity threshold for voice detection. A lower value increases sensitivity. Default is 0.01.\n    *   `visualizer`: `IVisualizer?` – An optional visualizer instance that can be attached to the analyzer for visualizing audio processing data, useful for debugging and tuning. Default is `null`.\n*   `Analyze(Span<float> buffer)`: перевіряє audio buffer та оновлює `IsVoiceActive` property на основі алгоритму детекції.\n    *   `buffer`: `Span<float>` – The audio buffer to analyze for voice activity. The audio data in this buffer is processed to determine if voice is present.\n*   `GenerateAudio(Span<float> buffer)`: This method is inherited from `AudioAnalyzer` but not directly used in `VoiceActivityDetector`. Voice Activity Detector works by analyzing the input audio buffer provided to the `Analyze` method and does not generate audio; use `Process` method instead.\n\n**Remarks:**\n\n*   **Frequency Range:** The `SpeechLowBand` and `SpeechHighBand` properties allow you to customize the frequency range that the VAD focuses on for speech detection. Speech typically falls within the 300Hz to 3400Hz range, but you may need to adjust these values depending on the characteristics of your audio and the type of speech you are detecting.\n*   **Threshold Sensitivity:** The `Threshold` property is crucial for controlling the sensitivity of the voice activity detection. Adjusting this threshold may be necessary to achieve optimal performance in different environments and with varying audio input levels.\n*   **FFT Size:** The `fftSize` parameter in the constructor determines the FFT window size. A larger FFT size provides better frequency resolution, which can be beneficial in noisy environments or when detecting subtle voice activity. However, it also increases the computational cost and latency. Ensure that the FFT size is always a power of 2 for optimal performance and compatibility with FFT algorithms.\n*   **Performance Tuning:** For optimal performance, especially in real-time applications, carefully tune the `fftSize` and `Threshold` parameters. Larger FFT sizes are more computationally intensive but offer better frequency resolution. Adjust the `Threshold` based on the ambient noise level and the desired sensitivity of voice detection.\n*   **Environment Considerations:** The ideal settings for `fftSize`, `Threshold`, `SpeechLowBand`, and `SpeechHighBand` may vary depending on the environment in which the voice activity detector is used. In noisy environments, you might need to increase the `fftSize` and adjust the `Threshold` to minimize false positives.\n*   **Visualizer for Debugging:** The optional `visualizer` parameter in the constructor is highly useful for debugging and tuning the voice activity detector. By attaching a visualizer, you can visually inspect the audio data and the detector's response, which can help in understanding and adjusting the detector's parameters for optimal performance in your specific use case.\n\n### Enums `Capability`\n\n```csharp\n[Flags]\npublic enum Capability\n{\n    Playback = 1,\n    Record = 2,\n    Mixed = Playback | Record,\n    Loopback = 4\n}\n```\n\n**Values:**\n\n*   `Playback`: Indicates playback capability.\n*   `Record`: Indicates recording capability.\n*   `Mixed`: Indicates both playback and recording capability.\n*   `Loopback`: Indicates loopback capability (recording system audio output).\n\n### Enums `DeviceType`\n\n```csharp\npublic enum DeviceType\n{\n    Playback,\n    Capture\n}\n```\n**Values:**\n*   `Playback`: Device used for audio playback.\n*   `Capture`: Device used for audio capture.\n\n### Enums `EncodingFormat`\n\n```csharp\npublic enum EncodingFormat\n{\n    Unknown = 0,\n    Wav,\n    Flac,\n    Mp3,\n    Vorbis\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown encoding format.\n*   `Wav`: Waveform Audio File Format (WAV).\n*   `Flac`: Free Lossless Audio Codec (FLAC).\n*   `Mp3`: MPEG-1 Audio Layer III (MP3).\n*   `Vorbis`: Ogg Vorbis.\n\n### Enums `PlaybackState`\n\n```csharp\npublic enum PlaybackState\n{\n    Stopped,\n    Playing,\n    Paused\n}\n```\n\n**Values:**\n\n*   `Stopped`: Playback is stopped.\n*   `Playing`: Playback is currently in progress.\n*   `Paused`: Playback is paused.\n\n### Enums `Result`\n\n```csharp\npublic enum Result\n{\n    Success = 0,\n    Error = -1,\n    // ... (other error codes)\n    CrcMismatch = -100,\n    FormatNotSupported = -200,\n    // ... (other backend-specific error codes)\n    DeviceNotInitialized = -300,\n    // ... (other device-related error codes)\n    FailedToInitBackend = -400\n    // ... (other backend initialization error codes)\n}\n```\n\n**Values:**\n\n*   `Success`: The operation was successful.\n*   `Error`: A generic error occurred.\n*   `CrcMismatch`: CRC checksum mismatch.\n*   `FormatNotSupported`: The requested audio format is not supported.\n*   `DeviceNotInitialized`: The audio device is not initialized.\n*   `FailedToInitBackend`: Failed to initialize the audio backend.\n*   **(Many other error codes representing various error conditions)**\n\n### Enums `SampleFormat`\n\n```csharp\npublic enum SampleFormat\n{\n    Default = 0,\n    U8 = 1,\n    S16 = 2,\n    S24 = 3,\n    S32 = 4,\n    F32 = 5\n}\n```\n\n**Values:**\n\n*   `Default`: The default sample format (typically `F32`).\n*   `U8`: Unsigned 8-bit integer.\n*   `S16`: Signed 16-bit integer.\n*   `S24`: Signed 24-bit integer packed in 3 bytes.\n*   `S32`: Signed 32-bit integer.\n*   `F32`: 32-bit floating-point.\n\n### Enums `FilterType`\n\n```csharp\npublic enum FilterType\n{\n    Peaking,\n    LowShelf,\n    HighShelf,\n    BandPass,\n    Notch,\n    LowPass,\n    HighPass\n}\n```\n\n**Values:**\n\n*   `Peaking`: Peaking filter.\n*   `LowShelf`: Low-shelf filter.\n*   `HighShelf`: High-shelf filter.\n*   `BandPass`: Band-pass filter.\n*   `Notch`: Notch filter.\n*   `LowPass`: Low-pass filter.\n*   `HighPass`: High-pass filter.\n\n### Enums `EnvelopeGenerator.EnvelopeState`\n\n```csharp\npublic enum EnvelopeState\n{\n    Idle,\n    Attack,\n    Decay,\n    Sustain,\n    Release\n}\n```\n\n**Values:**\n\n*   `Idle`: The envelope is inactive.\n*   `Attack`: The attack stage of the envelope.\n*   `Decay`: The decay stage of the envelope.\n*   `Sustain`: The sustain stage of the envelope.\n*   `Release`: The release stage of the envelope.\n\n### Enums `EnvelopeGenerator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    NoteOn,\n    Gate,\n    Trigger\n}\n```\n\n**Values:**\n\n* `NoteOn`: The envelope will go directly from attack to sustain, without a decay stage.\n* `Gate`: The envelope will progress normally, and will only enter release stage when trigger is off.\n* `Trigger`: The envelope will always progress to the end, including the release stage.\n\n### Enums `LowFrequencyOscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Triangle,\n    Sawtooth,\n    ReverseSawtooth,\n    Random,\n    SampleAndHold\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Triangle`: Triangle wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `ReverseSawtooth`: Reverse sawtooth wave.\n*   `Random`: Random values.\n*   `SampleAndHold`: Sample and hold random values.\n\n### Enums `LowFrequencyOscillator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    FreeRunning,\n    NoteTrigger\n}\n```\n\n**Values:**\n\n* `FreeRunning`: The LFO will run continuously without needing a trigger.\n* `NoteTrigger`: The LFO will only start when triggered.\n\n### Enums `Oscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Sawtooth,\n    Triangle,\n    Noise,\n    Pulse\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `Triangle`: Triangle wave.\n*   `Noise`: White noise.\n*   `Pulse`: Pulse wave.\n\n### Enums `SurroundPlayer.SpeakerConfiguration`\n\n```csharp\npublic enum SpeakerConfiguration\n{\n    Stereo,\n    Quad,\n    Surround51,\n    Surround71,\n    Custom\n}\n```\n\n**Values:**\n\n*   `Stereo`: Standard stereo configuration (2 speakers).\n*   `Quad`: Quadraphonic configuration (4 speakers).\n*   `Surround51`: 5.1 surround sound configuration (6 speakers).\n*   `Surround71`: 7.1 surround sound configuration (8 speakers).\n* *   `Custom`: A custom speaker configuration defined by the user.\n\n### Enums `SurroundPlayer.PanningMethod`\n\n```csharp\npublic enum PanningMethod\n{\n    Linear,\n    EqualPower,\n    Vbap\n}\n```\n\n**Values:**\n\n*   `Linear`: Linear panning.\n*   `EqualPower`: Equal power panning.\n*   `Vbap`: Vector Base Amplitude Panning (VBAP).\n\n### Editing `Composition`\nSee [Editing and Persistence Guide](./editing-engine.mdx#composition) for details.\n```csharp\npublic class Composition : ISoundDataProvider, IDisposable\n{\n    public Composition(string name = \"Composition\", int? targetChannels = null);\n\n    public string Name { get; set; }\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public List<Track> Tracks { get; }\n    public float MasterVolume { get; set; }\n    public bool IsDirty { get; }\n    public int SampleRate { get; set; } // Target sample rate for rendering\n    public int TargetChannels { get; set; }\n\n    // ISoundDataProvider implementation\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public bool IsDisposed { get; private set; }\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    \n    public void AddTrack(Track track);\n    public bool RemoveTrack(Track track);\n    public TimeSpan CalculateTotalDuration();\n    public float[] Render(TimeSpan startTime, TimeSpan duration);\n    public int Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer);\n    public void MarkDirty();\n    internal void ClearDirtyFlag();\n    public void Dispose();\n    // ... other methods like ReplaceSegment, RemoveSegment, SilenceSegment, InsertSegment ...\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `Track`\nSee [Editing and Persistence Guide](./editing-engine.mdx#track) for details.\n```csharp\npublic class Track\n{\n    public Track(string name = \"Track\", TrackSettings? settings = null);\n\n    public string Name { get; set; }\n    public List<AudioSegment> Segments { get; }\n    public TrackSettings Settings { get; set; }\n    internal Composition? ParentComposition { get; set; }\n\n    public void MarkDirty();\n    public void AddSegment(AudioSegment segment);\n    public bool RemoveSegment(AudioSegment segment, bool shiftSubsequent = false);\n    public void InsertSegmentAt(AudioSegment segmentToInsert, TimeSpan insertionTime, bool shiftSubsequent = true);\n    public TimeSpan CalculateDuration();\n    public int Render(TimeSpan overallStartTime, TimeSpan durationToRender, Span<float> outputBuffer, int targetSampleRate, int targetChannels);\n}\n```\n\n### Editing `AudioSegment`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegment) for details.\n```csharp\npublic class AudioSegment : IDisposable\n{\n    public AudioSegment(\n        ISoundDataProvider sourceDataProvider,\n        TimeSpan sourceStartTime,\n        TimeSpan sourceDuration,\n        TimeSpan timelineStartTime,\n        string name = \"Segment\",\n        AudioSegmentSettings? settings = null,\n        bool ownsDataProvider = false);\n\n    public string Name { get; set; }\n    public ISoundDataProvider SourceDataProvider { get; private set; }\n    public TimeSpan SourceStartTime { get; set; }\n    public TimeSpan SourceDuration { get; set; }\n    public TimeSpan TimelineStartTime { get; set; }\n    public AudioSegmentSettings Settings { get; set; }\n    internal Track? ParentTrack { get; set; }\n\n    public TimeSpan StretchedSourceDuration { get; }\n    public TimeSpan EffectiveDurationOnTimeline { get; }\n    public TimeSpan TimelineEndTime { get; }\n    public TimeSpan GetTotalLoopedDurationOnTimeline();\n    public AudioSegment Clone(TimeSpan? newTimelineStartTime = null);\n    internal void ReplaceSource(ISoundDataProvider newSource, TimeSpan newSourceStartTime, TimeSpan newSourceDuration);\n    public int ReadProcessedSamples(TimeSpan segmentTimelineOffset, TimeSpan durationToRead, Span<float> outputBuffer, int outputBufferOffset, int targetSampleRate, int targetChannels);\n    internal void FullResetState();\n    public void Dispose();\n    public void MarkDirty();\n}\n```\n\n### Editing `AudioSegmentSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegmentsettings) for details.\n```csharp\npublic class AudioSegmentSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public TimeSpan FadeInDuration { get; set; }\n    public FadeCurveType FadeInCurve { get; set; }\n    public TimeSpan FadeOutDuration { get; set; }\n    public FadeCurveType FadeOutCurve { get; set; }\n    public bool IsReversed { get; set; }\n    public LoopSettings Loop { get; set; }\n    public float SpeedFactor { get; set; }\n    public float TimeStretchFactor { get; set; } // Overridden by TargetStretchDuration if set\n    public TimeSpan? TargetStretchDuration { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public AudioSegmentSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `TrackSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#tracksettings) for details.\n```csharp\npublic class TrackSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public bool IsMuted { get; set; }\n    public bool IsSoloed { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public TrackSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `LoopSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#loopsettings) for details.\n```csharp\npublic record struct LoopSettings\n{\n    public int Repetitions { get; }\n    public TimeSpan? TargetDuration { get; }\n    public LoopSettings(int repetitions = 0, TimeSpan? targetDuration = null);\n    public static LoopSettings PlayOnce { get; }\n}\n```\n\n### Editing `FadeCurveType`\nSee [Editing and Persistence Guide](./editing-engine.mdx#fadecurvetype) for details.\n```csharp\npublic enum FadeCurveType\n{\n    Linear,\n    Logarithmic,\n    SCurve\n}\n```\n\n### Editing.Persistence\nThese are Data Transfer Objects (DTOs) for serialization. See [Editing and Persistence Guide](./editing-engine.mdx#project-persistence) for their purpose.\n*   `CompositionProjectManager` (static class): `SaveProjectAsync`, `LoadProjectAsync`, `RelinkMissingMediaAsync`.\n*   `ProjectData`\n*   `ProjectTrack`\n*   `ProjectSegment`\n*   `ProjectAudioSegmentSettings`\n*   `ProjectTrackSettings`\n*   `ProjectSourceReference`\n*   `ProjectEffectData`\n\n\n### Extensions.WebRtc.Apm\n\n#### `AudioProcessingModule` (Class)\n```csharp\npublic class AudioProcessingModule : IDisposable\n{\n    public AudioProcessingModule();\n    public ApmError ApplyConfig(ApmConfig config);\n    public ApmError Initialize();\n    public ApmError ProcessStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    public ApmError ProcessReverseStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    // ... other methods for setting delays, levels, runtime settings, getting info, AEC dump ...\n    public static int GetFrameSize(int sampleRateHz);\n    public void Dispose();\n}\n```\n**Description:** Provides low-level access to the WebRTC Audio Processing Module. It's responsible for initializing the APM, applying configurations, and processing audio frames. Generally used internally by `WebRtcApmModifier` and `NoiseSuppressor`.\n\n#### `ApmConfig` (Class)\n```csharp\npublic class ApmConfig : IDisposable\n{\n    public ApmConfig();\n    public void SetEchoCanceller(bool enabled, bool mobileMode);\n    public void SetNoiseSuppression(bool enabled, NoiseSuppressionLevel level);\n    public void SetGainController1(bool enabled, GainControlMode mode, int targetLevelDbfs, int compressionGainDb, bool enableLimiter);\n    public void SetGainController2(bool enabled);\n    public void SetHighPassFilter(bool enabled);\n    public void SetPreAmplifier(bool enabled, float fixedGainFactor);\n    public void SetPipeline(int maxInternalRate, bool multiChannelRender, bool multiChannelCapture, DownmixMethod downmixMethod);\n    public void Dispose();\n}\n```\n**Description:** Used to configure the features of the `AudioProcessingModule` such as echo cancellation, noise suppression, gain control, etc.\n\n#### `StreamConfig` (Class)\n```csharp\npublic class StreamConfig : IDisposable\n{\n    public StreamConfig(int sampleRateHz, int numChannels);\n    public int SampleRateHz { get; }\n    public int NumChannels { get; }\n    public void Dispose();\n}\n```\n**Description:** Defines the properties (sample rate, number of channels) of an audio stream being processed by the APM.\n\n#### `ProcessingConfig` (Class)\nThis class holds multiple `StreamConfig` instances for different parts of the APM pipeline (input, output, reverse input, reverse output).\n\n#### `NoiseSuppressor` (Component - `SoundFlow.Extensions.WebRtc.Apm.Components`)\n```csharp\npublic class NoiseSuppressor : IDisposable\n{\n    public NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High, bool useMultichannelProcessing = false);\n    public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;\n    public float[] ProcessAll();\n    public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null);\n    public void Dispose();\n}\n```\n**Description:** A component for offline/batch noise suppression using WebRTC APM. It takes an `ISoundDataProvider`, processes its audio, and outputs the cleaned audio either as a whole or in chunks.\n**Key Members:**\n*   `NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, ...)`: Constructor.\n*   `OnAudioChunkProcessed` (event): Raised when a chunk of audio is processed.\n*   `ProcessAll()`: Processes the entire audio stream and returns it.\n*   `ProcessChunks()`: Processes audio in chunks, raising `OnAudioChunkProcessed`.\n\n#### `WebRtcApmModifier` (Modifier - `SoundFlow.Extensions.WebRtc.Apm.Modifiers`)\n```csharp\npublic sealed class WebRtcApmModifier : SoundModifier, IDisposable\n{\n    public WebRtcApmModifier(\n        bool aecEnabled = false, bool aecMobileMode = false, int aecLatencyMs = 40,\n        bool nsEnabled = false, NoiseSuppressionLevel nsLevel = NoiseSuppressionLevel.High,\n        // ... other AGC, HPF, PreAmp, Pipeline settings ...\n    );\n\n    public override string Name { get; set; }\n    public EchoCancellationSettings EchoCancellation { get; }\n    public NoiseSuppressionSettings NoiseSuppression { get; }\n    public AutomaticGainControlSettings AutomaticGainControl { get; }\n    public ProcessingPipelineSettings ProcessingPipeline { get; }\n    public bool HighPassFilterEnabled { get; set; }\n    public bool PreAmplifierEnabled { get; set; }\n    public float PreAmplifierGainFactor { get; set; }\n    public float PostProcessGain { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n    public void Dispose();\n}\n```\n**Description:** A `SoundModifier` that applies various WebRTC APM features (AEC, NS, AGC, HPF, PreAmp) to an audio stream in real-time.\n**Key Members:**\n*   Constructor with detailed initial settings.\n*   Properties for configuring each APM feature (`EchoCancellation`, `NoiseSuppression`, `AutomaticGainControl`, `ProcessingPipeline`, `HighPassFilterEnabled`, etc.).\n*   `Process(Span<float> buffer)`: Core processing logic.\n*   `Dispose()`: Releases native APM resources.\n\n#### Enums for WebRTC APM\n*   `ApmError`: Error codes.\n*   `NoiseSuppressionLevel`: Low, Moderate, High, VeryHigh.\n*   `GainControlMode`: AdaptiveAnalog, AdaptiveDigital, FixedDigital.\n*   `DownmixMethod`: AverageChannels, UseFirstChannel.\n*   `RuntimeSettingType`: Types for runtime APM settings.\n\n### Interfaces `ISoundDataProvider`\n\n```csharp\npublic interface ISoundDataProvider : IDisposable\n{\n    int Position { get; }\n    int Length { get; }\n    bool CanSeek { get; }\n    SampleFormat SampleFormat { get; }\n    int SampleRate { get; set; }\n    bool IsDisposed { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n    event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    int ReadBytes(Span<float> buffer);\n    void Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples).\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio data.\n*   `Dispose()`: Releases resources held by the data provider.\n\n### Interfaces `ISoundDecoder`\n\n```csharp\npublic interface ISoundDecoder : IDisposable\n{\n    bool IsDisposed { get; }\n    int Length { get; }\n    SampleFormat SampleFormat { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n\n    int Decode(Span<float> samples);\n    bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the decoder has been disposed.\n*   `Length`: The total length of the decoded audio data (in samples).\n*   `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n*   `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio stream.\n*   `Dispose()`: Releases the resources used by the decoder.\n\n### Interfaces `ISoundEncoder`\n\n```csharp\npublic interface ISoundEncoder : IDisposable\n{\n    bool IsDisposed { get; }\n\n    int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples.\n*   `Dispose()`: Releases the resources used by the encoder.\n\n\n### Interfaces `ISoundPlayer`\n\n```csharp\npublic interface ISoundPlayer\n{\n    PlaybackState State { get; }\n    bool IsLooping { get; set; }\n    float PlaybackSpeed { get; set; }\n    float Volume { get; set; }\n    float Time { get; }\n    float Duration { get; }\n    float LoopStartSeconds { get; }\n    float LoopEndSeconds { get; }\n    int LoopStartSamples { get; }\n    int LoopEndSamples { get; }\n\n    void Play();\n    void Pause();\n    void Stop();\n    bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    bool Seek(float time);\n    bool Seek(int sampleOffset);\n    void SetLoopPoints(float startTime, float? endTime = -1f);\n    void SetLoopPoints(int startSample, int endSample = -1);\n    void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `State`: The current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Whether looping is enabled or disabled (`get`, `set`).\n*   `PlaybackSpeed`: Gets or sets the playback speed. 1.0 is normal speed.\n*   `Volume`: Gets or sets the volume of the sound player (0.0 to 1.0 or higher for gain).\n*   `Time`: The current playback position (in seconds).\n*   `Duration`: The total duration of the audio (in seconds).\n*   `LoopStartSeconds`: Gets the configured loop start point in seconds.\n*   `LoopEndSeconds`: Gets the configured loop end point in seconds.  -1 indicates looping to the natural end.\n*   `LoopStartSamples`: Gets the configured loop start point in samples.\n*   `LoopEndSamples`: Gets the configured loop end point in samples. -1 indicates looping to the natural end.\n\n**Methods:**\n\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to the specified time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to the specified time (in seconds). Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset (in samples). Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures custom loop points using start and end times in seconds. `endTime` is optional;  use -1 or `null` to loop to the natural end.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures custom loop points using start and end sample indices. `endSample` is optional; use -1 to loop to the natural end.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures custom loop points using `TimeSpan`.\n\n### Interfaces `IVisualizationContext`\n\n```csharp\npublic interface IVisualizationContext\n{\n    void Clear();\n    void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f);\n    void DrawRectangle(float x, float y, float width, float height, Color color);\n}\n```\n\n**Methods:**\n\n*   `Clear()`: Clears the drawing surface.\n*   `DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)`: Draws a line from (`x1`, `y1`) to (`x2`, `y2`) with the specified color and thickness.\n*   `DrawRectangle(float x, float y, float width, float height, Color color)`: Draws a rectangle with the specified position, dimensions, and color.\n\n### Interfaces `IVisualizer`\n\n```csharp\npublic interface IVisualizer : IDisposable\n{\n    string Name { get; }\n\n    event EventHandler VisualizationUpdated;\n\n    void ProcessOnAudioData(Span<float> audioData);\n    void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes a chunk of audio data for visualization.\n*   `Render(IVisualizationContext context)`: Renders the visualization using the provided `IVisualizationContext`.\n*   `Dispose()`: Releases the resources used by the visualizer.\n\n### Modifiers `AlgorithmicReverbModifier`\n\n```csharp\npublic class AlgorithmicReverbModifier : SoundModifier\n{\n    public AlgorithmicReverbModifier();\n\n    public float Damp { get; set; }\n    public override string Name { get; set; }\n    public float PreDelay { get; set; }\n    public float RoomSize { get; set; }\n    public float Wet { get; set; }\n    public float Width { get; set; }\n    public float Mix { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Damp`: The damping factor of the reverb.\n*   `Name`: The name of the modifier.\n*   `PreDelay`: The pre-delay time (in milliseconds).\n*   `RoomSize`: The simulated room size.\n*   `Wet`: The wet/dry mix of the reverb (0 = dry, 1 = wet).\n*   `Width`: The stereo width of the reverb.\n*   `Mix`: The mix level of the reverb.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the reverb effect.\n\n### Modifiers `AmbientReverbModifier`\n\n```csharp\npublic class AmbientReverbModifier : SoundModifier\n{\n    public AmbientReverbModifier(int baseDelayLength, float baseDecayFactor, float roomSize, float stereoWidth, float diffusion);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `AmbientReverbModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `baseDelayLength`: The base delay length (in samples) for the reverb.\n*   `baseDecayFactor`: The base decay factor for the reverb.\n*   `roomSize`: The simulated room size.\n*   `stereoWidth`: The stereo width of the reverb.\n*   `diffusion`: The diffusion factor of the reverb.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the ambient reverb effect.\n\n### Modifiers `BassBoostModifier`\n\n```csharp\npublic class BassBoostModifier : SoundModifier\n{\n    public BassBoostModifier(float cutoffFrequency);\n\n    public float CutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency below which the bass boost is applied.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the bass boost effect.\n\n### Modifiers `ChorusModifier`\n\n```csharp\npublic class ChorusModifier : SoundModifier\n{\n    public ChorusModifier(float depth, float rate, float feedback, float wetDryMix, int maxDelayLength);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `ChorusModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `depth`: The depth of the chorus effect.\n*   `rate`: The rate of the chorus effect.\n*   `feedback`: The feedback amount of the chorus effect.\n*   `wetDryMix`: The wet/dry mix of the chorus effect.\n*   `maxDelayLength`: The maximum delay length (in samples) used by the chorus effect.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the chorus effect.\n\n### Modifiers `CompressorModifier`\n\n```csharp\npublic class CompressorModifier : SoundModifier\n{\n    public CompressorModifier(float threshold, float ratio, float attack, float release, float knee, float makeupGain);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `CompressorModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `threshold`: The threshold (in dB) above which compression is applied.\n*   `ratio`: The compression ratio.\n*   `attack`: The attack time (in milliseconds).\n*   `release`: The release time (in milliseconds).\n*   `knee`: The knee width (in dB).\n*   `makeupGain`: The amount of makeup gain to apply after compression.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the compression effect.\n\n### Modifiers `DelayModifier`\n\n```csharp\npublic class DelayModifier : SoundModifier\n{\n    public DelayModifier(int delayLength, float feedback, float wetMix, float cutoffFrequency);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `DelayModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `delayLength`: The delay length (in samples).\n*   `feedback`: The feedback amount of the delay.\n*   `wetMix`: The wet/dry mix of the delay (0 = dry, 1 = wet).\n*   `cutoffFrequency`: The cutoff frequency for the low-pass filter applied to the delayed signal.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the delay effect.\n\n### Modifiers `FrequencyBandModifier`\n\n```csharp\npublic class FrequencyBandModifier : SoundModifier\n{\n    public FrequencyBandModifier(float lowCutoffFrequency, float highCutoffFrequency);\n\n    public float HighCutoffFrequency { get; set; }\n    public float LowCutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `HighCutoffFrequency`: The high cutoff frequency of the frequency band.\n*   `LowCutoffFrequency`: The low cutoff frequency of the frequency band.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the frequency band modification.\n\n### Modifiers `NoiseReductionModifier`\n\n```csharp\npublic class NoiseReductionModifier : SoundModifier\n{\n    public NoiseReductionModifier(int fftSize = 2048, float alpha = 3f, float beta = 0.001f, float smoothingFactor = 0.9f, float gain = 1.5f, int noiseFrames = 5, VoiceActivityDetector? vad = null);\n\n    public override string Name { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying noise reduction.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `NoiseReductionModifier` operates on buffers, not individual samples.\n\n### Modifiers `ParametricEqualizer`\n\n```csharp\npublic class ParametricEqualizer : SoundModifier\n{\n    public ParametricEqualizer(int channels);\n\n    public override string Name { get; set; }\n    \n    public void AddBand(EqualizerBand band);\n    public void AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth);\n    public void AddBands(IEnumerable<EqualizerBand> bands);\n    public void ClearBands();\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n    public void RemoveBand(EqualizerBand band);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `AddBand(EqualizerBand band)`: Adds an `EqualizerBand` to the equalizer.\n*   `AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth)`: Adds an equalizer band with the specified parameters.\n*   `AddBands(IEnumerable<EqualizerBand> bands)`: Adds multiple equalizer bands.\n*   `ClearBands()`: Removes all equalizer bands.\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying equalization.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `ParametricEqualizer` operates on buffers, not individual samples.\n*   `RemoveBand(EqualizerBand band)`: Removes a specific equalizer band.\n\n### Modifiers `StereoChorusModifier`\n\n```csharp\npublic class StereoChorusModifier : SoundModifier\n{\n    public StereoChorusModifier(float depthLeft, float depthRight, float rateLeft, float rateRight, float feedbackLeft, float feedbackRight, float wetDryMix, int maxDelayLength);\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `StereoChorusModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `depthLeft`: The depth of the chorus effect for the left channel.\n*   `depthRight`: The depth of the chorus effect for the right channel.\n*   `rateLeft`: The rate of the chorus effect for the left channel.\n*   `rateRight`: The rate of the chorus effect for the right channel.\n*   `feedbackLeft`: The feedback amount for the left channel.\n*   `feedbackRight`: The feedback amount for the right channel.\n*   `wetDryMix`: The wet/dry mix of the chorus effect.\n*   `maxDelayLength`: The maximum delay length (in samples) used by the chorus effect.\n\n**Methods:**\n\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying the stereo chorus effect.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the stereo chorus effect.\n\n### Modifiers `TrebleBoostModifier`\n\n```csharp\npublic class TrebleBoostModifier : SoundModifier\n{\n    public TrebleBoostModifier(float cutoffFrequency);\n\n    public float CutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency above which the treble boost is applied.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the treble boost effect.\n\n### Providers `AssetDataProvider`\n\n```csharp\npublic sealed class AssetDataProvider : ISoundDataProvider, IDisposable\n{\n    public AssetDataProvider(Stream stream, int? sampleRate = null);\n    public AssetDataProvider(byte[] data, int? sampleRate = null);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; }\n    public int? SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; private set; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public void Dispose();\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (always true for `AssetDataProvider`).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the provider.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio data.\n\n### Providers `StreamDataProvider`\n\n```csharp\npublic sealed class StreamDataProvider : ISoundDataProvider\n{\n    public StreamDataProvider(Stream stream, int? sampleRate = null);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; private set; }\n    public int? SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the stream into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio stream (if supported).\n*   `Dispose()`: Releases resources used by the provider.\n\n### Providers `MicrophoneDataProvider`\n\n```csharp\npublic class MicrophoneDataProvider : ISoundDataProvider, IDisposable\n{\n    public MicrophoneDataProvider(int bufferSize = 8, int? sampleRate = null);\n\n    public int Position { get; private set; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void StartCapture();\n    public void StopCapture();    \n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the captured audio data (in samples).\n*   `Length`: Returns -1, indicating an unknown length for the live microphone stream.\n*   `CanSeek`: Returns `false` because seeking is not supported for live microphone input.\n*   `SampleFormat`: The sample format of the captured audio data, which matches the `AudioEngine`'s sample format.\n*   `SampleRate`: The sample rate of the captured audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when `StopCapture()` is called, signaling the end of the microphone input stream.\n*   `PositionChanged`: Raised after reading data, indicating that the read position has changed.\n\n**Methods:**\n\n*   `MicrophoneDataProvider(int bufferSize = 8, int? sampleRate = null)`: Constructor that initializes the `MicrophoneDataProvider`. It sets the buffer queue size (default is 8), the sample rate (defaults to the `AudioEngine`'s sample rate), and subscribes to the `AudioEngine.OnAudioProcessed` event to capture audio data.\n    * `bufferSize`: The number of audio sample arrays to hold in internal queue. Higher values will lead to higher latency but will be more resilient to performance spikes.\n    * `sampleRate`: The sample rate of the microphone, will use the audioEngine sample rate if not set.\n*   `StartCapture()`: Starts capturing audio data from the microphone.\n*   `StopCapture()`: Stops capturing audio data and raises the `EndOfStreamReached` event.\n*   `ReadBytes(Span<float> buffer)`: Reads captured audio samples into the provided buffer. If not enough data is available in the queue it will fill the rest of the buffer with silence.\n*   `Seek(int offset)`: Throws `NotSupportedException` because seeking is not supported for live microphone input.\n*   `Dispose()`: Releases resources used by the `MicrophoneDataProvider`, unsubscribes from the `AudioEngine.OnAudioProcessed` event, and clears the internal buffer queue.\n\n### Providers `ChunkedDataProvider`\n\n```csharp\npublic sealed class ChunkedDataProvider : ISoundDataProvider, IDisposable\n{\n    public ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize);\n    public ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data in samples.\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a `Stream`. It creates an `ISoundDecoder` to decode the stream, sets the default chunk size (220500 samples per channel, which is 10 seconds at 44.1 kHz), and starts prefetching data.\n*   `ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a file path. It opens a `FileStream` and calls the other constructor.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is prefilled with decoded audio. If the buffer runs out, it decodes another chunk from the stream.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported by the stream and decoder). It disposes of the current decoder, creates a new one, and seeks within the stream. Then, it clears the internal buffer and refills it from the new position.\n*   `Dispose()`: Releases the resources used by the `ChunkedDataProvider`, including the decoder and the stream.\n\n**Remarks:**\n\nThe `ChunkedDataProvider` is designed to handle large audio files efficiently by reading and decoding them in chunks. This prevents the entire file from being loaded into memory at once. The default chunk size is set to 10 seconds of audio at 44.1 kHz, but you can adjust this value in the constructor. The class uses an internal buffer (`Queue<float>`) to store decoded audio samples and prefetches data in the background.\n\n\n### Providers `NetworkDataProvider`\n\n```csharp\npublic sealed class NetworkDataProvider : ISoundDataProvider, IDisposable\n{\n    public NetworkDataProvider(string url, int? sampleRate = null);\n\n    public int Position { get; }\n    public int Length { get; private set; }\n    public bool CanSeek { get; private set; }\n    public SampleFormat SampleFormat { get; private set; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Returns -1 for HLS streams without an `#EXT-X-ENDLIST` tag, indicating an unknown or continuously growing length.\n*   `CanSeek`: Indicates whether seeking is supported. It's `true` for direct audio URLs if the server supports range requests and for HLS streams with an `#EXT-X-ENDLIST` tag; otherwise, it's `false`.\n*   `SampleFormat`: The format of the audio samples. Determined after the initial connection to the stream.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `NetworkDataProvider(string url, int? sampleRate = null)`: Constructor that initializes the `NetworkDataProvider` with a network URL. It determines whether the URL points to a direct audio file or an HLS playlist.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is filled asynchronously.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported). The behavior differs for direct URLs and HLS streams:\n    *   **Direct URLs:** Performs an HTTP range request to fetch data starting from the desired offset.\n    *   **HLS Streams:** Locates the HLS segment containing the desired time offset and starts downloading from that segment.\n*   `Dispose()`: Releases the resources used by the `NetworkDataProvider`, including the `HttpClient`, decoder, and stream.\n\n**Remarks:**\n\nThe `NetworkDataProvider` can handle both direct audio URLs and HLS (HTTP Live Streaming) playlists. It automatically detects the stream type and behaves accordingly.\n\n**Direct Audio URLs:**\n\n*   It uses `HttpClient` to make requests to the URL.\n*   It supports seeking if the server responds with an \"Accept-Ranges: bytes\" header.\n*   It creates an `ISoundDecoder` to decode the audio stream.\n*   It buffers audio data asynchronously in a background thread.\n\n**HLS Playlists:**\n\n*   It downloads and parses the M3U(8) playlist file.\n*   It identifies the individual media segments (e.g., `.ts` files).\n*   It downloads and decodes segments sequentially.\n*   It refreshes the playlist periodically for live streams.\n*   It supports seeking by selecting the appropriate segment based on the desired time offset.\n*   It determines whether the playlist has a defined end by checking for the `#EXT-X-ENDLIST` tag, which affects whether `Length` is known and `CanSeek` is true.\n\nThe class uses an internal `Queue<float>` to buffer audio samples. The `ReadBytes` method waits for data to become available in the buffer if it's empty.\n\n### Providers `RawDataProvider`\n\n```csharp\npublic sealed class RawDataProvider : ISoundDataProvider, IDisposable\n{\n    public RawDataProvider(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from a stream containing raw PCM audio data.\n**Properties:**\n*   `Position`: The current read position in samples.\n*   `Length`: The total length of the stream in samples.\n*   `CanSeek`: Indicates if the underlying stream is seekable.\n*   `SampleFormat`: The sample format of the raw audio data.\n*   `SampleRate`: The sample rate of the raw audio data.\n    **Events:**\n*   `EndOfStreamReached`: Raised when the end of the stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n    **Methods:**\n*   `ReadBytes(Span<float> buffer)`: Reads raw PCM data from the stream and converts it to `float` if necessary.\n*   `Seek(int offset)`: Seeks to the specified offset in the underlying stream if `CanSeek` is true.\n*   `Dispose()`: Disposes the underlying stream.\n\n\n### Structs\n\n#### `DeviceInfo`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct DeviceInfo\n{\n    public IntPtr Id;\n    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 256)]\n    public string Name;\n    [MarshalAs(UnmanagedType.U1)]\n    public bool IsDefault;\n    public uint NativeDataFormatCount;\n    public IntPtr NativeDataFormats; // Pointer to an array of NativeDataFormat\n}\n```\n**Description:** Represents information about an audio device, including its native ID, name, whether it's the default system device, and a count/pointer to its supported native data formats.\n\n#### `NativeDataFormat`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct NativeDataFormat\n{\n    public SampleFormat Format;\n    public uint Channels;\n    public uint SampleRate;\n    public uint Flags;\n}\n```\n**Description:** Represents a specific native data format (sample format, channels, sample rate) supported by an audio device. Accessed via the `NativeDataFormats` pointer in `DeviceInfo`.\n\n### Utils `Extensions`\n\n```csharp\npublic static class Extensions\n{\n    public static int GetBytesPerSample(this SampleFormat sampleFormat);\n    public static unsafe Span<T> GetSpan<T>(nint ptr, int length) where T : unmanaged;\n    public static T[] ReadArray<T>(this nint pointer, int count) where T : struct; // New method\n}\n```\n**New Methods:**\n*   `ReadArray<T>(this nint pointer, int count) where T : struct`: Reads an array of structures of type `T` from a native memory pointer.\n\n### Utils `MathHelper`\n```csharp\npublic static class MathHelper\n{\n    public static void Fft(Complex[] data);\n    public static float[] HammingWindow(int size);\n    public static void InverseFft(Complex[] data);\n    public static double Mod(this double x, double y); // New method\n    public static float PrincipalAngle(float angle);   // New method\n    // ... other existing methods ...\n}\n```\n**New Methods:**\n*   `Mod(this double x, double y)`: Returns the remainder after division, in the range [-0.5, 0.5).\n*   `PrincipalAngle(float angle)`: Returns the principal angle of a number in the range [-PI, PI).\n\n### Visualization `LevelMeterAnalyzer`\n\n```csharp\npublic class LevelMeterAnalyzer : AudioAnalyzer\n{\n    public LevelMeterAnalyzer(IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public float Peak { get; }\n    public float Rms { get; }\n\n    protected override void Analyze(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Peak`: The peak level of the audio signal.\n*   `Rms`: The RMS (root mean square) level of the audio signal.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer)`: Analyzes the audio buffer to calculate the RMS and peak levels.\n\n### Visualization `LevelMeterVisualizer`\n\n```csharp\npublic class LevelMeterVisualizer : IVisualizer\n{\n    public LevelMeterVisualizer(LevelMeterAnalyzer levelMeterAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public Color PeakHoldColor { get; set; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the level meter bar.\n*   `Name`: The name of the visualizer.\n*   `PeakHoldColor`: The color of the peak hold indicator.\n*   `Size`: The size of the level meter.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the level meter.\n*   `Render(IVisualizationContext context)`: Renders the level meter visualization.\n\n### Visualization `SpectrumAnalyzer`\n\n```csharp\npublic class SpectrumAnalyzer : AudioAnalyzer\n{\n    public SpectrumAnalyzer(int fftSize, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public ReadOnlySpan<float> SpectrumData { get; }\n\n    protected override void Analyze(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `SpectrumData`: The calculated frequency spectrum data.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer)`: Analyzes the audio buffer to compute the frequency spectrum using an FFT.\n\n### Visualization `SpectrumVisualizer`\n\n```csharp\npublic class SpectrumVisualizer : IVisualizer\n{\n    public SpectrumVisualizer(SpectrumAnalyzer spectrumAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the spectrum bars.\n*   `Name`: The name of the visualizer.\n*   `Size`: The size of the spectrum visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the spectrum.\n*   `Render(IVisualizationContext context)`: Renders the spectrum visualization.\n\n### Visualization `WaveformVisualizer`\n\n```csharp\npublic class WaveformVisualizer : IVisualizer\n{\n    public WaveformVisualizer();\n\n    public string Name { get; }\n    public List<float> Waveform { get; }\n    public Color WaveformColor { get; set; }\n    public Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n*   `Waveform`: The waveform data.\n*   `WaveformColor`: The color of the waveform.\n*   `Size`: The size of the waveform visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the waveform.\n*   `Render(IVisualizationContext context)`: Renders the waveform visualization."
  },
  {
    "id": 17,
    "slug": "advanced-topics",
    "version": "1.1.2",
    "title": "Advanced Topics",
    "description": "Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.",
    "navOrder": 6,
    "category": "Core",
    "content": "---\ntitle: Advanced Topics\ndescription: Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.\nnavOrder: 6\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\nThis section delves into more advanced topics related to SoundFlow, including extending the engine with custom components, optimizing performance, and understanding threading considerations.\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Advanced Topics\">\n    <Tab\n        key=\"extending\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ph:puzzle-piece-bold' />\n                <span>Extending SoundFlow</span>\n            </div>\n        }\n    >\n        ## Extending SoundFlow\n\n        One of SoundFlow's key strengths is its extensibility. You can tailor the engine to your specific needs by creating custom:\n\n        *   **Sound Components (`SoundComponent`)**\n        *   **Sound Modifiers (`SoundModifier`)**\n        *   **Visualizers (`IVisualizer`)**\n        *   **Audio Backends (`AudioEngine`)**\n        *   **Sound Data Providers (`ISoundDataProvider`)**\n        *   **Extensions (e.g., for specific DSP libraries)**: SoundFlow supports integration with external audio processing libraries. For instance, the `SoundFlow.Extensions.WebRtc.Apm` package provides features like noise suppression and echo cancellation by wrapping the WebRTC Audio Processing Module. You can create similar extensions for other libraries.\n\n        ### Custom Sound Components\n\n        Creating custom `SoundComponent` classes allows you to implement unique audio processing logic and integrate it seamlessly into the SoundFlow audio graph.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundComponent\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundComponent` class.\n            </Step>\n            <Step title=\"Implement GenerateAudio\" icon='lucide:audio-lines'>\n                Override the `GenerateAudio(Span<float> buffer)` method. This is where you'll write the core audio processing code for your component.\n                *   If your component generates audio (e.g., an oscillator), write samples to the provided `buffer`.\n                *   If your component modifies audio, read from connected input components (using a temporary buffer if necessary), process the audio, and then write to the provided `buffer`.\n            </Step>\n            <Step title=\"Override other methods (optional)\" icon='icon-park-outline:switch-one'>\n                You can override methods like `ConnectInput`, `AddAnalyzer`, `AddModifier`, etc., to customize how your component interacts with the audio graph.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your component to expose configurable parameters that users can adjust.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using System;\n\n        public class CustomGainComponent : SoundComponent\n        {\n            public float Gain { get; set; } = 1.0f; // Default gain\n\n            public override string Name { get; set; } = \"Custom Gain\";\n\n            protected override void GenerateAudio(Span<float> buffer)\n        {\n            // Multiply each sample by the gain factor\n            for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] *= Gain;\n        }\n        }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        // Create an instance of your custom component\n        var gainComponent = new CustomGainComponent { Gain = 0.5f };\n\n        // Connect it to the audio graph\n        var player = new SoundPlayer(new StreamDataProvider(File.OpenRead(\"audio.wav\")));\n        gainComponent.AddInput(player);\n        Mixer.Master.AddComponent(gainComponent);\n\n        // ...\n        ```\n\n        ### Custom Sound Modifiers\n\n        Custom `SoundModifier` classes allow you to implement your own audio effects.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundModifier\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundModifier` class.\n            </Step>\n            <Step title=\"Implement ProcessSample\" icon='icon-park-outline:sound-wave'>\n                Implement `ProcessSample(float sample, int channel)` (or override `Process` for buffer-level):\n                *   `ProcessSample(float sample, int channel)`: This method takes a single audio sample and the channel index as input and returns the modified sample.\n                *   `Process(Span<float> buffer)`: Override this for more complex effects that operate on entire buffers (e.g., FFT-based effects). By default, it calls `ProcessSample` for each sample.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your modifier to expose configurable parameters.\n            </Step>\n            <Step title=\"Use 'Enabled' property\" icon='material-symbols:toggle-on-outline'>\n                Your modifier will have an `Enabled` property (defaulting to `true`) to allow toggling its effect.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using System;\n\n        public class CustomDistortionModifier : SoundModifier\n        {\n            public float Threshold { get; set; } = 0.5f;\n\n            public override string Name { get; set; } = \"Custom Distortion\";\n\n            public override float ProcessSample(float sample, int channel)\n        {\n            // Simple hard clipping distortion\n            if (sample > Threshold)\n        {\n            return Threshold;\n        }\n            else if (sample < -Threshold)\n        {\n            return -Threshold;\n        }\n            else\n        {\n            return sample;\n        }\n        }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        // Create an instance of your custom modifier\n        var distortion = new CustomDistortionModifier { Threshold = 0.7f };\n        // distortion.Enabled = false; // To disable it\n\n        // Add it to a SoundComponent\n        using var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(dataProvider);\n        player.AddModifier(distortion);\n        Mixer.Master.AddComponent(player);\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Visualizers\n\n        Custom `IVisualizer` classes allow you to create unique visual representations of audio data.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement IVisualizer\" icon='ph:plugs-connected-bold'>\n                Create a new class that implements the `IVisualizer` interface.\n            </Step>\n            <Step title=\"Implement ProcessOnAudioData\" icon='carbon:data-vis-4'>\n                This method receives a `Span<float>` containing audio data. You should process this data and store the relevant information needed for rendering.\n            </Step>\n            <Step title=\"Implement Render\" icon='material-symbols:draw-outline'>\n                This method receives an `IVisualizationContext`. Use the drawing methods provided by the context (e.g., `DrawLine`, `DrawRectangle`) to render your visualization.\n            </Step>\n            <Step title=\"Raise VisualizationUpdated\" icon='mdi:bell-ring-outline'>\n                When the visualization data changes (e.g., after processing new audio data), raise the `VisualizationUpdated` event to notify the UI to update the display.\n            </Step>\n            <Step title=\"Implement Dispose\" icon='material-symbols:delete-outline'>\n                Release any unmanaged resources or unsubscribe from events.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Interfaces;\n        using System;\n        using System.Numerics;\n\n        // Assuming a simple Color struct/class exists:\n        // public struct Color { public float R, G, B, A; ... }\n\n        public class CustomBarGraphVisualizer : IVisualizer\n        {\n            private float _level;\n\n            public string Name => \"Custom Bar Graph\";\n\n            public event EventHandler? VisualizationUpdated;\n\n            public void ProcessOnAudioData(Span<float> audioData)\n        {\n            if (audioData.IsEmpty) return;\n            // Calculate the average level (simplified for this example)\n            float sum = 0;\n            for (int i = 0; i < audioData.Length; i++)\n        {\n            sum += Math.Abs(audioData[i]);\n        }\n            _level = sum / audioData.Length;\n\n            // Notify that the visualization needs to be updated\n            VisualizationUpdated?.Invoke(this, EventArgs.Empty);\n        }\n\n            public void Render(IVisualizationContext context)\n        {\n            // Clear the drawing area\n            context.Clear();\n\n            // Draw a simple bar graph based on the calculated level\n            float barHeight = _level * 200; // Scale the level for visualization\n            // Assuming Color constructor: new Color(r,g,b) or similar\n            context.DrawRectangle(10, 200 - barHeight, 30, barHeight, new Color(0, 1, 0));\n        }\n\n            public void Dispose()\n        {\n            // Unsubscribe from events, release resources if any\n            VisualizationUpdated = null;\n        }\n        }\n        ```\n\n        ### Adding Audio Backends\n\n        SoundFlow is designed to support multiple audio backends. Currently, it includes a `MiniAudio` backend. You can add support for other audio APIs (e.g., WASAPI, ASIO, CoreAudio) by creating a new backend.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from AudioEngine\" icon='ph:engine-bold'>\n                Create a new class that inherits from `AudioEngine`.\n            </Step>\n            <Step title=\"Implement Abstract Methods\" icon='material-symbols:function'>\n                Implement the abstract methods:\n                *   `InitializeAudioDevice()`: Initialize the audio device using the new backend's API, including context creation if needed.\n                *   `ProcessAudioData()`: Implement the main audio processing loop, or set up callbacks if the backend works that way.\n                *   `CleanupAudioDevice()`: Clean up any resources used by the audio device and context.\n                *   `CreateEncoder(...)`: Create an `ISoundEncoder` implementation for the new backend.\n                *   `CreateDecoder(...)`: Create an `ISoundDecoder` implementation for the new backend.\n                *   `UpdateDevicesInfo()`: Implement logic to enumerate playback and capture devices using the backend's API, populating `PlaybackDevices`, `CaptureDevices`, `PlaybackDeviceCount`, and `CaptureDeviceCount`.\n                *   `SwitchDevice(...)` and `SwitchDevices(...)`: Implement logic to reinitialize or reconfigure the audio device to use the specified new device(s).\n            </Step>\n            <Step title=\"Implement Codec Interfaces\" icon='mdi:file-code-outline'>\n                Create classes that implement `ISoundEncoder` and `ISoundDecoder` to handle audio encoding and decoding for your chosen backend.\n            </Step>\n        </Steps>\n\n        **Example (Skeleton):**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Enums;\n        using SoundFlow.Structs;\n        using System;\n        using System.IO;\n        using System.Linq;\n\n        public class MyNewAudioEngine : AudioEngine\n        {\n            private nint _context; // Example: native context handle\n            private nint _device;  // Example: native device handle\n\n            public MyNewAudioEngine(int sampleRate, Capability capability, SampleFormat sampleFormat, int channels)\n            : base(sampleRate, capability, sampleFormat, channels)\n        {\n            // Base constructor calls InitializeAudioDevice\n        }\n\n            protected override void InitializeAudioDevice()\n        {\n            // Initialize your audio device using the new backend's API\n\n            // 1. Initialize backend context (e.g., ma_context_init for MiniAudio)\n            // _context = NativeApi.InitContext();\n            // 2. Get default device IDs or use null for system default\n            // UpdateDevicesInfo(); // Populate device lists\n            // var defaultPlaybackId = PlaybackDevices.FirstOrDefault(d => d.IsDefault)?.Id ?? IntPtr.Zero;\n            // var defaultCaptureId = CaptureDevices.FirstOrDefault(d => d.IsDefault)?.Id ?? IntPtr.Zero;\n            // 3. Initialize the actual device with these IDs\n            // _device = NativeApi.InitDevice(_context, ..., defaultPlaybackId, defaultCaptureId);\n            // NativeApi.StartDevice(_device);\n            Console.WriteLine(\"MyNewAudioEngine: Audio device initialized.\");\n        }\n\n            protected override void ProcessAudioData()\n        {\n            // If backend uses callbacks, this might be empty.\n            // If backend needs a manual loop, implement it here.\n        }\n\n            protected override void CleanupAudioDevice()\n        {\n            // NativeApi.StopDevice(_device);\n            // NativeApi.UninitDevice(_device);\n            // NativeApi.UninitContext(_context);\n            Console.WriteLine(\"MyNewAudioEngine: Audio device cleaned up.\");\n        }\n\n            public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate)\n        {\n            // Return an instance of your custom ISoundEncoder implementation\n            // return new MyNewAudioEncoder(stream, encodingFormat, sampleFormat, encodingChannels, sampleRate);\n            throw new NotImplementedException();\n        }\n\n            public override ISoundDecoder CreateDecoder(Stream stream)\n        {\n            // Return an instance of your custom ISoundDecoder implementation\n            // return new MyNewAudioDecoder(stream);\n            throw new NotImplementedException();\n        }\n\n            public override void UpdateDevicesInfo()\n        {\n            // Use backend API to get list of playback and capture devices\n            // Populate this.PlaybackDevices, this.CaptureDevices,\n            // this.PlaybackDeviceCount, this.CaptureDeviceCount.\n            // Example:\n            // (var nativePlaybackDevices, var nativeCaptureDevices) = NativeApi.GetDeviceList(_context);\n            // this.PlaybackDevices = ConvertNativeToDeviceInfo(nativePlaybackDevices);\n            // ...\n            Console.WriteLine(\"MyNewAudioEngine: Device info updated.\");\n        }\n\n            public override void SwitchDevice(DeviceInfo deviceInfo, DeviceType type)\n        {\n            // CleanupCurrentDevice(); // Stop and uninit current device\n            // if (type == DeviceType.Playback) InitializeWithDeviceIds(deviceInfo.Id, CurrentCaptureDevice?.Id ?? IntPtr.Zero);\n            // else InitializeWithDeviceIds(CurrentPlaybackDevice?.Id ?? IntPtr.Zero, deviceInfo.Id);\n            // Update CurrentPlaybackDevice/CurrentCaptureDevice properties\n            Console.WriteLine($\"MyNewAudioEngine: Switched {type} device to {deviceInfo.Name}.\");\n        }\n\n            public override void SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo)\n        {\n            // IntPtr playbackId = playbackDeviceInfo?.Id ?? CurrentPlaybackDevice?.Id ?? IntPtr.Zero;\n            // IntPtr captureId = captureDeviceInfo?.Id ?? CurrentCaptureDevice?.Id ?? IntPtr.Zero;\n            // CleanupCurrentDevice();\n            // InitializeWithDeviceIds(playbackId, captureId);\n            Console.WriteLine(\"MyNewAudioEngine: Switched devices.\");\n        }\n            // Helper method for device initialization logic\n            // private void InitializeWithDeviceIds(IntPtr playbackId, IntPtr captureId) { /* ... */ }\n            // private void CleanupCurrentDevice() { /* ... */ }\n        }\n        ```\n    </Tab>\n\n    <Tab\n        key=\"performance\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ic:round-speed' />\n                <span>Performance Optimization</span>\n            </div>\n        }\n    >\n        ## Performance Optimization\n\n        Here are some tips for optimizing the performance of your SoundFlow applications:\n\n        *   **Buffer Sizes:** Choose appropriate buffer sizes for your use case. Smaller buffers reduce latency but increase CPU overhead. Larger buffers can improve efficiency but may introduce latency. Experiment to find the optimal balance. The audio backend (e.g., MiniAudio) often manages its own internal buffer sizes based on system capabilities and requests.\n        *   **SIMD:** SoundFlow uses SIMD instructions (when available) in the `Mixer` and `MathHelper` classes, and for some operations within `SoundComponent` and the audio conversion pipeline. Ensure your target platform supports SIMD for better performance.\n        *   **Profiling:** Use a profiler (like the one built into Visual Studio) to identify performance bottlenecks in your audio processing pipeline.\n        *   **Asynchronous Operations:** For long-running operations (e.g., loading large files, network requests in `NetworkDataProvider`, project saving/loading), use asynchronous programming (`async` and `await`) to avoid blocking the main thread or the audio thread.\n        *   **Avoid Allocations:** Minimize memory allocations within the `GenerateAudio` method of `SoundComponent` and the `ProcessSample` or `Process` method of `SoundModifier`. Allocate buffers and other resources in advance, if possible. Use `ArrayPool<T>.Shared` for temporary buffers when unavoidable.\n        *   **Efficient Algorithms:** Use efficient algorithms for audio processing, especially in performance-critical sections.\n        *   **Modifier Overhead:** Each `SoundModifier` added to a `SoundComponent` or `AudioSegment`, `Track`, `Composition` introduces some overhead. For very simple operations, integrating them directly into a custom `SoundComponent` might be more performant than using many tiny modifiers. However, modifiers offer better reusability and modularity.\n        *   **Effect Toggling:** Use the `Enabled` property on `SoundModifier`, `AudioAnalyzer`, `AudioSegmentSettings`, and `TrackSettings` to non-destructively disable effects or entire processing paths instead of removing and re-adding them, which can be more efficient.\n    </Tab>\n\n    <Tab\n        key=\"threading\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='carbon:thread' />\n                <span>Threading Considerations</span>\n            </div>\n        }\n    >\n        ## Threading Considerations\n\n        SoundFlow uses a dedicated, high-priority thread for audio processing. This ensures that audio is processed in real time and minimizes the risk of glitches or dropouts.\n\n        **Key Considerations:**\n\n        *   **Audio Thread:** The `AudioEngine`'s audio processing logic (e.g., within `ProcessGraph`, `ProcessAudioInput`, or backend callbacks which then call these) and consequently the `GenerateAudio` method of your `SoundComponent` and `Process` method of your `AudioAnalyzer` classes, are all called from the audio thread(s). Avoid performing any long-running or blocking operations (like I/O, complex non-audio computations, or UI updates) on this thread.\n        *   **UI Thread:** Never perform audio processing directly on the UI thread. This can lead to unresponsiveness and glitches. Use the `AudioEngine`'s audio thread for all audio-related operations. For UI updates based on audio events (e.g., from an `IVisualizer`), marshal the calls to the UI thread (e.g., `Dispatcher.Invoke` in WPF, `Control.Invoke` in WinForms).\n        *   **Thread Safety:** If you need to access or modify shared data from both the audio thread and another thread (e.g., the UI thread updating a `SoundModifier`'s property), use appropriate synchronization mechanisms (like `lock`, `Monitor`, or thread-safe collections) to ensure data integrity and prevent race conditions. Many properties on `SoundComponent` and `SoundModifier` are internally locked for thread-safe access.\n    </Tab>\n</Tabs>"
  }
]
