[
  {
    "id": 11,
    "slug": "webrtc-apm",
    "version": "1.3.0",
    "title": "WebRTC Audio Processing Module (APM) Extension",
    "description": "Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.",
    "navOrder": 11,
    "category": "Extensions",
    "content": "﻿---\r\nid: 11\r\ntitle: WebRTC Audio Processing Module (APM) Extension\r\ndescription: Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.\r\nnavOrder: 11\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# WebRTC Audio Processing Module (APM) Extension for SoundFlow\r\n\r\nThe `SoundFlow.Extensions.WebRtc.Apm` package integrates a native library based on the high-quality WebRTC Audio Processing Module (APM) into the SoundFlow audio engine. This extension provides advanced voice processing features essential for real-time communication and audio enhancement.\r\n\r\n## Features\r\n\r\nThe WebRTC APM extension offers several key audio processing features:\r\n\r\n*   **Acoustic Echo Cancellation (AEC):** Reduces or eliminates echoes that occur when audio played through speakers is picked up by the microphone.\r\n*   **Noise Suppression (NS):** Attenuates steady-state background noise (e.g., fans, hums) to improve speech clarity. Multiple suppression levels are available.\r\n*   **Automatic Gain Control (AGC):** Dynamically adjusts the microphone input volume to maintain a consistent audio level, preventing clipping or overly quiet audio. Supports different modes and target levels.\r\n*   **High Pass Filter (HPF):** Removes low-frequency components (typically below 80Hz) to reduce rumble and DC offset.\r\n*   **Pre-Amplifier:** Applies a configurable fixed gain to the audio signal before other APM processing steps.\r\n*   **Multi-channel Processing Configuration:** Allows specifying how multi-channel audio is handled and downmixed.\r\n\r\nThese features can be configured and applied primarily through the `WebRtcApmModifier` for real-time processing within the SoundFlow audio graph, or using the `NoiseSuppressor` component for offline batch processing.\r\n\r\n**Important Note on Sample Rates:** The WebRTC APM native library primarily supports specific sample rates: **8000 Hz, 16000 Hz, 32000 Hz, and 48000 Hz**. Ensure your SoundFlow `AudioEngine` is initialized with one of these sample rates when using this extension for optimal performance and compatibility.\r\n\r\n## Installation\r\n\r\nTo use this extension, you need to have the core `SoundFlow` library installed. Then, add the `SoundFlow.Extensions.WebRtc.Apm` package to your project:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\nThis package includes the necessary C# wrapper and the native WebRTC APM binaries for supported platforms.\r\n\r\n## Usage\r\n\r\n### Real-time Processing with `WebRtcApmModifier`\r\n\r\nThe `WebRtcApmModifier` is a `SoundModifier` that can be added to any `SoundComponent` to process its audio output in real-time. This is ideal for applications like voice chat, live audio input processing, or any scenario requiring real-time audio enhancement. The following steps demonstrate a typical full-duplex setup for echo cancellation.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine & Devices\" description=\"Create the engine and a full-duplex device\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine` and Devices\r\n        First, create an instance of the `AudioEngine`. Then, define an `AudioFormat` compatible with WebRTC (e.g., 48kHz mono). Finally, initialize a `FullDuplexDevice`, which manages both a capture (microphone) and a playback (speaker) device, making it perfect for AEC scenarios.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Abstracts.Devices;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Structs;\r\n\r\n        // Initialize the audio engine\r\n        var audioEngine = new MiniAudioEngine();\r\n\r\n        // Define a WebRTC APM compatible audio format\r\n        var audioFormat = new AudioFormat\r\n        {\r\n            SampleRate = 48000,\r\n            Channels = 1, // Mono for typical voice processing\r\n            Format = SampleFormat.F32\r\n        };\r\n\r\n        // Initialize a full-duplex device using the system's default microphone and speakers\r\n        var fullDuplexDevice = audioEngine.InitializeFullDuplexDevice(\r\n            playbackDeviceInfo: audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault),\r\n            captureDeviceInfo: audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault),\r\n            format: audioFormat\r\n        );\r\n\r\n        // Start the devices so they are ready for processing\r\n        fullDuplexDevice.Start();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create SoundComponent for Mic\" description=\"Set up a data provider and player for the mic\" icon='ph:microphone-bold'>\r\n        ### 2. Set Up the Microphone Input Component\r\n        Create a `MicrophoneDataProvider` to receive live audio from the capture device, and a `SoundPlayer` to process this data through the SoundFlow graph. This `SoundPlayer` will act as our microphone source component.\r\n\r\n        ```csharp\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Providers;\r\n\r\n        // Create a data provider that reads from our duplex device\r\n        var microphoneDataProvider = new MicrophoneDataProvider(fullDuplexDevice);\r\n\r\n        // Create a sound player to process the live mic data\r\n        // We will add the APM modifier to this player\r\n        var micAudioComponent = new SoundPlayer(audioEngine, audioFormat, microphoneDataProvider);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Configure APM Modifier\" description=\"Instantiate and set up the modifier\" icon='material-symbols:settings-outline'>\r\n        ### 3. Instantiate and Configure `WebRtcApmModifier`\r\n        The modifier's constructor allows setting initial states for all features. You can also adjust them dynamically via its public properties.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Modifiers;\r\n\r\n        var apmModifier = new WebRtcApmModifier(\r\n            // Echo Cancellation (AEC) settings\r\n            aecEnabled: true,\r\n            aecMobileMode: false, // Desktop mode is generally more robust\r\n            aecLatencyMs: 40,     // Estimated system latency for AEC (tune this)\r\n\r\n            // Noise Suppression (NS) settings\r\n            nsEnabled: true,\r\n            nsLevel: NoiseSuppressionLevel.High,\r\n\r\n            // Automatic Gain Control (AGC) - Version 1 (legacy)\r\n            agc1Enabled: true,\r\n            agcMode: GainControlMode.AdaptiveDigital,\r\n            agcTargetLevel: -3,   // Target level in dBFS (0 is max, typical is -3 to -18)\r\n            agcCompressionGain: 9, // Only for FixedDigital mode\r\n            agcLimiter: true,\r\n\r\n            // Automatic Gain Control (AGC) - Version 2 (newer, often preferred)\r\n            agc2Enabled: false, // Set to true to use AGC2, potentially disable AGC1\r\n\r\n            // High Pass Filter (HPF)\r\n            hpfEnabled: true,\r\n\r\n            // Pre-Amplifier\r\n            preAmpEnabled: false,\r\n            preAmpGain: 1.0f,\r\n\r\n            // Pipeline settings for multi-channel audio (if numChannels > 1)\r\n            useMultichannelCapture: false, // Process capture (mic) as mono/stereo as configured by AudioEngine\r\n            useMultichannelRender: false,  // Process render (playback for AEC) as mono/stereo\r\n            downmixMethod: DownmixMethod.AverageChannels // Method if downmixing is needed\r\n        );\r\n\r\n        // Example of changing a setting dynamically:\r\n        // apmModifier.NoiseSuppression.Level = NoiseSuppressionLevel.VeryHigh;\r\n        ```\r\n    </Step>\r\n    <Step title=\"Add Modifier\" description=\"Attach the modifier to the mic component\" icon='ic:baseline-plus'>\r\n        ### 4. Add the Modifier to the Microphone Component\r\n\r\n        ```csharp\r\n        micAudioComponent.AddModifier(apmModifier);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Start Processing\" description=\"Add to mixer and start playback\" icon='mdi:play-box-outline'>\r\n        ### 5. Add the Component to the Mixer and Start Processing\r\n        To complete the loop for testing, add the processed microphone component to the playback device's `MasterMixer`. This will route the cleaned microphone audio to the speakers.\r\n\r\n        ```csharp\r\n        // Add the processed mic component to the playback device's master mixer\r\n        fullDuplexDevice.MasterMixer.AddComponent(micAudioComponent);\r\n\r\n        // Start the microphone provider and the player component\r\n        microphoneDataProvider.StartCapture();\r\n        micAudioComponent.Play();\r\n\r\n        Console.WriteLine(\"WebRTC APM processing microphone input. Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        // Cleanup\r\n        fullDuplexDevice.Dispose(); // Disposes underlying devices and their components\r\n        apmModifier.Dispose();      // Important to release native resources\r\n        microphoneDataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n**AEC Far-End (Playback) Signal:** For Acoustic Echo Cancellation to work, the processor needs both the microphone signal (near-end) and the speaker signal (far-end). The `WebRtcApmModifier` is designed to integrate deeply with SoundFlow's architecture. It automatically detects and captures the audio being processed by the master mixer of any active playback device within the same `AudioEngine` context. This allows it to correlate the audio being sent to the speakers with the audio coming from the microphone to perform echo cancellation seamlessly.\r\n\r\n### Offline Processing with `NoiseSuppressor`\r\n\r\nThe `NoiseSuppressor` component is designed for batch processing of audio from an `ISoundDataProvider` (e.g., an audio file). It applies only the WebRTC Noise Suppression feature.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Required for encoding/decoding\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        The engine is required for its decoding and encoding capabilities, even if not playing audio back.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Structs;\r\n\r\n        var audioEngine = new MiniAudioEngine();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create Data Provider\" description=\"Load your noisy audio file\" icon='mdi:file-music-outline'>\r\n        ### 2. Create an `ISoundDataProvider` for your noisy audio file\r\n        Use a `StreamDataProvider` to read from an audio file.\r\n\r\n        ```csharp\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Define the format of the source file. This is needed by the data provider.\r\n        var fileFormat = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        string noisyFilePath = \"path/to/your/noisy_audio.wav\";\r\n        var dataProvider = new StreamDataProvider(audioEngine, fileFormat, File.OpenRead(noisyFilePath));\r\n        ```\r\n    </Step>\r\n    <Step title=\"Instantiate NoiseSuppressor\" description=\"Set up the offline processor\" icon='icon-park-outline:sound-wave'>\r\n        ### 3. Instantiate `NoiseSuppressor`\r\n        Provide the data source and its audio parameters. These MUST match the actual properties of the audio from the data provider.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Components;\r\n\r\n        // Parameters: dataProvider, sampleRate, numChannels, suppressionLevel\r\n        var noiseSuppressor = new NoiseSuppressor(dataProvider, 48000, 1, NoiseSuppressionLevel.VeryHigh);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Process the Audio\" description=\"Process all at once or in chunks\" icon='carbon:cics-transaction-server-zos'>\r\n        ### 4. Process the audio\r\n        You can process all audio at once (for smaller files) or chunk by chunk.\r\n\r\n        **Option A: Process All (returns `float[]`)**\r\n        ```csharp\r\n        float[] cleanedAudio = noiseSuppressor.ProcessAll();\r\n        // Now 'cleanedAudio' contains the noise-suppressed audio data.\r\n        \r\n        // You can save it using an ISoundEncoder:\r\n        var outputFormat = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        using var fileStream = new FileStream(\"cleaned_audio.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        using var encoder = audioEngine.CreateEncoder(fileStream, EncodingFormat.Wav, outputFormat);\r\n        encoder.Encode(cleanedAudio.AsSpan());\r\n        ```\r\n\r\n        **Option B: Process Chunks (via event or direct handler)**\r\n        ```csharp\r\n        var outputFormatChunked = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        using var chunkFileStream = new FileStream(\"cleaned_audio_chunked.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        using var chunkEncoder = audioEngine.CreateEncoder(chunkFileStream, EncodingFormat.Wav, outputFormatChunked);\r\n\r\n        noiseSuppressor.OnAudioChunkProcessed += (processedChunk) =>\r\n        {\r\n            if (!chunkEncoder.IsDisposed)\r\n            {\r\n                chunkEncoder.Encode(processedChunk.ToArray());\r\n            }\r\n        };\r\n\r\n        // ProcessChunks is a blocking call until the entire provider is processed.\r\n        noiseSuppressor.ProcessChunks();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Dispose Resources\" description=\"Clean up all IDisposable objects\" icon='material-symbols:delete-outline'>\r\n        ### 5. Dispose resources\r\n\r\n        ```csharp\r\n        noiseSuppressor.Dispose();\r\n        dataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n## Configuration Details\r\n\r\n### `WebRtcApmModifier` Properties:\r\n\r\n*   **`Enabled` (bool):** Enables/disables the entire APM modifier.\r\n*   **`EchoCancellation` (`EchoCancellationSettings`):**\r\n*   `Enabled` (bool): Enables/disables AEC.\r\n*   `MobileMode` (bool): Toggles between desktop (false) and mobile (true) AEC modes.\r\n*   `LatencyMs` (int): Estimated system audio latency in milliseconds. Crucial for AEC performance. Tune this value for your setup.\r\n*   **`NoiseSuppression` (`NoiseSuppressionSettings`):**\r\n*   `Enabled` (bool): Enables/disables NS.\r\n*   `Level` (`NoiseSuppressionLevel`): Sets the aggressiveness (Low, Moderate, High, VeryHigh).\r\n*   **`AutomaticGainControl` (`AutomaticGainControlSettings`):**\r\n*   `Agc1Enabled` (bool): Enables/disables the legacy AGC1.\r\n*   `Mode` (`GainControlMode`): Sets the mode for AGC1 (AdaptiveAnalog, AdaptiveDigital, FixedDigital).\r\n*   `TargetLevelDbfs` (int): Target level for AGC1 AdaptiveDigital mode (-31 to 0 dBFS).\r\n*   `CompressionGainDb` (int): Gain for AGC1 FixedDigital mode (0 to 90 dB).\r\n*   `LimiterEnabled` (bool): Enables/disables the limiter for AGC1.\r\n*   `Agc2Enabled` (bool): Enables/disables the newer AGC2.\r\n*   **`HighPassFilterEnabled` (bool):** Enables/disables the HPF.\r\n*   **`PreAmplifierEnabled` (bool):** Enables/disables the pre-amplifier.\r\n*   **`PreAmplifierGainFactor` (float):** Gain factor for the pre-amplifier (e.g., 1.0 is no change, 2.0 is +6dB).\r\n*   **`ProcessingPipeline` (`ProcessingPipelineSettings`):**\r\n*   `UseMultichannelCapture` (bool): If true and input is multi-channel, APM processes it as such. Otherwise, it might downmix.\r\n*   `UseMultichannelRender` (bool): Similar to capture, but for the far-end/render signal for AEC.\r\n*   `DownmixMethod` (`DownmixMethod`): Specifies how to downmix if multi-channel processing is disabled for a stream (AverageChannels, UseFirstChannel).\r\n*   **`PostProcessGain` (float):** A final gain applied after all APM processing (default 1.0f).\r\n\r\n### `NoiseSuppressor` Constructor:\r\n\r\n*   `dataProvider` (`ISoundDataProvider`): The audio source.\r\n*   `sampleRate` (int): Sample rate of the source audio (must be 8k, 16k, 32k, or 48k).\r\n*   `numChannels` (int): Number of channels in the source audio.\r\n*   `suppressionLevel` (`NoiseSuppressionLevel`): Desired noise suppression level.\r\n*   `useMultichannelProcessing` (bool): If true and `numChannels > 1`, attempts to process channels independently.\r\n\r\n## Licensing\r\n\r\n*   The C# code (`SoundFlow.Extensions.WebRtc.Apm` wrapper and components) is licensed under the **MIT License**.\r\n*   The native `webrtc-apm` library used by this extension is based on the WebRTC Audio Processing Module, which is typically licensed under the **BSD 3-Clause \"New\" or \"Revised\" License**. The specific version included is derived from the [PulseAudio project's extraction](https://gitlab.freedesktop.org/pulseaudio/webrtc-audio-processing).\r\n\r\n**Users must comply with the terms of both licenses.** This generally involves including the copyright notice and license text of the WebRTC code if distributing applications using this extension. Please consult the native library's specific distribution for exact requirements.\r\n\r\n## Troubleshooting\r\n\r\n*   **No effect or poor quality:**\r\n*   Verify the `AudioEngine` sample rate (set in the `AudioFormat` struct) matches one supported by WebRTC APM (8k, 16k, 32k, 48k Hz).\r\n*   For AEC, ensure `aecLatencyMs` is tuned appropriately for your system. Too low or too high values can degrade performance.\r\n*   Ensure the far-end signal is correctly being captured if AEC is enabled (usually is handled automatically by the modifier, which monitors active playback devices in the same engine context).\r\n*   **Errors during initialization:** Check the console output for any specific error messages from the native APM library. Ensure the native binaries are correctly deployed with your application.\r\n*   **Performance issues:** While WebRTC APM is optimized, processing many channels or enabling all features at very high settings can be CPU intensive. Monitor performance and adjust settings if needed."
  },
  {
    "id": 13.8,
    "slug": "utilities-and-debugging",
    "version": "1.3.0",
    "title": "Utilities & Debugging",
    "description": "A guide to the essential utility classes in SoundFlow, including centralized logging, high-performance channel mixing, and signal processing math helpers.",
    "navOrder": 13.8,
    "category": "Core",
    "content": "﻿---\r\nid: 13.8\r\ntitle: Utilities & Debugging\r\ndescription: A guide to the essential utility classes in SoundFlow, including centralized logging, high-performance channel mixing, and signal processing math helpers.\r\nnavOrder: 13.8\r\ncategory: Core\r\n---\r\n\r\n# Utilities & Debugging\r\n\r\nThe `SoundFlow.Utils` namespace provides critical infrastructure for debugging your application and performing high-performance audio manipulation. While many of these tools are used internally by the engine, they are exposed publicly to help you build robust, optimized audio applications.\r\n\r\n## Centralized Logging (`Log`)\r\n\r\nSoundFlow v1.3 introduces a completely decoupled, centralized logging system. The library no longer writes directly to `Console.WriteLine`. Instead, it routes all diagnostic messages through the static `SoundFlow.Utils.Log` class.\r\n\r\nTo receive error messages, warnings, or debug information from the internal engine (such as Codec loading failures or MIDI device connection errors), you **must** subscribe to the `Log.OnLog` event.\r\n\r\n### Log Levels\r\n\r\nThe system uses the `LogLevel` enumeration to categorize messages:\r\n\r\n| Level       | Description                                                                                                                                    |\r\n|:------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\r\n| **Debug**   | Detailed information intended for development and deep debugging. (e.g., \"Loaded chunk X from file\").                                          |\r\n| **Info**    | General informational messages about the library's operation (e.g., \"PortMidi backend enabled\").                                               |\r\n| **Warning** | Indicates a potential issue that does not prevent the operation from completing but may lead to unexpected behavior (e.g., \"Frame truncated\"). |\r\n| **Error**   | Indicates a definite error that has occurred, often preceding an Exception (e.g., \"Failed to initialize device\").                              |\r\n\r\n### Usage Example\r\n\r\nThis example demonstrates how to route SoundFlow logs to the System Console. In a GUI application (WPF/Avalonia/WinForms), you would route these strings to a text box or a log file.\r\n\r\n```csharp\r\nusing SoundFlow.Utils;\r\n\r\npublic static void Main()\r\n{\r\n    // 1. Subscribe to the static event BEFORE initializing the engine.\r\n    Log.OnLog += HandleSoundFlowLog;\r\n\r\n    // 2. Initialize your engine (logs will now be captured)\r\n    using var engine = new MiniAudioEngine();\r\n\r\n    // ... application logic ...\r\n}\r\n\r\n// The event handler receives the severity level and the message string.\r\nprivate static void HandleSoundFlowLog(LogLevel level, string message)\r\n{\r\n    // Optional: Color code the output based on severity\r\n    switch (level)\r\n    {\r\n        case LogLevel.Error:\r\n            Console.ForegroundColor = ConsoleColor.Red;\r\n            break;\r\n        case LogLevel.Warning:\r\n            Console.ForegroundColor = ConsoleColor.Yellow;\r\n            break;\r\n        case LogLevel.Info:\r\n            Console.ForegroundColor = ConsoleColor.White;\r\n            break;\r\n        case LogLevel.Debug:\r\n            Console.ForegroundColor = ConsoleColor.Gray;\r\n            break;\r\n    }\r\n\r\n    Console.WriteLine($\"[{level}] {message}\");\r\n    Console.ResetColor();\r\n}\r\n```\r\n\r\n## High-Performance Channel Mixing (`ChannelMixer`)\r\n\r\nThe `SoundFlow.Utils.ChannelMixer` is a static utility class designed for extremely efficient upmixing, downmixing, and channel interleaving of raw audio buffers.\r\n\r\nUnlike simple `for` loops, this class leverages **SIMD (Single Instruction, Multiple Data)** instructions—specifically **AVX** and **SSE**—to process multiple samples in parallel. This is critical for performance when handling multi-channel surround sound or processing large blocks of audio data.\r\n\r\n### Key Features\r\n\r\n*   **Automatic SIMD Optimization:** Automatically detects CPU capabilities and uses AVX2, AVX, SSE3, or SSE paths where available.\r\n*   **Upmixing:** Intelligently distributes mono signals to stereo or multi-channel buffers.\r\n*   **Downmixing:** Sums multi-channel signals down to mono.\r\n*   **General Mixing:** Handles arbitrary channel count conversions (e.g., 5.1 Surround to Stereo).\r\n\r\n### API Reference\r\n\r\n#### `Mix`\r\n```csharp\r\npublic static float[] Mix(float[] samples, int sourceChannels, int targetChannels)\r\n```\r\nCreates a *new* float array containing the mixed audio.\r\n*   **samples**: The source audio buffer (interleaved).\r\n*   **sourceChannels**: The number of channels in the source buffer.\r\n*   **targetChannels**: The desired number of channels in the output.\r\n\r\n#### Configuration Properties\r\nYou can manually disable specific SIMD instruction sets if necessary (e.g., for benchmarking or compatibility testing). Defaults to `true`.\r\n*   `public static bool EnableAvx { get; set; }`\r\n*   `public static bool EnableSse { get; set; }`\r\n\r\n### Usage Example\r\n\r\n```csharp\r\nusing SoundFlow.Utils;\r\n\r\n// Imagine we have a buffer of 5.1 Surround Sound audio (6 channels).\r\n// 100 frames * 6 channels = 600 samples.\r\nfloat[] surroundBuffer = GetSurroundData();\r\nint sourceChannels = 6;\r\n\r\n// We want to downmix this to Stereo (2 channels) for headphone playback.\r\nint targetChannels = 2;\r\n\r\ntry\r\n{\r\n    // Perform the mix. This will use AVX/SSE acceleration to sum\r\n    // the channels efficiently.\r\n    float[] stereoBuffer = ChannelMixer.Mix(surroundBuffer, sourceChannels, targetChannels);\r\n\r\n    Console.WriteLine($\"Input size: {surroundBuffer.Length}\"); // 600\r\n    Console.WriteLine($\"Output size: {stereoBuffer.Length}\");  // 200 (100 frames * 2 channels)\r\n}\r\ncatch (ArgumentException ex)\r\n{\r\n    // Thrown if the input buffer length is not a multiple of sourceChannels\r\n    Console.WriteLine($\"Mixing error: {ex.Message}\");\r\n}\r\n```\r\n\r\n## Audio Math Helpers (`MathHelper`)\r\n\r\nThe `SoundFlow.Utils.MathHelper` class exposes the optimized mathematical functions used internally by the engine's DSP components. These are useful if you are writing your own custom `SoundModifier` or `AudioAnalyzer`.\r\n\r\n### Fast Fourier Transform (FFT)\r\n\r\nSoundFlow includes a highly optimized, SIMD-accelerated FFT implementation.\r\n\r\n*   **`void Fft(Complex[] data)`**: Performs an in-place Fast Fourier Transform. The input length must be a power of two.\r\n*   **`void InverseFft(Complex[] data)`**: Performs an in-place Inverse Fast Fourier Transform (IFFT).\r\n\r\n### Offline Resampling\r\n\r\nWhile the `ResamplerModifier` is for real-time processing, `MathHelper` provides a utility for \"one-shot\" offline resampling of arrays. This affects both pitch and speed (varispeed).\r\n\r\n*   **`float[] ResampleLinear(float[] inputData, int channels, int sourceRate, int targetRate)`**\r\n*   Creates a new array with the audio resampled to the target rate using linear interpolation.\r\n*   This is useful for converting loaded assets to the engine's sample rate before playback to avoid real-time resampling overhead.\r\n\r\n```csharp\r\nusing SoundFlow.Utils;\r\n\r\n// Load raw audio data (e.g., 44.1kHz)\r\nfloat[] originalAudio = LoadAudioData();\r\nint originalRate = 44100;\r\n\r\n// The engine is running at 48kHz. Resample the asset upfront.\r\nint targetRate = 48000;\r\nint channels = 2;\r\n\r\nfloat[] readyToPlayAudio = MathHelper.ResampleLinear(originalAudio, channels, originalRate, targetRate);\r\n```\r\n\r\n### Windowing Functions\r\n\r\nUseful for spectral analysis (FFT) to reduce spectral leakage.\r\n*   **`float[] HammingWindow(int size)`**\r\n*   **`float[] HanningWindow(int size)`**\r\n\r\n## Format Helper Extensions\r\n\r\nThe `AudioFormat` struct includes a static helper utility that wraps the `SoundMetadataReader` to quickly infer format information from a raw stream. This is useful when initializing devices or providers without manually configuring the `AudioFormat`.\r\n\r\n```csharp\r\nusing SoundFlow.Structs;\r\nusing System.IO;\r\n\r\n// You have a stream, but you don't know if it's Wav, Mp3, or Flac,\r\n// and you don't know the sample rate.\r\nusing var fileStream = File.OpenRead(\"mystery_file.audio\");\r\n\r\n// AudioFormat.GetFormatFromStream inspects the header bytes.\r\nAudioFormat? format = AudioFormat.GetFormatFromStream(fileStream);\r\n\r\nif (format.HasValue)\r\n{\r\n    // Format detected!\r\n    Console.WriteLine($\"Detected: {format.Value.SampleRate}Hz, {format.Value.Channels} Channels\");\r\n\r\n    // You can now safely use this to initialize a device\r\n    // engine.InitializePlaybackDevice(null, format.Value);\r\n}\r\nelse\r\n{\r\n    Console.WriteLine(\"Could not determine audio format.\");\r\n}\r\n```"
  },
  {
    "id": 8,
    "slug": "tutorials-recording",
    "version": "1.3.0",
    "title": "Recording Audio",
    "description": "Comprehensive tutorials for recording audio from devices and processing it in real-time with SoundFlow.",
    "navOrder": 8,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 8\r\ntitle: Recording Audio\r\ndescription: Comprehensive tutorials for recording audio from devices and processing it in real-time with SoundFlow.\r\nnavOrder: 8\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Recording with SoundFlow\r\n\r\nWelcome to the SoundFlow audio recording tutorials! This guide will walk you through the essential steps to integrate audio recording capabilities into your .NET applications using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to record from the default device, perform custom real-time audio processing, or monitor microphone input, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Recording tutorials\">\r\n    <Tab key=\"basic-recording\" title=\"Basic Recording\">\r\n        This tutorial demonstrates how to record audio from the default recording device and save it to a WAV\r\n        file.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o BasicRecording\r\n                cd BasicRecording\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the basic recorder\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace BasicRecording;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default capture (recording) device.\r\n                        var defaultCaptureDevice = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultCaptureDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default capture device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define the audio format for recording. The backend will capture in this format.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 1 // Mono recording\r\n                        };\r\n                        \r\n                        // Initialize the capture device.\r\n                        using var device = audioEngine.InitializeCaptureDevice(defaultCaptureDevice, audioFormat);\r\n                        \r\n                        // Set up the output file stream.\r\n                        string outputFilePath = Path.Combine(Directory.GetCurrentDirectory(), \"output.wav\");\r\n                        using var fileStream = new FileStream(outputFilePath, FileMode.Create, FileAccess.Write, FileShare.None);\r\n                        \r\n                        // Create a recorder, linking it to the capture device and the output stream.\r\n                        using var recorder = new Recorder(device, fileStream, EncodingFormat.Wav);\r\n\r\n                        Console.WriteLine(\"Recording... Press any key to stop.\");\r\n                        device.Start(); // Start the device to begin capturing data.\r\n                        recorder.StartRecording(); // Start the recorder to begin encoding and writing.\r\n                        \r\n                        Console.ReadKey();\r\n                        \r\n                        recorder.StopRecording();\r\n                        device.Stop();\r\n\r\n                        Console.WriteLine($\"Recording stopped. Saved to {outputFilePath}\");\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        First, an `AudioEngine` is initialized. We find the `default capture device` from the engine's list of available recording devices. An `AudioFormat` is specified for the recording session (e.g., 48kHz, mono, 32-bit float).\r\n\r\n        The default capture device is then initialized with this format, creating an `AudioCaptureDevice`. This `device` instance is now the source of our audio data.\r\n\r\n        A `Recorder` is created, taking the `device` and an output `FileStream` as arguments. The recorder listens to the audio data processed by the device.\r\n\r\n        To begin, `device.Start()` is called to activate the hardware, and `recorder.StartRecording()` is called to start encoding the incoming audio from the device and writing it to the specified WAV file. After the user presses a key, both the recorder and the device are stopped, and all resources are automatically disposed by their `using` statements.\r\n    </Tab>\r\n\r\n    <Tab key=\"custom-processing\" title=\"Custom Processing\">\r\n        This tutorial demonstrates using a callback to process recorded audio in real-time.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o CustomProcessing\r\n                cd CustomProcessing\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement real-time processing\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n\r\n                namespace CustomProcessing;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultCaptureDevice = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultCaptureDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default capture device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 1\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializeCaptureDevice(defaultCaptureDevice, audioFormat);\r\n                        \r\n                        // Create a recorder that uses a callback instead of a file stream.\r\n                        using var recorder = new Recorder(device, ProcessAudio);\r\n\r\n                        Console.WriteLine(\"Recording with custom processing... Press any key to stop.\");\r\n                        device.Start();\r\n                        recorder.StartRecording();\r\n                        \r\n                        Console.ReadKey();\r\n                        \r\n                        recorder.StopRecording();\r\n                        device.Stop();\r\n                        Console.WriteLine(\"Recording stopped.\");\r\n                    }\r\n\r\n                    // This method will be called for each chunk of recorded audio.\r\n                    private static void ProcessAudio(Span<float> samples, Capability capability)\r\n                    {\r\n                        // Perform custom processing on the audio samples.\r\n                        // For example, calculate the average level:\r\n                        float sum = 0;\r\n                        for (int i = 0; i < samples.Length; i++)\r\n                        {\r\n                            sum += Math.Abs(samples[i]);\r\n                        }\r\n                        float averageLevel = sum / samples.Length;\r\n\r\n                        Console.WriteLine($\"Average level: {averageLevel:F4}\");\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This example demonstrates how to process audio data in real-time as it's being recorded. After the standard setup of initializing the `AudioEngine` and the default `AudioCaptureDevice`, a `Recorder` is created.\r\n\r\n        Instead of providing a file stream, this `Recorder` is given a callback method, `ProcessAudio`. The `Recorder` subscribes to the `device`'s `OnAudioProcessed` event. When the recorder is started, it will invoke our `ProcessAudio` method every time the device provides a new chunk of audio data.\r\n\r\n        The `ProcessAudio` method receives a `Span<float>` containing the latest audio samples, allowing for immediate analysis or processing, such as calculating the average level shown in this example. This approach is ideal for applications that need to react to live audio input without writing to a file, like voice activity detection, real-time visualizations, or triggering events based on sound.\r\n    </Tab>\r\n\r\n    <Tab key=\"mic-playback\" title=\"Mic Playback (Monitor)\">\r\n        This tutorial demonstrates capturing microphone audio and playing it back in real-time.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o MicrophonePlayback\r\n                cd MicrophonePlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement microphone loopback\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n\r\n                namespace MicrophonePlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback and capture devices.\r\n                        var defaultPlayback = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        var defaultCapture = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlayback.Id == IntPtr.Zero || defaultCapture.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"Default playback and/or capture device not found.\");\r\n                            return;\r\n                        }\r\n                        \r\n                        // Define a common audio format for both input and output.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2 // Stereo for playback\r\n                        };\r\n\r\n                        // Use FullDuplexDevice for simplified simultaneous input and output.\r\n                        using var fullDuplexDevice = audioEngine.InitializeFullDuplexDevice(defaultPlayback, defaultCapture, audioFormat);\r\n                        \r\n                        // Create a data provider that reads from the microphone.\r\n                        // It subscribes to the capture device's audio events.\r\n                        using var microphoneDataProvider = new MicrophoneDataProvider(fullDuplexDevice.CaptureDevice);\r\n                        \r\n                        // Create a SoundPlayer to play back the microphone data.\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, microphoneDataProvider);\r\n\r\n                        // Add the player to the playback device's master mixer.\r\n                        fullDuplexDevice.PlaybackDevice.MasterMixer.AddComponent(player);\r\n\r\n                        // Start capturing and playing.\r\n                        fullDuplexDevice.Start();\r\n                        microphoneDataProvider.StartCapture();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing live microphone audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop everything.\r\n                        fullDuplexDevice.Stop();\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This example creates a real-time microphone monitoring system. The key component is the `FullDuplexDevice`, a high-level abstraction that simplifies managing simultaneous input and output.\r\n\r\n        After initializing the `AudioEngine`, we find the default playback and capture devices and define a common `AudioFormat`. `audioEngine.InitializeFullDuplexDevice` is then called to create and manage a paired `AudioCaptureDevice` and `AudioPlaybackDevice`.\r\n\r\n        A `MicrophoneDataProvider` is created and linked to the capture part of the full duplex device (`fullDuplexDevice.CaptureDevice`). This provider listens for incoming audio data.\r\n\r\n        A `SoundPlayer` is instantiated with the `MicrophoneDataProvider` as its source. This player is then added to the `MasterMixer` of the playback part of the full duplex device (`fullDuplexDevice.PlaybackDevice.MasterMixer`).\r\n\r\n        Starting the `fullDuplexDevice` activates both the capture and playback hardware streams. `microphoneDataProvider.StartCapture()` begins queuing the incoming audio, and `player.Play()` starts reading from that queue and sending the audio to the output, creating a live monitoring effect. All resources are managed with `using` statements for automatic cleanup.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for recording audio with SoundFlow!\r\nNext, explore using audio effects with SoundFlow in our [Modifiers Tutorial](./tutorials-modifiers)."
  },
  {
    "id": 7,
    "slug": "tutorials-playback",
    "version": "1.3.0",
    "title": "Playback Fundamentals",
    "description": "Comprehensive tutorials for playing audio files, streams, and controlling playback with the SoundFlow C# audio library.",
    "navOrder": 7,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 7\r\ntitle: Playback Fundamentals\r\ndescription: Comprehensive tutorials for playing audio files, streams, and controlling playback with the SoundFlow C# audio library.\r\nnavOrder: 7\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Playback with SoundFlow\r\n\r\nWelcome to the SoundFlow audio playback tutorials! This guide will walk you through the essential steps to integrate audio playback into your .NET applications using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to play local files, stream from the web, control playback dynamics, implement looping, experiment with surround sound, or efficiently handle large audio assets, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Playback tutorials\">\r\n    <Tab key=\"basic-playback\" title=\"Basic Playback\">\r\n        This tutorial demonstrates how to play an audio file from disk using `SoundPlayer` and\r\n        `StreamDataProvider`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application:\r\n                ```bash\r\n                dotnet new console -o BasicPlayback\r\n                cd BasicPlayback\r\n                ```\r\n            </Step>\r\n            <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                ### 2. Install the SoundFlow NuGet package:\r\n                ```bash\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the basic player\" icon='ph:code-bold'>\r\n                ### 3. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace BasicPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine with the MiniAudio backend.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback device.\r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // The audio format for processing. We'll use 32-bit float, which is standard for processing.\r\n                        // The data provider will handle decoding the source file to this format.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        // Initialize the playback device. This manages the connection to the physical audio hardware.\r\n                        // The 'using' statement ensures it's properly disposed of.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a data provider for the audio file.\r\n                        // Replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n\r\n                        // Create a SoundPlayer, linking the engine, format, and data provider.\r\n                        // The player is also IDisposable.\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Add the player to the device's master mixer to route its audio for playback.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device. This opens the audio stream to the hardware.\r\n                        device.Start();\r\n\r\n                        // Start playback.\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until playback finishes or the user presses a key.\r\n                        Console.WriteLine(\"Playing audio... Press any key to exit.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop the device. This stops the audio stream.\r\n                        device.Stop();\r\n\r\n                        // The `using` statements for `audioEngine`, `device`, `dataProvider`, and `player`\r\n                        // will automatically handle disposal and resource cleanup.\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file on your\r\n                computer.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 4. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        First, a `MiniAudioEngine` is initialized to manage audio operations. We then identify the `default playback device` from the engine's available devices. An `AudioFormat` struct is defined, specifying the desired internal processing format (e.g., 32-bit float, 48kHz sample rate, 2 channels).\r\n\r\n        The chosen device is then initialized using `audioEngine.InitializePlaybackDevice`, creating an `AudioPlaybackDevice`. This device handles the interaction with the audio hardware and exposes a `MasterMixer` where `SoundFlow` components can be added.\r\n\r\n        A `StreamDataProvider` is created to load the audio file. Crucially, both `StreamDataProvider` and `SoundPlayer` are now instantiated with `audioEngine` and `audioFormat` to provide them with the necessary context for decoding and processing audio in the specified format. The `player` is added to the `device.MasterMixer`.\r\n\r\n        Finally, `device.Start()` is called to open the audio stream to the hardware, enabling sound output. The `player.Play()` then begins playback. The program waits for user input, after which the `using` statements automatically handle the proper disposal and cleanup of all allocated resources (`audioEngine`, `device`, `dataProvider`, and `player`).\r\n    </Tab>\r\n\r\n    <Tab key=\"web-playback\" title=\"Web Playback\">\r\n        This tutorial demonstrates how to play an audio stream from a URL using `SoundPlayer` and\r\n        `NetworkDataProvider`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application:\r\n                ```bash\r\n                dotnet new console -o WebPlayback\r\n                cd WebPlayback\r\n                ```\r\n            </Step>\r\n            <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                ### 2. Install the SoundFlow NuGet package:\r\n                ```bash\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the network player\" icon='ph:code-bold'>\r\n                ### 3. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n                using System.Threading.Tasks;\r\n\r\n                namespace WebPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static async Task Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback device.\r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define the audio format for processing.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        // Initialize the default playback device.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a NetworkDataProvider. Replace \"your-audio-stream-url\"\r\n                        // with the actual URL (direct audio file or HLS .m3u8 playlist).\r\n                        using var dataProvider = new NetworkDataProvider(audioEngine, audioFormat, \"your-audio-stream-url\");\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing stream... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop the device.\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"your-audio-stream-url\"` with the actual URL of an audio stream (e.g., direct\r\n                MP3/WAV or an HLS .m3u8 playlist).***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 4. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example follows the same foundational setup as the basic playback tutorial: initializing an `AudioEngine`, identifying a `default playback device`, defining an `AudioFormat`, and initializing the `AudioPlaybackDevice`.\r\n\r\n        The key distinction lies in using `NetworkDataProvider` instead of `StreamDataProvider`. `NetworkDataProvider` is designed to stream and decode audio directly from a web URL. Like other data providers and players in the new API, its constructor now requires both the `audioEngine` and `audioFormat`. It intelligently handles various web audio sources, including direct MP3/WAV files and HLS `.m3u8` playlists.\r\n\r\n        After the `SoundPlayer` is created and added to the `device.MasterMixer`, `device.Start()` and `player.Play()` initiate the audio stream. The `using` statements ensure all resources are properly managed and disposed upon program exit.\r\n    </Tab>\r\n\r\n    <Tab key=\"playback-control\" title=\"Playback Control\">\r\n        This tutorial demonstrates how to control audio playback using `Play`, `Pause`, `Stop`, `Seek`, and\r\n        `PlaybackSpeed`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o PlaybackControl\r\n                cd PlaybackControl\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the interactive player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace PlaybackControl;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider) { Volume = 0.8f }; // Example: set initial volume\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n                        Console.WriteLine(\"Playing audio... (p: pause/play, s: seek, +/-: speed, v/m: volume, any other: stop)\");\r\n\r\n                        // Handle user input for playback control.\r\n                        while (player.State != PlaybackState.Stopped)\r\n                        {\r\n                            var keyInfo = Console.ReadKey(true);\r\n                            switch (keyInfo.Key)\r\n                            {\r\n                                case ConsoleKey.P:\r\n                                    if (player.State == PlaybackState.Playing)\r\n                                        player.Pause();\r\n                                    else\r\n                                        player.Play();\r\n                                    Console.WriteLine(player.State == PlaybackState.Paused ? \"Paused\" : \"Playing\");\r\n                                    break;\r\n                                case ConsoleKey.S:\r\n                                    Console.Write(\"Enter seek time (in seconds, e.g., 10.5): \");\r\n                                    if (float.TryParse(Console.ReadLine(), out var seekTimeSeconds))\r\n                                    {\r\n                                        if (player.Seek(TimeSpan.FromSeconds(seekTimeSeconds)))\r\n                                            Console.WriteLine($\"Seeked to {seekTimeSeconds:F1}s. Current time: {player.Time:F1}s\");\r\n                                        else\r\n                                            Console.WriteLine(\"Seek failed.\");\r\n                                    }\r\n                                    else\r\n                                        Console.WriteLine(\"Invalid seek time.\");\r\n                                    break;\r\n                                case ConsoleKey.OemPlus:\r\n                                case ConsoleKey.Add:\r\n                                    player.PlaybackSpeed = Math.Min(2.0f, player.PlaybackSpeed + 0.1f);\r\n                                    Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                case ConsoleKey.OemMinus:\r\n                                case ConsoleKey.Subtract:\r\n                                    player.PlaybackSpeed = Math.Max(0.1f, player.PlaybackSpeed - 0.1f);\r\n                                    Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                case ConsoleKey.V:\r\n                                    player.Volume = Math.Min(1.5f, player.Volume + 0.1f); // Allow gain up to 150%\r\n                                    Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                case ConsoleKey.M:\r\n                                    player.Volume = Math.Max(0.0f, player.Volume - 0.1f);\r\n                                    Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                default:\r\n                                    player.Stop();\r\n                                    Console.WriteLine(\"Stopped\");\r\n                                    break;\r\n                            }\r\n                        }\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates interactive control over a `SoundPlayer` by extending the basic playback setup. After initializing the `AudioEngine`, selecting the `default playback device`, defining the `AudioFormat`, and initializing the `AudioPlaybackDevice`, a `StreamDataProvider` loads the audio file, and a `SoundPlayer` is instantiated (again, with `audioEngine` and `audioFormat`). The player is then added to the `device.MasterMixer`.\r\n\r\n        Once `device.Start()` and `player.Play()` are called, the application enters a loop to process user input from the console. The `SoundFlow` API provides intuitive properties and methods for common playback controls:\r\n        *   `P` toggles between `Play()` and `Pause()`.\r\n        *   `S` prompts for a time and calls `player.Seek(TimeSpan)`. The `Seek` method returns a boolean, indicating if the seek operation was successful (which depends on the underlying data provider's `CanSeek` capability).\r\n        *   `+` and `-` keys adjust the `player.PlaybackSpeed` property.\r\n        *   `V` and `M` keys control the `player.Volume` property, allowing for dynamic gain adjustment.\r\n        *   Any other key press calls `player.Stop()`, terminating playback and exiting the loop.\r\n\r\n        The `using` statements guarantee all resources are correctly released when the program concludes.\r\n    </Tab>\r\n\r\n    <Tab key=\"looping\" title=\"Looping\">\r\n        This tutorial demonstrates how to enable looping for a `SoundPlayer` and how to set custom loop points.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LoopingPlayback\r\n                cd LoopingPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the looping player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LoopingPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Enable looping.\r\n                        player.IsLooping = true;\r\n\r\n                        // Optional: Set custom loop points\r\n\r\n                        // Example 1: Loop from 2.5 seconds to 7.0 seconds (using float seconds)\r\n                        // player.SetLoopPoints(2.5f, 7.0f);\r\n\r\n                        // Example 2: Loop from sample 110250 to sample 308700 (using samples)\r\n                        // player.SetLoopPoints(110250, 308700); // Assuming 44.1kHz stereo, these are example values\r\n\r\n                        // Example 3: Loop from 1.5 seconds to the natural end of the audio (using TimeSpan, end point is optional)\r\n                        player.SetLoopPoints(TimeSpan.FromSeconds(1.5));\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio in a loop... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code extends the fundamental playback example to showcase audio looping capabilities. After the standard initialization of the `AudioEngine`, `AudioPlaybackDevice`, and `SoundPlayer` (including passing `audioEngine` and `audioFormat` to the data provider and player), the player's looping behavior is configured:\r\n        *   `player.IsLooping = true;`: This boolean property is the primary control to enable continuous looping. When set to `true`, upon reaching the end of the audio (or the defined loop end point), the player will automatically reset its position to the loop start point and continue playing.\r\n        *   `player.SetLoopPoints(...)`:: This method offers precise control over the section of audio that will loop. It provides multiple overloads, allowing you to specify the loop start and end points using `float` seconds, `int` samples, or `TimeSpan` values, catering to different precision requirements. If the `endTime` (or equivalent) parameter is omitted or set to a default indicating \"to end,\" the loop will extend to the natural conclusion of the audio data.\r\n\r\n        The player is added to the `device.MasterMixer`, and after starting the `device` and `player`, the application enters a wait state until the user presses a key, after which resources are automatically cleaned up.\r\n    </Tab>\r\n\r\n    <Tab key=\"surround-sound\" title=\"Surround Sound\">\r\n        This tutorial demonstrates how to use `SurroundPlayer` to play audio with surround sound configurations.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SurroundPlayback\r\n                cd SurroundPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the surround player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n                using System.Numerics;\r\n\r\n                namespace SurroundPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define an audio format with 8 channels for 7.1 surround sound.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 8\r\n                        };\r\n\r\n                        // Initialize the playback device with the 8-channel format.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a SurroundPlayer. It will upmix mono/stereo sources.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SurroundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Configure the SurroundPlayer for 7.1 surround sound.\r\n                        player.SpeakerConfig = SurroundPlayer.SpeakerConfiguration.Surround71;\r\n\r\n                        // Set the panning method (VBAP is often good for surround).\r\n                        player.Panning = SurroundPlayer.PanningMethod.Vbap;\r\n\r\n                        // Set the listener position (optional, (0,0) is center).\r\n                        player.ListenerPosition = new Vector2(0, 0);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing surround sound audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file (mono, stereo,\r\n                or multi-channel).***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example illustrates how to configure `SoundFlow` for surround sound playback.\r\n\r\n        The crucial first step, after initializing the `AudioEngine`, is to define an `AudioFormat` that specifies the desired number of output channels for your surround setup (e.g., 8 channels for 7.1 surround sound). The `AudioPlaybackDevice` is then initialized using this multi-channel `AudioFormat`, ensuring the audio hardware is configured to output to all relevant speakers.\r\n\r\n        Instead of `SoundPlayer`, a `SurroundPlayer` is instantiated, taking the `audioEngine`, `audioFormat`, and `dataProvider` as arguments. The `SurroundPlayer` is specialized for spatial audio. If the input `audiofile.wav` is mono or stereo, the `SurroundPlayer` will intelligently upmix and pan the audio across the configured speaker layout.\r\n\r\n        Key properties for configuring surround playback include:\r\n        *   `player.SpeakerConfig`: Set this to one of the predefined speaker layouts (e.g., `SurroundPlayer.SpeakerConfiguration.Surround71`).\r\n        *   `player.Panning`: Choose a panning algorithm, such as `SurroundPlayer.PanningMethod.Vbap` (Vector-Based Amplitude Panning), which often provides excellent spatialization for surround sound.\r\n        *   `player.ListenerPosition`: (Optional) Adjust the virtual listener's position within the soundfield using a `Vector2`.\r\n\r\n        The `SurroundPlayer` is added to the `device.MasterMixer`, and after `device.Start()` and `player.Play()`, the application will output sound through the configured surround channels. Resources are automatically managed by `using` statements.\r\n    </Tab>\r\n\r\n    <Tab key=\"chunked-data\" title=\"Chunked Data\">\r\n        This tutorial demonstrates how to use the `ChunkedDataProvider` for efficient playback of large audio\r\n        files.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ChunkedPlayback\r\n                cd ChunkedPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the chunked data player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ChunkedPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a ChunkedDataProvider and load a large audio file.\r\n                        // Replace \"path/to/your/large/audiofile.wav\" with the actual path.\r\n                        using var dataProvider = new ChunkedDataProvider(audioEngine, audioFormat, \"path/to/your/large/audiofile.wav\");\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio with ChunkedDataProvider... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/large/audiofile.wav\"` with the path to a large audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This tutorial showcases the `ChunkedDataProvider`, a specialized data provider designed for efficient playback of very large audio files. Unlike `StreamDataProvider` or `AssetDataProvider` which might load an entire file into memory or process it sequentially without optimized chunking, `ChunkedDataProvider` reads and decodes audio data in smaller, manageable chunks. This significantly reduces memory footprint and improves responsiveness, especially for long recordings or high-resolution audio.\r\n\r\n        The setup follows the familiar pattern: initialize `AudioEngine`, select `default playback device`, define `AudioFormat`, and initialize `AudioPlaybackDevice`. The `ChunkedDataProvider` is then instantiated, requiring `audioEngine`, `audioFormat`, and the path to the large audio file. A `SoundPlayer` is created with this provider, added to the `device.MasterMixer`, and playback begins.\r\n\r\n        The integration is seamless; simply swap `StreamDataProvider` (or `AssetDataProvider`) with `ChunkedDataProvider` in your setup. The underlying chunking mechanism works transparently, ensuring smooth playback while optimizing resource usage. All resources are released automatically via `using` statements.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for playing audio with SoundFlow!\r\nNext, explore recording audio with SoundFlow in our [Recording Audio Tutorial](./tutorials-recording)."
  },
  {
    "id": 9,
    "slug": "tutorials-modifiers",
    "version": "1.3.0",
    "title": "Audio Modifiers & Effects",
    "description": "Comprehensive tutorials for applying various audio effects and modifiers (reverb, EQ, compression, etc.) with SoundFlow.",
    "navOrder": 9,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 9\r\ntitle: Audio Modifiers & Effects\r\ndescription: Comprehensive tutorials for applying various audio effects and modifiers (reverb, EQ, compression, etc.) with SoundFlow.\r\nnavOrder: 9\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs, Card, CardBody} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# Applying Audio Modifiers & Effects with SoundFlow\r\n\r\nWelcome to the SoundFlow audio modifiers and effects tutorials! This guide will walk you through applying various digital audio effects to your playback streams using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to add reverb, equalize frequencies, compress dynamics, or mix multiple sources, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Modifier tutorials\">\r\n    <Tab key=\"reverb\" title=\"Reverb\">\r\n        Demonstrates how to apply a reverb effect using the `AlgorithmicReverbModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ReverbEffect\r\n                cd ReverbEffect\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with reverb\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ReverbEffect;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create an AlgorithmicReverbModifier. It now needs the audio format.\r\n                        var reverb = new AlgorithmicReverbModifier(audioFormat)\r\n                        {\r\n                            RoomSize = 0.8f,\r\n                            Damp = 0.5f,\r\n                            Wet = 0.3f, // Wet mix (0=dry, 1=fully wet)\r\n                            Width = 1f,\r\n                            PreDelay = 20f // Pre-delay in milliseconds\r\n                        };\r\n\r\n                        // Add the reverb modifier to the player.\r\n                        player.AddModifier(reverb);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with reverb... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        After initializing the `AudioEngine` and the `AudioPlaybackDevice`, an `AlgorithmicReverbModifier` is created. In the new API, the modifier's constructor requires an `AudioFormat` instance to correctly configure its internal delay lines and filters based on the sample rate and channel count. The `Wet` property now controls the blend between the original (dry) and processed (wet) signals, where 0 is fully dry and 1 is fully wet. We add this `reverb` modifier to the `player`, which is then added to the `device.MasterMixer` for playback. Experiment with `RoomSize`, `Damp`, `Wet`, `Width`, and `PreDelay` to shape the reverb's character.\r\n    </Tab>\r\n\r\n    <Tab key=\"vocal-extraction\" title=\"Vocal Extraction\">\r\n        Demonstrates how to isolate vocals from a mixed audio track using the `VocalExtractorModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o VocalExtractor\r\n                cd VocalExtractor\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with vocal extractor\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace VocalExtractor;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load a stereo music file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/song.mp3\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the VocalExtractorModifier.\r\n                        // We use a larger FFT size (4096) for better frequency resolution, which helps separate vocals from bass.\r\n                        // We set the frequency range to 100Hz - 10kHz to focus on the human voice.\r\n                        var vocalExtractor = new VocalExtractorModifier(\r\n                            sampleRate: audioFormat.SampleRate,\r\n                            minFrequency: 100f, \r\n                            maxFrequency: 10000f,\r\n                            fftSize: 4096,\r\n                            hopSize: 1024\r\n                        );\r\n\r\n                        // Add the modifier to the player.\r\n                        player.AddModifier(vocalExtractor);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio with Vocal Extraction.\");\r\n                        Console.WriteLine(\"Press 'T' to toggle the effect on/off. Press any other key to stop.\");\r\n\r\n                        // Simple loop to toggle the effect\r\n                        while (true)\r\n                        {\r\n                            var key = Console.ReadKey(true).Key;\r\n                            if (key == ConsoleKey.T)\r\n                            {\r\n                                vocalExtractor.Enabled = !vocalExtractor.Enabled;\r\n                                Console.WriteLine($\"Vocal Extractor: {(vocalExtractor.Enabled ? \"ON\" : \"OFF\")}\");\r\n                            }\r\n                            else\r\n                            {\r\n                                break;\r\n                            }\r\n                        }\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/song.mp3\"` with the actual path to a stereo music file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and toggle the effect\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the use of the `VocalExtractorModifier`. This advanced effect uses different algorithms depending on the input channels:\r\n        *   **Stereo:** It uses **Center-Channel Extraction (Mid-Side)**. Since vocals are typically mixed in the center and instruments are often panned to the sides, the modifier subtracts the side information from the mid signal, effectively removing panned instruments.\r\n        *   **Mono:** It falls back to **Spectral Gating** and **Bandpass Filtering**. It attenuates frequencies outside the vocal range (`MinFrequency` to `MaxFrequency`) and suppresses quiet frequency bands likely to be noise.\r\n\r\n        We instantiate the modifier with an `fftSize` of 4096. A larger FFT size provides better frequency resolution, which is crucial for distinguishing low-frequency vocal fundamentals from bass instruments. The loop allows you to toggle the effect on and off in real-time to hear the isolation result.\r\n    </Tab>\r\n\r\n    <Tab key=\"equalization\" title=\"Equalization\">\r\n        Demonstrates how to use the `ParametricEqualizer` to adjust frequency balance.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Equalization\r\n                cd Equalization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with EQ\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Collections.Generic;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Equalization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a ParametricEqualizer, which now requires the audio format.\r\n                        var equalizer = new ParametricEqualizer(audioFormat);\r\n\r\n                        // Add some equalizer bands.\r\n                        var bands = new List<EqualizerBand>\r\n                        {\r\n                            // Boost low frequencies (bass)\r\n                            new(FilterType.LowShelf, 100, 6, 0.7f),\r\n                            // Cut mid frequencies\r\n                            new(FilterType.Peaking, 1000, -4, 2f),\r\n                            // Boost high frequencies (treble)\r\n                            new(FilterType.HighShelf, 10000, 5, 0.7f)\r\n                        };\r\n                        equalizer.AddBands(bands);\r\n\r\n                        // Add the equalizer to the player.\r\n                        player.AddModifier(equalizer);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with equalization... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust bands\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        Following the new API structure, this code sets up the `AudioEngine` and an `AudioPlaybackDevice`. A `ParametricEqualizer` is instantiated, and like the reverb modifier, its constructor now requires an `AudioFormat` to correctly calculate filter coefficients based on the sample rate.\r\n        We define a `List<EqualizerBand>` to configure our EQ settings: a low-shelf filter to boost bass, a peaking filter to cut a specific mid-range frequency, and a high-shelf filter to boost treble. The `equalizer.AddBands()` method applies these settings. The `equalizer` is then added as a modifier to the `player`, which is subsequently added to the `device.MasterMixer`. You will hear the audio with the equalization applied. Experiment with different `FilterType` enums, frequencies, gain values, and Q factors to shape the sound to your liking.\r\n    </Tab>\r\n\r\n    <Tab key=\"chorus-delay\" title=\"Chorus & Delay\">\r\n        Demonstrates how to apply chorus and delay effects using `ChorusModifier` and `DelayModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ChorusDelay\r\n                cd ChorusDelay\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with effects\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ChorusDelay;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a ChorusModifier.\r\n                        var chorus = new ChorusModifier(\r\n                            audioFormat,\r\n                            depthMs: 2f,           // Depth (in milliseconds)\r\n                            rateHz: 0.8f,          // LFO Rate (in Hz)\r\n                            feedback: 0.5f,      // Feedback amount\r\n                            wetDryMix: 0.5f      // Wet/dry mix (0 = dry, 1 = wet)\r\n                        );\r\n\r\n                        // Create a DelayModifier.\r\n                        var delaySamples = (int)(audioFormat.SampleRate * 0.5); // 500ms delay\r\n                        var delay = new DelayModifier(\r\n                            audioFormat,\r\n                            delaySamples: delaySamples, \r\n                            feedback: 0.6f,      \r\n                            wetMix: 0.4f,       \r\n                            cutoff: 4000f \r\n                        );\r\n\r\n                        // Add the chorus and delay modifiers to the player.\r\n                        player.AddModifier(chorus);\r\n                        player.AddModifier(delay);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with chorus and delay... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust effects\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates chaining multiple effects. After the standard `AudioEngine` and `AudioPlaybackDevice` setup, we instantiate `ChorusModifier` and `DelayModifier`. Both modifiers now require an `AudioFormat` object in their constructors to configure internal buffers and timing calculations correctly based on the sample rate. The delay length for the `DelayModifier` is now specified in samples instead of milliseconds.\r\n        Both `chorus` and `delay` modifiers are added to the `player` instance. They will be processed in the order they were added: the audio will first pass through the chorus effect, and the output of the chorus will then be fed into the delay effect. This chain is then added to the `device.MasterMixer` for playback.\r\n    </Tab>\r\n\r\n    <Tab key=\"compression\" title=\"Compression\">\r\n        Demonstrates how to use the `CompressorModifier` to reduce the dynamic range of an audio stream.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Compression\r\n                cd Compression\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with compressor\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Compression;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a CompressorModifier.\r\n                        var compressor = new CompressorModifier(\r\n                            audioFormat,\r\n                            thresholdDb: -20f, // Threshold (in dB)\r\n                            ratio: 4f,        // Compression ratio\r\n                            attackMs: 10f,      // Attack time (in milliseconds)\r\n                            releaseMs: 100f,    // Release time (in milliseconds)\r\n                            kneeDb: 5f,         // Knee width (in dB)\r\n                            makeupGainDb: 6f   // Makeup gain (in dB)\r\n                        );\r\n\r\n                        // Add the compressor to the player.\r\n                        player.AddModifier(compressor);\r\n                        \r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n                        \r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with compression... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code showcases dynamic range compression. After initializing the `AudioEngine` and an `AudioPlaybackDevice`, a `CompressorModifier` is created. Its constructor now requires an `AudioFormat` to accurately time its attack and release stages based on the sample rate. The compressor is configured with a -20dB threshold, a 4:1 ratio, a soft knee of 5dB, and 6dB of makeup gain to compensate for the volume reduction. This modifier is then added to the `player`, which is routed to the `device.MasterMixer`. When played, the audio's dynamic range will be reduced, making quiet parts louder and loud parts quieter, resulting in a more consistent overall volume level.\r\n    </Tab>\r\n\r\n    <Tab key=\"resampler\" title=\"Real-time Resampling\">\r\n        Demonstrates how to use the `ResamplerModifier` to change playback speed and pitch in real-time.\r\n\r\n        <Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 mb-6\">\r\n            <CardBody>\r\n                <div className=\"flex items-center gap-3\">\r\n                    <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                    <div className=\"text-sm\">\r\n                        <strong>Varispeed Effect:</strong> Unlike the `TimeStretch` feature in `AudioSegment` (which preserves pitch), the `ResamplerModifier` links pitch and speed together.\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li>Factor > 1.0: Faster speed, higher pitch (Chipmunk effect).</li>\r\n                            <li>Factor < 1.0: Slower speed, lower pitch (Slow-motion effect).</li>\r\n                        </ul>\r\n                    </div>\r\n                </div>\r\n            </CardBody>\r\n        </Card>\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create project:\r\n                ```bash\r\n                dotnet new console -o ResamplerDemo\r\n                cd ResamplerDemo\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement variable speed player\" icon='ph:code-bold'>\r\n                ### 2. Code `Program.cs`:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        using var engine = new MiniAudioEngine();\r\n                        var format = AudioFormat.DvdHq;\r\n                        using var device = engine.InitializePlaybackDevice(null, format);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\r\n                        using var player = new SoundPlayer(engine, format, dataProvider);\r\n\r\n                        // Create the resampler. Start at normal speed (1.0).\r\n                        var resampler = new ResamplerModifier(1.0f);\r\n                        player.AddModifier(resampler);\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Control Speed/Pitch:\");\r\n                        Console.WriteLine(\"[+] Increase Speed\");\r\n                        Console.WriteLine(\"[-] Decrease Speed\");\r\n                        Console.WriteLine(\"[R] Reset to 1.0\");\r\n                        Console.WriteLine(\"[Q] Quit\");\r\n\r\n                        while (true)\r\n                        {\r\n                            var key = Console.ReadKey(true).Key;\r\n                            if (key == ConsoleKey.Q) break;\r\n\r\n                            switch (key)\r\n                            {\r\n                                case ConsoleKey.OemPlus:\r\n                                case ConsoleKey.Add:\r\n                                    resampler.ResampleFactor += 0.1f;\r\n                                    break;\r\n                                case ConsoleKey.OemMinus:\r\n                                case ConsoleKey.Subtract:\r\n                                    resampler.ResampleFactor = Math.Max(0.1f, resampler.ResampleFactor - 0.1f);\r\n                                    break;\r\n                                case ConsoleKey.R:\r\n                                    resampler.ResampleFactor = 1.0f;\r\n                                    break;\r\n                            }\r\n                            Console.WriteLine($\"Current Factor: {resampler.ResampleFactor:F1}x\");\r\n                        }\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n    </Tab>\r\n\r\n    <Tab key=\"mixing\" title=\"Mixing\">\r\n        Demonstrates how to use the `Mixer` to combine multiple audio sources.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Mixing\r\n                cd Mixing\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Combine multiple audio sources\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Mixing;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create two SoundPlayer instances and load different audio files.\r\n                        using var dataProvider1 = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile1.wav\"));\r\n                        using var player1 = new SoundPlayer(audioEngine, audioFormat, dataProvider1);\r\n\r\n                        using var dataProvider2 = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile2.wav\"));\r\n                        using var player2 = new SoundPlayer(audioEngine, audioFormat, dataProvider2);\r\n\r\n                        // Create an Oscillator that generates a sine wave.\r\n                        using var oscillator = new Oscillator(audioEngine, audioFormat)\r\n                        {\r\n                            Frequency = 440, // 440 Hz (A4 note)\r\n                            Amplitude = 0.5f,\r\n                            Type = Oscillator.WaveformType.Sine\r\n                        };\r\n\r\n                        // Add the players and the oscillator to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player1);\r\n                        device.MasterMixer.AddComponent(player2);\r\n                        device.MasterMixer.AddComponent(oscillator);\r\n\r\n                        // Start playback for both players.\r\n                        device.Start();\r\n                        player1.Play();\r\n                        player2.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing mixed audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile1.wav\"` and `\"path/to/your/audiofile2.wav\"` with the actual\r\n                paths to two different audio files.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust levels\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the core mixing capability of SoundFlow. After initializing the `AudioEngine` and an `AudioPlaybackDevice`, we create multiple `SoundComponent` instances: two `SoundPlayer`s for audio files and one `Oscillator` for a synthesized sine wave. Note that the `Oscillator` now also requires the `audioEngine` and `audioFormat` in its constructor.\r\n        Instead of a static master mixer, all components are added directly to the `device.MasterMixer`. The mixer automatically sums the audio output from all its added components. When the device is started, you will hear both audio files and the sine wave playing simultaneously, mixed together. You can control the individual contribution of each component to the mix by adjusting its `Volume` and `Pan` properties.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for applying audio effects with SoundFlow!\r\nNext, explore using audio analyzers with SoundFlow in our [Analysis Tutorial](./tutorials-analysis)."
  },
  {
    "id": 10,
    "slug": "tutorials-analysis",
    "version": "1.3.0",
    "title": "Audio Analysis & Visualization",
    "description": "Comprehensive tutorials for analyzing audio streams (level, spectrum, VAD) and visualizing data with SoundFlow.",
    "navOrder": 10,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 10\r\ntitle: Audio Analysis & Visualization\r\ndescription: Comprehensive tutorials for analyzing audio streams (level, spectrum, VAD) and visualizing data with SoundFlow.\r\nnavOrder: 10\r\ncategory: Tutorials and Examples\r\n---\r\nimport { Icon } from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Analysis & Visualization with SoundFlow\r\n\r\nWelcome to the SoundFlow audio analysis and visualization tutorials! This guide will walk you through extracting valuable data from audio streams, such as level information, frequency spectrum, and voice activity, as well as visualizing this data, using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're building a custom meter, a real-time spectrum display, or a smart voice assistant, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Audio Analysis and Visualization tutorials\">\r\n    <Tab key=\"level-metering\" title=\"Level Metering\">\r\n        This tutorial demonstrates how to use the `LevelMeterAnalyzer` to measure the RMS and peak levels of an\r\n        audio stream.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LevelMetering\r\n                cd LevelMetering\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LevelMetering;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a LevelMeterAnalyzer, passing the audio format.\r\n                        var levelMeter = new LevelMeterAnalyzer(audioFormat);\r\n\r\n                        // Attach the analyzer to the player.\r\n                        player.AddAnalyzer(levelMeter);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Create a timer to periodically display the RMS and peak levels.\r\n                        var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            Console.WriteLine($\"RMS Level: {levelMeter.Rms:F4}, Peak Level: {levelMeter.Peak:F4}\");\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop playback and clean up.\r\n                        timer.Stop();\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        After setting up the `AudioEngine` and initializing an `AudioPlaybackDevice`, a `SoundPlayer` is created. A `LevelMeterAnalyzer` is then instantiated, requiring an `AudioFormat` in its constructor. The analyzer is attached directly to the `player` using `player.AddAnalyzer(levelMeter)`.\r\n        When the `player` processes audio, it automatically passes its audio data to the attached `levelMeter`. A timer then periodically reads the `Rms` and `Peak` properties from the analyzer and displays them in the console, providing a real-time level readout.\r\n    </Tab>\r\n\r\n    <Tab key=\"spectrum-analysis\" title=\"Spectrum Analysis\">\r\n        This tutorial demonstrates how to use the `SpectrumAnalyzer` to analyze frequency content using FFT.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SpectrumAnalysis\r\n                cd SpectrumAnalysis\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace SpectrumAnalysis;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                        var spectrumAnalyzer = new SpectrumAnalyzer(audioFormat, fftSize: 2048);\r\n\r\n                        // Attach the spectrum analyzer to the player.\r\n                        player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Create a timer to periodically display the spectrum data.\r\n                        var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            // Get the spectrum data from the analyzer.\r\n                            var spectrumData = spectrumAnalyzer.SpectrumData;\r\n\r\n                            // Print the magnitude of the first few frequency bins.\r\n                            if (spectrumData.Length > 0)\r\n                            {\r\n                                Console.Write(\"Spectrum: \");\r\n                                for (int i = 0; i < Math.Min(10, spectrumData.Length); i++)\r\n                                {\r\n                                    Console.Write($\"{spectrumData[i]:F2} \");\r\n                                }\r\n                                Console.WriteLine();\r\n                            }\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying spectrum data... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop playback and clean up.\r\n                        timer.Stop();\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code follows a similar pattern to level metering. A `SpectrumAnalyzer` is created, requiring the `audioFormat` and an `fftSize` (which must be a power of two). It is then attached to the `player` using `AddAnalyzer`. As the `player` processes audio, it feeds the data to the `spectrumAnalyzer`, which performs a Fast Fourier Transform (FFT).\r\n        A timer periodically accesses the `spectrumAnalyzer.SpectrumData` property, which contains the magnitudes of the frequency bins, and prints the first few values to the console for a simple real-time display of the frequency content.\r\n    </Tab>\r\n\r\n    <Tab key=\"vad\" title=\"Voice Activity Detection\">\r\n        This tutorial demonstrates how to use the `VoiceActivityDetector` to detect the presence of human voice.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o VoiceActivityDetection\r\n                cd VoiceActivityDetection\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement VAD on a source\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace VoiceActivityDetection;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file with speech.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/speechfile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a VoiceActivityDetector.\r\n                        var vad = new VoiceActivityDetector(audioFormat);\r\n\r\n                        // Attach the VAD as an analyzer to the player's output.\r\n                        player.AddAnalyzer(vad);\r\n\r\n                        // Subscribe to the SpeechDetected event.\r\n                        vad.SpeechDetected += isDetected => Console.WriteLine($\"Speech detected: {isDetected}\");\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Analyzing audio for voice activity... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/speechfile.wav\"` with the path to an audio file containing speech.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates how to detect voice in an audio stream. After initializing the `AudioEngine`, `AudioPlaybackDevice`, and `SoundPlayer`, a `VoiceActivityDetector` is created, passing the `audioFormat` to its constructor. It is then attached to the `player` with `AddAnalyzer`.\r\n        The key part of this example is subscribing to the `vad.SpeechDetected` event. This event fires whenever the VAD's state changes (from silence to speech, or vice versa), providing a boolean value. The event handler simply prints the new state to the console. This event-driven approach is efficient for building applications that need to react to the presence or absence of speech.\r\n    </Tab>\r\n\r\n    <Tab key=\"level-meter-viz\" title=\"Console Level Meter\">\r\n        Demonstrates creating a console-based level meter using the `LevelMeterAnalyzer` and\r\n        `LevelMeterVisualizer`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LevelMeterVisualization\r\n                cd LevelMeterVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Interfaces;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LevelMeterVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard engine and device setup.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a player for an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the LevelMeterAnalyzer and LevelMeterVisualizer.\r\n                        // The visualizer is linked to the analyzer.\r\n                        var levelMeterAnalyzer = new LevelMeterAnalyzer(audioFormat);\r\n                        var levelMeterVisualizer = new LevelMeterVisualizer(levelMeterAnalyzer);\r\n                        \r\n                        // Attach the analyzer to the player. The analyzer will automatically\r\n                        // pass its data to the linked visualizer.\r\n                        player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                        // Add the player to the mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        levelMeterVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawLevelMeter(levelMeterAnalyzer.Rms, levelMeterAnalyzer.Peak);\r\n                        };\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            levelMeterVisualizer.ProcessOnAudioData(System.Array.Empty<float>());\r\n                            levelMeterVisualizer.Render(new ConsoleVisualizationContext());\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        levelMeterVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based level meter.\r\n                    private static void DrawLevelMeter(float rms, float peak)\r\n                    {\r\n                        int barLength = (int)(rms * 40); \r\n                        int peakMarkerPos = (int)(peak * 40);\r\n\r\n                        Console.SetCursorPosition(0, 0);\r\n                        Console.Write(\"RMS:  [\");\r\n                        Console.Write(new string('#', barLength));\r\n                        Console.Write(new string(' ', 40 - barLength));\r\n                        Console.Write(\"]\\n\");\r\n\r\n                        Console.SetCursorPosition(0, 1);\r\n                        Console.Write(\"Peak: [\");\r\n                        Console.Write(new string(' ', 40));\r\n                        Console.Write(\"]\\r\"); // Carriage return to move back\r\n                        Console.Write(\"Peak: [\");\r\n                        if(peakMarkerPos < 40) Console.SetCursorPosition(7 + peakMarkerPos, 1);\r\n                        Console.Write(\"|\");\r\n                        \r\n                        Console.SetCursorPosition(0, 3);\r\n                    }\r\n                }\r\n\r\n                // Simple IVisualizationContext implementation for console output.\r\n                public class ConsoleVisualizationContext : IVisualizationContext\r\n                {\r\n                    public void Clear() { }\r\n                    public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1) { }\r\n                    public void DrawRectangle(float x, float y, float width, float height, Color color) { }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example demonstrates the analyzer-visualizer pattern. A `LevelMeterAnalyzer` is created to process the audio, and a `LevelMeterVisualizer` is created, linked to the analyzer. When `player.AddAnalyzer(levelMeterAnalyzer)` is called, the analyzer is attached to the player's audio stream. Inside its processing logic, the analyzer automatically calls its linked visualizer's `ProcessOnAudioData` method.\r\n        We subscribe to the `levelMeterVisualizer.VisualizationUpdated` event, which is fired by the visualizer whenever it receives new data. The event handler calls our `DrawLevelMeter` helper function to render a simple text-based meter in the console, providing a direct visual representation of the audio levels calculated by the analyzer.\r\n    </Tab>\r\n\r\n    <Tab key=\"waveform-viz\" title=\"Console Waveform\">\r\n        Demonstrates using `WaveformVisualizer` to display an audio waveform.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o WaveformVisualization\r\n                cd WaveformVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement waveform visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.Collections.Generic;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace WaveformVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard setup\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a LevelMeterAnalyzer or any analyzer you want.\r\n                        var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                        // Create a WaveformVisualizer.\r\n                        var waveformVisualizer = new WaveformVisualizer();\r\n\r\n                        // Add the player to the master mixer.\r\n                        Mixer.Master.AddComponent(player);\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        waveformVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawWaveform(waveformVisualizer.Waveform);\r\n                        };\r\n\r\n                        // Connect the player's output to the level meter analyzer's input.\r\n                        player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                        // Add the player to the master mixer.\r\n                        Mixer.Master.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        player.Play();\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            waveformVisualizer.Render(new ConsoleVisualizationContext()); // ConsoleVisualizationContext is just a placeholder\r\n                        };\r\n                        timer.Start();\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio and displaying waveform... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        waveformVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based waveform.\r\n                    private static void DrawWaveform(IReadOnlyList<float> waveform)\r\n                    {\r\n                        Console.Clear();\r\n                        int consoleWidth = Console.WindowWidth;\r\n                        int consoleHeight = Console.WindowHeight;\r\n\r\n                        if (waveform.Count == 0) return;\r\n\r\n                        for (int i = 0; i < consoleWidth; i++)\r\n                        {\r\n                            int waveformIndex = (int)(i * (waveform.Count / (float)consoleWidth));\r\n                            waveformIndex = Math.Clamp(waveformIndex, 0, waveform.Count - 1);\r\n\r\n                            float sampleValue = waveform[waveformIndex];\r\n                            int consoleY = (int)((sampleValue + 1) * 0.5 * (consoleHeight - 1));\r\n                            consoleY = Math.Clamp(consoleY, 0, consoleHeight - 1);\r\n\r\n                            if (i < consoleWidth && (consoleHeight - consoleY - 1) < consoleHeight)\r\n                            {\r\n                                Console.SetCursorPosition(i, consoleHeight - consoleY - 1);\r\n                                Console.Write(\"*\");\r\n                            }\r\n                        }\r\n                        Console.SetCursorPosition(0, consoleHeight - 1);\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        In this example, the `WaveformVisualizer` is used to directly visualize the audio samples. While it implements `IVisualizer`, it doesn't need a separate `AudioAnalyzer`. It is attached directly to the `player` using `AddAnalyzer`. When the player processes its audio buffer, it passes a copy of that buffer to the `WaveformVisualizer`, which stores it.\r\n        We subscribe to the `VisualizationUpdated` event, which the visualizer raises after receiving a new buffer. The event handler calls `DrawWaveform` to render a simple ASCII representation of the waveform to the console, providing a real-time oscilloscope-like view of the audio signal.\r\n    </Tab>\r\n\r\n    <Tab key=\"spectrum-viz\" title=\"Console Spectrum\">\r\n        Demonstrates creating a console-based spectrum analyzer using the `SpectrumAnalyzer` and\r\n        `SpectrumVisualizer`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SpectrumAnalyzerVisualization\r\n                cd SpectrumAnalyzerVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement spectrum visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Interfaces;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace SpectrumAnalyzerVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard setup.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the SpectrumAnalyzer and SpectrumVisualizer.\r\n                        var spectrumAnalyzer = new SpectrumAnalyzer(audioFormat, fftSize: 2048);\r\n                        var spectrumVisualizer = new SpectrumVisualizer(spectrumAnalyzer);\r\n\r\n                        // Attach the analyzer to the player.\r\n                        player.AddAnalyzer(spectrumAnalyzer);\r\n                        \r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        spectrumVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawSpectrum(spectrumAnalyzer.SpectrumData);\r\n                        };\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            spectrumVisualizer.ProcessOnAudioData(Array.Empty<float>());\r\n                            spectrumVisualizer.Render(new ConsoleVisualizationContext());\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying spectrum analyzer... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        spectrumVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based spectrum analyzer.\r\n                    private static void DrawSpectrum(ReadOnlySpan<float> spectrumData)\r\n                    {\r\n                        Console.Clear();\r\n                        int consoleWidth = Console.WindowWidth;\r\n                        int consoleHeight = Console.WindowHeight -1;\r\n\r\n                        if (spectrumData.IsEmpty) return;\r\n\r\n                        for (int i = 0; i < consoleWidth; i++)\r\n                        {\r\n                            // Logarithmic mapping of frequency bins to console columns for better visualization\r\n                            double logIndex = Math.Log10(1 + 9 * ((double)i / consoleWidth));\r\n                            int spectrumIndex = (int)(logIndex * (spectrumData.Length - 1));\r\n                            \r\n                            float magnitude = spectrumData[spectrumIndex];\r\n                            int barHeight = (int)(magnitude * consoleHeight);\r\n                            barHeight = Math.Clamp(barHeight, 0, consoleHeight);\r\n\r\n                            for (int j = 0; j < barHeight; j++)\r\n                            {\r\n                                Console.SetCursorPosition(i, consoleHeight - 1 - j);\r\n                                Console.Write(\"█\");\r\n                            }\r\n                        }\r\n                        Console.SetCursorPosition(0, consoleHeight - 1);\r\n                    }\r\n                }\r\n\r\n                // Simple IVisualizationContext implementation for console output.\r\n                public class ConsoleVisualizationContext : IVisualizationContext\r\n                {\r\n                    public void Clear() { }\r\n                    public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f) { }\r\n                    public void DrawRectangle(float x, float y, float width, float height, Color color) { }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the analyzer-visualizer pattern for frequency analysis. A `SpectrumAnalyzer` is created to perform the FFT, and a `SpectrumVisualizer` is linked to it. The analyzer is attached to the `player`, which feeds it audio data. The analyzer processes this data and automatically informs the visualizer.\r\n        We subscribe to the `spectrumVisualizer.VisualizationUpdated` event. In the handler, `DrawSpectrum` is called, which reads the `spectrumAnalyzer.SpectrumData` and renders a simple bar chart of the frequency magnitudes to the console. The drawing logic uses a logarithmic scale for the frequency axis, which provides a more musically intuitive display of the spectrum.\r\n    </Tab>\r\n\t\r\n\t<Tab key=\"ui-integration\" title={<div className=\"flex items-center gap-2\">\r\n\t\t<Icon icon=\"lucide:layout-template\"/><span>UI Integration</span></div>}>\r\n        These examples use basic console output for simplicity. To integrate SoundFlow's visualizers with a GUI\r\n        framework (like WPF, WinForms, Avalonia, or MAUI), you'll need to:\r\n        <Steps layout='vertical'>\r\n            <Step title=\"Implement IVisualizationContext\" description=\"Wrap your UI framework's drawing primitives\"\r\n                  icon='material-symbols:draw-outline'>\r\n                This class will wrap the drawing primitives of your chosen UI framework. For example, in WPF, you might\r\n                use `DrawingContext` methods to draw shapes on a `Canvas`.\r\n            </Step>\r\n            <Step title=\"Update UI from Event\" description=\"Trigger a redraw on the UI thread\" icon='mdi:update'>\r\n                In the `VisualizationUpdated` event handler, trigger a redraw of your UI element that hosts the\r\n                visualization. Make sure to marshal the update to the UI thread using `Dispatcher.Invoke` or a similar\r\n                mechanism if the event is raised from a different thread.\r\n            </Step>\r\n            <Step title=\"Call Render Method\" description=\"Pass your context to the visualizer\" icon='lucide:render'>\r\n                In your UI's rendering logic, call the `Render` method of the visualizer, passing your\r\n                `IVisualizationContext` implementation.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Example (Conceptual WPF):**\r\n\r\n        ```csharp\r\n        // In your XAML:\r\n        // <Canvas x:Name=\"VisualizationCanvas\"/>\r\n\r\n        // In your code-behind:\r\n        public partial class MainWindow : Window\r\n        {\r\n            private readonly WaveformVisualizer _visualizer;\r\n\r\n            public MainWindow()\r\n            {\r\n                InitializeComponent();\r\n\r\n                // ... Initialize AudioEngine, SoundPlayer, etc. ...\r\n\r\n                _visualizer = new WaveformVisualizer();\r\n                _visualizer.VisualizationUpdated += OnVisualizationUpdated;\r\n\r\n                // ...\r\n            }\r\n\r\n            private void OnVisualizationUpdated(object? sender, EventArgs e)\r\n            {\r\n                // Marshal the update to the UI thread\r\n                Dispatcher.Invoke(() =>\r\n                {\r\n                    VisualizationCanvas.Children.Clear(); // Clear previous drawing\r\n\r\n                    // Create a custom IVisualizationContext that wraps the Canvas\r\n                    var context = new WpfVisualizationContext(VisualizationCanvas);\r\n\r\n                    // Render the visualization\r\n                    _visualizer.Render(context);\r\n                });\r\n            }\r\n\r\n            // ...\r\n        }\r\n\r\n        // IVisualizationContext implementation for WPF\r\n        public class WpfVisualizationContext : IVisualizationContext\r\n        {\r\n            private readonly Canvas _canvas;\r\n\r\n            public WpfVisualizationContext(Canvas canvas)\r\n            {\r\n                _canvas = canvas;\r\n            }\r\n\r\n            public void Clear()\r\n            {\r\n                _canvas.Children.Clear();\r\n            }\r\n\r\n            public void DrawLine(float x1, float y1, float x2, float y2, SoundFlow.Interfaces.Color color, float thickness = 1f)\r\n            {\r\n                var line = new Line\r\n                {\r\n                    X1 = x1,\r\n                    Y1 = y1,\r\n                    X2 = x2,\r\n                    Y2 = y2,\r\n                    Stroke = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255))),\r\n                    StrokeThickness = thickness\r\n                };\r\n                _canvas.Children.Add(line);\r\n            }\r\n\r\n            public void DrawRectangle(float x, float y, float width, float height, SoundFlow.Interfaces.Color color)\r\n            {\r\n                var rect = new Rectangle\r\n                {\r\n                    Width = width,\r\n                    Height = height,\r\n                    Fill = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255)))\r\n                };\r\n                Canvas.SetLeft(rect, x);\r\n                Canvas.SetTop(rect, y);\r\n                _canvas.Children.Add(rect);\r\n            }\r\n        }\r\n        ```\r\n\r\n        Remember to adapt this conceptual example to your specific UI framework and project structure.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for audio analysis and visualization with SoundFlow!"
  },
  {
    "id": 14,
    "slug": "synthesis-synthesizer-component",
    "version": "1.3.0",
    "title": "The Synthesizer Component",
    "description": "A comprehensive guide to the core Synthesizer component, its architecture, how it generates sound from MIDI, and its role in the audio graph.",
    "navOrder": 14,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 14\r\ntitle: The Synthesizer Component\r\ndescription: A comprehensive guide to the core Synthesizer component, its architecture, how it generates sound from MIDI, and its role in the audio graph.\r\nnavOrder: 14\r\ncategory: Synthesis\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# The Synthesizer Component\r\n\r\nThe `Synthesizer` (`SoundFlow.Synthesis.Synthesizer`) is a new, fundamental `SoundComponent` introduced in SoundFlow v1.3.0. It serves as a powerful, polyphonic, and multi-timbral sound source capable of generating complex audio from MIDI messages. It can be thought of as a virtual instrument or a software synthesizer that lives within your SoundFlow audio graph.\r\n\r\n## Features\r\n\r\n*   **Polyphonic & Multi-timbral:** The synthesizer can play multiple notes simultaneously (polyphony) and can respond to all 16 MIDI channels independently, playing a different instrument on each channel (multi-timbrality).\r\n*   **Extensible Instrument Model:** It does not produce sound on its own but relies on a pluggable `IInstrumentBank` to source its sounds. This allows you to load instruments from various sources, including built-in basic synths or complex sample libraries like SoundFonts.\r\n*   **MIDI Controllable:** As an implementation of `IMidiControllable`, it is the primary target for MIDI data from a `Sequencer`, a live `MidiInputDevice`, or programmatic MIDI messages.\r\n*   **MPE (MIDI Polyphonic Expression) Support:** The synthesizer can be switched into MPE mode to respond to per-note expression data from compatible controllers, allowing for incredibly nuanced performances.\r\n*   **Standard `SoundComponent`:** It inherits from `SoundComponent`, meaning it integrates seamlessly into the audio graph. You can connect its output to `Mixer`s and apply `SoundModifier`s and `AudioAnalyzer`s to it just like any other audio source.\r\n\r\n## Core Concepts\r\n\r\nThe `Synthesizer` is a `SoundComponent`, so it must be added to a `Mixer` (typically a device's `MasterMixer`) to be heard. Its `GenerateAudio` method is called on the high-priority audio thread, where it renders the audio for all its currently active voices.\r\n\r\nCrucially, it is also an `IMidiControllable`. Its `ProcessMidiMessage` method is the entry point for all MIDI data. This method is thread-safe and can be called from any thread (e.g., the UI thread for programmatic notes, or a MIDI backend's thread for live input) to control the synthesizer in real-time.\r\n\r\n## Internal MIDI Effects & Temporal Processing\r\n\r\nThe `Synthesizer` includes a built-in MIDI processing pipeline that sits before the voice generation stage. This allows you to attach MIDI effects—including time-based effects like Arpeggiators—directly to the synth.\r\n\r\n### Managing the Modifier Chain\r\n\r\nYou can dynamically add, remove, or reorder modifiers using the following methods:\r\n\r\n*   **`AddMidiModifier(MidiModifier modifier)`**: Adds an effect to the end of the chain.\r\n*   **`RemoveMidiModifier(MidiModifier modifier)`**: Removes an effect from the chain.\r\n*   **`MidiModifiers`**: A read-only property exposing the current list of modifiers.\r\n\r\n```csharp\r\n// Create an Arpeggiator\r\nvar arp = new ArpeggiatorModifier { Mode = ArpMode.Up, Rate = 0.25 };\r\n\r\n// Attach it to the synthesizer\r\nsynthesizer.AddMidiModifier(arp);\r\n\r\n// Later, you can disable it non-destructively\r\narp.IsEnabled = false;\r\n```\r\n\r\n### Temporal Processing & BPM\r\n\r\nA unique feature of the `Synthesizer` is its automatic handling of `ITemporalMidiModifier` instances.\r\n\r\nWhen you add a modifier that implements `ITemporalMidiModifier` (like the `ArpeggiatorModifier`), the `Synthesizer` automatically drives it using the audio thread's clock.\r\n\r\n1.  **BPM Control:** The `Synthesizer` has a **`Bpm`** property (float, default 120). This sets the tempo for all attached temporal modifiers.\r\n2.  **Sample-Accurate Timing:** Inside the `Synthesizer.GenerateAudio` loop, the synth calculates the exact duration of the audio buffer it is about to render.\r\n3.  **The Tick Loop:** It calls the `Tick(duration, Bpm)` method on all temporal modifiers.\r\n4.  **Event Injection:** Any MIDI messages returned by the `Tick` method (e.g., the Arpeggiator deciding it's time to play the next note) are immediately injected into the sound generation engine for the current audio buffer.\r\n\r\nThis ensures that arpeggiators and sequencers running inside the `Synthesizer` stay perfectly synchronized with the audio render clock, free from the jitter often associated with thread-based timers.\r\n\r\n```csharp\r\n// Set the tempo for the internal arpeggiator\r\nsynthesizer.Bpm = 140;\r\n```\r\n\r\n## Getting Started: Playing Your First Synthesized Note\r\n\r\nThis example demonstrates the most basic usage: creating a synthesizer, giving it a simple instrument bank, and playing a note programmatically.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.Structs;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing System.Linq;\r\nusing System.Threading;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"Synthesizer Basic Usage Example\");\r\n\r\n        // 1. Initialize the audio engine and a playback device.\r\n        using var engine = new MiniAudioEngine();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format); // Use default device\r\n\r\n        // 2. Create an Instrument Bank.\r\n        // The BasicInstrumentBank provides a few simple, hardcoded synth sounds.\r\n        var instrumentBank = new BasicInstrumentBank(format);\r\n\r\n        // 3. Create the Synthesizer instance.\r\n        // It requires the engine, format, and an instrument bank.\r\n        var synthesizer = new Synthesizer(engine, format, instrumentBank);\r\n\r\n        // 4. Add the synthesizer to the device's master mixer to make it audible.\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n\r\n        // 5. Start the audio device's processing thread.\r\n        device.Start();\r\n        Console.WriteLine($\"Audio device '{device.Info?.Name}' started.\");\r\n\r\n        // Playing Notes Programmatically\r\n\r\n        // A Note On message: Command 0x90, Channel 1, Note 60 (Middle C), Velocity 127\r\n        var noteOn = new MidiMessage(0x90, 60, 127);\r\n        // A Note Off message for the same note\r\n        var noteOff = new MidiMessage(0x80, 60, 0);\r\n\r\n        Console.WriteLine(\"\\nPlaying Middle C (Note 60)...\");\r\n        synthesizer.ProcessMidiMessage(noteOn);\r\n        Thread.Sleep(1000); // Hold the note for 1 second\r\n        synthesizer.ProcessMidiMessage(noteOff);\r\n        Console.WriteLine(\"Note Off sent.\");\r\n        Thread.Sleep(1000); // Wait for the release tail to fade out\r\n\r\n        // Switching Instruments\r\n\r\n        // A Program Change message for Channel 1, Program 80 (Lead Synth in BasicInstrumentBank)\r\n        var programChange = new MidiMessage(0xC0, 80, 0);\r\n\r\n        Console.WriteLine(\"\\nSwitching to Program 80 (Lead Synth)...\");\r\n        synthesizer.ProcessMidiMessage(programChange);\r\n\r\n        Console.WriteLine(\"Playing a higher note (A4, Note 69)...\");\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 69, 110));\r\n        Thread.Sleep(1000);\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 69, 0));\r\n        Console.WriteLine(\"Note Off sent.\");\r\n        Thread.Sleep(1000);\r\n\r\n        // 6. Clean up.\r\n        device.Stop();\r\n        synthesizer.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n## Key Properties & Methods\r\n\r\n### `MpeEnabled`\r\n```csharp\r\npublic bool MpeEnabled { get; set; }\r\n```\r\nGets or sets whether the synthesizer operates in MPE (MIDI Polyphonic Expression) mode. When `false` (the default), it operates as a standard multi-timbral instrument. When `true`, it interprets incoming MIDI according to MPE specifications, allowing for per-note expression.\r\n\r\n<div className=\"flex items-center gap-3 my-4 p-4 rounded-lg bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n    <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n    <p className=\"text-sm\">\r\n        Switching this property will trigger an \"All Notes Off\" command internally to prevent stuck notes that might occur from changing the voice allocation logic. For a detailed guide, see the **<a href=\"/docs/synthesis/mpe-support\">MPE Support</a>** page.\r\n    </p>\r\n</div>\r\n\r\n### `ProcessMidiMessage(MidiMessage message)`\r\n```csharp\r\npublic void ProcessMidiMessage(MidiMessage message)\r\n```\r\nThis is the primary method for controlling the synthesizer. It accepts a `MidiMessage` struct and routes it to the correct internal `MidiChannel` based on the message's channel, which then triggers the appropriate action (Note On, Note Off, parameter change, etc.). This method is thread-safe.\r\n\r\n### `Reset()`\r\n```csharp\r\npublic void Reset()\r\n```\r\nImmediately resets the synthesizer to a clean state. This action performs the following on all 16 MIDI channels:\r\n*   Kills all currently sounding voices with a very fast fade-out to prevent clicks.\r\n*   Resets all channel parameters (Volume, Pan, Pitch Bend, etc.) to their default values.\r\n*   Resets the selected instrument on each channel to the default (Bank 0, Program 0).\r\n\r\nThis is useful for clearing the synthesizer's state after stopping playback or when loading a new song.\r\n\r\n### `ProcessMpeEvent(object mpeEvent)` (Internal)\r\nThis method is designed for internal use by the `MidiManager`. When a device is configured for MPE, the `MidiManager`'s internal `MpeParser` translates the raw MIDI stream into higher-level MPE event objects. The `MidiManager` then calls this method to deliver these events to the synthesizer, which routes them to the correct active voice for per-note expression. You should not call this method directly."
  },
  {
    "id": 19,
    "slug": "synthesis-soundfont-support",
    "version": "1.3.0",
    "title": "SoundFont (.sf2) Support",
    "description": "A practical guide to loading and using industry-standard SoundFont 2 files as instruments within the SoundFlow synthesizer.",
    "navOrder": 19,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 19\r\ntitle: SoundFont (.sf2) Support\r\ndescription: A practical guide to loading and using industry-standard SoundFont 2 files as instruments within the SoundFlow synthesizer.\r\nnavOrder: 19\r\ncategory: Synthesis\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# SoundFont (.sf2) Support\r\n\r\nSoundFlow v1.3.0 includes robust support for the industry-standard **SoundFont 2 (.sf2)** file format. This allows you to load vast libraries of high-quality, multi-sampled instruments and use them directly within the `Synthesizer` component. This feature is enabled by the `SoundFontBank` class.\r\n\r\n## What is a SoundFont?\r\n\r\nA SoundFont is a standardized file format that bundles everything needed for a sample-based instrument bank:\r\n*   **PCM Audio Samples:** The raw audio recordings of notes.\r\n*   **Instrument Definitions:** Defines how samples are mapped across the keyboard (key splits) and respond to velocity (velocity layers).\r\n*   **Modulators & Generators:** A complex system of parameters that define envelopes (ADSR), filters, LFOs, and other synthesis properties for each sample.\r\n*   **Preset Hierarchy:** Organizes instruments into banks and programs, matching the MIDI specification for instrument selection.\r\n\r\nThe `SoundFontBank` class parses this entire structure, converting it into SoundFlow's native `Instrument` model.\r\n\r\n## The `SoundFontBank` Class\r\n\r\nThe `SoundFontBank` (`SoundFlow.Synthesis.Banks.SoundFontBank`) is a concrete implementation of the `IInstrumentBank` interface. It is the bridge between the SF2 file format and the SoundFlow synthesis engine.\r\n\r\n*   **Constructor:** `public SoundFontBank(string filePath, AudioFormat format)`\r\n    When you create a `SoundFontBank`, it performs several intensive one-time operations:\r\n    1.  It opens and reads the `.sf2` file stream.\r\n    2.  It parses all metadata chunks (presets, instruments, zones, generators, and sample headers).\r\n    3.  It reads the raw sample data.\r\n    4.  It **resamples** all audio samples from their original sample rate to match the `AudioFormat` of your engine. This is crucial for high-quality, pitch-accurate playback and is done upfront for performance.\r\n*   **`IDisposable`:** The `SoundFontBank` holds an open file stream to the `.sf2` file, so it must be disposed of properly (e.g., with a `using` statement) to release the file handle.\r\n\r\n### Listing Available Presets\r\n\r\nA key feature of the `SoundFontBank` is its ability to enumerate all the presets contained within the file, which is essential for building a UI for instrument selection.\r\n\r\n*   **`AvailablePresets` Property:** A `IReadOnlyList<PresetInfo>` that contains information for every preset in the SoundFont.\r\n*   **`PresetInfo` Record:** A simple record struct with three properties:\r\n    *   `int Bank`: The MIDI bank number.\r\n    *   `int Program`: The MIDI program number (0-127).\r\n    *   `string Name`: The human-readable name of the preset (e.g., \"Acoustic Grand Piano\").\r\n\r\n## Complete Example: Loading and Playing a SoundFont\r\n\r\nThis example demonstrates the full workflow: loading an SF2 file, listing its presets, selecting an instrument, and playing it with the `Synthesizer`.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.Structs;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing System;\r\nusing System.IO;\r\nusing System.Linq;\r\nusing System.Threading;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"SoundFont Bank Example\");\r\n\r\n        // Replace with the path to a SoundFont file on your system.\r\n        var soundFontPath = @\"C:\\path\\to\\your\\soundfont.sf2\";\r\n        if (!File.Exists(soundFontPath))\r\n        {\r\n            Console.WriteLine($\"Error: SoundFont file not found at '{soundFontPath}'.\");\r\n            return;\r\n        }\r\n\r\n        // 1. Standard engine and device setup\r\n        using var engine = new MiniAudioEngine();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n\r\n        // 2. Load the .sf2 file into a SoundFontBank. This can take a moment.\r\n        Console.WriteLine($\"Loading SoundFont: {Path.GetFileName(soundFontPath)}...\");\r\n        using var soundFontBank = new SoundFontBank(soundFontPath, format);\r\n        Console.WriteLine(\"SoundFont loaded successfully.\");\r\n\r\n        // 3. List the available presets.\r\n        Console.WriteLine(\"\\n--- Available Presets ---\");\r\n        foreach (var preset in soundFontBank.AvailablePresets)\r\n        {\r\n            Console.WriteLine($\"Bank: {preset.Bank}, Program: {preset.Program}, Name: {preset.Name}\");\r\n        }\r\n\r\n        // 4. Create a Synthesizer and give it the loaded bank.\r\n        var synthesizer = new Synthesizer(engine, format, soundFontBank);\r\n\r\n        // 5. Add the synthesizer to the mixer and start the device.\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n        device.Start();\r\n\r\n        // Select an instrument and play it\r\n\r\n        // Let's choose the first available preset.\r\n        var presetToPlay = soundFontBank.AvailablePresets.FirstOrDefault();\r\n        if (presetToPlay == null)\r\n        {\r\n            Console.WriteLine(\"No presets found in the SoundFont.\");\r\n            return;\r\n        }\r\n\r\n        Console.WriteLine($\"\\nSelecting preset: '{presetToPlay.Name}' (Bank: {presetToPlay.Bank}, Program: {presetToPlay.Program})\");\r\n\r\n        int channel = 1; // MIDI Channel 1\r\n\r\n        // Bank Select MSB (CC 0)\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0xB0 + channel - 1), 0, (byte)(presetToPlay.Bank / 128)));\r\n        // Bank Select LSB (CC 32)\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0xB0 + channel - 1), 32, (byte)(presetToPlay.Bank % 128)));\r\n\r\n        // Program Change\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0xC0 + channel - 1), (byte)presetToPlay.Program, 0));\r\n\r\n        Console.WriteLine(\"Playing a C Major chord...\");\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0x90 + channel - 1), 60, 100)); // C4\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0x90 + channel - 1), 64, 100)); // E4\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0x90 + channel - 1), 67, 100)); // G4\r\n\r\n        Thread.Sleep(3000); // Hold the chord\r\n\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0x80 + channel - 1), 60, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0x80 + channel - 1), 64, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage((byte)(0x80 + channel - 1), 67, 0));\r\n\r\n        Console.WriteLine(\"Playback finished.\");\r\n        Thread.Sleep(1000);\r\n\r\n        // 6. Clean up\r\n        device.Stop();\r\n        synthesizer.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n## Using SoundFonts with a Sequencer\r\n\r\nThe `SoundFontBank` integrates seamlessly with the `Sequencer` component, allowing you to create a powerful standalone MIDI file player that uses high-quality sampled instruments. Simply replace the `BasicInstrumentBank` in the `Sequencer` example with a `SoundFontBank`.\r\n\r\n```csharp\r\n// In the Sequencer example...\r\n\r\n// using var instrumentBank = new BasicInstrumentBank(format);\r\n// BECOMES:\r\nusing var instrumentBank = new SoundFontBank(\"path/to/your/soundfont.sf2\", format);\r\n\r\n// The rest of the code remains the same.\r\nvar synthesizer = new Synthesizer(engine, format, instrumentBank);\r\nvar sequencer = new Sequencer(engine, format, midiDataProvider, synthesizer);\r\n// ...\r\n```\r\nThe `Sequencer` will play the MIDI file, sending Program Change and Bank Select messages from the file to the `Synthesizer`, which will then load the correct instruments from your SoundFont."
  },
  {
    "id": 18,
    "slug": "synthesis-sequencer-component",
    "version": "1.3.0",
    "title": "The Sequencer Component",
    "description": "Learn to use the Sequencer component for high-precision, tempo-aware playback of MIDI data from files or memory.",
    "navOrder": 18,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 18\r\ntitle: The Sequencer Component\r\ndescription: Learn to use the Sequencer component for high-precision, tempo-aware playback of MIDI data from files or memory.\r\nnavOrder: 18\r\ncategory: Synthesis\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# The Sequencer & Sample-Accurate MIDI Playback\r\n\r\nThe `Sequencer` (`SoundFlow.Synthesis.Sequencer`) is a specialized `SoundComponent` designed for high-precision playback of MIDI data. Unlike other components that generate or process audio, the `Sequencer`'s primary role is to act as a **sample-accurate MIDI event dispatcher**.\r\n\r\nIt reads MIDI events from a `MidiDataProvider` and sends them to a target (like a `Synthesizer`) at the precise moment they are meant to occur, synchronized perfectly with the audio rendering clock. This eliminates the timing jitter that can occur with less precise, system-timer-based MIDI playback.\r\n\r\n## Core Concepts\r\n\r\n*   **Input (`MidiDataProvider`):** The `Sequencer` is driven by a `MidiDataProvider`. This provider holds a pre-processed, time-ordered list of all MIDI events for a given musical sequence. It's typically created from a `.mid` file or a `MidiSequence` object from the editing engine.\r\n\r\n*   **Output (`IMidiControllable` Target):** The `Sequencer` sends its MIDI messages to a target that implements the `IMidiControllable` interface. In most cases, this will be a `Synthesizer` instance.\r\n\r\n*   **Sample-Accurate Timing:** The `Sequencer` achieves its timing precision by hooking into the audio processing graph. Its `GenerateAudio` method (which does not write any audio samples) is called by the `Mixer` on the high-priority audio thread for each audio buffer. Inside this method, it calculates the exact time range of the current buffer and dispatches any MIDI events from its `MidiDataProvider` that fall within that slice of time.\r\n\r\n*   **Tempo Awareness (`ISequencerContext`):** To correctly translate MIDI ticks into real time, the `Sequencer` needs access to a tempo map. It does this via the `ISequencerContext` interface.\r\n    *   When used within a `Composition`, the `composition.Renderer` automatically handles all sequencing logic, using the `Composition` itself as the context to sync with the master `TempoTrack`.\r\n    *   When used **standalone** (as shown in the example below), the `Sequencer` can be given a `Context` property. If `null`, it falls back to using the `MidiDataProvider`'s own internal tempo map (derived from the original MIDI file).\r\n\r\n<div className=\"flex items-center gap-3 my-4 p-4 rounded-lg bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n    <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n    <p className=\"text-sm\">\r\n        **Key takeaway:** You typically only need to create a `Sequencer` instance manually when you want to build a standalone MIDI player. For multi-track projects, the `Composition`'s built-in playback system handles all sequencing for you.\r\n    </p>\r\n</div>\r\n\r\n\r\n## The `MidiDataProvider`: The Heart of Sequencing\r\n\r\nBefore diving into the Sequencer, it's crucial to understand the `MidiDataProvider`. This class transforms raw MIDI data (from a file or a recording) into a format optimized for real-time playback.\r\n\r\n### Key Features\r\n\r\n*   **Event Linearization:** A Standard MIDI File contains multiple tracks, each with its own timeline. The `MidiDataProvider` merges all events into a single, time-ordered list of `TimedMidiEvent` structures. This allows the sequencer to simply iterate through the list without juggling multiple track pointers.\r\n*   **Absolute Timing:** Instead of the relative \"delta-time\" used in MIDI files (ticks since the *previous* event), the `MidiDataProvider` converts everything to **absolute ticks** from the beginning of the sequence. This makes seeking and random access trivial.\r\n*   **Tempo Mapping & Time Conversion:** MIDI files often contain variable tempos. The `MidiDataProvider` scans the file to build an internal **Tempo Map**. It exposes two vital methods:\r\n    *   `GetTimeSpanForTick(long tick)`: Calculates the exact real-time timestamp (`TimeSpan`) for a given MIDI tick, accounting for all tempo changes up to that point.\r\n    *   `GetTickForTimeSpan(TimeSpan time)`: The inverse operation, essential for locating the playback position.\r\n\r\n### Usage Example\r\n\r\n```csharp\r\n// Load a MIDI file\r\nvar midiFile = MidiFileParser.Parse(File.OpenRead(\"song.mid\"));\r\nvar provider = new MidiDataProvider(midiFile);\r\n\r\n// Check duration\r\nConsole.WriteLine($\"Song Duration: {provider.Duration}\");\r\n\r\n// Find out when the 1000th tick happens in real time\r\nvar timeAtTick1000 = provider.GetTimeSpanForTick(1000);\r\nConsole.WriteLine($\"Tick 1000 happens at: {timeAtTick1000}\");\r\n```\r\n\r\n## Standalone MIDI File Player Example\r\n\r\nThis complete, runnable example demonstrates the primary use case for the `Sequencer`: building a simple, high-quality MIDI file player.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Metadata.Midi;\r\nusing SoundFlow.Providers;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing System.IO;\r\nusing System.Linq;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main(string[] args)\r\n    {\r\n        Console.WriteLine(\"Standalone Sequencer MIDI Player Example\");\r\n\r\n        // User Configuration\r\n\r\n        // Replace with the path to your MIDI file and a SoundFont file.\r\n        var midiFilePath = @\"C:\\path\\to\\your\\song.mid\";\r\n        var soundFontPath = @\"C:\\path\\to\\your\\soundfont.sf2\";\r\n\r\n        if (!File.Exists(midiFilePath) || !File.Exists(soundFontPath))\r\n        {\r\n            Console.WriteLine(\"Error: Please provide valid paths for the MIDI and SoundFont files.\");\r\n            return;\r\n        }\r\n\r\n        // 1. Standard engine and device setup\r\n        using var engine = new MiniAudioEngine();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n\r\n        // 2. Load MIDI data and an instrument bank\r\n        var midiFile = MidiFileParser.Parse(File.OpenRead(midiFilePath));\r\n        var midiDataProvider = new MidiDataProvider(midiFile);\r\n\r\n        // The SoundFontBank is IDisposable and must be managed.\r\n        using var instrumentBank = new SoundFontBank(soundFontPath, format);\r\n\r\n        Console.WriteLine($\"Loaded MIDI file: {Path.GetFileName(midiFilePath)}, Duration: {midiDataProvider.Duration:mm\\\\:ss}\");\r\n        Console.WriteLine($\"Loaded SoundFont with {instrumentBank.AvailablePresets.Count} presets.\");\r\n\r\n        // 3. Create the Synthesizer (the sound source)\r\n        var synthesizer = new Synthesizer(engine, format, instrumentBank);\r\n\r\n        // 4. Create the Sequencer (the MIDI event dispatcher)\r\n        var sequencer = new Sequencer(engine, format, midiDataProvider, synthesizer)\r\n        {\r\n            IsLooping = true // Let's loop the playback\r\n        };\r\n\r\n        // 5. Build the audio graph\r\n        // The Synthesizer generates the audio.\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n        // The Sequencer is also added so its GenerateAudio method is called for timing, even though it doesn't output audio itself.\r\n        device.MasterMixer.AddComponent(sequencer);\r\n\r\n        // 6. Start playback\r\n        device.Start();\r\n        sequencer.Play(); // This enables the sequencer's processing.\r\n\r\n        Console.WriteLine(\"\\nPlayback started. Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        // 7. Clean up\r\n        sequencer.Stop();\r\n        device.Stop();\r\n        synthesizer.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n## Playback Control\r\n\r\nThe `Sequencer` provides a simple set of methods to control playback:\r\n\r\n*   **`Play()`**: Starts or resumes sequencing. This sets `State` to `Playing` and `Enabled` to `true`.\r\n*   **`Pause()`**: Pauses sequencing. This sets `State` to `Paused` and `Enabled` to `false`.\r\n*   **`Stop()`**: Stops sequencing and resets the playback position to the beginning (`_currentTick = 0`). It also sends an \"All Notes Off\" CC message to its target to prevent stuck notes.\r\n*   **`Seek(TimeSpan time)`**: Jumps to a specific time in the sequence. It also sends an \"All Notes Off\" message before seeking.\r\n*   **`IsLooping` (property)**: If `true`, the `Sequencer` will automatically jump back to the beginning when it reaches the end of the `MidiDataProvider`.\r\n\r\n```csharp\r\n// Inside your application logic\r\n\r\n// Pause playback\r\nsequencer.Pause();\r\n\r\n// Resume playback\r\nsequencer.Play();\r\n\r\n// Jump to the 30-second mark\r\nsequencer.Seek(TimeSpan.FromSeconds(30));\r\n\r\n// Stop and reset to the beginning\r\nsequencer.Stop();\r\n```"
  },
  {
    "id": 16,
    "slug": "synthesis-multi-timbrality-in-depth",
    "version": "1.3.0",
    "title": "Multitimbrality in Depth",
    "description": "Learn how the SoundFlow synthesizer handles Multitimbrality, allowing you to play different instruments on different MIDI channels simultaneously from a single component.",
    "navOrder": 16,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 16\r\ntitle: Multitimbrality in Depth\r\ndescription: Learn how the SoundFlow synthesizer handles Multitimbrality, allowing you to play different instruments on different MIDI channels simultaneously from a single component.\r\nnavOrder: 16\r\ncategory: Synthesis\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# Multitimbrality in Depth\r\n\r\nMultitimbrality is the ability of a synthesizer to play different sounds, or \"timbres,\" on different MIDI channels at the same time. The SoundFlow `Synthesizer` is a fully 16-part multi-timbral instrument, making it a powerful central sound module for your compositions.\r\n\r\n## How it Works: The `MidiChannel` Class\r\n\r\nInternally, the `Synthesizer` component doesn't handle MIDI messages directly. Instead, it manages an array of 16 `MidiChannel` objects, one for each MIDI channel (1-16).\r\n\r\nWhen `synthesizer.ProcessMidiMessage(message)` is called:\r\n1.  The `Synthesizer` inspects the message's `Channel` property.\r\n2.  It forwards the message to the corresponding `MidiChannel` instance in its internal array.\r\n3.  Each `MidiChannel` object is a state machine that maintains its own, independent state.\r\n\r\n**Each `MidiChannel` instance tracks:**\r\n*   The currently selected `Instrument` (changed by Program Change messages).\r\n*   The current MIDI Bank (set by CC 0 and CC 32).\r\n*   The list of its own active `IVoice`s.\r\n*   Channel-wide parameters like Volume (CC 7), Pan (CC 10), and Pitch Bend.\r\n*   The state of the Damper Pedal (Sustain, CC 64).\r\n\r\nThis architecture allows you to send a flurry of MIDI messages for different instruments on different channels to a single `Synthesizer` instance, and it will correctly route the data and generate the mixed audio output.\r\n\r\n## Complete Example: Playing Multiple Instruments\r\n\r\nThis example demonstrates how to control two different instruments on two different MIDI channels from a single `Synthesizer`.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.Structs;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing System;\r\nusing System.Linq;\r\nusing System.Threading;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"Synthesizer Multitimbrality Example\");\r\n\r\n        // 1. Standard setup\r\n        using var engine = new MiniAudioEngine();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n\r\n        // 2. Create a synthesizer with a bank that has multiple instruments.\r\n        var instrumentBank = new BasicInstrumentBank(format);\r\n        var synthesizer = new Synthesizer(engine, format, instrumentBank);\r\n\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n        device.Start();\r\n\r\n        Console.WriteLine(\"Playing a C Major chord on Channel 1 (Default Piano)...\");\r\n        // Status 0x90 = Note On, Channel 1\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 60, 100)); // C4\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 64, 100)); // E4\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 67, 100)); // G4\r\n\r\n        Thread.Sleep(1500);\r\n\r\n        Console.WriteLine(\"\\nNow, playing a melody on Channel 2 (Music Box)...\");\r\n\r\n        // Control Channel 2\r\n\r\n        // Status 0xC1 = Program Change, Channel 2. Program 10 is the Music Box in BasicInstrumentBank.\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0xC1, 10, 0));\r\n\r\n        // Status 0x91 = Note On, Channel 2\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x91, 72, 120)); // C5\r\n        Thread.Sleep(500);\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x91, 74, 120)); // D5\r\n        Thread.Sleep(500);\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x91, 76, 120)); // E5\r\n        Thread.Sleep(500);\r\n\r\n        Console.WriteLine(\"\\nReleasing all notes...\");\r\n        // Status 0x80 = Note Off, Channel 1\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 60, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 64, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 67, 0));\r\n\r\n        // Status 0x81 = Note Off, Channel 2\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x81, 72, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x81, 74, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x81, 76, 0));\r\n\r\n        Thread.Sleep(2000); // Wait for release tails to fade\r\n\r\n        device.Stop();\r\n        synthesizer.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n## Bank Select Messages\r\n\r\nThe MIDI specification allows for more than 128 programs by using Bank Select messages. A MIDI bank can be thought of as a folder or collection of 128 programs. To select an instrument from a specific bank, you must send Control Change messages **before** you send the Program Change message.\r\n\r\n*   **CC 0 (Bank Select MSB):** Sets the Most Significant Byte of the bank number.\r\n*   **CC 32 (Bank Select LSB):** Sets the Least Significant Byte of the bank number.\r\n\r\nThe final bank number is calculated as `(MSB * 128) + LSB`.\r\n\r\nMost modern synthesizers and SoundFonts simply use the MSB and ignore the LSB. For compatibility, the SoundFlow `Synthesizer` respects both.\r\n\r\n### Example: Selecting an Instrument from a Specific Bank\r\n\r\nThis is especially important when using a `SoundFontBank`, which often organizes presets across different banks.\r\n\r\n```csharp\r\n// Assume 'synthesizer' is an active instance.\r\nint targetBank = 1;   // The bank number you want to select\r\nint targetProgram = 24; // The program number within that bank\r\nint channel = 3;      // The MIDI channel to change\r\n\r\n// Calculate MSB and LSB from the target bank number\r\nbyte bankMsb = (byte)(targetBank / 128);\r\nbyte bankLsb = (byte)(targetBank % 128);\r\n\r\n// The MIDI status byte for CC on channel 3 is 0xB2 (0xB0 + (3 - 1))\r\nbyte ccStatus = (byte)(0xB0 + (channel - 1));\r\n\r\n// 1. Send Bank Select MSB (CC 0)\r\nsynthesizer.ProcessMidiMessage(new MidiMessage(ccStatus, 0, bankMsb));\r\n\r\n// 2. Send Bank Select LSB (CC 32)\r\nsynthesizer.ProcessMidiMessage(new MidiMessage(ccStatus, 32, bankLsb));\r\n\r\n// 3. Send the Program Change message\r\n// The MIDI status byte for Program Change on channel 3 is 0xC2\r\nbyte pcStatus = (byte)(0xC0 + (channel - 1));\r\nsynthesizer.ProcessMidiMessage(new MidiMessage(pcStatus, (byte)targetProgram, 0));\r\n\r\n// Now, any notes sent on channel 3 will play instrument 24 from bank 1.\r\n```\r\n\r\n<div className=\"flex items-center gap-3 my-4 p-4 rounded-lg bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n    <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n    <p className=\"text-sm\">\r\n        **GM Drum Channel:** In the General MIDI specification, Channel 10 is reserved for percussion. SoundFont banks often place their drum kits on Bank 128. To select a drum kit, you would typically send Bank Select messages to choose Bank 128, followed by a Program Change on Channel 10.\r\n    </p>\r\n</div>"
  },
  {
    "id": 17,
    "slug": "synthesis-mpe-support",
    "version": "1.3.0",
    "title": "MPE (MIDI Polyphonic Expression) Support",
    "description": "A guide to enabling and using MPE with the SoundFlow synthesizer for advanced, per-note expressive control.",
    "navOrder": 17,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 17\r\ntitle: MPE (MIDI Polyphonic Expression) Support\r\ndescription: A guide to enabling and using MPE with the SoundFlow synthesizer for advanced, per-note expressive control.\r\nnavOrder: 17\r\ncategory: Synthesis\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# MPE (MIDI Polyphonic Expression) Support\r\n\r\nSoundFlow v1.3.0 introduces comprehensive support for **MIDI Polyphonic Expression (MPE)**, an extension to the MIDI specification that allows for polyphonic, per-note expressive control. The `Synthesizer` component can be configured to interpret MPE data, enabling incredibly nuanced and dynamic performances that are not possible with standard MIDI.\r\n\r\n## What is MPE?\r\n\r\nIn standard MIDI, channel-wide messages like Pitch Bend and Channel Pressure (Aftertouch) affect all notes being played on that channel simultaneously. If you bend a note in a chord, all notes in the chord bend.\r\n\r\nMPE solves this by using a clever channel-routing system:\r\n1.  A **Master Channel** (typically 1 or 16) is used for global messages like Program Change and Sustain Pedal.\r\n2.  A block of adjacent channels, called **Member Channels**, are reserved for note data.\r\n3.  When a new note is played, the MPE controller temporarily assigns it its own unique Member Channel.\r\n4.  All subsequent expression for that note—such as pitch bend (sliding a finger left/right), pressure (pressing harder), and timbre (sliding up/down, often CC 74)—is sent as standard channel-wide messages on that note's dedicated channel.\r\n5.  When the note is released, its channel is freed up for a new note.\r\n\r\nThis allows for individual, polyphonic expression for each note in a chord, unlocking the full potential of modern expressive controllers like the ROLI Seaboard, LinnStrument, or Osmose.\r\n\r\n## How SoundFlow Handles MPE\r\n\r\nSoundFlow's MPE implementation is a collaboration between the `MidiManager` and the `Synthesizer`.\r\n\r\n1.  **Configuration:** You first tell the `MidiManager` that a specific input device is an MPE controller by calling `MidiManager.ConfigureMpeZone()`.\r\n2.  **Parsing:** The `MidiManager` then activates a special `MpeParser` for that device. This parser intercepts the raw MIDI stream and translates it into higher-level MPE event objects (e.g., `PerNotePitchBendEvent`, `PerNotePressureEvent`). It also handles the voice-to-channel allocation.\r\n3.  **Dispatching:** These high-level MPE events, along with the parsed Note On/Off messages, are then sent to the `Synthesizer`'s internal `ProcessMpeEvent()` method.\r\n4.  **Voice Modulation:** The `Synthesizer` routes these per-note expression events to the specific `IVoice` instance that is playing the target note. The `Voice` then uses this data to modulate its parameters in real-time (e.g., adjusting oscillator frequency for pitch bend, or filter cutoff for timbre).\r\n\r\n## Enabling and Using MPE\r\n\r\nHere is a complete, runnable example that demonstrates how to set up an MPE-enabled synthesizer and respond to per-note expression.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Routing;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing System;\r\nusing System.Linq;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"Synthesizer MPE Example\");\r\n\r\n        // 1. Standard engine and MIDI backend setup\r\n        using var engine = new MiniAudioEngine();\r\n        var portMidiBackend = engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n        var format = AudioFormat.DvdHq;\r\n\r\n        // 2. Find your MPE controller\r\n        var mpeControllerInfo = engine.MidiInputDevices.FirstOrDefault(d => d.Name.Contains(\"Your MPE Controller Name\"));\r\n        if (mpeControllerInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"MPE controller not found. Please update the device name in the code.\");\r\n            Console.ReadKey();\r\n            return;\r\n        }\r\n\r\n        // 3. Configure the MPE Zone in the MidiManager\r\n        // This example uses the ROLI Seaboard RISE default MPE configuration.\r\n        // Check your controller's manual for its specific settings.\r\n        var mpeZone = new MidiManager.MpeZone(\r\n            MasterChannel: 1,\r\n            MemberChannelsStart: 2,\r\n            MemberChannelCount: 15\r\n        );\r\n        engine.MidiManager.ConfigureMpeZone(mpeControllerInfo, mpeZone);\r\n        Console.WriteLine($\"Configured '{mpeControllerInfo.Name}' as an MPE device.\");\r\n\r\n        // 4. Create a Synthesizer and enable MPE mode\r\n        var instrumentBank = new BasicInstrumentBank(format);\r\n        var synthesizer = new Synthesizer(engine, format, instrumentBank)\r\n        {\r\n            Name = \"MPE Synth\",\r\n            MpeEnabled = true // This is the crucial step!\r\n        };\r\n        Console.WriteLine(\"Synthesizer MPE mode enabled.\");\r\n\r\n        // 5. Create a direct route from the MPE controller to the synthesizer\r\n        engine.MidiManager.CreateRoute(mpeControllerInfo, synthesizer);\r\n\r\n        // 6. Setup audio output\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n        device.Start();\r\n\r\n        Console.WriteLine(\"\\nMPE Synthesizer is active. Play your MPE controller.\");\r\n        Console.WriteLine(\"Try sliding your fingers left/right (pitch), up/down (timbre), and applying pressure.\");\r\n        Console.WriteLine(\"Press any key to exit.\");\r\n        Console.ReadKey();\r\n\r\n        // 7. Clean up\r\n        device.Stop();\r\n        synthesizer.Dispose();\r\n    }\r\n}\r\n```\r\n**Before running:**\r\n*   Replace `\"Your MPE Controller Name\"` with a unique part of your controller's name as it appears in MIDI device lists.\r\n*   Verify your controller's MPE settings match the `MpeZone` configuration (Master Channel, Member Channels). Most MPE controllers have a software utility to configure this.\r\n\r\n## How the `IVoice` Responds to MPE\r\n\r\nThe default oscillator-based `Voice` implementation is pre-wired to respond to MPE data:\r\n\r\n*   **Per-Note Pitch Bend:** Directly modulates the frequency of the voice's oscillators.\r\n*   **Per-Note Pressure:** Modulates the cutoff frequency of the voice's filter (if enabled). Higher pressure opens the filter, making the sound brighter.\r\n*   **Per-Note Timbre (CC 74):** Also modulates the filter cutoff frequency, providing a second dimension of timbral control.\r\n\r\n<div className=\"flex items-center gap-3 my-4 p-4 rounded-lg bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n    <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n    <p className=\"text-sm\">\r\n        When creating custom `IVoice` implementations, you can implement the `SetPerNotePitchBend`, `SetPerNotePressure`, and `SetPerNoteTimbre` methods to define how your custom voice responds to these rich expression events.\r\n    </p>\r\n</div>\r\n\r\nSample-based voices (from `SoundFontBank`) will respond to per-note pitch bend, but by default do not have built-in responses to pressure or timbre, as this requires specific modulation routing that is not standard in the SF2 format."
  },
  {
    "id": 20,
    "slug": "synthesis-layering-with-multiinstrumentbank",
    "version": "1.3.0",
    "title": "Layering with MultiInstrumentBank",
    "description": "Learn how to layer multiple instrument banks to create complex, layered sounds or custom fallback behaviors using the MultiInstrumentBank.",
    "navOrder": 20,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 20\r\ntitle: Layering with MultiInstrumentBank\r\ndescription: Learn how to layer multiple instrument banks to create complex, layered sounds or custom fallback behaviors using the MultiInstrumentBank.\r\nnavOrder: 20\r\ncategory: Synthesis\r\n---\r\n\r\n# Layering with MultiInstrumentBank\r\n\r\nThe `MultiInstrumentBank` (`SoundFlow.Synthesis.Banks.MultiInstrumentBank`) is a powerful container that allows you to manage and prioritize a collection of `IInstrumentBank` instances. It acts as a high-level bank that queries its child banks in a specific order, enabling sophisticated instrument layering and custom fallback behaviors.\r\n\r\n## Core Concept: Prioritized Searching\r\n\r\nWhen the `Synthesizer` requests an instrument from a `MultiInstrumentBank`, the `MultiInstrumentBank` searches through its list of child banks in **reverse order of how they were added**. The last bank added has the highest priority.\r\n\r\nThe search follows these rules:\r\n1.  It asks the highest-priority bank for the requested instrument (bank/program).\r\n2.  If that bank returns a valid, non-fallback `Instrument`, that instrument is used immediately, and the search stops.\r\n3.  If that bank returns an instrument marked as a `fallback`, the `MultiInstrumentBank` ignores it and proceeds to query the next-lower-priority bank.\r\n4.  This process continues until a non-fallback instrument is found.\r\n5.  If no non-fallback instrument is found after checking all child banks, the `MultiInstrumentBank` returns its own master fallback instrument.\r\n\r\nThis priority system allows you to **override** sounds. For example, you can load a large General MIDI SoundFont as a base layer (low priority) and then add a smaller bank with a few high-quality, custom synth patches on top (high priority). If a MIDI Program Change message requests a program that exists in your custom bank, it will be used; otherwise, the request will \"fall through\" to the General MIDI SoundFont.\r\n\r\n## Complete Example: Layering a SoundFont and a Basic Synth Bank\r\n\r\nThis example demonstrates how to create a `MultiInstrumentBank` to layer a custom synthesizer patch on top of a full SoundFont bank. We will override the \"Acoustic Grand Piano\" (Program 0) from the SoundFont with a custom electric piano sound from `BasicInstrumentBank`.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.Structs;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing SoundFlow.Synthesis.Instruments;\r\nusing System;\r\nusing System.IO;\r\nusing System.Linq;\r\nusing System.Threading;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"MultiInstrumentBank Layering Example\");\r\n\r\n        var soundFontPath = @\"C:\\path\\to\\your\\soundfont.sf2\";\r\n        if (!File.Exists(soundFontPath))\r\n        {\r\n            Console.WriteLine($\"Error: SoundFont file not found at '{soundFontPath}'.\");\r\n            return;\r\n        }\r\n\r\n        // 1. Standard engine and device setup\r\n        using var engine = new MiniAudioEngine();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n\r\n        // 2. Create the individual instrument banks\r\n        Console.WriteLine(\"Loading instrument banks...\");\r\n        // Bank 1: The large SoundFont (will have lower priority)\r\n        using var soundFontBank = new SoundFontBank(soundFontPath, format);\r\n        // Bank 2: Our simple synth bank (will have higher priority)\r\n        var basicSynthBank = new BasicInstrumentBank(format);\r\n\r\n        // 3. Create the master fallback instrument. This will be used if a sound is in neither bank.\r\n        var masterFallbackDef = new VoiceDefinition(format, SoundFlow.Components.Oscillator.WaveformType.Triangle, 1, 0, 0.1f, 0.1f, 1.0f, 0.1f);\r\n        var masterFallbackInstrument = new Instrument([], masterFallbackDef, isFallback: true);\r\n\r\n        // 4. Create and populate the MultiInstrumentBank\r\n        using var multiBank = new MultiInstrumentBank(masterFallbackInstrument); // The MultiInstrumentBank is also IDisposable to manage its child banks.\r\n\r\n        // Add the SoundFont bank first (it will have lower priority)\r\n        multiBank.AddBank(soundFontBank);\r\n        Console.WriteLine($\"Added '{Path.GetFileName(soundFontPath)}' as the base layer.\");\r\n\r\n        // Add the basic synth bank second (it will have higher priority)\r\n        multiBank.AddBank(basicSynthBank);\r\n        Console.WriteLine(\"Added 'BasicInstrumentBank' as the override layer.\");\r\n\r\n        // 5. Create the Synthesizer with the MultiInstrumentBank\r\n        var synthesizer = new Synthesizer(engine, format, multiBank);\r\n\r\n        // 6. Add to mixer and start device\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n        device.Start();\r\n\r\n        // Test the override\r\n\r\n        Console.WriteLine(\"\\n--- Playing Program 0 (Acoustic Grand Piano) ---\");\r\n        // We send a Program Change for program 0.\r\n        // The MultiInstrumentBank will first check 'basicSynthBank'.\r\n        // 'basicSynthBank' HAS an instrument for program 0 (the electric piano).\r\n        // Since it's not a fallback, it will be used, and the SoundFont's piano will be ignored.\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0xC0, 0, 0)); // Program Change to 0 on Channel 1\r\n\r\n        // Play a chord\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 60, 100));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 64, 100));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 67, 100));\r\n        Console.WriteLine(\"You should hear the electric piano from BasicInstrumentBank.\");\r\n        Thread.Sleep(3000);\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 60, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 64, 0));\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 67, 0));\r\n        Thread.Sleep(1000);\r\n\r\n        // Test the fallback\r\n\r\n        Console.WriteLine(\"\\n--- Playing Program 10 (Music Box) ---\");\r\n        // We send a Program Change for program 10.\r\n        // MultiInstrumentBank checks 'basicSynthBank'. It also has an instrument for program 10, so it will be used.\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0xC0, 10, 0));\r\n\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 72, 120));\r\n        Console.WriteLine(\"You should hear the Music Box synth from BasicInstrumentBank.\");\r\n        Thread.Sleep(2000);\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 72, 0));\r\n        Thread.Sleep(1000);\r\n\r\n        Console.WriteLine(\"\\n--- Playing a program that only exists in the SoundFont (e.g., Program 5) ---\");\r\n        // We send a Program Change for program 5.\r\n        // MultiInstrumentBank checks 'basicSynthBank'. It does NOT have an instrument for program 5. It returns its fallback.\r\n        // Because it was a fallback, MultiInstrumentBank continues and checks 'soundFontBank'.\r\n        // Assuming the SoundFont has an instrument for program 5, it will be returned and used.\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0xC0, 5, 0));\r\n\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x90, 65, 100));\r\n        Console.WriteLine(\"You should hear the instrument for Program 5 from your SoundFont file.\");\r\n        Thread.Sleep(2000);\r\n        synthesizer.ProcessMidiMessage(new MidiMessage(0x80, 65, 0));\r\n        Thread.Sleep(1000);\r\n\r\n        // 7. Clean up\r\n        device.Stop();\r\n        synthesizer.Dispose();\r\n    }\r\n}\r\n```\r\n\r\nThis example clearly illustrates the power of the `MultiInstrumentBank`. It provides a clean, predictable way to combine multiple sound sources, allowing you to build up a custom sound library by layering different banks and overriding specific instruments where needed."
  },
  {
    "id": 15,
    "slug": "synthesis-instrument-model",
    "version": "1.3.0",
    "title": "The Instrument Model",
    "description": "An in-depth look at the data model that defines a synthesizer sound, including Instrument Banks, Voice Mappings, and Voice Definitions.",
    "navOrder": 15,
    "category": "Synthesis",
    "content": "﻿---\r\nid: 15\r\ntitle: The Instrument Model\r\ndescription: An in-depth look at the data model that defines a synthesizer sound, including Instrument Banks, Voice Mappings, and Voice Definitions.\r\nnavOrder: 15\r\ncategory: Synthesis\r\n---\r\n\r\n# The Instrument Model\r\n\r\nThe SoundFlow synthesizer's power and flexibility come from its hierarchical data model for defining sounds.\r\nThis model allows for everything from simple single-oscillator patches to complex, multi-layered, velocity-switched sampled instruments with unison stacking (\"Supersaw\").\r\nUnderstanding this hierarchy is key to creating your own sounds or using pre-existing sound libraries like SoundFonts.\r\n\r\n## The Hierarchy of Sound\r\n\r\nA sound is defined by a chain of objects, each responsible for a different level of abstraction. The relationship can be visualized as follows:\r\n\r\n```\r\nSynthesizer\r\n└── IInstrumentBank (A collection of instruments)\r\n└── Instrument (A single patch, e.g., \"Grand Piano\")\r\n└── VoiceMapping (A rule, e.g., \"Play this sound for soft notes on the lower keys\")\r\n└── VoiceDefinition (The recipe for that sound, e.g., \"Use a sine wave with 7 voices and 20% detune\")\r\n└── IVoice (The active sound instance playing a single note)\r\n```\r\n\r\nLet's break down each component in detail.\r\n\r\n### 1. `IInstrumentBank` (The Library)\r\n\r\nThe `IInstrumentBank` is an interface representing a collection of instruments. The `Synthesizer` uses an implementation of this interface to find which sound to play when it receives a MIDI Program Change message.\r\n\r\n*   **Core Method:** `Instrument GetInstrument(int bank, int program)`\r\nThis method is the heart of the interface. The synthesizer calls it with the current MIDI bank and program number, and the bank is expected to return the corresponding `Instrument` object.\r\n\r\n*   **Implementations:** SoundFlow provides several implementations:\r\n*   **`BasicInstrumentBank`:** A simple, hardcoded bank with a few basic synth sounds. Perfect for getting started and for testing.\r\n*   **`SoundFontBank`:** A powerful implementation that loads a standard `.sf2` SoundFont file, parsing all of its presets into playable `Instrument`s.\r\n*   **`MultiInstrumentBank`:** A container that holds a prioritized list of other banks, allowing you to layer them.\r\n\r\n### 2. `Instrument` (The Patch)\r\n\r\nAn `Instrument` represents a single, complete sound or preset, such as \"Acoustic Grand Piano,\" \"Synth Bass,\" or \"Drum Kit.\" It doesn't define the sound itself but holds a list of `VoiceMapping` rules that determine *how* the sound is constructed in response to different performance inputs.\r\n\r\n*   **Key Property:** A list of `VoiceMapping` objects.\r\n*   **Fallback Definition:** Each instrument also has a fallback `VoiceDefinition` that is used if an incoming note doesn't match any of its specific mapping rules.\r\n*   **`IsFallback` Property:** A boolean that indicates if this instrument is just a placeholder. When searching through banks, the synthesizer will keep looking if it only finds fallback instruments.\r\n\r\n### 3. `VoiceMapping` (The Rule)\r\n\r\nA `VoiceMapping` is the core of a multi-sampled or layered instrument. It's a rule that says, \"If a MIDI note falls within this key range and this velocity range, then use this specific `VoiceDefinition`.\"\r\n\r\nThis allows for creating expressive instruments:\r\n*   **Velocity Layers:** Different samples or synth settings for soft, medium, and hard key presses.\r\n*   **Key Splits:** Different sounds across the keyboard (e.g., a bass sound on the left hand and a piano on the right).\r\n\r\n**Key Properties:**\r\n*   `Definition`: The `VoiceDefinition` to trigger if this mapping's conditions are met.\r\n*   `MinKey` / `MaxKey`: The MIDI note number range (inclusive, 0-127).\r\n*   `MinVelocity` / `MaxVelocity`: The velocity range (inclusive, 0-127).\r\n\r\nWhen a `NoteOn` event occurs, the `Instrument` iterates through its list of `VoiceMapping`s and uses the first one whose `IsMatch(noteNumber, velocity)` method returns `true`.\r\n\r\n### 4. `VoiceDefinition` (The Recipe)\r\n\r\nThe `VoiceDefinition` is the \"recipe\" for a sound. It contains all the parameters needed to construct a single, playable `IVoice`. This is where the actual sound design happens.\r\n\r\n**Core Parameters (in constructor):**\r\n*   `Format`: The `AudioFormat` of the engine.\r\n*   `OscillatorType`: The waveform for the oscillator(s) (e.g., `Sine`, `Sawtooth`).\r\n*   `AttackTime`, `DecayTime`, `SustainLevel`, `ReleaseTime`: The parameters for the ADSR amplitude envelope.\r\n*   `UseFilter`: A boolean indicating whether a low-pass filter (with its own envelope) should be applied.\r\n*   `Sample`: An optional `SampleData` object. If this is provided, the voice will be a sample player instead of an oscillator-based synth.\r\n\r\n#### Unison and Detune (Creating \"Thick\" Sounds)\r\n\r\nThe `VoiceDefinition` supports unison stacking, allowing a single note to trigger multiple oscillators simultaneously. This is critical for creating rich, wide synthesizer sounds like the classic \"Supersaw\" lead or lush pads.\r\n\r\n*   **`Unison` (int):** The number of oscillators to stack per note. A value of `1` is a standard mono voice. A value of `7` creates 7 distinct oscillators for each key press.\r\n*   **`Detune` (float):** The spread of frequencies for the unison voices.\r\n*   A value of `0` means all voices play the exact same pitch (causing phasing).\r\n*   A value of `0.01` represents a 1% frequency deviation for the outermost voices, creating a thick, chorused effect.\r\n\r\n**Internal Behavior:**\r\nWhen a `Voice` is created with `Unison > 1`, SoundFlow automatically:\r\n1.  Instantiates multiple `UnisonLayer` objects.\r\n2.  Spreads the pitch of each layer symmetrically around the center frequency based on the `Detune` amount.\r\n3.  Spreads the **pan** of each layer across the stereo field (e.g., voice 1 hard left, voice 7 hard right, voice 4 center) to create a wide stereo image.\r\n4.  Normalizes the gain to prevent clipping (so 7 voices aren't 7x louder than 1).\r\n\r\n**Example: Creating a \"Supersaw\" Recipe**\r\n```csharp\r\nvar superSawDef = new VoiceDefinition(\r\n    format: myAudioFormat,\r\n    oscType: Oscillator.WaveformType.Sawtooth,\r\n    unison: 7,        // Stack 7 sawtooth waves\r\n    detune: 0.015f,   // 1.5% detune spread for a thick sound\r\n    attack: 0.01f,    // Fast attack\r\n    decay: 0.5f,\r\n    sustain: 1.0f,    // Full sustain\r\n    release: 0.5f\r\n);\r\n```\r\n\r\n**Core Method:** `IVoice CreateVoice(VoiceContext context)`\r\nThis factory method is called by the synthesizer to create a new, active `IVoice` instance based on the recipe's parameters and the specific context of the incoming note (frequency, velocity, etc.).\r\n\r\n### 5. `IVoice` (The Active Sound)\r\n\r\nAn `IVoice` is the final, active object that actually generates audio samples.\r\n*   It is created by a `VoiceDefinition` when a `NoteOn` message is processed.\r\n*   It has a lifecycle: it's created, it renders audio for the duration of the note, it enters a release phase on `NoteOff`, and it signals it's finished (`IsFinished`) once its envelope has faded to silence.\r\n*   The `Synthesizer` manages a pool of active voices, adding new ones on `NoteOn` and removing them once they are finished.\r\n\r\n## Putting It Together: A Manual Instrument Example\r\n\r\nThis conceptual example demonstrates how you would manually construct an instrument with two velocity layers.\r\n\r\n```csharp\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis.Instruments;\r\nusing System.Collections.Generic;\r\n\r\n// Assume 'format' is a valid AudioFormat instance (e.g., AudioFormat.DvdHq)\r\n\r\n// Define the Voice \"Recipes\"\r\n\r\n// A soft, gentle sine wave for low velocities\r\nvar softVoiceDef = new VoiceDefinition(\r\n    format,\r\n    oscType: Oscillator.WaveformType.Sine,\r\n    unison: 1,\r\n    detune: 0,\r\n    attack: 0.05f,\r\n    decay: 0.8f,\r\n    sustain: 0.1f,\r\n    release: 0.5f\r\n);\r\n\r\n// A brighter, thick \"Supersaw\" for high velocities\r\nvar hardVoiceDef = new VoiceDefinition(\r\n    format,\r\n    oscType: Oscillator.WaveformType.Sawtooth,\r\n    unison: 5,        // 5 stacked oscillators\r\n    detune: 0.008f,   // Mild detune\r\n    attack: 0.01f,\r\n    decay: 0.5f,\r\n    sustain: 0.6f,\r\n    release: 0.3f\r\n);\r\n\r\n// A fallback definition in case no mapping matches (though our mappings cover all cases)\r\nvar fallbackDef = new VoiceDefinition(format, Oscillator.WaveformType.Triangle, 1, 0, 0.1f, 0.1f, 1.0f, 0.1f);\r\n\r\n// Create the Mapping Rules\r\n\r\n// Rule 1: For velocities 1-64, use the soft definition\r\nvar softMapping = new VoiceMapping(softVoiceDef)\r\n{\r\n    MinVelocity = 1,\r\n    MaxVelocity = 64\r\n    // MinKey and MaxKey default to 0-127, covering the whole keyboard\r\n};\r\n\r\n// Rule 2: For velocities 65-127, use the hard definition\r\nvar hardMapping = new VoiceMapping(hardVoiceDef)\r\n{\r\n    MinVelocity = 65,\r\n    MaxVelocity = 127\r\n};\r\n\r\n// Assemble the Instrument\r\n\r\nvar velocityPiano = new Instrument(\r\n    mappings: new List<VoiceMapping> { softMapping, hardMapping },\r\n    fallbackDefinition: fallbackDef\r\n);\r\n\r\n// Now, 'velocityPiano' can be added to an IInstrumentBank and used by a Synthesizer.\r\n// When the synthesizer calls instrument.GetVoiceDefinition(note, velocity), it will get:\r\n// - softVoiceDef for a note with velocity 50.\r\n// - hardVoiceDef for a note with velocity 110.\r\n```"
  },
  {
    "id": 13,
    "slug": "portmidi-backend",
    "version": "1.3.0",
    "title": "PortMidi Backend for MIDI I/O & Synchronization",
    "description": "A guide to the SoundFlow.Midi.PortMidi package, explaining how to enable cross-platform MIDI input/output, routing, and advanced synchronization (MIDI Clock, MTC).",
    "navOrder": 13,
    "category": "Extensions",
    "content": "﻿---\r\nid: 13\r\ntitle: PortMidi Backend for MIDI I/O & Synchronization\r\ndescription: A guide to the SoundFlow.Midi.PortMidi package, explaining how to enable cross-platform MIDI input/output, routing, and advanced synchronization (MIDI Clock, MTC).\r\nnavOrder: 13\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\n\r\n# PortMidi Backend for MIDI I/O & Synchronization\r\n\r\nThe `SoundFlow.Midi.PortMidi` package is an extension that provides a robust, cross-platform MIDI backend for the SoundFlow audio engine.\r\n\r\nThis package implements the required `IMidiBackend` interface and wraps the widely-supported PortMidi library. This integration enables your applications to communicate seamlessly with external MIDI hardware and software for both input and output (I/O).\r\n\r\nBeyond basic I/O, this backend integrates deeply with the `AudioEngine`'s new synchronization events. This offers advanced features critical for complex timing setups. For instance, the backend allows you to slave your composition's transport to an external MIDI Clock. Alternatively, it can act as a sample-accurate master clock source for your external hardware.\r\n\r\n## Features\r\n\r\n*   **Cross-Platform MIDI I/O:** Send and receive MIDI channel messages (Note On/Off, CC, Pitch Bend, etc.) on Windows, macOS, and Linux.\r\n*   **System Exclusive (SysEx):** Full support for sending and receiving large SysEx messages for deep hardware integration.\r\n*   **Seamless Integration:** A simple extension method, `UsePortMidi()`, enables the backend and configures the `MidiManager`.\r\n*   **MIDI Clock Synchronization (Master Mode):** Act as a master clock source, sending sample-accurate MIDI Clock, Start, Stop, and Continue messages to synchronize external hardware (drum machines, sequencers) to your SoundFlow application's transport.\r\n*   **MIDI Clock Synchronization (Slave Mode):** Synchronize a `Composition`'s playback to an external MIDI Clock source. SoundFlow will listen for Start, Stop, and Continue messages and adjust its transport to follow the incoming clock ticks.\r\n*   **MIDI Time Code (MTC) Synchronization (Slave Mode):** Synchronize a `Composition`'s playback position to incoming MIDI Time Code, perfect for video and post-production workflows.\r\n\r\n## Installation\r\n\r\nTo use the PortMidi backend, add the `SoundFlow.Midi.PortMidi` package to your project. This will also bring in the core `SoundFlow` library if it's not already there.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Midi.PortMidi\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Midi.PortMidi\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Getting Started: Basic MIDI I/O\r\n\r\nThis example demonstrates the fundamental workflow: enabling the backend, listing devices, receiving messages from an input, and sending a message to an output.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.Devices;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Structs;\r\nusing System;\r\nusing System.Linq;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        // 1. Initialize the audio engine.\r\n        using var engine = new MiniAudioEngine();\r\n\r\n        // 2. Enable the PortMidi backend. This is the crucial step.\r\n        // It returns the backend instance, which we can ignore for basic I/O.\r\n        engine.UsePortMidi();\r\n        Console.WriteLine(\"PortMidi backend enabled.\");\r\n\r\n        // 3. Refresh the list of available MIDI devices.\r\n        engine.UpdateMidiDevicesInfo();\r\n\r\n        // 4. Select input and output devices.\r\n        var inputDevice = engine.MidiInputDevices.FirstOrDefault();\r\n        var outputDevice = engine.MidiOutputDevices.FirstOrDefault();\r\n\r\n        if (inputDevice.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI input device found. Cannot receive messages.\");\r\n        }\r\n        else\r\n        {\r\n            // 5. Get a managed input node from the MidiManager.\r\n            var inputNode = engine.MidiManager.GetOrCreateInputNode(inputDevice);\r\n            Console.WriteLine($\"Listening for MIDI messages on '{inputDevice.Name}'...\");\r\n\r\n            // 6. Subscribe to its output to receive messages.\r\n            inputNode.OnMessageOutput += OnMidiMessageReceived;\r\n        }\r\n\r\n        if (outputDevice.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI output device found. Cannot send messages.\");\r\n        }\r\n        else\r\n        {\r\n            // 7. Get a managed output node.\r\n            var outputNode = engine.MidiManager.GetOrCreateOutputNode(outputDevice);\r\n            Console.WriteLine($\"Ready to send MIDI messages to '{outputDevice.Name}'.\");\r\n\r\n            // 8. Send a Middle C Note On message to the output device.\r\n            var noteOnMessage = new MidiMessage(0x90, 60, 100); // Note On, Channel 1, Middle C, Velocity 100\r\n            outputNode.ProcessMessage(noteOnMessage);\r\n            Console.WriteLine(\"Sent a Middle C Note On message.\");\r\n        }\r\n\r\n        Console.WriteLine(\"\\nPress any key to exit.\");\r\n        Console.ReadKey();\r\n    }\r\n\r\n    private static void OnMidiMessageReceived(MidiMessage message)\r\n    {\r\n        Console.WriteLine($\"Received MIDI Message: \" +\r\n                          $\"Cmd: {message.Command}, \" +\r\n                          $\"Ch: {message.Channel}, \" +\r\n                          $\"Data1: {message.Data1}, \" +\r\n                          $\"Data2: {message.Data2}\");\r\n    }\r\n}\r\n```\r\n\r\n## Advanced Topic: MIDI Synchronization\r\n\r\nThe `PortMidiBackend` provides powerful synchronization capabilities by integrating with the `AudioEngine`'s transport events.\r\n\r\n### Slave Mode: Syncing SoundFlow to External Gear\r\n\r\nIn Slave Mode, your `Composition`'s transport (play, stop, position) is controlled by an external MIDI source.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.PortMidi.Enums;\r\nusing SoundFlow.Structs;\r\nusing System.Linq;\r\n\r\n// Setup\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\n\r\n// Enable the PortMidi backend and get a reference to it.\r\nvar portMidiBackend = engine.UsePortMidi();\r\n\r\nengine.UpdateMidiDevicesInfo();\r\nvar syncInputDevice = engine.MidiInputDevices.FirstOrDefault();\r\n\r\nif (syncInputDevice.Name == null)\r\n{\r\n    Console.WriteLine(\"No MIDI input device found for sync.\");\r\n    return;\r\n}\r\n\r\n// Create a composition that will be controlled.\r\nvar composition = new Composition(engine, format, \"My Synced Project\");\r\n// ... add audio and MIDI tracks to the composition ...\r\n\r\n// Configuration\r\nportMidiBackend.ConfigureSync(\r\n    SyncMode.Slave,\r\n    SyncSource.MidiClock, // Or SyncSource.Mtc\r\n    syncInputDevice,\r\n    null, // No output device needed for slave mode\r\n    composition.Renderer // The renderer's transport will be controlled\r\n);\r\nConsole.WriteLine($\"Composition is now slaved to MIDI Clock from '{syncInputDevice.Name}'.\");\r\n\r\n// Monitoring\r\nportMidiBackend.OnSyncStatusChanged += status => Console.WriteLine($\"Sync Status: {status}\");\r\nportMidiBackend.OnBpmChanged += bpm => Console.WriteLine($\"Detected BPM: {bpm:F2}\");\r\n\r\n// Playback\r\nusing var device = engine.InitializePlaybackDevice(null, format);\r\nvar player = new SoundPlayer(engine, format, composition.Renderer);\r\ndevice.MasterMixer.AddComponent(player);\r\ndevice.Start();\r\nplayer.Play(); // Start the player, it will now wait for MIDI transport commands.\r\n\r\nConsole.WriteLine(\"Waiting for MIDI Start/Clock messages. Press any key to exit.\");\r\nConsole.ReadKey();\r\n```\r\n\r\n### Master Mode: Making SoundFlow the Clock Source\r\n\r\nIn Master Mode, your `Composition`'s playback drives the MIDI clock, synchronizing external hardware to your application.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.PortMidi.Enums;\r\nusing SoundFlow.Structs;\r\nusing System.Linq;\r\n\r\n// Setup\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\n\r\nvar portMidiBackend = engine.UsePortMidi();\r\n\r\nengine.UpdateMidiDevicesInfo();\r\nvar syncOutputDevice = engine.MidiOutputDevices.FirstOrDefault();\r\n\r\nif (syncOutputDevice.Name == null)\r\n{\r\n    Console.WriteLine(\"No MIDI output device found for sync.\");\r\n    return;\r\n}\r\n\r\nvar composition = new Composition(engine, format, \"My Master Project\");\r\n// ... add tracks ...\r\n\r\n// Configuration\r\nportMidiBackend.ConfigureSync(\r\n    SyncMode.Master,\r\n    SyncSource.Internal, // Not used in Master mode\r\n    null, // No input device\r\n    syncOutputDevice,\r\n    composition.Renderer // The renderer is the master clock source\r\n);\r\nConsole.WriteLine($\"SoundFlow is now Master MIDI Clock, outputting to '{syncOutputDevice.Name}'.\");\r\n\r\n// Playback\r\nusing var device = engine.InitializePlaybackDevice(null, format);\r\nvar player = new SoundPlayer(engine, format, composition.Renderer);\r\ndevice.MasterMixer.AddComponent(player);\r\ndevice.Start();\r\n\r\nConsole.WriteLine(\"Press any key to start sending MIDI Clock and playing...\");\r\nConsole.ReadKey();\r\n\r\n// Starting the player will now automatically send MIDI Start and Clock messages.\r\nplayer.Play();\r\n\r\nConsole.WriteLine(\"Sending MIDI Clock. Press any key to stop.\");\r\nConsole.ReadKey();\r\n```\r\n\r\n## Error Handling\r\n\r\nErrors originating from the PortMidi library will throw a `PortBackendException`. This exception contains a `PortMidiError` enum that provides specific details about the failure.\r\n\r\n```csharp\r\ntry\r\n{\r\n    var node = engine.MidiManager.GetOrCreateOutputNode(someDeviceInfo);\r\n}\r\ncatch (PortBackendException ex)\r\n{\r\n    Console.WriteLine($\"A PortMidi error occurred: {ex.ErrorCode}\");\r\n    // ex.ErrorCode could be...\r\n    // - HostError: An OS-level error occurred.\r\n    // - InvalidDeviceId: The device ID was invalid (e.g., device disconnected).\r\n    // - DeviceIsBusy: The device is already in use by another application.\r\n    // ... and others.\r\n}\r\n```\r\n\r\nKey `PortMidiError` values include:\r\n*   `HostError`: An error was reported by the host operating system's MIDI API.\r\n*   `InvalidDeviceId`: The provided device ID is no longer valid, which can happen if a device is disconnected.\r\n*   `DeviceIsBusy`: The device is already in use by another application in an exclusive mode.\r\n*   `InsufficientMemory`: A memory allocation failed within the native library."
  },
  {
    "id": 1.1,
    "slug": "migration-guide",
    "version": "1.3.0",
    "title": "Migration Guide (v1.2 to v1.3)",
    "description": "A guide to updating your code from SoundFlow v1.2.x to the new v1.3.0 architecture, covering all breaking changes.",
    "navOrder": 1.1,
    "category": "Core",
    "content": "﻿---\r\nid: 1.1\r\ntitle: Migration Guide (v1.2 to v1.3)\r\ndescription: A guide to updating your code from SoundFlow v1.2.x to the new v1.3.0 architecture, covering all breaking changes.\r\nnavOrder: 1.1\r\ncategory: Core\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Card, CardHeader, CardBody } from \"@heroui/react\";\r\n\r\n# Migration Guide: v1.2.1 to v1.3.0\r\n\r\nWelcome to SoundFlow v1.3.0! This is a major feature release that introduces a powerful pluggable architecture for codecs and MIDI, a built-in software synthesizer, and a comprehensive real-time control system.\r\n\r\nTo enable these new capabilities, some core APIs have been refactored, resulting in a few necessary breaking changes. This guide is designed to walk you through each change with clear before-and-after examples to make the migration process as smooth as possible.\r\n\r\n## High-Level Architectural Changes\r\n\r\nThe breaking changes in v1.3.0 were made to support three key architectural improvements:\r\n\r\n1.  **Pluggable Codec System:** The old, hardcoded audio format handling is replaced by a flexible, priority-based factory system. This allows SoundFlow to support virtually any audio format by adding a codec extension.\r\n2.  **Pluggable MIDI System:** The engine is now fully MIDI-aware through a new backend system, enabling MIDI I/O, routing, synchronization, and real-time control.\r\n3.  **Composition Refactor:** The `Composition` class has been refactored into a cleaner data model with dedicated service classes (`Editor`, `Renderer`, `Recorder`) to better organize its expanding feature set, including MIDI tracks and recording.\r\n\r\n## Breaking Changes in Detail\r\n\r\nHere is a step-by-step guide to updating your code for each breaking change.\r\n\r\n<div className=\"mt-6\"> {/* Optional: Adds initial space before the first card */}\r\n    <Card className=\"bg-content1\">\r\n        <CardHeader>\r\n            <div className=\"flex items-center gap-3\">\r\n                <Icon icon=\"mdi:file-code-outline\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                <h3 className=\"font-semibold text-lg\">1. Codec and Provider System Overhaul</h3>\r\n            </div>\r\n        </CardHeader>\r\n        <CardBody>\r\n            The `EncodingFormat` enum has been removed in favor of extensible string-based format identifiers (e.g., \"wav\", \"mp3\"). Additionally, `ISoundDataProvider` constructors have been updated to support automatic format detection.\r\n\r\n            <p className=\"mt-2 text-sm text-foreground-500\">\r\n                <strong>Reason for Change:</strong> To support an unlimited number of audio formats via the new `ICodecFactory` system and simplify data provider creation.\r\n            </p>\r\n            <Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Codec and Provider Changes\" className=\"mt-4\">\r\n                <Tab key=\"before\" title=\"Before (v1.2.1)\">\r\n                    ```csharp\r\n                    // Creating a provider required a known format\r\n                    using var dataProvider = new StreamDataProvider(engine, format, stream);\r\n\r\n                    // Creating an encoder used the EncodingFormat enum\r\n                    using var encoder = engine.CreateEncoder(stream, EncodingFormat.Wav, format);\r\n\r\n                    // Creating a recorder also used the enum\r\n                    using var recorder = new Recorder(captureDevice, stream, EncodingFormat.Wav);\r\n                    ```\r\n                </Tab>\r\n                <Tab key=\"after\" title=\"After (v1.3.0)\">\r\n                    ```csharp\r\n                    // Creating a provider can now auto-detect the format\r\n                    using var dataProvider = new StreamDataProvider(engine, stream);\r\n                    // Or you can still provide a target format hint\r\n                    using var dataProviderWithHint = new StreamDataProvider(engine, format, stream);\r\n\r\n                    // Creating an encoder now uses a string format ID\r\n                    using var encoder = engine.CreateEncoder(stream, \"wav\", format);\r\n\r\n                    // Creating a recorder also uses a string format ID\r\n                    using var recorder = new Recorder(captureDevice, stream, \"wav\");\r\n                    ```\r\n                </Tab>\r\n            </Tabs>\r\n        </CardBody>\r\n    </Card>\r\n\r\n    <Card className=\"bg-content1 mt-6\">\r\n        <CardHeader>\r\n            <div className=\"flex items-center gap-3\">\r\n                <Icon icon=\"ph:stack-bold\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                <h3 className=\"font-semibold text-lg\">2. Composition and Editing Engine Refactor</h3>\r\n            </div>\r\n        </CardHeader>\r\n        <CardBody>\r\n            The `Composition` class is no longer an `ISoundDataProvider` itself. Its methods for manipulation and playback have been moved to dedicated service classes: `Editor`, `Renderer`, and `Recorder`.\r\n\r\n            <p className=\"mt-2 text-sm text-foreground-500\">\r\n                <strong>Reason for Change:</strong> To separate the `Composition` data model from its operations, making the architecture cleaner and more extensible for new features like MIDI tracks, recording, and mapping.\r\n            </p>\r\n            <Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Composition Refactor Changes\" className=\"mt-4\">\r\n                <Tab key=\"before\" title=\"Before (v1.2.1)\">\r\n                    ```csharp\r\n                    // Composition did not require an engine in constructor\r\n                    var composition = new Composition(format, \"My Song\");\r\n                    composition.AddTrack(myTrack);\r\n\r\n                    // The composition itself was the data provider for playback\r\n                    var player = new SoundPlayer(engine, format, composition);\r\n\r\n                    // Rendering was a method on Composition\r\n                    var audio = composition.Render(TimeSpan.Zero, duration);\r\n                    ```\r\n                </Tab>\r\n                <Tab key=\"after\" title=\"After (v1.3.0)\">\r\n                    ```csharp\r\n                    // Composition now requires the AudioEngine instance in its constructor\r\n                    var composition = new Composition(engine, format, \"My Song\");\r\n                    // Editing methods are now on the Editor service\r\n                    composition.Editor.AddTrack(myTrack);\r\n\r\n                    // Playback now uses the new Renderer property, which is an ISoundDataProvider\r\n                    var player = new SoundPlayer(engine, format, composition.Renderer);\r\n\r\n                    // Rendering is also done via the Renderer service\r\n                    var audio = composition.Renderer.Render(TimeSpan.Zero, duration);\r\n                    ```\r\n                </Tab>\r\n            </Tabs>\r\n        </CardBody>\r\n    </Card>\r\n\r\n    <Card className=\"bg-content1 mt-6\">\r\n        <CardHeader>\r\n            <div className=\"flex items-center gap-3\">\r\n                <Icon icon=\"icon-park-outline:sound-wave\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                <h3 className=\"font-semibold text-lg\">3. `Filter` is now a `SoundModifier`</h3>\r\n            </div>\r\n        </CardHeader>\r\n        <CardBody>\r\n            The `Filter` class now inherits from `SoundModifier` instead of `SoundComponent`.\r\n\r\n            <p className=\"mt-2 text-sm text-foreground-500\">\r\n                <strong>Reason for Change:</strong> A filter's role is to modify an existing audio stream, which aligns perfectly with the `SoundModifier` pattern. This makes its usage consistent with other effects and allows it to be used in the powerful modifier chains of the editing engine.\r\n            </p>\r\n            <Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Filter Change\" className=\"mt-4\">\r\n                <Tab key=\"before\" title=\"Before (v1.2.1)\">\r\n                    ```csharp\r\n                    var player = new SoundPlayer(engine, format, dataProvider);\r\n                    var filter = new Filter(engine, format) { CutoffFrequency = 500f };\r\n\r\n                    // Filter was connected as a separate component in the graph\r\n                    filter.ConnectInput(player);\r\n                    device.MasterMixer.AddComponent(filter);\r\n                    ```\r\n                </Tab>\r\n                <Tab key=\"after\" title=\"After (v1.3.0)\">\r\n                    ```csharp\r\n                    var player = new SoundPlayer(engine, format, dataProvider);\r\n\r\n                    // Filter constructor no longer needs the engine\r\n                    var filter = new Filter(format) { CutoffFrequency = 500f };\r\n\r\n                    // Filter is now added as a modifier directly to the component\r\n                    player.AddModifier(filter);\r\n                    device.MasterMixer.AddComponent(player);\r\n                    ```\r\n                </Tab>\r\n            </Tabs>\r\n        </CardBody>\r\n    </Card>\r\n\r\n    <Card className=\"bg-content1 mt-6\">\r\n        <CardHeader>\r\n            <div className=\"flex items-center gap-3\">\r\n                <Icon icon=\"lucide:refresh-cw\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                <h3 className=\"font-semibold text-lg\">4. Splitting of `UpdateDevicesInfo()`</h3>\r\n            </div>\r\n        </CardHeader>\r\n        <CardBody>\r\n            The `AudioEngine.UpdateDevicesInfo()` method has been marked as obsolete and replaced by two specific methods for audio and MIDI devices.\r\n\r\n            <p className=\"mt-2 text-sm text-foreground-500\">\r\n                <strong>Reason for Change:</strong> To separate the enumeration of audio hardware from MIDI hardware, which is now managed by its own pluggable backend system.\r\n            </p>\r\n            <Tabs color=\"primary\" variant=\"bordered\" aria-label=\"UpdateDevicesInfo Change\" className=\"mt-4\">\r\n                <Tab key=\"before\" title=\"Before (v1.2.1)\">\r\n                    ```csharp\r\n                    engine.UpdateDevicesInfo();\r\n                    // Now you can access engine.PlaybackDevices and engine.CaptureDevices\r\n                    ```\r\n                </Tab>\r\n                <Tab key=\"after\" title=\"After (v1.3.0)\">\r\n                    ```csharp\r\n                    // For audio devices\r\n                    engine.UpdateAudioDevicesInfo();\r\n\r\n                    // For MIDI devices (after enabling a MIDI backend)\r\n                    engine.UpdateMidiDevicesInfo();\r\n                    ```\r\n                </Tab>\r\n            </Tabs>\r\n        </CardBody>\r\n    </Card>\r\n\r\n    <Card className=\"bg-content1 mt-6\">\r\n        <CardHeader>\r\n            <div className=\"flex items-center gap-3\">\r\n                <Icon icon=\"lucide:alert-triangle\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                <h3 className=\"font-semibold text-lg\">5. New <code>Result&lt;T&gt;</code> Error Handling Pattern</h3>\r\n            </div>\r\n        </CardHeader>\r\n        <CardBody>\r\n            The simple `SoundFlow.Enums.Result` enum has been removed. Many new features, especially the `SoundMetadataReader`, now return a `SoundFlow.Structs.Result<T>` struct instead of throwing exceptions for common, expected failures (like reading a file with an unknown format). The `BackendException` constructor has also been updated to use an integer result code.\r\n\r\n            <p className=\"mt-2 text-sm text-foreground-500\">\r\n                <strong>Reason for Change:</strong> To adopt a more modern, explicit, and robust approach to error handling that avoids costly exceptions for predictable failure scenarios.\r\n            </p>\r\n            <Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Result Pattern Change\" className=\"mt-4\">\r\n                <Tab key=\"before\" title=\"Before (v1.2.1)\">\r\n                    ```csharp\r\n                    // Old BackendException\r\n                    throw new BackendException(\"MyBackend\", Result.Error, \"Something failed.\");\r\n\r\n                    // Hypothetical old metadata reader that throws exceptions\r\n                    try\r\n                    {\r\n                        var info = OldMetadataReader.Read(\"file.mp3\");\r\n                    }\r\n                    catch (SomeFormatException ex)\r\n                    {\r\n                        Console.WriteLine($\"Error: {ex.Message}\");\r\n                    }\r\n                    ```\r\n                </Tab>\r\n                <Tab key=\"after\" title=\"After (v1.3.0)\">\r\n                    ```csharp\r\n                    // New BackendException uses an int for the result code\r\n                    throw new BackendException(\"MyBackend\", -1, \"Something failed.\");\r\n\r\n                    // New metadata reader uses the Result<T> pattern\r\n                    var result = SoundMetadataReader.Read(\"file.mp3\");\r\n                    if (result.IsFailure)\r\n                    {\r\n                        // Handle the error without a try-catch block\r\n                        Console.WriteLine($\"Error reading metadata: {result.Error?.Message}\");\r\n                    }\r\n                    else\r\n                    {\r\n                        var info = result.Value;\r\n                        Console.WriteLine($\"Duration: {info.Duration}\");\r\n                    }\r\n                    ```\r\n                </Tab>\r\n            </Tabs>\r\n        </CardBody>\r\n    </Card>\r\n</div>\r\n\r\n## A Glimpse at New Features\r\n\r\nThis update is packed with new capabilities. Here's a brief overview to get you started on your exploration:\r\n\r\n<div className=\"grid grid-cols-1 md:grid-cols-2 gap-4 mt-6\">\r\n    <Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n        <CardHeader><div className=\"flex items-center gap-3\"><Icon icon=\"ph:puzzle-piece-bold\" className=\"text-primary text-xl\" /> <h4 className=\"font-semibold\">Pluggable Codecs & FFmpeg</h4></div></CardHeader>\r\n        <CardBody className=\"pt-0\">\r\n            Add the `SoundFlow.Codecs.FFMpeg` package and call `engine.RegisterCodecFactory(new FFmpegCodecFactory())` to instantly add support for MP3, AAC, OGG, FLAC, and dozens more. See the **[FFmpeg Codec Extension](./ffmpeg-codec-extension)** page for more.\r\n        </CardBody>\r\n    </Card>\r\n    <Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n        <CardHeader><div className=\"flex items-center gap-3\"><Icon icon=\"cbi:midi\" className=\"text-primary text-xl\" /> <h4 className=\"font-semibold\">Pluggable MIDI & PortMidi</h4></div></CardHeader>\r\n        <CardBody className=\"pt-0\">\r\n            Add the `SoundFlow.Midi.PortMidi` package and call `engine.UsePortMidi()` to enable MIDI I/O. Use `engine.MidiManager` to list devices and create routes. See the **[PortMidi Backend](./portmidi-backend)** page for details.\r\n        </CardBody>\r\n    </Card>\r\n    <Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n        <CardHeader><div className=\"flex items-center gap-3\"><Icon icon=\"icon-park-outline:piano\" className=\"text-primary text-xl\" /> <h4 className=\"font-semibold\">Synthesizer & Sequencer</h4></div></CardHeader>\r\n        <CardBody className=\"pt-0\">\r\n            Use the new `Synthesizer` component as a MIDI destination. Load instruments from `.sf2` SoundFont files with `SoundFontBank`. Drive it all with the sample-accurate `Sequencer`. Explore the **[Synthesizer & Instruments](./synthesizer-and-instruments)** documentation.\r\n        </CardBody>\r\n    </Card>\r\n    <Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20\">\r\n        <CardHeader><div className=\"flex items-center gap-3\"><Icon icon=\"icon-park-outline:equalizer\" className=\"text-primary text-xl\" /> <h4 className=\"font-semibold\">MIDI Control & Mapping</h4></div></CardHeader>\r\n        <CardBody className=\"pt-0\">\r\n            Decorate properties with `[ControllableParameter]` to expose them for real-time control. Use `composition.MappingManager` to link your MIDI controller to any parameter in your project. Dive into the **[MIDI Control & Mapping](./midi-control-and-mapping)** guide.\r\n        </CardBody>\r\n    </Card>\r\n</div>\r\n\r\n## New Namespaces to Import\r\n\r\nAs you update your code, you will likely need to add some of these new `using` directives:\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Midi;\r\nusing SoundFlow.Interfaces;\r\nusing SoundFlow.Structs; // For the new Result<T> and IError types\r\n\r\n// For extensions\r\nusing SoundFlow.Codecs.FFMpeg;\r\nusing SoundFlow.Midi.PortMidi;\r\n```\r\n\r\n---\r\n\r\nWe believe these changes make SoundFlow a significantly more powerful and flexible audio framework. We encourage you to explore the updated documentation for in-depth guides on all the new features. If you have any questions or run into issues, please feel free to open an issue on our [GitHub repository](https://github.com/LSXPrime/SoundFlow)."
  },
  {
    "id": 13.2,
    "slug": "midi-routing",
    "version": "1.3.0",
    "title": "MIDI Routing (The Virtual Patch Bay)",
    "description": "A guide to connecting MIDI sources to destinations using the MidiManager's routing system to create live signal paths.",
    "navOrder": 13.2,
    "category": "MIDI",
    "content": "﻿---\r\nid: 13.2\r\ntitle: MIDI Routing (The Virtual Patch Bay)\r\ndescription: A guide to connecting MIDI sources to destinations using the MidiManager's routing system to create live signal paths.\r\nnavOrder: 13.2\r\ncategory: MIDI\r\n---\r\n\r\n# MIDI Routing (The Virtual Patch Bay)\r\n\r\nWhile direct device interaction is useful, the true power of SoundFlow's MIDI system lies in its routing capabilities. The `MidiManager` acts as a **virtual patch bay**, allowing you to create flexible, live signal paths that connect various MIDI sources to various destinations. This guide explains the core concepts of the routing graph and provides practical examples for common scenarios.\r\n\r\n## 1. Introduction to the Routing Graph\r\n\r\nThe routing system is built on a simple but powerful concept: connecting an `IMidiSourceNode` to an `IMidiDestinationNode` via a `MidiRoute`.\r\n\r\n*   **`IMidiSourceNode`**: Represents any component that can **originate** a stream of MIDI messages.\r\n*   **`IMidiDestinationNode`**: Represents any component that can **receive** and process MIDI messages.\r\n*   **`MidiRoute`**: Represents a single, active connection or \"patch cable\" between one source and one destination. It listens for messages from the source and forwards them to the destination.\r\n\r\nThis model allows you to decouple your MIDI hardware from your internal sound-generating or processing components, creating a flexible and modular system.\r\n\r\n## 2. The Nodes of the Graph\r\n\r\nSoundFlow provides several concrete node implementations for building your routing graph.\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph Sources\r\n        direction LR\r\n        InputNode[\"<div style='text-align: center'><b>MidiInputNode</b><br>(Physical Keyboard)</div>\"]:::sourceStyle\r\n    end\r\n\r\n    subgraph Destinations\r\n        direction LR\r\n        OutputNode[\"<div style='text-align: center'><b>MidiOutputNode</b><br>(Hardware Synth)</div>\"]:::destStyle\r\n        TargetNode[\"<div style='text-align: center'><b>MidiTargetNode</b><br>(Internal Synthesizer)</div>\"]:::destStyle\r\n    end\r\n\r\n    subgraph Internal Sequencer\r\n        direction LR\r\n        Sequencer[\"<div style='text-align: center'><b>Sequencer</b><br>(MIDI File Player)</div>\"]:::sourceStyle\r\n    end\r\n\r\n    InputNode -- \"MidiRoute 1: Live Control\" --> TargetNode\r\n    InputNode -- \"MidiRoute 2: MIDI Thru\" --> OutputNode\r\n    Sequencer -- \"MidiRoute 3: Sequencing\" --> OutputNode\r\n    Sequencer -- \"MidiRoute 4: Internal Seq\" --> TargetNode\r\n\r\n    classDef sourceStyle fill:#e6f2ff,stroke:#007acc,stroke-width:2px,color:#000\r\n    classDef destStyle fill:#e6fffa,stroke:#009688,stroke-width:2px,color:#000\r\n```\r\n\r\n### Source Nodes (`IMidiSourceNode`)\r\n\r\nThese are the starting points of your MIDI signal flow.\r\n\r\n*   **`MidiInputNode`**: This node wraps a physical MIDI input device, like a USB keyboard or MIDI interface. It is created and managed by the `MidiManager` via the `GetOrCreateInputNode` method. When you play your keyboard, this node emits `MidiMessage` events into the routing system.\r\n\r\n### Destination Nodes (`IMidiDestinationNode`)\r\n\r\nThese are the endpoints of your MIDI signal flow.\r\n\r\n*   **`MidiOutputNode`**: This node wraps a physical MIDI output device, such as an external hardware synthesizer or a MIDI interface connected to other gear. It is created and managed by the `MidiManager` via the `GetOrCreateOutputNode` method. When it receives a `MidiMessage`, it sends it out to the physical hardware.\r\n\r\n*   **`MidiTargetNode`**: This is a crucial wrapper that makes any internal, `IMidiControllable` component a valid MIDI destination. The most common target is the `Synthesizer` component. When a `MidiTargetNode` receives a message, it simply calls the `ProcessMidiMessage` method on the component it wraps.\r\n\r\n## 3. Creating and Managing Routes\r\n\r\nThe `MidiManager` provides simple, high-level methods for managing these connections.\r\n\r\n*   **The `MidiRoute` Class:** This object represents a single, active connection. It is responsible for subscribing to the source node's events and calling the destination node's processing methods. It also serves as the container for real-time `MidiModifier`s, which will be covered in the next guide.\r\n\r\n*   **`MidiManager.CreateRoute(...)`:** These convenience methods are the primary way to establish a connection. You provide the source and destination (either as `MidiDeviceInfo` structs or direct `IMidiControllable` instances), and the `MidiManager` handles the creation of the nodes and the `MidiRoute` object for you.\r\n\r\n*   **`MidiManager.RemoveRoute(route)`:** This method disconnects a route, unsubscribes it from its source, and removes it from the manager's active list.\r\n\r\n## 4. Practical Routing Scenarios\r\n\r\nThe following complete, runnable examples illustrate the most common routing configurations.\r\n\r\n### Scenario 1: MIDI Thru (Physical Input → Physical Output)\r\n\r\n**Use Case:** Connecting a MIDI controller directly to an external hardware synthesizer through your computer, effectively using the computer as a \"MIDI patch bay\" or \"MIDI thru box.\"\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing System;\r\nusing System.Linq;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n\r\n        var inputDeviceInfo = engine.MidiInputDevices.FirstOrDefault();\r\n        var outputDeviceInfo = engine.MidiOutputDevices.FirstOrDefault();\r\n\r\n        if (inputDeviceInfo.Name == null || outputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"A MIDI input and output device must be connected.\");\r\n            return;\r\n        }\r\n\r\n        Console.WriteLine($\"Creating MIDI Thru route: '{inputDeviceInfo.Name}' -> '{outputDeviceInfo.Name}'\");\r\n\r\n        // 1. Create the route.\r\n        // The MidiManager automatically finds/creates the required input and output nodes.\r\n        var thruRoute = engine.MidiManager.CreateRoute(inputDeviceInfo, outputDeviceInfo);\r\n\r\n        Console.WriteLine(\"Route is active. MIDI data is now flowing from input to output.\");\r\n        Console.WriteLine(\"Press any key to disconnect the route and exit.\");\r\n        Console.ReadKey();\r\n\r\n        // 2. Remove the route to stop the connection.\r\n        engine.MidiManager.RemoveRoute(thruRoute);\r\n        Console.WriteLine(\"Route disconnected.\");\r\n    }\r\n}\r\n```\r\n\r\n### Scenario 2: Live Instrument Control (Physical Input → Internal Synthesizer)\r\n\r\n**Use Case:** The most common scenario for a software synthesizer—playing an internal instrument with a physical MIDI keyboard.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing System;\r\nusing System.Linq;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        // 1. Standard engine and device setup\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n\r\n        var inputDeviceInfo = engine.MidiInputDevices.FirstOrDefault();\r\n        if (inputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI input device found.\");\r\n            return;\r\n        }\r\n\r\n        // 2. Create the internal sound source (the destination).\r\n        var instrumentBank = new BasicInstrumentBank(format);\r\n        var synthesizer = new Synthesizer(engine, format, instrumentBank);\r\n\r\n        // 3. Add the synthesizer to the audio graph to be heard.\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n\r\n        Console.WriteLine($\"Creating live control route: '{inputDeviceInfo.Name}' -> Synthesizer\");\r\n\r\n        // 4. Create the route from the physical input to the internal synthesizer.\r\n        var liveRoute = engine.MidiManager.CreateRoute(inputDeviceInfo, synthesizer);\r\n\r\n        // 5. Start audio playback.\r\n        device.Start();\r\n\r\n        Console.WriteLine(\"Route is active. Play your MIDI keyboard to hear the synthesizer.\");\r\n        Console.WriteLine(\"Press any key to exit.\");\r\n        Console.ReadKey();\r\n\r\n        device.Stop();\r\n        engine.MidiManager.RemoveRoute(liveRoute);\r\n    }\r\n}\r\n```\r\n\r\n### Scenario 3: Sequencing to External Hardware (Internal Sequencer → Physical Output)\r\n\r\n**Use Case:** Using SoundFlow as a MIDI sequencer to play a MIDI file on an external hardware synthesizer.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Metadata.Midi;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Routing.Nodes;\r\nusing SoundFlow.Providers;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing System;\r\nusing System.IO;\r\nusing System.Linq;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        var midiFilePath = \"path/to/your/song.mid\";\r\n        if (!File.Exists(midiFilePath))\r\n        {\r\n            Console.WriteLine($\"MIDI file not found: {midiFilePath}\");\r\n            return;\r\n        }\r\n\r\n        // 1. Standard engine and device setup\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n        var format = AudioFormat.DvdHq;\r\n        using var device = engine.InitializePlaybackDevice(null, format); // Device is still needed to drive the Sequencer's clock\r\n\r\n        var outputDeviceInfo = engine.MidiOutputDevices.FirstOrDefault();\r\n        if (outputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI output device found to send sequence data to.\");\r\n            return;\r\n        }\r\n\r\n        // 2. Load the MIDI file.\r\n        var midiFile = MidiFileParser.Parse(File.OpenRead(midiFilePath));\r\n        var midiDataProvider = new MidiDataProvider(midiFile);\r\n\r\n        // 3. Get the destination node for the physical MIDI output.\r\n        var outputNode = engine.MidiManager.GetOrCreateOutputNode(outputDeviceInfo);\r\n\r\n        // 4. Create the Sequencer.\r\n        // The target is the MidiOutputNode, NOT an internal synthesizer.\r\n        var sequencer = new Sequencer(engine, format, midiDataProvider, outputNode);\r\n\r\n        // 5. Add the Sequencer to the audio graph.\r\n        // This is crucial! Although the sequencer doesn't produce audio, it needs to be\r\n        // part of the processing graph to get sample-accurate timing updates.\r\n        device.MasterMixer.AddComponent(sequencer);\r\n\r\n        // 6. Start the device and sequencer.\r\n        device.Start();\r\n        sequencer.Play();\r\n\r\n        Console.WriteLine($\"Playing MIDI file '{Path.GetFileName(midiFilePath)}' to '{outputDeviceInfo.Name}'.\");\r\n        Console.WriteLine(\"Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        sequencer.Stop();\r\n        device.Stop();\r\n    }\r\n}\r\n```\r\n\r\n## 5. Handling Route Failures\r\n\r\nA `MidiRoute` can fail during operation. The most common cause is a physical device being disconnected (e.g., a USB cable is unplugged). When this happens, the route enters a **faulted state** (`IsFaulted = true`) and stops processing messages to prevent further errors.\r\n\r\nYou can be notified of these failures by subscribing to the `MidiManager.OnRouteFaulted` event.\r\n\r\n```csharp\r\n// In your application setup code:\r\nengine.MidiManager.OnRouteFaulted += HandleRouteFault;\r\n\r\n// ... create routes ...\r\n\r\nprivate static void HandleRouteFault(MidiRoute faultedRoute, IError? error)\r\n{\r\n    // This handler will be called if a route fails.\r\n    Console.WriteLine($\"\\n--- MIDI ROUTE FAULT ---\");\r\n    Console.WriteLine($\"Route from '{faultedRoute.Source.Name}' to '{faultedRoute.Destination.Name}' has failed.\");\r\n    Console.WriteLine($\"Reason: {error?.Message ?? \"Unknown error\"}\");\r\n\r\n    // In a real application, you might update the UI to show the disconnected status\r\n    // or attempt to automatically re-establish the route.\r\n}\r\n```"
  },
  {
    "id": 13.3,
    "slug": "midi-real-time-modifiers",
    "version": "1.3.0",
    "title": "Real-time MIDI Modifiers",
    "description": "Learn how to create custom real-time MIDI effects like arpeggiators, chord generators, and velocity curves using the MidiModifier class.",
    "navOrder": 13.3,
    "category": "MIDI",
    "content": "﻿---\r\nid: 13.3\r\ntitle: Real-time MIDI Modifiers\r\ndescription: Learn how to create custom real-time MIDI effects like arpeggiators, chord generators, and velocity curves using the MidiModifier class.\r\nnavOrder: 13.3\r\ncategory: MIDI\r\n---\r\n\r\n# Real-time MIDI Modifiers\r\n\r\nThe `MidiModifier` (`SoundFlow.Midi.Abstracts.MidiModifier`) is a powerful component for creating real-time MIDI effects. It acts as an interceptor in a MIDI signal path, allowing you to filter, transform, or even generate new MIDI messages on the fly before they reach their destination.\r\n\r\nThis contrasts with `SoundModifier`, which operates on audio samples. `MidiModifier`s work on MIDI data, which is the instructional data *before* any sound is produced. This makes them ideal for creating musical tools like arpeggiators, chord generators, velocity curve shapers, and more.\r\n\r\n## 1. The `MidiModifier` Abstract Class\r\n\r\nTo create a MIDI effect, you inherit from the `MidiModifier` base class.\r\n\r\n```csharp\r\npublic abstract class MidiModifier : IMidiMappable\r\n{\r\n    // A unique ID for this instance, used for MIDI mapping its parameters.\r\n    public Guid Id { get; }\r\n\r\n    // A user-friendly name for the modifier.\r\n    public virtual string Name { get; set; }\r\n\r\n    // Toggles the effect on or off. If false, messages pass through unprocessed.\r\n    public bool IsEnabled { get; set; } = true;\r\n\r\n    // The core method you must implement.\r\n    public abstract IEnumerable<MidiMessage> Process(MidiMessage message);\r\n}\r\n```\r\n\r\nThe key to its flexibility is the `Process` method. By returning an `IEnumerable<MidiMessage>`, you can control the MIDI stream in three fundamental ways.\r\n\r\n## 2. The `Process` Method: A Deep Dive\r\n\r\nThe `Process` method is a generator function. Whatever `MidiMessage` objects you `yield return` from this method will be passed on to the next stage in the chain.\r\n\r\n### Scenario 1: Filtering (Returning Zero Messages)\r\n\r\nTo filter out or \"drop\" a MIDI message, simply do not `yield return` anything. The message will be consumed by your modifier and will not continue down the signal path.\r\n\r\n#### Full Example: `ChannelFilterModifier`\r\n\r\nThis modifier will only allow messages on a specific MIDI channel to pass through, dropping all others.\r\n\r\n```csharp\r\nusing SoundFlow.Midi.Abstracts;\r\nusing SoundFlow.Midi.Structs;\r\nusing System.Collections.Generic;\r\n\r\n/// <summary>\r\n/// A MIDI modifier that filters messages, only allowing those on a specific channel to pass.\r\n/// </summary>\r\npublic sealed class ChannelFilterModifier : MidiModifier\r\n{\r\n    /// <summary>\r\n    /// Gets or sets the MIDI channel to allow (1-16).\r\n    /// </summary>\r\n    public int AllowedChannel { get; set; }\r\n\r\n    public ChannelFilterModifier(int channel)\r\n    {\r\n        AllowedChannel = channel;\r\n        Name = $\"Channel Filter ({channel})\";\r\n    }\r\n\r\n    /// <inheritdoc />\r\n    public override IEnumerable<MidiMessage> Process(MidiMessage message)\r\n    {\r\n        // Check if the incoming message's channel matches the allowed channel.\r\n        if (message.Channel == AllowedChannel)\r\n        {\r\n            // If it matches, yield return the original message to let it pass through.\r\n            yield return message;\r\n        }\r\n        // If it does not match, we do nothing. The message is effectively dropped.\r\n    }\r\n}\r\n```\r\n\r\n### Scenario 2: Transforming (Returning One Message)\r\n\r\nTo modify a message, you create a new `MidiMessage` instance with the altered data and `yield return` it. The original message is discarded, and the new one takes its place in the stream.\r\n\r\n#### Full Example: `TransposeModifier`\r\n\r\nThis modifier changes the pitch of `NoteOn` and `NoteOff` messages, passing all other message types through unchanged.\r\n\r\n```csharp\r\nusing SoundFlow.Midi.Abstracts;\r\nusing SoundFlow.Midi.Enums;\r\nusing SoundFlow.Midi.Structs;\r\nusing SoundFlow.Structs;\r\n\r\nnamespace SoundFlow.Midi.Modifier;\r\n\r\n/// <summary>\r\n/// A MIDI modifier that transposes the pitch of Note On and Note Off messages.\r\n/// </summary>\r\npublic sealed class TransposeModifier : MidiModifier\r\n{\r\n    /// <inheritdoc />\r\n    public override string Name => $\"Transpose ({Semitones} st)\";\r\n\r\n    /// <summary>\r\n    /// Gets or sets the amount to transpose in semitones. Can be positive or negative.\r\n    /// </summary>\r\n    public int Semitones { get; set; }\r\n\r\n    /// <summary>\r\n    /// Initializes a new instance of the <see cref=\"TransposeModifier\"/> class.\r\n    /// </summary>\r\n    /// <param name=\"semitones\">The transposition amount in semitones.</param>\r\n    public TransposeModifier(int semitones)\r\n    {\r\n        Semitones = semitones;\r\n    }\r\n\r\n    /// <inheritdoc />\r\n    public override IEnumerable<MidiMessage> Process(MidiMessage message)\r\n    {\r\n        // Only process Note On and Note Off messages.\r\n        if (message.Command is MidiCommand.NoteOn or MidiCommand.NoteOff)\r\n        {\r\n            // Transpose the note number and clamp it to the valid MIDI range (0-127).\r\n            var newNote = Math.Clamp(message.NoteNumber + Semitones, 0, 127);\r\n\r\n            // Create a new message with the transposed note number.\r\n            yield return new MidiMessage(message.StatusByte, (byte)newNote, message.Data2, message.Timestamp);\r\n        }\r\n        else\r\n        {\r\n            // Pass through all other message types unchanged.\r\n            yield return message;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n### Scenario 3: Generating (Returning Multiple Messages)\r\n\r\nTo generate new MIDI events from a single input event, you can `yield return` multiple `MidiMessage` objects. This is the basis for effects like arpeggiators or chord generators.\r\n\r\n#### Full Example: `ChordModifier`\r\n\r\nThis modifier takes a single `NoteOn` message and generates a major chord. It also correctly handles the `NoteOff` message to silence all the notes it generated.\r\n\r\n```csharp\r\nusing SoundFlow.Midi.Abstracts;\r\nusing SoundFlow.Midi.Enums;\r\nusing SoundFlow.Midi.Structs;\r\n\r\n/// <summary>\r\n/// A MIDI modifier that generates a major chord from a single incoming note.\r\n/// </summary>\r\npublic sealed class ChordModifier : MidiModifier\r\n{\r\n    // Stores the original note that triggered the chord, so we know which chord to turn off.\r\n    private readonly Dictionary<int, int[]> _activeChords = new(); // Key: rootNote, Value: [third, fifth]\r\n\r\n    public ChordModifier()\r\n    {\r\n        Name = \"Major Chord Generator\";\r\n    }\r\n\r\n    /// <inheritdoc />\r\n    public override IEnumerable<MidiMessage> Process(MidiMessage message)\r\n    {\r\n        // Pass through disabled modifiers.\r\n        if (!IsEnabled)\r\n        {\r\n            yield return message;\r\n            yield break;\r\n        }\r\n\r\n        if (message is { Command: MidiCommand.NoteOn, Velocity: > 0 })\r\n        {\r\n            var rootNote = message.NoteNumber;\r\n            var third = rootNote + 4; // Major third\r\n            var fifth = rootNote + 7; // Perfect fifth\r\n\r\n            // Store the notes we're about to create.\r\n            _activeChords[rootNote] = [third, fifth];\r\n\r\n            // 1. Yield the original root note.\r\n            yield return message;\r\n\r\n            // 2. Generate and yield the new third note, if valid.\r\n            if (third <= 127)\r\n                yield return new MidiMessage((byte)((byte)message.Command | (message.Channel - 1)), (byte)third, message.Data2, message.Timestamp);\r\n\r\n            // 3. Generate and yield the new fifth note, if valid.\r\n            if (fifth <= 127)\r\n                yield return new MidiMessage((byte)((byte)message.Command | (message.Channel - 1)), (byte)fifth, message.Data2, message.Timestamp);\r\n        }\r\n        else if (message.Command == MidiCommand.NoteOff || (message.Command == MidiCommand.NoteOn && message.Velocity == 0))\r\n        {\r\n            var rootNote = message.NoteNumber;\r\n\r\n            // 1. Yield the original Note Off message to turn off the root note.\r\n            yield return message;\r\n\r\n            // 2. If this root note triggered a chord, generate Note Off messages for the other notes.\r\n            if (_activeChords.Remove(rootNote, out var chordNotes))\r\n            {\r\n                var noteOffCommand = (byte)((byte)message.Command | (message.Channel - 1));\r\n                yield return new MidiMessage(noteOffCommand, (byte)chordNotes[0], 0); // Third\r\n                yield return new MidiMessage(noteOffCommand, (byte)chordNotes[1], 0); // Fifth\r\n            }\r\n        }\r\n        else\r\n        {\r\n            // Pass all other messages through.\r\n            yield return message;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## 4. How and Where to Use MIDI Modifiers\r\n\r\nThere are two primary places you can insert a `MidiModifier` into the signal path.\r\n\r\n### Use Case 1: Modifying a Live `MidiRoute`\r\n\r\nYou can add modifiers directly to a `MidiRoute` to process live MIDI input in real time.\r\n\r\n```csharp\r\n// (Continuing from the \"Live Instrument Control\" example in the Routing guide)\r\n\r\n// ... setup engine, synthesizer, and get inputDeviceInfo ...\r\n\r\n// Create a live route from the keyboard to the synthesizer.\r\nvar liveRoute = engine.MidiManager.CreateRoute(inputDeviceInfo, synthesizer);\r\n\r\n// Create an instance of our custom ChordModifier.\r\nvar chordModifier = new ChordModifier();\r\n\r\n// Insert the modifier into the route's processing chain.\r\nliveRoute.InsertProcessor(0, chordModifier);\r\n\r\nConsole.WriteLine(\"Chord Modifier is active. Play single notes on your keyboard to hear major chords.\");\r\n// ... start device and wait for input ...\r\n```\r\n\r\n### Use Case 2: Modifying a Sequenced `MidiTrack`\r\n\r\nYou can add modifiers to the `Settings` of a `MidiTrack` in a `Composition`. The modifiers will be applied to all MIDI data from that track during playback or rendering.\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\n\r\n// Assuming 'composition' and 'myMidiTrack' are existing instances.\r\n\r\n// Create a modifier to transpose the track up by one octave.\r\nvar octaveUpModifier = new TransposeModifier(12);\r\n\r\n// Add the modifier to the track's settings.\r\nmyMidiTrack.Settings.AddMidiModifier(octaveUpModifier);\r\n\r\n// Now, when the composition is played, all notes from 'myMidiTrack' will be\r\n// transposed up by 12 semitones before they reach the track's target synthesizer.\r\n```\r\n\r\n---\r\n\r\n## 5. Built-in Modifiers\r\n\r\nSoundFlow includes a suite of robust, ready-to-use MIDI modifiers in the `SoundFlow.Midi.Modifier` namespace. These cover the most common use cases for MIDI processing.\r\n\r\n### Transpose Modifier (`TransposeModifier`)\r\nShifts the pitch of incoming `NoteOn` and `NoteOff` messages by a fixed number of semitones. Useful for key changes or octave shifting.\r\n\r\n```csharp\r\nusing SoundFlow.Midi.Modifier;\r\n\r\n// Shift up by one octave (12 semitones)\r\nvar octaveUp = new TransposeModifier(12);\r\n\r\n// Shift down by a perfect fifth (-7 semitones)\r\nvar fifthDown = new TransposeModifier(-7);\r\n```\r\n\r\n### Channel Filter (`ChannelFilterModifier`)\r\nActs as a gate, allowing only messages on a specific MIDI channel to pass through. All other messages are dropped. This is essential for complex routing setups where a single stream contains data for multiple instruments.\r\n\r\n```csharp\r\n// Allow only messages on Channel 10 (Drums)\r\nvar drumFilter = new ChannelFilterModifier(10);\r\n```\r\n\r\n### Velocity Processor (`VelocityModifier`)\r\nReshapes the dynamic range of a performance by modifying the velocity of `NoteOn` messages.\r\n\r\n*   **`MinVelocity` / `MaxVelocity`**: Clamps the output velocity to a specific range.\r\n*   **`Add`**: Adds a fixed offset to the velocity (can be negative).\r\n*   **`Curve`**: Applies a non-linear curve to the velocity response.\r\n*   `0.0`: Linear (1:1).\r\n*   `> 0.0` (e.g., `0.5`): Exponential. Harder to play loud notes; increases dynamic contrast.\r\n*   `< 0.0` (e.g., `-0.5`): Logarithmic. Easier to play loud notes; compresses dynamics.\r\n\r\n```csharp\r\nvar compressor = new VelocityModifier\r\n{\r\n    MinVelocity = 40,  // Raise the noise floor (nothing softer than 40)\r\n    MaxVelocity = 110, // Cap the ceiling\r\n    Curve = -0.3f      // Slight logarithmic boost to make it feel \"punchier\"\r\n};\r\n```\r\n\r\n### Harmonizer (`HarmonizerModifier`)\r\nInstantly creates chords from single notes. It listens for `NoteOn` and `NoteOff` messages and generates additional parallel notes based on the specified intervals.\r\n\r\n```csharp\r\n// Create a minor triad (Root + Minor 3rd + Perfect 5th)\r\n// Intervals are in semitones relative to the root.\r\nvar minorChord = new HarmonizerModifier([0, 3, 7]);\r\n\r\n// Create a \"Power Chord\" (Root + 5th + Octave)\r\nvar powerChord = new HarmonizerModifier([0, 7, 12]);\r\n```\r\n\r\n### Randomizer (`RandomizerModifier`)\r\nA powerful tool for generative music or humanizing robotic performances. It can introduce probability and variation to note events.\r\n\r\n*   **`Chance`**: Probability (0.0-1.0) that a note will play at all.\r\n*   **`VelocityRandomness`**: Amount of random deviation applied to velocity.\r\n*   **`PitchRandomness`**: Probability (0.0-1.0) that a note's pitch will be changed.\r\n*   **`PitchRange`**: The range (+/- semitones) for pitch randomization.\r\n*   **`MinNote` / `MaxNote`**: Restricts the effect to a specific key range.\r\n\r\n```csharp\r\nvar humanizer = new RandomizerModifier\r\n{\r\n    Chance = 0.95f,           // 5% chance a note is skipped (ghost notes)\r\n    VelocityRandomness = 0.2f, // +/- 10% velocity variation\r\n    VelocityBias = -0.1f       // Tend slightly towards softer velocities\r\n};\r\n\r\nvar glitchEffect = new RandomizerModifier\r\n{\r\n    PitchRandomness = 0.3f, // 30% chance to glitch the pitch\r\n    PitchRange = 24         // Jump up to +/- 2 octaves\r\n};\r\n```\r\n\r\n### Arpeggiator (`ArpeggiatorModifier`)\r\nA stateful modifier that turns held chords into rhythmic patterns. Because it generates notes over time, it implements the `ITemporalMidiModifier` interface (see below).\r\n\r\n*   **`Mode`**: The pattern type (`Up`, `Down`, `UpDown`, `Random`).\r\n*   **`Rate`**: The step speed relative to the beat (e.g., `0.25` for 1/16th notes).\r\n*   **`Octaves`**: The number of octaves the pattern spans.\r\n*   **`Gate`**: The duration of each note as a percentage of the step time (0.0-1.0).\r\n\r\n```csharp\r\nvar arp = new ArpeggiatorModifier\r\n{\r\n    Mode = ArpMode.UpDown,\r\n    Octaves = 2,\r\n    Rate = 0.125, // 1/32nd notes\r\n    Gate = 0.8    // Staccato feel\r\n};\r\n```\r\n\r\n## 6. Temporal Modifiers (`ITemporalMidiModifier`)\r\n\r\nStandard MIDI modifiers process input messages instantly. However, effects like Arpeggiators or Sequencers need to generate events based on **time**, even when no input messages are being received.\r\n\r\nSoundFlow handles this via the `ITemporalMidiModifier` interface.\r\n\r\n```csharp\r\npublic interface ITemporalMidiModifier\r\n{\r\n    /// <summary>\r\n    /// Advances the modifier's internal state by a specified duration and returns any generated MIDI messages.\r\n    /// </summary>\r\n    /// <param name=\"deltaSeconds\">The time elapsed since the last tick, in seconds.</param>\r\n    /// <param name=\"bpm\">The current tempo in beats per minute.</param>\r\n    IEnumerable<MidiMessage> Tick(double deltaSeconds, double bpm);\r\n}\r\n```\r\n\r\n### How it Works\r\n1.  **Input Handling:** The modifier still receives standard messages via `Process(message)`. An Arpeggiator, for example, uses `NoteOn` messages to build a buffer of \"held notes\" but **suppresses** them (returns nothing) so they don't sound immediately.\r\n2.  **Time Advancement:** The host component (like the `Synthesizer`) calls the `Tick()` method periodically (typically once per audio buffer render).\r\n3.  **Event Generation:** Inside `Tick()`, the modifier calculates if enough time has passed to trigger the next step in its pattern. If so, it returns new `NoteOn` or `NoteOff` messages, which the host then processes immediately."
  },
  {
    "id": 13.1,
    "slug": "midi-io-and-device-management",
    "version": "1.3.0",
    "title": "MIDI I/O & Device Management",
    "description": "A foundational guide to discovering, initializing, and interacting with physical MIDI devices for basic input and output operations.",
    "navOrder": 13.1,
    "category": "MIDI",
    "content": "﻿---\r\nid: 13.1\r\ntitle: MIDI I/O & Device Management\r\ndescription: A foundational guide to discovering, initializing, and interacting with physical MIDI devices for basic input and output operations.\r\nnavOrder: 13.1\r\ncategory: MIDI\r\n---\r\n\r\n\r\n# MIDI I/O & Device Management\r\n\r\nThis guide provides a foundational walkthrough of how to enable and manage MIDI devices in SoundFlow. You will learn how to discover connected MIDI hardware, initialize devices for input and output, and handle basic MIDI messages.\r\n\r\n## 1. Introduction to MIDI in SoundFlow\r\n\r\nSoundFlow's MIDI capabilities are designed to be extensible through a **pluggable backend system**. The core library provides the abstractions, but the actual communication with the operating system's MIDI APIs is handled by a separate backend implementation.\r\n\r\n*   **`IMidiBackend` Interface:** This is the contract that all MIDI backends must implement. It defines the core responsibilities for device enumeration and creation.\r\n*   **`SoundFlow.Midi.PortMidi`:** This is the official, cross-platform backend implementation provided by SoundFlow. It wraps the widely-used PortMidi library, ensuring compatibility across Windows, macOS, and Linux.\r\n\r\n## 2. Enabling MIDI Functionality\r\n\r\nBefore you can perform any MIDI operations, you must enable a MIDI backend.\r\n\r\n### Step 1: Install the Backend Package\r\n\r\nFirst, add the `SoundFlow.Midi.PortMidi` NuGet package to your project.\r\n\r\n```bash\r\n# Using .NET CLI\r\ndotnet add package SoundFlow.Midi.PortMidi\r\n\r\n# Using Package Manager Console\r\nInstall-Package SoundFlow.Midi.PortMidi\r\n```\r\n\r\n### Step 2: Enable the Backend in Code\r\n\r\nIn your application's startup code, after creating your `AudioEngine` instance, you must call the `UsePortMidi()` extension method. This method creates an instance of the `PortMidiBackend`, registers it with the engine, and initializes it. This call is mandatory and should only be done once.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi; // Import the extension method\r\n\r\n// 1. Initialize the audio engine.\r\nusing var engine = new MiniAudioEngine();\r\n\r\n// 2. Enable the PortMidi backend.\r\n// This activates all MIDI functionality in the engine.\r\nengine.UsePortMidi();\r\n\r\nConsole.WriteLine(\"MIDI backend enabled.\");\r\n```\r\n\r\n### Step 3: Update Device Lists\r\n\r\nAfter enabling the backend, you must explicitly ask the engine to enumerate the available MIDI devices. This is done by calling `engine.UpdateMidiDevicesInfo()`. This method populates the `MidiInputDevices` and `MidiOutputDevices` properties on your engine instance.\r\n\r\n```csharp\r\n// After enabling the backend:\r\nengine.UpdateMidiDevicesInfo();\r\n```\r\n\r\n## 3. Listing and Identifying Devices\r\n\r\nOnce the device lists have been updated, you can iterate through them to find the hardware you want to use. Each device is represented by a `MidiDeviceInfo` struct, which contains its `Name` and a backend-specific `Id`.\r\n\r\n### Complete Example: Listing All MIDI Devices\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n\r\n        Console.WriteLine(\"Enumerating MIDI devices...\");\r\n        engine.UpdateMidiDevicesInfo();\r\n\r\n        Console.WriteLine(\"\\n--- MIDI Input Devices ---\");\r\n        if (engine.MidiInputDevices.Length != 0)\r\n        {\r\n            foreach (var device in engine.MidiInputDevices)\r\n            {\r\n                Console.WriteLine($\"ID: {device.Id}, Name: {device.Name}\");\r\n            }\r\n        }\r\n        else\r\n        {\r\n            Console.WriteLine(\"No MIDI input devices found.\");\r\n        }\r\n\r\n        Console.WriteLine(\"\\n--- MIDI Output Devices ---\");\r\n        if (engine.MidiOutputDevices.Length != 0)\r\n        {\r\n            foreach (var device in engine.MidiOutputDevices)\r\n            {\r\n                Console.WriteLine($\"ID: {device.Id}, Name: {device.Name}\");\r\n            }\r\n        }\r\n        else\r\n        {\r\n            Console.WriteLine(\"No MIDI output devices found.\");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## 4. The `MidiManager` and Managed Device Nodes\r\n\r\nYou do not create `MidiInputDevice` or `MidiOutputDevice` instances directly using a `new` keyword. Instead, the `MidiManager` (accessed via `engine.MidiManager`) acts as a factory and lifecycle manager. It ensures that only one instance of a physical device is active at a time and provides a stable \"node\" for routing purposes.\r\n\r\n*   **`GetOrCreateInputNode(MidiDeviceInfo)`:** This `MidiManager` method takes a `MidiDeviceInfo` struct and returns a `MidiInputNode`. The node contains the live, initialized `MidiInputDevice` instance. Calling this method multiple times for the same device will return the same node instance, preventing resource conflicts.\r\n*   **`GetOrCreateOutputNode(MidiDeviceInfo)`:** This method works similarly for output devices, returning a `MidiOutputNode` that contains an active `MidiOutputDevice`.\r\n\r\n## 5. Practical Examples of Basic I/O\r\n\r\nThe following examples demonstrate the complete workflow for basic MIDI input and output operations.\r\n\r\n### Example 1: Receiving MIDI Messages\r\n\r\nThis example shows how to listen for incoming MIDI messages from a specific input device and print their contents to the console.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Structs;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n\r\n        var inputDeviceInfo = engine.MidiInputDevices.FirstOrDefault();\r\n        if (inputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI input device found. Exiting.\");\r\n            return;\r\n        }\r\n\r\n        Console.WriteLine($\"Found input device: '{inputDeviceInfo.Name}'. Attempting to listen...\");\r\n\r\n        try\r\n        {\r\n            // 1. Get the managed node for the device from the MidiManager.\r\n            // This initializes the physical device if it's the first time it's being accessed.\r\n            var inputNode = engine.MidiManager.GetOrCreateInputNode(inputDeviceInfo);\r\n\r\n            // 2. Subscribe to the node's output event to receive channel messages.\r\n            inputNode.OnMessageOutput += OnMidiMessageReceived;\r\n\r\n            Console.WriteLine(\"Successfully listening for MIDI messages. Play some notes on your device.\");\r\n            Console.WriteLine(\"Press any key to exit.\");\r\n            Console.ReadKey();\r\n\r\n            // 3. Unsubscribe when done to be a good citizen.\r\n            inputNode.OnMessageOutput -= OnMidiMessageReceived;\r\n        }\r\n        catch (Exception ex)\r\n        {\r\n            Console.WriteLine($\"An error occurred: {ex.Message}\");\r\n        }\r\n    }\r\n\r\n    /// <summary>\r\n    /// Event handler for incoming MIDI messages.\r\n    /// </summary>\r\n    private static void OnMidiMessageReceived(MidiMessage message)\r\n    {\r\n        // The MidiMessage struct provides convenient properties to interpret the data.\r\n        Console.WriteLine(\r\n            $\"[Received] Command: {message.Command}, \" +\r\n            $\"Channel: {message.Channel}, \" +\r\n            $\"Note/CC: {message.Data1}, \" +\r\n            $\"Value: {message.Data2}, \" +\r\n            $\"Timestamp: {message.Timestamp}\"\r\n        );\r\n    }\r\n}\r\n```\r\n\r\n### Example 2: Sending MIDI Messages\r\n\r\nThis example shows how to send MIDI messages to a specific output device.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Structs;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n\r\n        var outputDeviceInfo = engine.MidiOutputDevices.FirstOrDefault();\r\n        if (outputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI output device found. Exiting.\");\r\n            return;\r\n        }\r\n\r\n        Console.WriteLine($\"Found output device: '{outputDeviceInfo.Name}'. Attempting to send a note...\");\r\n\r\n        try\r\n        {\r\n            // 1. Get the managed node for the output device.\r\n            var outputNode = engine.MidiManager.GetOrCreateOutputNode(outputDeviceInfo);\r\n\r\n            // 2. Create MIDI messages to send.\r\n            // Status 0x90 = Note On on Channel 1.\r\n            var noteOnMessage = new MidiMessage(0x90, 60, 100); // Note 60 (Middle C), Velocity 100\r\n\r\n            // Status 0x80 = Note Off on Channel 1.\r\n            var noteOffMessage = new MidiMessage(0x80, 60, 0);  // Note 60 (Middle C), Velocity 0\r\n\r\n            // 3. Send the Note On message.\r\n            var result = outputNode.ProcessMessage(noteOnMessage);\r\n            if(result.IsSuccess)\r\n            {\r\n                Console.WriteLine(\"Sent Note On for Middle C.\");\r\n            }\r\n            else\r\n            {\r\n                Console.WriteLine($\"Failed to send Note On: {result.Error?.Message}\");\r\n                return;\r\n            }\r\n\r\n            // 4. Wait for one second.\r\n            Thread.Sleep(1000);\r\n\r\n            // 5. Send the Note Off message.\r\n            result = outputNode.ProcessMessage(noteOffMessage);\r\n            if(result.IsSuccess)\r\n            {\r\n                Console.WriteLine(\"Sent Note Off for Middle C.\");\r\n            }\r\n            else\r\n            {\r\n                 Console.WriteLine($\"Failed to send Note Off: {result.Error?.Message}\");\r\n            }\r\n        }\r\n        catch (Exception ex)\r\n        {\r\n            Console.WriteLine($\"An error occurred: {ex.Message}\");\r\n        }\r\n    }\r\n}\r\n```"
  },
  {
    "id": 13.4,
    "slug": "midi-control-and-mapping-introduction",
    "version": "1.3.0",
    "title": "MIDI Control & Mapping (Introduction)",
    "description": "Learn how to expose parameters on your custom components for real-time control using the IMidiMappable interface and ControllableParameter attribute.",
    "navOrder": 13.4,
    "category": "MIDI",
    "content": "﻿---\r\nid: 13.4\r\ntitle: MIDI Control & Mapping (Introduction)\r\ndescription: Learn how to expose parameters on your custom components for real-time control using the IMidiMappable interface and ControllableParameter attribute.\r\nnavOrder: 13.4\r\ncategory: MIDI\r\n---\r\n\r\n# MIDI Control & Mapping (Introduction)\r\n\r\nSoundFlow provides a powerful and flexible system for controlling the parameters of your audio components in real-time using MIDI. This allows you to create dynamic, interactive experiences where physical knobs, faders, and buttons on a MIDI controller can manipulate properties like filter cutoff, reverb wetness, or track volume.\r\n\r\nThis guide covers the foundational step: how to prepare your custom components and effects to be \"seen\" by this mapping system.\r\n\r\n## 1. Two Ways to Control Parameters\r\n\r\nThere are two primary methods for an object to respond to MIDI messages in SoundFlow:\r\n\r\n1.  **Direct Control (`IMidiControllable`):** This is for components that have a hardcoded, intrinsic response to specific MIDI messages. The prime example is the `Synthesizer`, which *must* respond to `NoteOn` and `NoteOff` messages to function. This is achieved by implementing the `IMidiControllable` interface and its `ProcessMidiMessage` method.\r\n\r\n2.  **Dynamic Mapping:** This is a flexible, user-configurable system for linking **any** MIDI message to **any** exposed parameter at runtime. For example, you might decide on the fly to map CC knob #74 on your keyboard to a filter's cutoff frequency, or a button to toggle a delay effect on and off. This is the focus of this guide.\r\n\r\nThe goal of this guide is to make your own custom classes **mappable**.\r\n\r\n## 2. Step 1: The `IMidiMappable` Interface\r\n\r\nFor an object to be a valid target for the dynamic mapping system, it **must** implement the `IMidiMappable` interface. This is a simple but critical contract.\r\n\r\n```csharp\r\npublic interface IMidiMappable\r\n{\r\n    /// <summary>\r\n    /// Gets the unique identifier for this mappable instance.\r\n    /// This ID is used to persist and restore MIDI mappings.\r\n    /// </summary>\r\n    Guid Id { get; }\r\n}\r\n```\r\n\r\n### The `Id` Property\r\n\r\nThe single requirement of this interface is the `Id` property, which must return a `Guid` that is **unique and stable** for the lifetime of that object instance.\r\n\r\nIts role is crucial: when you save a project, a MIDI mapping is stored as a link between a MIDI message and the `Id` of a target object (e.g., \"CC #74 on MyKeyboard controls the 'Cutoff' property on the object with ID `8f1b...`\"). When you load the project, the `MidiMappingManager` uses this `Id` to find the exact same object instance and re-establish the link.\r\n\r\n### Built-in Mappable Classes\r\n\r\nYou often don't need to implement this yourself, as SoundFlow's core building blocks are already mappable out-of-the-box:\r\n\r\n*   `SoundComponent` (and all derivatives like `Mixer`, `SoundPlayer`, `Synthesizer`)\r\n*   `SoundModifier` (and all derivatives like `Filter`, `DelayModifier`, `CompressorModifier`)\r\n*   `MidiModifier` (and all derivatives)\r\n*   `AudioAnalyzer`\r\n*   `Track`\r\n*   `TrackSettings`\r\n*   `AudioSegmentSettings`\r\n*   `Composition`\r\n\r\nWhen you create a `new Filter(...)`, it automatically gets a new, unique `Id` from its base class constructor, ready for mapping.\r\n\r\n## 3. Step 2: The `[ControllableParameter]` Attribute\r\n\r\nSimply implementing `IMidiMappable` tells the system that an object *can* be a target. To specify *which properties* on that object can be controlled, you must decorate them with the `ControllableParameterAttribute`.\r\n\r\nThis attribute acts as metadata, providing the mapping system (and any UI you build on top of it) with the necessary information to correctly scale MIDI input and display the parameter to the user.\r\n\r\n```csharp\r\n[AttributeUsage(AttributeTargets.Property)]\r\npublic sealed class ControllableParameterAttribute : Attribute\r\n{\r\n    public string DisplayName { get; }\r\n    public double MinValue { get; }\r\n    public double MaxValue { get; }\r\n    public MappingScale Scale { get; }\r\n\r\n    public ControllableParameterAttribute(string displayName, double minValue, double maxValue, MappingScale scale = MappingScale.Linear);\r\n}\r\n```\r\n\r\n### Attribute Properties Explained\r\n\r\n*   **`DisplayName` (string):** The user-friendly name that would appear in a UI dropdown (e.g., \"Filter Cutoff\", \"Reverb Time\").\r\n*   **`MinValue` / `MaxValue` (double):** The valid numeric range for the property. This is absolutely critical for correctly scaling an incoming MIDI value (e.g., 0-127) to the property's expected range (e.g., 20.0 to 20000.0).\r\n*   **`Scale` (`MappingScale` enum):** This tells the mapping engine how to interpret the range.\r\n*   **`MappingScale.Linear`**: Use this for parameters that change linearly, such as **Volume**, **Pan**, or a **Wet/Dry Mix**. A MIDI value of 64 will map to the exact midpoint of the `MinValue`/`MaxValue` range.\r\n*   **`MappingScale.Logarithmic`**: This is essential for parameters related to human perception, especially **frequency**. Human hearing perceives frequency logarithmically (the difference between 100 Hz and 200 Hz is much more noticeable than between 10,000 Hz and 10,100 Hz). Using this scale ensures that turning a physical knob feels smooth and musically responsive across the entire frequency range. It is highly recommended for filter cutoffs, EQ frequencies, etc.\r\n\r\nThe attribute can be applied to public, settable properties of type `float`, `double`, `int`, `long`, `bool`, and `enum`. For `bool` and `enum` types, `MinValue` and `MaxValue` are typically set to `0` and `1` (for bool) or the range of enum values.\r\n\r\n## 4. Complete Example: Creating a Mappable \"BitCrusher\" Effect\r\n\r\nLet's build a custom `SoundModifier` from scratch and make its parameters fully controllable via MIDI mapping. This `BitCrusher` effect will reduce the audio's sample rate (downsampling) and bit depth (quantization) to create a lo-fi, retro digital sound.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Interfaces;\r\n\r\n// Define an enum for one of our parameters.\r\npublic enum NoiseType { None, White, Pink }\r\n\r\n/// <summary>\r\n/// A custom SoundModifier that applies a bit-crushing effect to audio.\r\n/// It inherits from SoundModifier, which already implements IMidiMappable.\r\n/// </summary>\r\npublic class BitCrusherModifier : SoundModifier\r\n{\r\n    private float _lastSample;\r\n    private int _downsampleCounter;\r\n    private readonly Random _random = new();\r\n\r\n    public override string Name { get; set; } = \"BitCrusher\";\r\n\r\n    // Mappable Parameters\r\n\r\n    [ControllableParameter(\"Bit Depth\", 1.0, 16.0)]\r\n    public float BitDepth { get; set; } = 8.0f;\r\n\r\n    [ControllableParameter(\"Downsampling\", 1.0, 50.0, MappingScale.Logarithmic)]\r\n    public float DownsampleFactor { get; set; } = 1.0f;\r\n\r\n    [ControllableParameter(\"Add Noise\", 0, 1)] // Min/Max for bool\r\n    public bool AddNoise { get; set; } = false;\r\n\r\n    [ControllableParameter(\"Noise Type\", 0, 2)] // Min/Max for enum values\r\n    public NoiseType Noise { get; set; } = NoiseType.White;\r\n\r\n    [ControllableParameter(\"Mix\", 0.0, 1.0)]\r\n    public float WetDryMix { get; set; } = 1.0f;\r\n\r\n    // Processing Logic\r\n\r\n    public override float ProcessSample(float sample, int channel)\r\n    {\r\n        // 1. Downsampling (Sample Rate Reduction)\r\n        float processedSample;\r\n        if (_downsampleCounter >= DownsampleFactor)\r\n        {\r\n            _downsampleCounter = 0;\r\n            _lastSample = sample;\r\n            processedSample = sample;\r\n        }\r\n        else\r\n        {\r\n            processedSample = _lastSample;\r\n            _downsampleCounter++;\r\n        }\r\n\r\n        // 2. Quantization (Bit Depth Reduction)\r\n        var steps = MathF.Pow(2, BitDepth);\r\n        processedSample = MathF.Round(processedSample * (steps - 1)) / (steps - 1);\r\n\r\n        // 3. Add Noise (Optional)\r\n        if (AddNoise)\r\n        {\r\n            float noise = 0;\r\n            if (Noise == NoiseType.White)\r\n            {\r\n                noise = ((float)_random.NextDouble() * 2.0f - 1.0f) / steps;\r\n            }\r\n            // A more complex Pink noise implementation would go here.\r\n            processedSample += noise;\r\n        }\r\n\r\n        // 4. Mix original and processed signal\r\n        return sample * (1.0f - WetDryMix) + processedSample * WetDryMix;\r\n    }\r\n\r\n    public override void Process(Span<float> buffer, int channels)\r\n    {\r\n        if (!Enabled) return;\r\n\r\n        for (var i = 0; i < buffer.Length; i++)\r\n        {\r\n            // We assume mono processing for simplicity, using channel 0's state\r\n            buffer[i] = ProcessSample(buffer[i], 0);\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis `BitCrusherModifier` is now fully prepared. The `MidiMappingManager` can discover its `Id` and its five controllable parameters, along with their names, ranges, and scaling types.\r\n\r\nThe next guide, **MIDI Control & Mapping (Advanced)**, will show you how to take this component and programmatically create `MidiMapping` objects to link it to a physical controller."
  },
  {
    "id": 13.5,
    "slug": "midi-control-and-mapping-advanced",
    "version": "1.3.0",
    "title": "MIDI Control & Mapping (Advanced)",
    "description": "A deep dive into programmatically creating complex MIDI mappings, handling relative encoders, triggering methods, and using High-Resolution (14-bit) MIDI.",
    "navOrder": 13.5,
    "category": "MIDI",
    "content": "﻿---\r\nid: 13.5\r\ntitle: MIDI Control & Mapping (Advanced)\r\ndescription: A deep dive into programmatically creating complex MIDI mappings, handling relative encoders, triggering methods, and using High-Resolution (14-bit) MIDI.\r\nnavOrder: 13.5\r\ncategory: MIDI\r\n---\r\n\r\n# MIDI Control & Mapping (Advanced)\r\n\r\nThe introductory guide covered how to expose parameters on your custom components using the `[ControllableParameter]` attribute. This advanced guide focuses on the **consumer side**: how to programmatically create, configure, and manage `MidiMapping` objects to link physical hardware controls to those exposed parameters.\r\n\r\nThis level of control is essential for building applications with:\r\n*   **User-Configurable MIDI Maps:** allowing users to \"MIDI Learn\" knobs and faders.\r\n*   **Dynamic Control Surfaces:** swapping mappings on the fly based on context.\r\n*   **Complex Behaviors:** triggering methods with arguments, handling endless encoders, or using high-resolution 14-bit MIDI messages.\r\n\r\n## The `MidiMapping` Anatomy\r\n\r\nA `MidiMapping` is a data structure that defines a single connection in the routing graph. It consists of four distinct parts: **Source**, **Target**, **Transformer**, and **Behavior**.\r\n\r\n### 1. The Source (`MidiInputSource`)\r\n\r\nThis record defines exactly which incoming MIDI message will trigger the mapping.\r\n\r\n```csharp\r\npublic record MidiInputSource\r\n{\r\n    public string DeviceName { get; init; }\r\n    public int Channel { get; init; }\r\n    public MidiMappingSourceType MessageType { get; init; }\r\n    public int MessageParameter { get; init; }\r\n}\r\n```\r\n\r\n| Property               | Description                                                                                                                                                                                                                    |\r\n|:-----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| **`DeviceName`**       | The exact name of the physical MIDI input device (e.g., `\"Launchkey Mini MK3\"`). This allows mappings to distinguish between identical messages (like CC #1) coming from different controllers.                                |\r\n| **`Channel`**          | The MIDI channel (1-16) to listen to. Set this to **0** for **Omni Mode** (listen to all channels).                                                                                                                            |\r\n| **`MessageType`**      | The type of message. Options: `ControlChange`, `NoteOn`, `NoteOff`, `PitchBend`, or `HighResolutionControlChange`.                                                                                                             |\r\n| **`MessageParameter`** | The specific identifier for the message type:<br/>• **CC:** The Controller Number (0-127).<br/>• **Note:** The Note Number (0-127).<br/>• **HighRes CC:** The 14-bit NRPN/RPN number.<br/>• **PitchBend:** Ignored (set to 0). |\r\n\r\n### 2. The Target (`MidiMappingTarget`)\r\n\r\nThis record defines the destination within your SoundFlow project.\r\n\r\n```csharp\r\npublic record MidiMappingTarget\r\n{\r\n    public Guid TargetObjectId { get; init; }\r\n    public MidiMappingTargetType TargetType { get; init; }\r\n    public string TargetMemberName { get; init; }\r\n    public List<MethodArgument> MethodArguments { get; init; }\r\n}\r\n```\r\n\r\n*   **`TargetObjectId`**: The unique `Guid` of the object implementing `IMidiMappable` (e.g., a specific `Filter` instance or `TrackSettings`). This ID allows the system to find the exact object instance even after a project is saved and reloaded.\r\n*   **`TargetType`**: Specifies if the target is a `Property` (for setting values) or a `Method` (for triggering actions).\r\n*   **`TargetMemberName`**: The exact, case-sensitive string name of the public property or method you want to control (e.g., `\"CutoffFrequency\"` or `\"TriggerSample\"`).\r\n\r\n### 3. The Transformation (`ValueTransformer`)\r\n\r\nThis record defines the math for converting the MIDI input range into the parameter's native range.\r\n\r\n```csharp\r\npublic record ValueTransformer\r\n{\r\n    public float SourceMin { get; init; }\r\n    public float SourceMax { get; init; }\r\n    public float TargetMin { get; init; }\r\n    public float TargetMax { get; init; }\r\n    public MidiMappingCurveType CurveType { get; init; }\r\n}\r\n```\r\n\r\nThe transformation pipeline works in three stages:\r\n1.  **Normalization:** The incoming MIDI value (bounded by `SourceMin`/`SourceMax`) is converted to a normalized `0.0` to `1.0` float.\r\n2.  **Curve Application:** The normalized value is warped by the `CurveType` (Linear, Exponential, or Logarithmic).\r\n3.  **De-normalization:** The curved value (bounded by `TargetMin`/`TargetMax`) is scaled to the parameter's actual `MinValue`/`MaxValue` defined in its attribute.\r\n\r\n### 4. The Behavior (`MidiMappingBehavior`)\r\n\r\nThis enum dictates how the transformed value is applied to the target.\r\n\r\n| Behavior       | Best Use Case    | Description                                                                                    |\r\n|:---------------|:-----------------|:-----------------------------------------------------------------------------------------------|\r\n| **`Absolute`** | Faders, Knobs    | The MIDI value directly sets the property value. 0 = Min, 127 = Max.                           |\r\n| **`Relative`** | Endless Encoders | Interprets values around 64. < 64 decrements the property, > 64 increments it.                 |\r\n| **`Toggle`**   | Buttons          | Flips a boolean property between `true` and `false` each time the input exceeds the threshold. |\r\n| **`Trigger`**  | Buttons, Pads    | Executes a method on the target object.                                                        |\r\n\r\n---\r\n\r\n## Programmatic Mapping Examples\r\n\r\n### 1. Absolute Control (Knob to Filter Cutoff)\r\n\r\nThis example maps MIDI CC #74 to a Filter's Cutoff Frequency. It uses a **Logarithmic** curve, which is critical for frequency parameters to ensure the knob feels musical (smooth changes in pitch) rather than linear (where 0-1000Hz takes up only 5% of the knob).\r\n\r\n```csharp\r\n// 1. Create the components\r\nvar filter = new Filter(AudioFormat.DvdHq) { CutoffFrequency = 1000f };\r\nvar track = new Track(\"Synth\");\r\ntrack.Settings.AddModifier(filter);\r\ncomposition.Editor.AddTrack(track);\r\n\r\n// 2. Define Source: CC 74 on Channel 1\r\nvar source = new MidiInputSource\r\n{\r\n    DeviceName = \"My MIDI Keyboard\",\r\n    Channel = 1,\r\n    MessageType = MidiMappingSourceType.ControlChange,\r\n    MessageParameter = 74\r\n};\r\n\r\n// 3. Define Target: The 'CutoffFrequency' property on our filter instance\r\nvar target = new MidiMappingTarget\r\n{\r\n    TargetObjectId = filter.Id,\r\n    TargetType = MidiMappingTargetType.Property,\r\n    TargetMemberName = nameof(Filter.CutoffFrequency)\r\n};\r\n\r\n// 4. Define Transformer: Logarithmic mapping\r\n// Source: Standard MIDI 0-127\r\n// Target: 0.0-1.0 (Full range of the parameter's Min/Max defined in attribute)\r\nvar transformer = new ValueTransformer\r\n{\r\n    SourceMin = 0,\r\n    SourceMax = 127,\r\n    TargetMin = 0.0f,\r\n    TargetMax = 1.0f,\r\n    CurveType = MidiMappingCurveType.Logarithmic\r\n};\r\n\r\n// 5. Create and Register\r\nvar mapping = new MidiMapping(source, target, transformer, MidiMappingBehavior.Absolute);\r\ncomposition.MappingManager.AddMapping(mapping);\r\n```\r\n\r\n### Complete Example: Creating a Property Mapping\r\n\r\nThis example shows the full process of creating a mapping from scratch to control a property on a `SoundModifier`. We will use the `BitCrusherModifier` defined in the Introduction guide.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Editing.Mapping;\r\nusing SoundFlow.Interfaces;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Structs;\r\nusing System.Linq;\r\n\r\n/// <summary>\r\n/// This program demonstrates how to programmatically create a real-time MIDI mapping.\r\n/// It links a physical knob on a MIDI controller (CC #21) to the 'DownsampleFactor'\r\n/// parameter of a custom BitCrusher audio effect.\r\n///\r\n/// A sound player is used to play audio segment, which is processed by the BitCrusher.\r\n/// When the user turns the mapped knob, the bit-crushing effect is audible in real-time.\r\n/// </summary>\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        // 1. Standard Engine & MIDI Backend Setup\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UsePortMidi();\r\n        engine.UpdateMidiDevicesInfo();\r\n        var format = AudioFormat.DvdHq;\r\n\r\n        // Initialize a normal output device for audio playback\r\n        engine.UpdateAudioDevicesInfo();\r\n        var playbackDevice = engine.InitializePlaybackDevice(engine.PlaybackDevices.FirstOrDefault(x => x.IsDefault), format);\r\n\r\n        // 2. Create a Composition and Target Component\r\n        var composition = new Composition(engine, format);\r\n        var audioTrack = new Track(\"My Audio Track\");\r\n        composition.Editor.AddTrack(audioTrack);\r\n\r\n        // Create our custom modifier\r\n        var bitCrusher = new BitCrusherModifier();\r\n        audioTrack.Settings.AddModifier(bitCrusher);\r\n\r\n        // IMPORTANT: We need the ID to link the mapping.\r\n        Console.WriteLine($\"Created BitCrusher with ID: {bitCrusher.Id}\");\r\n\r\n        // Create an audio segment to play and test the modifier on, and add it to the track\r\n        var audioSegment = composition.Editor.CreateSegmentFromFile(\"path/to/audio.wav\", TimeSpan.Zero);\r\n        audioTrack.AddSegment(audioSegment);\r\n\r\n        // 3. Define the Mapping\r\n        var inputDeviceInfo = engine.MidiInputDevices.FirstOrDefault();\r\n        if (inputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"No MIDI input device found. Cannot create mapping.\");\r\n            return;\r\n        }\r\n\r\n        // a) Define the MIDI Source (e.g., CC #21 on our keyboard)\r\n        var source = new MidiInputSource\r\n        {\r\n            DeviceName = inputDeviceInfo.Name,\r\n            Channel = 0, // Omni: listen on all channels\r\n            MessageType = MidiMappingSourceType.ControlChange,\r\n            MessageParameter = 21 // The CC number to listen for\r\n        };\r\n\r\n        // b) Define the Target (the 'DownsampleFactor' property of our BitCrusher)\r\n        var target = new MidiMappingTarget\r\n        {\r\n            TargetObjectId = bitCrusher.Id, // Link to our specific BitCrusher instance\r\n            TargetType = MidiMappingTargetType.Property,\r\n            TargetMemberName = nameof(BitCrusherModifier.DownsampleFactor) // Must match the property name exactly\r\n        };\r\n\r\n        // c) Define the Transformer (map MIDI 0-127 to parameter 1.0-50.0)\r\n        // Note: The [ControllableParameter] attribute on BitCrusher already defines the Min/Max (1.0 to 50.0)\r\n        // and the Scale (Logarithmic). The transformer below maps the MIDI range to the\r\n        // *normalized* range (0.0 to 1.0) of that parameter.\r\n        var transformer = new ValueTransformer\r\n        {\r\n            SourceMin = 0,\r\n            SourceMax = 127,\r\n            TargetMin = 0.0f,\r\n            TargetMax = 1.0f,\r\n            CurveType = MidiMappingCurveType.Linear // Use Linear here, the attribute handles the log scale.\r\n        };\r\n\r\n        // 4. Create the MidiMapping Object\r\n        var mapping = new MidiMapping(source, target, transformer, MidiMappingBehavior.Absolute);\r\n\r\n        // 5. Add the mapping to the manager\r\n        composition.MappingManager.AddMapping(mapping);\r\n        Console.WriteLine($\"Mapping created: CC #21 on '{inputDeviceInfo.Name}' -> BitCrusher.DownsampleFactor\");\r\n\r\n        // 6. To make it listen, we must add the device to the manager.\r\n        // GetOrCreateInputNode initializes the device and makes the manager aware of it.\r\n        var inputNode = engine.MidiManager.GetOrCreateInputNode(inputDeviceInfo);\r\n        composition.MappingManager.AddInputDevice(inputNode.Device);\r\n\r\n        // 7. Start Playback\r\n        var soundPlayer = new SoundPlayer(engine, format, composition.Renderer);\r\n        playbackDevice.MasterMixer.AddComponent(soundPlayer);\r\n        playbackDevice.Start();\r\n        soundPlayer.Play();\r\n\r\n        Console.WriteLine(\"\\nAudio is now playing. Turn CC knob #21 on your controller to hear the effect.\");\r\n        Console.WriteLine(\"Press any key to exit.\");\r\n        Console.ReadKey();\r\n\r\n        // Clean up resources\r\n        playbackDevice.Stop();\r\n        composition.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n---\r\n\r\n### 2. Triggering Methods (Pad to Effect)\r\n\r\nThe `Trigger` behavior allows you to map a MIDI message to a method call, passing arguments derived from the MIDI message itself or as fixed constants.\r\n\r\n### Method Target Example\r\n\r\nConsider a custom component with this method:\r\n```csharp\r\npublic class EffectController : SoundComponent\r\n{\r\n    // ... constructor ...\r\n    public void TriggerStutter(float durationMs, bool quantize) { ... }\r\n    // ...\r\n}\r\n```\r\n\r\nWe will map a **Note On** message to this method. The **Velocity** will control the `durationMs`, and `quantize` will be fixed to `true`.\r\n\r\n```csharp\r\nvar target = new MidiMappingTarget\r\n{\r\n    TargetObjectId = effectController.Id,\r\n    TargetType = MidiMappingTargetType.Method,\r\n    TargetMemberName = \"TriggerStutter\",\r\n    MethodArguments = new List<MethodArgument>\r\n    {\r\n        // Argument 1: 'durationMs' (float)\r\n        // Mapped dynamically from Note Velocity\r\n        new MethodArgument\r\n        {\r\n            Source = MidiMappingArgumentSource.MidiValue,\r\n            // Scale velocity (0-127) to normalized float range (0.0-1.0).\r\n            // The method logic inside 'TriggerStutter' should handle scaling this\r\n            // to actual milliseconds, or rely on [ControllableParameter] metadata if present.\r\n            Transformer = new ValueTransformer { SourceMin = 0, SourceMax = 127 }\r\n        },\r\n\r\n        // Argument 2: 'quantize' (bool)\r\n        // Fixed constant\r\n        new MethodArgument\r\n        {\r\n            Source = MidiMappingArgumentSource.Constant,\r\n            ConstantValue = true\r\n        }\r\n    }\r\n};\r\n\r\nvar mapping = new MidiMapping(source, target, new ValueTransformer(), MidiMappingBehavior.Trigger)\r\n{\r\n    ActivationThreshold = 1 // Only trigger if velocity > 0\r\n};\r\n\r\ncomposition.MappingManager.AddMapping(mapping);\r\n```\r\n\r\n### 3. Relative Encoders (Endless Knobs)\r\n\r\n\"Endless\" encoders often send relative MIDI messages (e.g., `65` for +1 step right, `63` for -1 step left) rather than absolute values. SoundFlow's `Relative` behavior handles this automatically using specific math.\r\n\r\n**The Internal Math:**\r\n\r\n\r\n\r\n<p>\r\n  <span class=\"katex-display\">\r\n    $$ \\Delta = \\text{Input} - 64 $$\r\n\r\n    $$ \\text{StepSize} = \\frac{\\text{AttrMax} - \\text{AttrMin}}{\\text{SourceMax} - \\text{SourceMin}} $$\r\n\r\n    $$ \\text{NewValue} = \\text{OldValue} + (\\Delta \\times \\text{StepSize}) $$\r\n  </span>\r\n</p>\r\n\r\n**Tuning Sensitivity:**\r\nTo change the speed (sensitivity) of the knob, you adjust the `SourceMax` and `SourceMin` of the transformer.\r\n*   **Faster/Coarse Control:** Set a **smaller range** (e.g., 0 to 20). This decreases the denominator, making the `StepSize` larger.\r\n*   **Slower/Fine Control:** Set a **larger range** (e.g., 0 to 1000). This increases the denominator, making the `StepSize` smaller.\r\n\r\n```csharp\r\n// Configure for a relative encoder\r\nvar transformer = new ValueTransformer\r\n{\r\n    // A range of 100 (0 to 100) gives a moderate sensitivity.\r\n    // Reduce this range (e.g., 0 to 50) to make the knob move the parameter FASTER.\r\n    SourceMin = 0,\r\n    SourceMax = 100,\r\n    TargetMin = 0.0f,\r\n    TargetMax = 1.0f\r\n};\r\n\r\nvar mapping = new MidiMapping(source, target, transformer, MidiMappingBehavior.Relative);\r\n```\r\n\r\n---\r\n\r\n## High-Resolution MIDI (NRPN/RPN)\r\n\r\nStandard MIDI CCs are limited to 128 steps (7-bit). This can cause \"zipper noise\" when controlling sensitive parameters like filter cutoffs.\r\n\r\nSoundFlow v1.3 includes a dedicated, built-in **High-Resolution CC Parser** (`HighResCcParser`) within the `MidiMappingManager`. It automatically detects and reassembles standard 14-bit NRPN (Non-Registered Parameter Number) and RPN sequences.\r\n\r\n**How it works internally:**\r\n1.  The manager listens for CC 99 (NRPN MSB) and CC 98 (NRPN LSB) to identify the parameter index.\r\n2.  It waits for CC 6 (Data Entry MSB) and optionally CC 38 (Data Entry LSB).\r\n3.  It combines the Data MSB and LSB into a single **14-bit value** (0-16383).\r\n4.  It triggers any mapping configured with `MessageType = HighResolutionControlChange` matching the NRPN index.\r\n\r\n**How to use it:**\r\nYou simply treat the NRPN number as the `MessageParameter`.\r\n\r\n```csharp\r\n// Example: Mapping NRPN #1024 to a parameter.\r\n// NRPN 1024 corresponds to MSB 8, LSB 0 (8 * 128 + 0).\r\n\r\nvar source = new MidiInputSource\r\n{\r\n    DeviceName = \"My High-Res Synth\",\r\n    Channel = 1,\r\n    MessageType = MidiMappingSourceType.HighResolutionControlChange, // <--- Key Type\r\n    MessageParameter = 1024 // The raw 14-bit NRPN number\r\n};\r\n\r\nvar transformer = new ValueTransformer\r\n{\r\n    SourceMin = 0,\r\n    SourceMax = 16383, // Map the full 14-bit input range\r\n    TargetMin = 0.0f,\r\n    TargetMax = 1.0f\r\n};\r\n\r\nvar mapping = new MidiMapping(source, target, transformer, MidiMappingBehavior.Absolute);\r\ncomposition.MappingManager.AddMapping(mapping);\r\n```\r\n\r\n## Mapping Lifecycle\r\n\r\n### Persistence\r\nMappings are automatically saved and loaded as part of the `.sfproj` file via the `CompositionProjectManager`. The `Guid` (`TargetObjectId`) ensures that when a project is loaded, the mapping reconnects to the correct `SoundModifier` instance, even if tracks were reordered.\r\n\r\n### Resolution\r\nWhen a project is loaded, the `MidiMappingManager` attempts to \"resolve\" mappings.\r\n*   **`IsResolved` Property:** Each mapping has this boolean flag.\r\n*   If `IsResolved` is **true**, the mapping is active.\r\n*   If `IsResolved` is **false**, it means the target object (Guid) or the specific property name could not be found (e.g., the effect was deleted from the track). The mapping remains in the list (so you can inspect it or fix it) but will not process MIDI.\r\n\r\n### Debugging\r\nTo debug mapping issues, you can subscribe to the `MidiInputDevice` events directly before they reach the mapping manager to ensure hardware is sending what you expect.\r\n\r\n```csharp\r\n// Debugging helper\r\nvar inputNode = engine.MidiManager.GetOrCreateInputNode(myDeviceInfo);\r\ninputNode.OnMessageOutput += (msg) =>\r\n{\r\n    Console.WriteLine($\"DEBUG MIDI: Ch{msg.Channel} Cmd{msg.Command} D1:{msg.Data1} D2:{msg.Data2}\");\r\n};\r\n```"
  },
  {
    "id": 2.5,
    "slug": "metadata-and-tagging",
    "version": "1.3.0",
    "title": "Metadata & Tagging",
    "description": "Learn how to read and write metadata tags (ID3, Vorbis, MP4) and analyze audio file formats using the new SoundFlow.Metadata namespace.",
    "navOrder": 2.5,
    "category": "Core",
    "content": "﻿---\r\nid: 2.5\r\ntitle: Metadata & Tagging\r\ndescription: Learn how to read and write metadata tags (ID3, Vorbis, MP4) and analyze audio file formats using the new SoundFlow.Metadata namespace.\r\nnavOrder: 2.5\r\ncategory: Core\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Card, CardBody, CardHeader} from \"@heroui/react\";\r\n\r\n# Metadata & Tagging\r\n\r\nNew in SoundFlow v1.3.0 is a comprehensive `SoundFlow.Metadata` namespace. This subsystem provides tools for reading and writing tags (Artist, Title, Album Art) and analyzing file headers to determine format, duration, and bitrate without needing to fully decode the audio stream.\r\n\r\nSupported formats include:\r\n*   **MP3** (ID3v1, ID3v2.3)\r\n*   **FLAC, OGG** (Vorbis Comments)\r\n*   **WAV** (RIFF INFO, ID3 chunks)\r\n*   **AIFF** (IFF Chunks, ID3 chunks)\r\n*   **M4A / MP4** (iTunes-style atoms/boxes)\r\n\r\n## Reading Metadata\r\n\r\nThe `SoundMetadataReader` is your entry point. It can automatically detect the format of a file (even if the extension is wrong) by inspecting the header bytes.\r\n\r\n### Basic Usage\r\n\r\nTo inspect a file, use the static `Read` method.\r\n\r\n```csharp\r\nusing SoundFlow.Metadata;\r\nusing SoundFlow.Metadata.Models;\r\n\r\nvar filePath = \"path/to/song.mp3\";\r\n\r\n// Returns a Result<SoundFormatInfo>\r\nvar result = SoundMetadataReader.Read(filePath);\r\n\r\nif (result.IsSuccess)\r\n{\r\n    SoundFormatInfo info = result.Value;\r\n\r\n    Console.WriteLine($\"Format: {info.FormatName} ({info.CodecName})\");\r\n    Console.WriteLine($\"Duration: {info.Duration}\");\r\n    Console.WriteLine($\"Sample Rate: {info.SampleRate} Hz\");\r\n    Console.WriteLine($\"Bitrate: {info.Bitrate / 1000} kbps ({info.BitrateMode})\");\r\n\r\n    if (info.Tags != null)\r\n    {\r\n        Console.WriteLine($\"Title: {info.Tags.Title}\");\r\n        Console.WriteLine($\"Artist: {info.Tags.Artist}\");\r\n        Console.WriteLine($\"Album: {info.Tags.Album}\");\r\n\r\n        if (info.Tags.AlbumArt != null)\r\n        {\r\n            Console.WriteLine($\"Album Art found: {info.Tags.AlbumArt.Length} bytes\");\r\n        }\r\n    }\r\n}\r\nelse\r\n{\r\n    Console.WriteLine($\"Error: {result.Error.Message}\");\r\n}\r\n```\r\n\r\n### `ReadOptions`\r\n\r\nYou can optimize the reading process by passing a `ReadOptions` object.\r\n\r\n```csharp\r\nvar options = new ReadOptions\r\n{\r\n    // Set to false to skip parsing metadata tags entirely (faster).\r\n    ReadTags = true,\r\n\r\n    // Set to false to skip extracting Album Art images.\r\n    // This saves significant memory when processing many files.\r\n    ReadAlbumArt = false,\r\n\r\n    // Set to false to skip parsing Cue Sheets (common in large FLAC files).\r\n    ReadCueSheet = false,\r\n\r\n    // Controls duration calculation strategy.\r\n    // FastEstimate: Calculates duration based on file size and header bitrate. Instant but less accurate for VBR.\r\n    // AccurateScan: Scans the entire file to count frames. Slower but 100% precise.\r\n    DurationAccuracy = DurationAccuracy.FastEstimate\r\n};\r\n\r\nvar info = SoundMetadataReader.Read(filePath, options).Value;\r\n```\r\n\r\n## The `SoundFormatInfo` Model\r\n\r\nThe reader returns a `SoundFormatInfo` object containing detailed technical data.\r\n\r\n| Property           | Description                                                                    |\r\n|:-------------------|:-------------------------------------------------------------------------------|\r\n| `FormatName`       | Common name (e.g., \"MPEG Layer III\").                                          |\r\n| `FormatIdentifier` | Lowercase ID used by the `AudioEngine` codec system (e.g., `\"mp3\"`, `\"flac\"`). |\r\n| `Duration`         | The length of the audio.                                                       |\r\n| `SampleRate`       | Hz (e.g., 44100).                                                              |\r\n| `ChannelCount`     | Number of channels (1=Mono, 2=Stereo).                                         |\r\n| `BitsPerSample`    | Bit depth (e.g., 16, 24). 0 for lossy formats.                                 |\r\n| `Bitrate`          | Bits per second.                                                               |\r\n| `IsLossless`       | Boolean indicating if the codec is lossless.                                   |\r\n| `Tags`             | A `SoundTags` object containing standard metadata fields.                      |\r\n| `Cues`             | A `CueSheet` object if one was embedded (e.g., in FLAC).                       |\r\n\r\n## Writing Metadata\r\n\r\nThe `SoundMetadataWriter` allows you to update or strip tags from audio files.\r\n\r\n<Card className=\"bg-warning-50/50 dark:bg-warning-500/10 border-1 border-warning-200/50 dark:border-warning-500/20 mb-6\">\r\n    <CardHeader>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:alert-triangle\" className=\"text-warning text-2xl flex-shrink-0\" />\r\n            <h4 className=\"font-semibold text-lg\">Destructive Operation</h4>\r\n        </div>\r\n    </CardHeader>\r\n    <CardBody className=\"pt-0 text-sm\">\r\n        Metadata writing in SoundFlow is a **destructive operation**. It creates a temporary copy of the file with the new tags and then **overwrites the original file**. Always ensure you have backups or handle errors gracefully.\r\n    </CardBody>\r\n</Card>\r\n\r\n### Writing Tags\r\n\r\n```csharp\r\nusing SoundFlow.Metadata;\r\nusing SoundFlow.Metadata.Models;\r\n\r\nvar filePath = \"song.mp3\";\r\n\r\n// 1. Create a new tags object\r\nvar newTags = new SoundTags\r\n{\r\n    Title = \"My New Title\",\r\n    Artist = \"The Developers\",\r\n    Year = 2023,\r\n    Genre = \"Synthwave\"\r\n};\r\n\r\n// 2. Write to file (Async recommended for I/O)\r\nvar result = await SoundMetadataWriter.WriteTagsAsync(filePath, newTags);\r\n\r\nif (result.IsSuccess)\r\n{\r\n    Console.WriteLine(\"Tags updated successfully.\");\r\n}\r\n```\r\n\r\n### Removing Tags\r\n\r\nYou can also strip all supported metadata from a file, which is useful for privacy or preparing samples.\r\n\r\n```csharp\r\nvar result = await SoundMetadataWriter.RemoveTagsAsync(filePath);\r\n```\r\n\r\n## Format-Specific Details\r\n\r\nSoundFlow handles the complexities of different tagging standards automatically.\r\n\r\n*   **MP3:** Writes ID3v2.3 tags at the start of the file. ID3v1 tags at the end of the file are read but not written (deprecated).\r\n*   **FLAC & OGG:** Uses Vorbis Comments. `ALBUMARTIST` is mapped correctly alongside standard fields.\r\n*   **M4A / MP4:** Manipulates the `moov.udta.meta.ilst` atom tree to write iTunes-style metadata.\r\n*   **WAV:** Prioritizes reading `id3 ` chunks. If absent, it reads standard RIFF `LIST INFO` chunks. When writing, it writes an `id3 ` chunk for maximum compatibility with modern players.\r\n\r\n## Helper: Quick Format Detection\r\n\r\nIf you just need to know the format of a stream to initialize an `AudioEngine` device (without parsing all tags), use the helper on `AudioFormat`.\r\n\r\n```csharp\r\nusing SoundFlow.Structs;\r\n\r\n// Open a stream\r\nusing var stream = File.OpenRead(\"unknown_file.bin\");\r\n\r\n// Inspects header bytes to guess format, channels, and sample rate.\r\n// Returns null if format is unrecognized.\r\nAudioFormat? format = AudioFormat.GetFormatFromStream(stream);\r\n\r\nif (format.HasValue)\r\n{\r\n    Console.WriteLine($\"Detected: {format.Value.SampleRate}Hz, {format.Value.Channels}ch\");\r\n    // You can now pass this 'format' to engine.InitializePlaybackDevice(...)\r\n}\r\n```"
  },
  {
    "id": 1,
    "slug": "getting-started",
    "version": "1.3.0",
    "title": "Getting Started with SoundFlow",
    "description": "Learn how to install the library, set up your development environment, and write your first SoundFlow application.",
    "navOrder": 1,
    "category": "Core",
    "content": "---\nid: 1\ntitle: Getting Started with SoundFlow\ndescription: Learn how to install the library, set up your development environment, and write your first SoundFlow application.\nnavOrder: 1\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\n# Getting Started with SoundFlow\n\nThis guide will help you get up and running with SoundFlow quickly. You'll learn how to install the library, set up your development environment, and write your first SoundFlow application.\n\n## Prerequisites\n\nBefore you begin, make sure you have the following installed:\n\n*   **[.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0) or later:** SoundFlow is built on .NET 8.0, so you'll need the corresponding SDK to build and run SoundFlow projects.\n*   **An IDE or code editor:** You can use any IDE or code editor that supports .NET development. Popular choices include:\n*   [Visual Studio](https://visualstudio.microsoft.com/) (Recommended for Windows)\n*   [Visual Studio Code](https://code.visualstudio.com/) (Cross-platform)\n*   [JetBrains Rider](https://www.jetbrains.com/rider/) (Cross-platform)\n*   **Basic knowledge of C# and .NET:** Familiarity with C# programming and .NET concepts will be helpful.\n\n**Supported Operating Systems:**\n\n*   Windows\n*   macOS\n*   Linux\n*   Android\n*   iOS\n*   FreeBSD\n\n## Installation\n\nYou can install SoundFlow in several ways:\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\n    <Tab\n        key=\"nuget\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:nuget' />\n                <span>NuGet Package Manager</span>\n            </div>\n        }\n    >\n        ### Option 1: Using the NuGet Package Manager (Recommended)\n\n        This is the easiest and recommended way to add SoundFlow to your .NET projects.\n\n        1. **Open the NuGet Package Manager Console:** In Visual Studio, go to `Tools` > `NuGet Package Manager` > `Package Manager Console`.\n        2. **Run the installation command:**\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        This command will download and install the latest version of SoundFlow and its dependencies into your current project.\n    </Tab>\n\n    <Tab\n        key=\"cli\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:dotnet' />\n                <span>.NET CLI</span>\n            </div>\n        }\n    >\n        ### Option 2: Using the .NET CLI\n\n        1. **Open your terminal or command prompt.**\n        2. **Navigate to your project directory.**\n        3. **Run the following command:**\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n\n        This command will add a reference to the SoundFlow package in your project file (`.csproj`).\n    </Tab>\n\n    <Tab\n        key=\"source\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:git' />\n                <span>Building from Source</span>\n            </div>\n        }\n    >\n        ### Option 3: Building from Source\n\n        If you want to use the latest development version of SoundFlow or contribute to the project, you can build it from source:\n\n        1. **Clone the SoundFlow repository:**\n\n        ```bash\n        git clone https://github.com/LSXPrime/SoundFlow.git\n        ```\n\n        2. **Navigate to the cloned directory:**\n\n        ```bash\n        cd SoundFlow\n        ```\n\n        3. **Build the project using the .NET CLI:**\n\n        ```bash\n        dotnet build\n        ```\n    </Tab>\n</Tabs>\n\n## Basic Usage Example\n\nLet's create a simple console application that plays an audio file using SoundFlow.\n\n<Steps layout='horizontal' nextLabel='Got it, Next' finishLabel='All Done!' resetLabel='Start Again' summaryMessage=\"Congratulations! You've built and run your first audio application with SoundFlow. Feel free to experiment with the code or start over.\">\n    <Step title=\"Create Project\" description=\"Use VS or .NET CLI\" icon='ic:outline-create-new-folder'>\n        ### 1. Create a new console application:\n        *   In Visual Studio, go to `File` > `New` > `Project`. Select `Console App` and give it a name (e.g., `SoundFlowExample`).\n        *   Or, use the .NET CLI:\n\n        ```bash\n        dotnet new console -o SoundFlowExample\n        cd SoundFlowExample\n        ```\n    </Step>\n\n    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\n        ### 2. Install the SoundFlow NuGet package: (If you haven't already)\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n    </Step>\n\n    <Step title=\"Write Code\" description=\"Implement the audio player\" icon='ph:code-bold'>\n        ### 3. Replace the contents of `Program.cs` with the following code:\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Abstracts.Devices;\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System;\n        using System.IO;\n        using System.Linq;\n\n        namespace SoundFlowExample;\n\n        internal static class Program\n        {\n            private static void Main(string[] args)\n        {\n            // 1. Initialize the engine context.\n            using var engine = new MiniAudioEngine();\n\n            // 2. Define the audio format for playback.\n            var format = AudioFormat.DvdHq; // 48kHz, 2-channel, 32-bit float\n\n            // 3. Initialize a specific playback device. Passing `null` uses the system default.\n            engine.UpdateAudioDevicesInfo();\n            var defaultDevice = engine.PlaybackDevices.FirstOrDefault(x => x.IsDefault);\n            using var playbackDevice = engine.InitializePlaybackDevice(defaultDevice, format);\n\n            // 4. Create a SoundPlayer, passing the engine and format context.\n            // Make sure you replace \"path/to/your/audiofile.wav\" with the actual path.\n            using var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"path/to/your/audiofile.wav\"));\n            var player = new SoundPlayer(engine, format, dataProvider);\n\n            // 5. Add the player to the device's master mixer.\n            playbackDevice.MasterMixer.AddComponent(player);\n\n            // 6. Start the device to begin the audio stream.\n            playbackDevice.Start();\n\n            // 7. Start the player.\n            player.Play();\n\n            Console.WriteLine($\"Playing audio on '{playbackDevice.Info?.Name}'... Press any key to stop.\");\n            Console.ReadKey();\n\n            // 8. Stop the device, which also stops the audio stream.\n            playbackDevice.Stop();\n        }\n        }\n        ```\n        ***Replace `path/to/your/audiofile.wav` with the actual path to an audio file on your computer.***\n    </Step>\n\n    <Step title=\"Run\" description=\"Build and run the app\" icon='lucide:audio-lines'>\n        ### 4. Build and run the application:\n        *   In Visual Studio, press `F5` or go to `Debug` > `Start Debugging`.\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        *  use the .NET CLI:\n\n        ```bash\n        dotnet run\n        ```\n    </Step>\n</Steps>\n\nYou should now hear the audio file playing through your default audio output device.\n\n**Code Explanation:**\n\n*   `using SoundFlow...`: These lines import the necessary namespaces from the SoundFlow library.\n*   `using var engine = new MiniAudioEngine()`: This creates an instance of the `AudioEngine`. In the new architecture, the engine acts as a central **context** or **factory** for creating and managing audio devices. It no longer represents a single device itself.\n*   `var format = new AudioFormat { ... }`: We define the desired audio format using the `AudioFormat` struct. This bundles the sample rate, channel count, and sample format, and is passed to devices and components to ensure they operate in the correct context.\n*   `var defaultDevice = engine.PlaybackDevices.FirstOrDefault(...);` Getting the first device from the available playback devices.\n*   `using var playbackDevice = engine.InitializePlaybackDevice(defaultDevice, format)`: This is the new way to initialize a playback device. We ask the engine to initialize one for us, passing `null` to select the system's default device. The device itself is `IDisposable` and manages its own lifecycle.\n*   `using var dataProvider = ...`, `var player = new SoundPlayer(...)`: Components like `ISoundDataProvider` and `SoundPlayer` now require the `engine` and `format` contexts in their constructors.\n*   `playbackDevice.MasterMixer.AddComponent(player)`: This adds the `SoundPlayer` to the device-specific `Master` mixer, Each `AudioPlaybackDevice` has its own `MasterMixer` property. The `Master` mixer is the root of the audio graph and represents the final output of the audio device solely.\n*   `playbackDevice.Start()`: You must now explicitly start the device to begin its audio processing thread.\n*   `player.Play()`: This starts the player's internal logic, causing it to generate audio when its `Process` method is called by the device's mixer.\n*   `playbackDevice.Stop()`: This stops the device's audio thread, gracefully halting all audio output from it.\n\n## Troubleshooting\n\n*   **\"Could not load file or assembly 'SoundFlow'...\"**: Make sure you have installed the SoundFlow NuGet package or added a reference to the SoundFlow library if you built it from source.\n*   **No audio output**:\n*   Verify that your audio device is properly configured and selected as the default output device in your operating system's sound settings.\n*   Check the volume levels in your operating system and in the SoundFlow application.\n*   Ensure that the audio file you are trying to play is in a supported format (WAV, FLAC, and MP3 are supported by default). For more formats like AAC or OGG, you'll need an extension like `SoundFlow.Codecs.FFMpeg`.\n*   **Errors during installation**: If you encounter errors while installing the NuGet package, try clearing your NuGet cache (`dotnet nuget locals all --clear`) and try again.\n\nIf you encounter any other issues, please open an issue on the [GitHub repository](https://github.com/LSXPrime/SoundFlow).\n\n## Next Steps\n\nNow that you have successfully set up SoundFlow and played your first audio file, you can explore the more advanced features and concepts:\n\n*   [Migration Guide (v1.2 to v1.3)](./migration-guide): **(Recommended for existing users)** A detailed guide on updating your code for the new architecture.\n*   [Core Concepts](./core-concepts): Learn more about the fundamental building blocks of SoundFlow.\n*   [Device Management](./device-management): Understand the new device-centric architecture, including multi-device playback, capture, and MIDI device handling.\n*   [Editing Engine & Persistence](./editing-engine): Discover the powerful non-destructive editing and project saving capabilities.\n*   [API Reference](./api-reference): Dive into the detailed documentation for each class and interface.\n*   [Tutorials and Examples](./tutorials-and-examples): Get hands-on experience with various SoundFlow features.\n*   [FFmpeg Codec Extension](./ffmpeg-codec-extension): Learn how to add support for dozens of new audio formats.\n*   [PortMidi Backend](./portmidi-backend): Discover how to enable MIDI I/O and synchronization.\n*   [Synthesizer & Instruments](./synthesizer-and-instruments): Dive into the new built-in software synthesizer.\n\nHappy coding!\n"
  },
  {
    "id": 12,
    "slug": "ffmpeg-codec-extension",
    "version": "1.3.0",
    "title": "FFmpeg Codec Extension",
    "description": "A detailed guide to using the SoundFlow.Codecs.FFMpeg package to add extensive audio format support for decoding and encoding (MP3, AAC, OGG, Opus, FLAC, and more).",
    "navOrder": 12,
    "category": "Extensions",
    "content": "﻿---\r\nid: 12\r\ntitle: FFmpeg Codec Extension\r\ndescription: A detailed guide to using the SoundFlow.Codecs.FFMpeg package to add extensive audio format support for decoding and encoding (MP3, AAC, OGG, Opus, FLAC, and more).\r\nnavOrder: 12\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# FFmpeg Codec Extension\r\n\r\nThe `SoundFlow.Codecs.FFMpeg` package is a powerful extension that integrates the world-class FFmpeg multimedia framework into SoundFlow. It provides a high-priority `ICodecFactory` implementation, enabling your applications to decode and encode a vast range of popular lossy and lossless audio formats that are not supported by the default engine.\r\n\r\nThis extension utilizes a custom, high-performance native wrapper library (`soundflow-ffmpeg`) to ensure efficient, low-overhead communication between the .NET runtime and the underlying FFmpeg libraries.\r\n\r\n## Features\r\n\r\n*   **Wide Format Support:** Decode and encode dozens of formats, including MP3, AAC, OGG Vorbis, Opus, FLAC, ALAC (Apple Lossless), WMA, and many more.\r\n*   **Automatic Format Detection (Probing):** Intelligently detect the format of an audio stream without relying on file extensions, making your application robust to misnamed files.\r\n*   **High-Priority Factory:** The `FFmpegCodecFactory` has a high priority, ensuring it is automatically chosen over the default built-in codecs for any formats it supports.\r\n*   **Efficient Streaming Architecture:** Decodes and encodes directly from/to .NET `Stream` objects. This is highly memory-efficient as it does not require loading entire large files into memory.\r\n*   **Cross-Platform:** The NuGet package includes pre-compiled native binaries for a wide array of platforms and architectures, including Windows (x86, x64, ARM64), macOS (x64, Apple Silicon), Linux (x64, ARM, ARM64), Android, iOS, and FreeBSD.\r\n\r\n## Installation\r\n\r\nTo use the FFmpeg extension, add the `SoundFlow.Codecs.FFMpeg` package to your project.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Codecs.FFMpeg\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Codecs.FFMpeg\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Getting Started: Basic Usage\r\n\r\nThis example demonstrates the core workflow: registering the factory, decoding an MP3 file into raw audio samples, and then encoding those samples into a new FLAC file.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Set Up Project\" icon='ph:folder-open-bold'>\r\n        Create a new console application and install the `SoundFlow` and `SoundFlow.Codecs.FFMpeg` packages. Place an audio file named `input.mp3` in your project's output directory (e.g., `bin/Debug/net8.0`).\r\n    </Step>\r\n    <Step title=\"Write the Code\" icon='ph:code-bold'>\r\n        Use the following code in your `Program.cs`. It demonstrates how to register the factory and use the engine's generic `CreateDecoder` and `CreateEncoder` methods, which will automatically select the FFmpeg implementation.\r\n    </Step>\r\n    <Step title=\"Run the Application\" icon='lucide:play'>\r\n        Run the application. It will produce an `output.flac` file in the same directory.\r\n    </Step>\r\n</Steps>\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Codecs.FFMpeg;\r\nusing SoundFlow.Structs;\r\nusing System;\r\nusing System.IO;\r\n\r\npublic static class Program\r\n{\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"SoundFlow FFmpeg Codec Demo\");\r\n\r\n        // 1. Initialize the audio engine.\r\n        using var engine = new MiniAudioEngine();\r\n\r\n        // 2. Instantiate the FFmpegCodecFactory.\r\n        var ffmpegFactory = new FFmpegCodecFactory();\r\n\r\n        // 3. Register the factory with the engine.\r\n        // This makes all of FFmpeg's supported formats available to the engine.\r\n        engine.RegisterCodecFactory(ffmpegFactory);\r\n        Console.WriteLine($\"Registered '{ffmpegFactory.FactoryId}' with priority {ffmpegFactory.Priority}.\");\r\n\r\n        // DECODING EXAMPLE\r\n        Console.WriteLine(\"\\n--- Decoding input.mp3 ---\");\r\n\r\n        // Prepare to store the raw audio data\r\n        float[]? rawAudioData = null;\r\n        AudioFormat decodedFormat = default;\r\n\r\n        var inputFilePath = \"input.mp3\";\r\n        if (!File.Exists(inputFilePath))\r\n        {\r\n            Console.WriteLine($\"Error: Input file not found at '{Path.GetFullPath(inputFilePath)}'.\");\r\n            return;\r\n        }\r\n\r\n        try\r\n        {\r\n            using var fileStream = new FileStream(inputFilePath, FileMode.Open, FileAccess.Read);\r\n\r\n            // The engine will now query the FFmpeg factory to handle the \"mp3\" format.\r\n            using var decoder = engine.CreateDecoder(fileStream, \"mp3\", AudioFormat.DvdHq);\r\n\r\n            Console.WriteLine($\"Decoder created. Format: {decoder.SampleFormat}, Channels: {decoder.Channels}, Rate: {decoder.SampleRate} Hz\");\r\n            Console.WriteLine($\"Length: {decoder.Length / (float)decoder.Channels / decoder.SampleRate:F2} seconds.\");\r\n\r\n            // Store the format for encoding later\r\n            decodedFormat = new AudioFormat\r\n            {\r\n                Format = decoder.SampleFormat,\r\n                Channels = decoder.Channels,\r\n                SampleRate = decoder.SampleRate,\r\n                Layout = AudioFormat.GetLayoutFromChannels(decoder.Channels)\r\n            };\r\n\r\n            // Decode the entire file into a float array\r\n            rawAudioData = new float[decoder.Length];\r\n            var samplesRead = decoder.Decode(rawAudioData);\r\n            Console.WriteLine($\"Successfully decoded {samplesRead} samples.\");\r\n        }\r\n        catch (Exception ex)\r\n        {\r\n            Console.WriteLine($\"An error occurred during decoding: {ex.Message}\");\r\n            return;\r\n        }\r\n\r\n\r\n        // ENCODING EXAMPLE\r\n        if (rawAudioData == null) return;\r\n\r\n        Console.WriteLine(\"\\n--- Encoding to output.flac ---\");\r\n        var outputFilePath = \"output.flac\";\r\n\r\n        try\r\n        {\r\n            using var fileStream = new FileStream(outputFilePath, FileMode.Create, FileAccess.Write);\r\n\r\n            // The engine will query the FFmpeg factory to handle the \"flac\" format.\r\n            // We pass the format we discovered during decoding.\r\n            using var encoder = engine.CreateEncoder(fileStream, \"flac\", decodedFormat);\r\n\r\n            var samplesWritten = encoder.Encode(rawAudioData);\r\n            Console.WriteLine($\"Successfully encoded {samplesWritten} samples to '{outputFilePath}'.\");\r\n        }\r\n        catch (Exception ex)\r\n        {\r\n            Console.WriteLine($\"An error occurred during encoding: {ex.Message}\");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## Core Concepts\r\n\r\n### The Codec Factory System (`ICodecFactory`)\r\n\r\nThe `FFmpegCodecFactory` is an implementation of the `ICodecFactory` interface, which is the cornerstone of SoundFlow's pluggable codec system. When you call `engine.CreateDecoder` or `engine.CreateEncoder`, the engine iterates through its registered factories, from highest to lowest priority, asking each one if it can handle the requested format ID.\r\n\r\nThe `FFmpegCodecFactory` has three key properties:\r\n*   `FactoryId`: A unique string, `\"SoundFlow.Codecs.FFMpeg\"`.\r\n*   `Priority`: An integer value of `100`. This high value ensures it is queried before the default built-in `MiniAudioCodecFactory` (which has a priority of 0) for any formats they might both support (like MP3).\r\n*   `SupportedFormatIds`: A comprehensive read-only collection of all format identifiers (e.g., \"mp3\", \"flac\") that this factory can handle. This list is derived from the specific build configuration of the underlying FFmpeg library.\r\n\r\n### Supported Formats\r\n\r\nThe `FFmpegCodecFactory` provides support for a vast array of audio formats. The exact list is determined by the native FFmpeg build included with the package, but it includes the following:\r\n\r\n*   **Lossless Formats:** `wav`, `aiff`, `flac`, `alac` (Apple Lossless), `ape` (Monkey's Audio), `wv` (WavPack), `tta` (True Audio), `shn` (Shorten)\r\n*   **Lossy Formats:** `mp3`, `mp2`, `ogg` (Vorbis), `opus`, `aac`, `m4a`, `wma` (Windows Media Audio), `ac3`\r\n*   **Container & Other Formats:** `mka` (Matroska Audio), `mpc` (Musepack), `tak` (Tom's lossless Audio Kompressor), `ra` (RealAudio), `dsf` (DSD Stream File), `au`, `gsm`\r\n\r\n### Automatic Format Detection (Probing)\r\n\r\nOne of the most powerful features enabled by the FFmpeg extension is the ability to decode a stream without knowing its format beforehand. The engine's `CreateDecoder(Stream stream, out AudioFormat detectedFormat)` method will ask each registered factory to \"probe\" the stream. The FFmpeg factory can read the first few bytes of the stream to identify the container and codec.\r\n\r\n```csharp\r\nusing var engine = new MiniAudioEngine();\r\nengine.RegisterCodecFactory(new FFmpegCodecFactory());\r\n\r\n// The file could be an MP3, FLAC, OGG, or any other supported format.\r\nvar unknownFilePath = \"some_audio_file.bin\";\r\n\r\nusing var fileStream = new FileStream(unknownFilePath, FileMode.Open, FileAccess.Read);\r\n\r\ntry\r\n{\r\n    // The engine will use the FFmpeg factory to probe the stream.\r\n    using var decoder = engine.CreateDecoder(fileStream, out AudioFormat detectedFormat);\r\n\r\n    // The 'detectedFormat' struct is now populated with the correct information.\r\n    Console.WriteLine($\"Successfully identified format!\");\r\n    Console.WriteLine($\"Channels: {detectedFormat.Channels}, Rate: {detectedFormat.SampleRate} Hz\");\r\n\r\n    // You can now use the decoder as normal.\r\n    var samples = new float[decoder.Length];\r\n    decoder.Decode(samples);\r\n}\r\ncatch (NotSupportedException)\r\n{\r\n    Console.WriteLine(\"Could not identify the audio format in the provided stream.\");\r\n}\r\n```\r\n\r\n## Advanced Topics & Internals\r\n\r\n### Streaming Architecture\r\n\r\nThe `FFmpegDecoder` and `FFmpegEncoder` are designed for efficiency and do not load entire files into memory. They operate directly on the provided `Stream`. This is achieved through a callback mechanism that bridges the managed .NET world with the native FFmpeg library's I/O functions.\r\n\r\n*   `ReadCallback` & `SeekCallback`: When the native decoder needs to read more data or seek within the input, it invokes these C# delegates, which in turn call `_stream.Read()` and `_stream.Seek()`.\r\n*   `WriteCallback`: When the native encoder produces a chunk of encoded data, it invokes this delegate, which calls `_stream.Write()` to save the data.\r\n\r\nThis design ensures that even very large files can be processed with a minimal memory footprint.\r\n\r\n### Error Handling: `FFmpegException` and `FFmpegResult`\r\n\r\nIf an unrecoverable error occurs within the native `soundflow-ffmpeg` library (e.g., a file is corrupt, a required codec is missing from the build), an `FFmpegException` is thrown. This exception contains a `Result` property of type `FFmpegResult`, which is a detailed enum that maps directly to the error codes from the native C API. This allows for precise error diagnosis.\r\n\r\nThe possible `FFmpegResult` values are:\r\n\r\n#### General Errors\r\n*   `Success = 0`: The operation completed successfully.\r\n*   `ErrorInvalidArgs = -1`: Invalid arguments (e.g., a null pointer) were provided to a native function.\r\n*   `ErrorAllocationFailed = -2`: A memory allocation failed within the native library.\r\n\r\n#### Decoder-specific Errors\r\n*   `DecoderErrorOpenInput = -10`: Failed to open the input stream. This can happen if the format is not recognized or the data is corrupt.\r\n*   `DecoderErrorFindStreamInfo = -11`: Could not find or parse stream information from the input.\r\n*   `DecoderErrorNoAudioStream = -12`: No suitable audio stream was found in the input file.\r\n*   `DecoderErrorCodecNotFound = -13`: A decoder for the audio format could not be found in the linked FFmpeg build.\r\n*   `DecoderErrorCodecContextAlloc = -14`: Failed to allocate a context for the audio decoder.\r\n*   `DecoderErrorCodecOpenFailed = -15`: The audio decoder could not be opened.\r\n*   `DecoderErrorInvalidTargetFormat = -16`: The requested target sample format for decoding is invalid or not supported.\r\n*   `DecoderErrorResamplerInitFailed = -17`: The audio resampler (`swresample`) could not be initialized for format conversion.\r\n*   `DecoderErrorPacketFrameAlloc = -18`: Failed to allocate an `AVFrame` or `AVPacket` for the decoding process.\r\n*   `DecoderErrorSeekFailed = -19`: The seek operation failed in the underlying FFmpeg format context.\r\n*   `DecoderErrorDecodingFailed = -20`: An unrecoverable error occurred during the decoding process, such as from a corrupt packet.\r\n\r\n#### Encoder-specific Errors\r\n*   `EncoderErrorFormatNotFound = -30`: The requested output format (e.g., \"mp3\", \"flac\") could not be found.\r\n*   `EncoderErrorCodecNotFound = -31`: An encoder for the requested output format could not be found in the linked FFmpeg build.\r\n*   `EncoderErrorStreamAlloc = -32`: Failed to create a new audio stream in the output format context.\r\n*   `EncoderErrorCodecContextAlloc = -33`: Failed to allocate a context for the audio encoder.\r\n*   `EncoderErrorCodecOpenFailed = -34`: The audio encoder could not be opened.\r\n*   `EncoderErrorContextParams = -35`: Failed to copy encoder parameters to the output stream.\r\n*   `EncoderErrorWriteHeader = -36`: Failed to write the header for the output audio file.\r\n*   `EncoderErrorInvalidInputFormat = -37`: The provided input sample format for encoding is invalid or not supported.\r\n*   `EncoderErrorResamplerInitFailed = -38`: The audio resampler (`swresample`) could not be initialized for encoding.\r\n*   `EncoderErrorPacketFrameAlloc = -39`: Failed to allocate an `AVFrame` or `AVPacket` for the encoding process.\r\n*   `EncoderErrorEncodingFailed = -40`: An unrecoverable error occurred during the encoding process.\r\n*   `EncoderErrorWriteFailed = -41`: An I/O error occurred while writing the encoded data to the output stream.\r\n\r\n### Native Library Management\r\n\r\nThe `SoundFlow.Codecs.FFMpeg` NuGet package is a self-contained unit. It includes the pre-compiled native `soundflow-ffmpeg` wrapper library for multiple platforms in its `runtimes` folder. When your application runs, a special `NativeLibraryResolver` automatically finds and loads the correct binary (`.dll`, `.so`, or `.dylib`) for the user's operating system and CPU architecture. This means you do not need to manually manage or deploy any native files.\r\n\r\n```\r\n/runtimes/\r\n├── win-x64/native/soundflow-ffmpeg.dll\r\n├── osx-arm64/native/libsoundflow-ffmpeg.dylib\r\n└── linux-x64/native/libsoundflow-ffmpeg.so\r\n... and so on for all supported platforms.\r\n```\r\n\r\nIf you encounter a `DllNotFoundException`, it typically means your deployment environment does not match one of the included runtimes or the files were not copied correctly."
  },
  {
    "id": 4.4,
    "slug": "editing-recording-and-sequencing",
    "version": "1.3.0",
    "title": "Editing - Recording & Sequencing",
    "description": "A comprehensive guide to recording live MIDI performances into your composition and using the Sequencer component for sample-accurate MIDI playback.",
    "navOrder": 4.4,
    "category": "Editing",
    "content": "---\r\nid: 4.4\r\ntitle: Editing - Recording & Sequencing\r\ndescription: A comprehensive guide to recording live MIDI performances into your composition and using the Sequencer component for sample-accurate MIDI playback.\r\nnavOrder: 4.4\r\ncategory: Editing\r\n---\r\n\r\n# MIDI Recording and Sequencing\r\n\r\nSoundFlow provides a sophisticated infrastructure for both capturing live MIDI performances and playing back MIDI data with sample-accurate timing. MIDI recording is managed by the powerful `composition.Recorder` service, while tempo-aware MIDI playback is handled by the `Sequencer` component.\r\n\r\n## MIDI Recording with `CompositionRecorder`\r\n\r\nThe `composition.Recorder` (`SoundFlow.Editing.CompositionRecorder`) is the dedicated service for managing the entire MIDI recording workflow. It is designed to be tightly integrated with the composition's transport (playback position), listening to armed `MidiTrack`s and capturing incoming MIDI data from physical devices. It then intelligently converts this captured data into new `MidiSegment`s on the timeline.\r\n\r\n### The Recording Workflow\r\n\r\nThe process is designed to mimic a real-world DAW workflow and can be broken down into four main steps:\r\n\r\n1.  **Get an Initialized MIDI Device:** Use the `MidiManager` to get a managed instance of the physical `MidiInputDevice` you want to record from.\r\n2.  **Arm a Track:** Tell the recorder which `MidiTrack` should receive MIDI from that specific, initialized device instance.\r\n3.  **Set Recording Mode:** Choose what happens when you record over existing MIDI data on the track.\r\n4.  **Start and Stop Recording:** Initiate and finalize the capture process.\r\n\r\n### Step 1 & 2: Obtaining a Device and Arming a Track\r\n\r\nYou cannot arm a track with just device information (`MidiDeviceInfo`). The recorder requires a live, initialized `MidiInputDevice` object. The **`MidiManager`** is responsible for creating and managing the lifecycle of these device instances.\r\n\r\nThe correct pattern is to use the `MidiManager` to get a managed device instance. For live recording, you will also want to set up a \"monitor\" route so you can hear what you are playing in real-time.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Midi.Devices;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Routing;\r\nusing SoundFlow.Midi.Routing.Nodes;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing System.Linq;\r\n\r\n// Setup the audio and MIDI engines\r\nusing var engine = new MiniAudioEngine();\r\nengine.UseMidiBackend(new PortMidiBackend());\r\nengine.UpdateMidiDevicesInfo();\r\n\r\n// Create a composition, a track to record onto, and a synthesizer to monitor the performance\r\nvar composition = new Composition(engine, AudioFormat.DvdHq, \"My Song\");\r\nvar myMidiTrack = new MidiTrack(\"Piano Performance\");\r\nvar synthesizer = new Synthesizer(engine, AudioFormat.DvdHq, new BasicInstrumentBank(AudioFormat.DvdHq)) { Name = \"Live Synth\" };\r\ncomposition.Editor.AddMidiTrack(myMidiTrack);\r\n\r\n// Register the synthesizer as a valid output target for MIDI tracks\r\ncomposition.MidiTargets.Add(new MidiTargetNode(synthesizer));\r\nmyMidiTrack.Target = composition.MidiTargets.First();\r\n\r\n// Select the physical MIDI input device\r\nvar inputDeviceInfo = engine.MidiInputDevices.FirstOrDefault();\r\nif (inputDeviceInfo.Name == null)\r\n{\r\n    Console.WriteLine(\"No MIDI input device found. Cannot arm track.\");\r\n    return;\r\n}\r\n\r\n// Create a monitoring route from the input device to the synthesizer\r\n// This allows the performer to hear what they are playing in real-time.\r\nvar monitorRoute = engine.MidiManager.CreateRoute(inputDeviceInfo, synthesizer);\r\nConsole.WriteLine($\"Live monitoring enabled: '{inputDeviceInfo.Name}' -> '{synthesizer.Name}'\");\r\n\r\n// The route's source node contains the initialized device required by the recorder\r\nvar inputNode = (MidiInputNode)monitorRoute.Source;\r\nvar initializedDevice = inputNode.Device;\r\n\r\n// Arm the track, telling the recorder to capture MIDI from this specific device\r\ncomposition.Recorder.ArmTrackForRecording(myMidiTrack, initializedDevice);\r\nConsole.WriteLine($\"Track '{myMidiTrack.Name}' is armed for recording from '{initializedDevice.Info.Name}'.\");\r\n```\r\n\r\n### Recording Modes\r\n\r\nThe `RecordingMode` enum gives you precise control over what happens when you record over a section of the timeline that already contains MIDI data.\r\n\r\n*   **`RecordingMode.Normal`:** This is the default \"destructive\" or \"tape-style\" recording. It always creates a brand new `MidiSegment` for the recorded performance. If you record over an existing segment, the new segment will be layered on top of it, and both will play back simultaneously unless you manually delete the old one.\r\n\r\n*   **`RecordingMode.OverdubMerge`:** This mode is for building up a performance. It merges the newly recorded notes and automation directly into an existing `MidiSegment`. If no segment exists at the recording start time, it will automatically create a new one. This is perfect for adding layers to a drum pattern or correcting a few notes in a piano performance without erasing the original take.\r\n\r\n### Starting and Stopping Recording\r\n\r\nOnce a track is armed, you can start and stop the recording process. The `StartRecording` method requires a `TimeSpan` to know where on the composition's timeline the recording should be placed.\r\n\r\n```csharp\r\nvar transportTime = composition.Renderer.CurrentTime;\r\n\r\n// Example 1: Standard recording mode, which creates a new segment.\r\ncomposition.Recorder.StartRecording(transportTime, RecordingMode.Normal);\r\n\r\n// Let playback and recording continue...\r\nConsole.WriteLine(\"Recording in Normal mode. Press Enter to stop.\");\r\nConsole.ReadLine();\r\n\r\nawait composition.Recorder.StopRecordingAsync();\r\nConsole.WriteLine(\"Recording stopped. New segment created.\");\r\n\r\n\r\n// Example 2: Overdub recording mode, which merges into an existing segment.\r\n// Find a segment at the current transport time to overdub into.\r\nMidiSegment targetSegment = myMidiTrack.Segments.FirstOrDefault(s => s.Contains(transportTime));\r\n\r\n// StartRecording will create a new segment if none is found, even in OverdubMerge mode.\r\ncomposition.Recorder.StartRecording(transportTime, RecordingMode.OverdubMerge, targetSegment);\r\n\r\n// Let playback and recording continue...\r\nConsole.WriteLine(\"Recording in OverdubMerge mode. Press Enter to stop.\");\r\nConsole.ReadLine();\r\n\r\nawait composition.Recorder.StopRecordingAsync();\r\nConsole.WriteLine(\"Recording stopped. Notes merged into segment.\");\r\n```\r\n\r\n### Advanced Recording: Punch-In/Out and Loop Recording\r\n\r\nThe `CompositionRecorder` supports features essential for professional recording workflows.\r\n\r\n*   **`PunchInTime` and `PunchOutTime`:** These `TimeSpan?` properties allow you to define a precise window for recording.\r\n*   When you call `StartRecording` and the playback time is before `PunchInTime`, the recorder enters a \"waiting\" state (`IsWaitingForPunchIn` will be `true`). It will automatically start capturing MIDI events only when playback reaches `PunchInTime`.\r\n*   If `PunchOutTime` is set, the recorder will automatically stop capturing when playback reaches that time.\r\n\r\n```csharp\r\n// Example: Correct a section from measure 5 to measure 9.\r\nvar measure5Time = TimeSpan.FromSeconds(10);\r\nvar measure9Time = TimeSpan.FromSeconds(20);\r\n\r\n// Set up the punch-in and punch-out times.\r\ncomposition.Recorder.PunchInTime = measure5Time;\r\ncomposition.Recorder.PunchOutTime = measure9Time;\r\n\r\n// Start the recorder in \"waiting\" mode before playback reaches the punch-in point.\r\n// For example, start playback from measure 1 (TimeSpan.Zero).\r\ncomposition.Recorder.StartRecording(TimeSpan.Zero);\r\n\r\n// ... start playback of the composition's renderer from the beginning ...\r\n\r\n// The recorder will automatically handle the start and stop of MIDI capture\r\n// precisely between 10 and 20 seconds.\r\n```\r\n*   **Loop Recording:** The recorder is aware of the composition renderer's looping. When `composition.Renderer.OnTransportLoop` is triggered (a feature you would implement in a custom transport), the recorder adds a sample offset. This ensures that notes recorded on subsequent loop passes are correctly placed on the timeline relative to the loop start, effectively creating \"takes\" or layers within a single recording pass.\r\n\r\n## MIDI Playback with the `Sequencer`\r\n\r\nThe `Sequencer` (`SoundFlow.Synthesis.Sequencer`) is a `SoundComponent` designed for high-precision, sample-accurate playback of MIDI data from a `MidiDataProvider`.\r\n\r\nUnlike a simple file player, the `Sequencer` does not generate audio itself. Instead, it hooks into the audio render callback, using it as a high-resolution clock to dispatch MIDI events to its target at the exact sample they are meant to occur.\r\n\r\n### Key Concepts\r\n\r\n*   **Input:** It takes a `MidiDataProvider` as its source of MIDI events. This provider contains a time-ordered list of all MIDI events.\r\n*   **Output:** It sends MIDI messages to an `IMidiControllable` target, most commonly a `Synthesizer`.\r\n*   **Timing:** It is driven by the audio graph. Its `GenerateAudio` method (which does not actually write audio) is called for each audio buffer. Inside this method, it calculates which MIDI events fall within that buffer's time slice and dispatches them.\r\n*   **Tempo Awareness:** It uses an `ISequencerContext` (which `Composition` implements) to query the master `TempoTrack`, allowing it to handle complex tempo changes correctly, ensuring MIDI playback speeds up and slows down with the project tempo.\r\n\r\n### Standalone Sequencer Example\r\n\r\nWhile the `composition.Renderer` handles all sequencing automatically when you play a full project, you can also use the `Sequencer` component directly to create a standalone MIDI file player.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Metadata.Midi;\r\nusing SoundFlow.Providers;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\n\r\n// User-defined asset paths\r\nconst string midiFilePath = @\"C:\\Path\\To\\Your\\Song.mid\";\r\nconst string soundFontPath = @\"C:\\Path\\To\\Your\\SoundFont.sf2\"; // We're using a SoundFont here, but you can use any other instrument bank supported by SoundFlow\r\n\r\n// Initialize the audio engine and output device\r\nusing var engine = new MiniAudioEngine();\r\nvar audioFormat = AudioFormat.DvdHq;\r\nusing var device = engine.InitializePlaybackDevice(null, audioFormat); // null for default device\r\n\r\n// Load the MIDI file and SoundFont instrument\r\nvar midiFile = MidiFileParser.Parse(File.OpenRead(midiFilePath));\r\nvar midiDataProvider = new MidiDataProvider(midiFile);\r\nvar instrumentBank = new SoundFontBank(soundFontPath, audioFormat);\r\n\r\n// Create the synthesizer and sequencer\r\nvar synthesizer = new Synthesizer(engine, audioFormat, instrumentBank);\r\nvar sequencer = new Sequencer(engine, audioFormat, midiDataProvider, synthesizer)\r\n{\r\n    IsLooping = true // Optional: make the playback loop\r\n};\r\n\r\n// Build the audio processing graph\r\ndevice.MasterMixer.AddComponent(synthesizer); // The synthesizer is what actually generates audio\r\ndevice.MasterMixer.AddComponent(sequencer); // The sequencer's timing is driven by the audio graph, allowing it to send MIDI events to the synthesizer\r\n\r\n// Start playback\r\ndevice.Start();\r\nsequencer.Play();\r\n\r\nConsole.WriteLine($\"Playing '{Path.GetFileName(midiFilePath)}'. Press any key to stop.\");\r\nConsole.ReadKey();\r\n\r\nsequencer.Stop();\r\ndevice.Stop();\r\n```\r\n\r\n\r\n## Complete Recording Workflow: A Practical Example\r\n\r\nTo bring all these concepts together, here is a complete, runnable console application that demonstrates the full record-and-playback workflow. It includes live monitoring, recording to a MIDI track, and then playing back the recorded performance.\r\n\r\n### Prerequisites\r\n\r\n*   A .NET Console Application project.\r\n*   The `SoundFlow` and `SoundFlow.Midi.PortMidi` NuGet packages installed.\r\n*   A physical/virtual MIDI keyboard connected to your computer.\r\n*   A SoundFont (`.sf2`) file (this example assumes one is available).\r\n\r\n### Full Example Code\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Midi.PortMidi;\r\nusing SoundFlow.Midi.Routing.Nodes;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing MidiTrack = SoundFlow.Editing.MidiTrack;\r\n\r\nnamespace ConsoleApp1;\r\n\r\npublic static class Program\r\n{\r\n    /// <summary>\r\n    /// The main entry point for the application.\r\n    /// </summary>\r\n    public static void Main()\r\n    {\r\n        Console.WriteLine(\"SoundFlow v1.3.0 - Full MIDI Recording Example\");\r\n        Console.WriteLine(\"==============================================\\n\");\r\n\r\n        // 1. Setup Engine & MIDI Backend\r\n        using var engine = new MiniAudioEngine();\r\n        engine.UseMidiBackend(new PortMidiBackend());\r\n        engine.UpdateAudioDevicesInfo();\r\n        engine.UpdateMidiDevicesInfo();\r\n\r\n        // 2. Prepare Composition & Instrument\r\n        var format = AudioFormat.DvdHq;\r\n        var composition = new Composition(engine, format, \"Live Recording Session\");\r\n        var midiTrack = new MidiTrack(\"My Performance\");\r\n        composition.Editor.AddMidiTrack(midiTrack);\r\n\r\n        var synthesizer = new Synthesizer(engine, format, new BasicInstrumentBank(format)) { Name = \"Session Synth\" };\r\n        composition.MidiTargets.Add(new MidiTargetNode(synthesizer));\r\n        midiTrack.Target = composition.MidiTargets.First();\r\n\r\n        // 3. Setup MIDI Input\r\n        var inputDeviceInfo = engine.MidiInputDevices.FirstOrDefault();\r\n        if (inputDeviceInfo.Name == null)\r\n        {\r\n            Console.WriteLine(\"ERROR: No MIDI input device found. Please connect a device and restart.\");\r\n            Console.ReadKey();\r\n            return;\r\n        }\r\n\r\n        var inputNode = engine.MidiManager.GetOrCreateInputNode(inputDeviceInfo);\r\n        var initializedDevice = inputNode.Device;\r\n\r\n\r\n        // 4. Arm the Track\r\n        composition.Recorder.ArmTrackForRecording(midiTrack, initializedDevice);\r\n        Console.WriteLine($\"Track '{midiTrack.Name}' is armed for recording.\\n\");\r\n\r\n        // 5. Setup Live MIDI Monitoring\r\n        var monitorRoute = engine.MidiManager.CreateRoute(inputDeviceInfo, synthesizer);\r\n        Console.WriteLine($\"Live monitoring enabled: '{inputDeviceInfo.Name}' -> '{synthesizer.Name}'\");\r\n\r\n\r\n        // 6. Initialize Audio Playback\r\n        using var device = engine.InitializePlaybackDevice(null, format);\r\n        Console.WriteLine($\"Audio output initialized on: {device.Info?.Name}\\n\");\r\n\r\n        // 7. Add Components to MasterMixer\r\n        device.MasterMixer.AddComponent(synthesizer);\r\n        var compositionPlayer = new SoundPlayer(engine, format, composition.Renderer);\r\n        device.MasterMixer.AddComponent(compositionPlayer);\r\n\r\n        // 8. Interactive Session\r\n        var isRecording = false;\r\n        try\r\n        {\r\n            device.Start();\r\n\r\n            while (true)\r\n            {\r\n                Console.WriteLine(\"--- MENU\");\r\n                Console.WriteLine(\"1. Start Recording (Normal)\");\r\n                Console.WriteLine(\"2. Stop Recording\");\r\n                Console.WriteLine(\"3. Playback Composition\");\r\n                Console.WriteLine(\"4. Exit\");\r\n                Console.Write(\"Select an option: \");\r\n                var key = Console.ReadKey(true).Key;\r\n                Console.WriteLine(key.ToString().Last());\r\n\r\n                switch (key)\r\n                {\r\n                    case ConsoleKey.D1:\r\n                        if (isRecording)\r\n                        {\r\n                            Console.WriteLine(\"Already recording.\");\r\n                            break;\r\n                        }\r\n\r\n                        Console.WriteLine(\"\\n*** RECORDING STARTED ***\\nPlay on your MIDI device. Press '2' to stop.\");\r\n                        isRecording = true;\r\n                        compositionPlayer.Seek(0);\r\n                        composition.Recorder.StartRecording(composition.Renderer.CurrentTime, RecordingMode.Normal);\r\n                        compositionPlayer.Play();\r\n                        break;\r\n\r\n                    case ConsoleKey.D2:\r\n                        if (!isRecording)\r\n                        {\r\n                            Console.WriteLine(\"Not currently recording.\");\r\n                            break;\r\n                        }\r\n\r\n                        Console.WriteLine(\"\\n*** RECORDING STOPPED ***\");\r\n                        isRecording = false;\r\n                        composition.Recorder.StopRecording();\r\n                        compositionPlayer.Stop();\r\n                        var recordedSegment = midiTrack.Segments.LastOrDefault();\r\n                        if (recordedSegment != null)\r\n                        {\r\n                            Console.WriteLine($\"A new MIDI segment '{recordedSegment.Name}' was created with a duration of {recordedSegment.SourceDuration:ss\\\\.fff}s.\");\r\n                        }\r\n\r\n                        break;\r\n\r\n                    case ConsoleKey.D3:\r\n                        if (isRecording)\r\n                        {\r\n                            Console.WriteLine(\"Please stop recording first.\");\r\n                            break;\r\n                        }\r\n\r\n                        if (midiTrack.Segments.Count == 0)\r\n                        {\r\n                            Console.WriteLine(\"Nothing to play back. Record something first.\");\r\n                            break;\r\n                        }\r\n\r\n                        // Remove Synthesizer from MasterMixer to prevent feedback\r\n                        device.MasterMixer.RemoveComponent(synthesizer);\r\n\r\n                        Console.WriteLine(\"\\n*** PLAYING BACK COMPOSITION ***\");\r\n                        compositionPlayer.Seek(0);\r\n                        compositionPlayer.Play();\r\n                        break;\r\n\r\n                    case ConsoleKey.D4:\r\n                        Console.WriteLine(\"Exiting...\");\r\n                        return;\r\n\r\n                    default:\r\n                        Console.WriteLine(\"Invalid option.\");\r\n                        break;\r\n                }\r\n\r\n                Console.WriteLine();\r\n            }\r\n        }\r\n        finally\r\n        {\r\n            Console.WriteLine(\"\\nShutting down...\");\r\n            compositionPlayer.Stop();\r\n            device.Stop();\r\n            engine.MidiManager.RemoveRoute(monitorRoute);\r\n            composition.Dispose();\r\n        }\r\n    }\r\n}\r\n```"
  },
  {
    "id": 4.3,
    "slug": "editing-playback-and-rendering",
    "version": "1.3.0",
    "title": "Editing - Playback and Rendering",
    "description": "Learn how to play back your entire composition in real-time and how to render it to an audio file for export.",
    "navOrder": 4.3,
    "category": "Editing",
    "content": "---\r\nid: 4.3\r\ntitle: Editing - Playback and Rendering\r\ndescription: Learn how to play back your entire composition in real-time and how to render it to an audio file for export.\r\nnavOrder: 4.3\r\ncategory: Editing\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# Playback and Rendering\r\n\r\nOnce you have built a `Composition` with audio and MIDI tracks, you can play it back in real time or render it offline to an audio file. Both of these operations are handled by the `composition.Renderer` service.\r\n\r\n## Live Playback\r\n\r\nThe `composition.Renderer` (`SoundFlow.Editing.CompositionRenderer`) is a special service that also implements the `ISoundDataProvider` interface. This clever design allows your entire, complex composition to be treated as a single, playable audio source.\r\n\r\nWhen a `SoundPlayer` requests audio from the `composition.Renderer`, the renderer performs several steps in real-time on the audio thread:\r\n1.  It calculates the current time and the time of the next audio buffer.\r\n2.  It determines which MIDI events from all active `MidiTrack`s fall within that time window, using the master `TempoTrack` for accurate timing.\r\n3.  It dispatches those MIDI events to their respective targets (e.g., `Synthesizer` instances).\r\n4.  It asks all active `Track`s to render their audio segments for the same time window.\r\n5.  It asks all active `Synthesizer` instances (and other audio-generating MIDI targets) to render their audio for the time window.\r\n6.  It mixes all the audio from audio tracks and synthesizers together.\r\n7.  It applies master effects (`Modifiers` and `Analyzers`) from the `Composition`.\r\n8.  It applies the `MasterVolume`.\r\n9.  It returns the final mixed audio buffer to the `SoundPlayer`.\r\n\r\n### Example: Playing a Full Composition\r\n\r\nThis example ties everything together, showing how to play a composition that contains both an audio track and a MIDI track driving a synthesizer.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Midi.Routing.Nodes;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing MidiTrack = SoundFlow.Editing.MidiTrack;\r\n\r\n// Set up the engine and a composition to hold our tracks\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\nvar composition = new Composition(engine, format, \"Full Playback Demo\");\r\n\r\n// Create a synthesizer and register it as a MIDI target within the composition.\r\n// The composition will manage sending MIDI to it and getting audio from it.\r\nvar instrumentBank = new BasicInstrumentBank(format);\r\nvar synthesizer = new Synthesizer(engine, format, instrumentBank) { Name = \"Piano\" };\r\ncomposition.MidiTargets.Add(new MidiTargetNode(synthesizer));\r\n\r\n// Create a MIDI track that sends its data to the synthesizer\r\nvar midiTrack = new MidiTrack(\"Piano Melody\") { Target = composition.MidiTargets[0] };\r\ncomposition.Editor.AddMidiTrack(midiTrack);\r\n\r\n// Create and populate the MIDI sequence\r\nvar sequence = new MidiSequence(composition.TicksPerQuarterNote, [], [], [], []);\r\nlong tick = 0;\r\nvar quarterNote = composition.TicksPerQuarterNote;\r\nvar wholeNote = quarterNote * 4;\r\nint[] scaleUp = { 60, 62, 64, 65, 67, 69, 71, 72 };\r\nforeach (var noteNumber in scaleUp) { sequence.AddNote(tick, quarterNote, noteNumber, 100); tick += quarterNote; }\r\nsequence.AddNote(tick, wholeNote, 60, 110);\r\nsequence.AddNote(tick, wholeNote, 64, 110);\r\nsequence.AddNote(tick, wholeNote, 67, 110);\r\nmidiTrack.AddSegment(new MidiSegment(sequence, TimeSpan.Zero));\r\n\r\n// Create an audio track and add a segment from a file\r\nvar audioTrack = new Track(\"Sound Effect\");\r\ncomposition.Editor.AddTrack(audioTrack);\r\ncomposition.Editor.CreateAndAddSegmentFromFile(\r\n    audioTrack,\r\n    \"path/to/your/drum_loop.wav\",\r\n    TimeSpan.FromSeconds(1) // Offset it slightly to hear it clearly after the scale starts\r\n);\r\n\r\n// Initialize the default audio output device\r\nusing var device = engine.InitializePlaybackDevice(engine.PlaybackDevices.First(d => d.IsDefault), format);\r\n\r\n\r\n// The composition.Renderer is a data provider that renders the entire project in real-time.\r\nvar compositionPlayer = new SoundPlayer(engine, format, composition.Renderer);\r\ndevice.MasterMixer.AddComponent(compositionPlayer);\r\n\r\n// Start the device and play the composition\r\ndevice.Start();\r\ncompositionPlayer.Play();\r\n\r\nConsole.WriteLine($\"Playing composition '{composition.Name}', with Duration {composition.Editor.CalculateTotalDuration()}. Press any key to stop.\");\r\nConsole.ReadKey();\r\n\r\n// Clean up\r\nConsole.WriteLine(\"\\nPlayback stopped.\");\r\ncompositionPlayer.Stop();\r\ndevice.Stop();\r\ncomposition.Dispose();\r\n```\r\n\r\n## Offline Rendering\r\n\r\nOffline rendering (also called \"bouncing\" or \"exporting\") is the process of generating the final mix of your composition into an audio buffer, which you can then save to a file. This is also handled by the `composition.Renderer`.\r\n\r\nThe `composition.Renderer.Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer)` method performs the same mixing logic as live playback but does it as fast as the CPU allows, filling a buffer you provide.\r\n\r\n### Example: Rendering to a WAV File\r\n\r\nThis example shows how to render the entire composition and save it as a WAV file.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Editing;\r\nusing System;\r\nusing System.IO;\r\nusing System.Threading.Tasks;\r\n\r\npublic async Task RenderCompositionToFile(AudioEngine engine, Composition composition, string outputFilePath)\r\n{\r\n    Console.WriteLine(\"Starting offline render...\");\r\n\r\n    // Get the total duration of the composition.\r\n    var totalDuration = composition.Editor.CalculateTotalDuration();\r\n    if (totalDuration <= TimeSpan.Zero)\r\n    {\r\n        Console.WriteLine(\"Composition is empty. Nothing to render.\");\r\n        return;\r\n    }\r\n\r\n    // var renderBuffer = composition.Renderer.Render(); // This will render the entire composition into a buffer in a single operation, but below is a more fine-grained example.\r\n\r\n    // Calculate the required buffer size based on the composition's format and duration.\r\n    var format = composition.Format;\r\n    var totalSamples = (int)(totalDuration.TotalSeconds * format.SampleRate * format.Channels);\r\n    var renderBuffer = new float[totalSamples];\r\n\r\n    // Render the entire composition into the buffer in a single operation.\r\n    composition.Renderer.Render(TimeSpan.Zero, totalDuration, renderBuffer);\r\n\r\n    Console.WriteLine(\"Render complete. Now encoding to WAV...\");\r\n\r\n    // Use the engine's built-in encoder to save the raw audio buffer to a WAV file.\r\n    await using var fileStream = new FileStream(outputFilePath, FileMode.Create);\r\n    using var encoder = engine.CreateEncoder(fileStream, \"wav\", format);\r\n    \r\n    encoder.Encode(renderBuffer);\r\n\r\n    Console.WriteLine($\"Successfully saved to {outputFilePath}\");\r\n}\r\n```"
  },
  {
    "id": 4.6,
    "slug": "editing-persistence",
    "version": "1.3.0",
    "title": "Editing - Project Persistence",
    "description": "Learn how to save and load entire compositions as .sfproj files, manage media assets, and handle missing files.",
    "navOrder": 4.6,
    "category": "Editing",
    "content": "---\r\nid: 4.6\r\ntitle: Editing - Project Persistence\r\ndescription: Learn how to save and load entire compositions as .sfproj files, manage media assets, and handle missing files.\r\nnavOrder: 4.6\r\ncategory: Editing\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs, Card, CardBody, CardHeader} from \"@heroui/react\";\r\n\r\n# Project Persistence\r\n\r\nThe `CompositionProjectManager` (`SoundFlow.Editing.Persistence.CompositionProjectManager`) is a static class that provides robust methods for saving and loading your entire `Composition` to and from a JSON-based format with the `.sfproj` extension.\r\n\r\nThis system is designed for portability, handling not just the project structure but also all associated media files.\r\n\r\n## What's Saved in a `.sfproj` File?\r\n\r\nThe project file is a comprehensive snapshot of your composition, including:\r\n*   **Project Settings:** Name, master volume, target sample rate, and channel count.\r\n*   **Master Tempo Track:** All `TempoMarker`s are saved to preserve your project's tempo map.\r\n*   **Audio Tracks:** All `Track`s, their settings, and all their `AudioSegment`s (including fades, loops, time-stretch settings, etc.).\r\n*   **MIDI Tracks:** All `MidiTrack`s, their settings (including `MidiModifier`s), their `MidiSegment`s, and the underlying MIDI note/automation data.\r\n*   **Effects:** All `SoundModifier` and `AudioAnalyzer` instances at every level (segment, track, and master).\r\n*   **MIDI Mappings:** All real-time MIDI control mappings created in the `MidiMappingManager`.\r\n*   **Media References:** A detailed manifest of all external audio and MIDI sources, with information for resolving their location.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Project persistence options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"save\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:save-outline' />\r\n                <span>Saving a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `CompositionProjectManager.SaveProjectAsync` method handles saving. It now accepts a `ProjectSaveOptions` object for fine-grained control over the process.\r\n\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using System.Threading.Tasks;\r\n\r\n                public async Task SaveMyProject(Composition composition, string filePath)\r\n                {\r\n                    // Create save options. Default is to consolidate and embed.\r\n                    var saveOptions = new ProjectSaveOptions\r\n                    {\r\n                        ConsolidateMedia = true,\r\n                        EmbedSmallMedia = true,\r\n                        MaxEmbedSizeBytes = 2 * 1024 * 1024 // Optional: Increase embed limit to 2MB\r\n                    };\r\n\r\n                    await CompositionProjectManager.SaveProjectAsync(\r\n                        composition.Renderer.Engine, // The engine is required\r\n                        composition,\r\n                        filePath,\r\n                        saveOptions\r\n                    );\r\n                    Console.WriteLine($\"Project saved to {filePath}\");\r\n                }\r\n                ```\r\n\r\n                **Saving Options (`ProjectSaveOptions`):**\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-2\">\r\n                    <li>\r\n                        <strong>`ConsolidateMedia` (bool):</strong> If `true`, SoundFlow copies all unique external audio and MIDI files referenced by segments into an `Assets` subfolder next to your `.sfproj` file. This makes the project self-contained and portable.\r\n                    </li>\r\n                    <li>\r\n                        <strong>`EmbedSmallMedia` (bool):</strong> If `true`, audio sources smaller than `MaxEmbedSizeBytes` will be embedded directly into the `.sfproj` file as Base64-encoded strings. This is useful for short sound effects, avoiding the need for separate files.\r\n                    </li>\r\n                    <li><strong>`MaxEmbedSizeBytes` (long):</strong> The size threshold in bytes for embedding. Defaults to 1MB.</li>\r\n                    <li><strong>`ConsolidatedMediaFolderName` (string):</strong> The name of the folder for consolidated assets. Defaults to \"Assets\".</li>\r\n                </ul>\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"load\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:folder-open-outline' />\r\n                <span>Loading a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `CompositionProjectManager.LoadProjectAsync` method reconstructs a `Composition` from a `.sfproj` file. It returns the loaded `Composition` and a list of any media sources that could not be found.\r\n\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using SoundFlow.Structs;\r\n                using System.Threading.Tasks;\r\n                using System.Linq;\r\n                using System.Collections.Generic;\r\n\r\n                public async Task<(Composition?, List<ProjectSourceReference>)> LoadMyProject(AudioEngine engine, string filePath)\r\n                {\r\n                    if (!File.Exists(filePath))\r\n                    {\r\n                        Console.WriteLine($\"Project file not found: {filePath}\");\r\n                        return (null, new List<ProjectSourceReference>());\r\n                    }\r\n                    \r\n                    var (loadedComposition, unresolvedSources) = await CompositionProjectManager.LoadProjectAsync(engine, AudioFormat.DvdHq, filePath);\r\n\r\n                    if (unresolvedSources.Any())\r\n                    {\r\n                        Console.WriteLine(\"Warning: Some media sources could not be found:\");\r\n                        foreach (var missing in unresolvedSources)\r\n                        {\r\n                            Console.WriteLine($\" - Missing ID: {missing.Id}, Original Path: {missing.OriginalAbsolutePath ?? \"N/A\"}\");\r\n                        }\r\n                    }\r\n\r\n                    Console.WriteLine($\"Project '{loadedComposition.Name}' loaded successfully!\");\r\n                    return (loadedComposition, unresolvedSources);\r\n                }\r\n                ```\r\n                Upon loading, the system automatically rehydrates everything: audio tracks, MIDI tracks (linking them to internal synthesizers), the tempo map, and all MIDI mappings.\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"relink\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='ph:link-bold' />\r\n                <span>Media Management & Relinking</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                SoundFlow attempts to locate media in this order:\r\n                1.  **Embedded Data** in the `.sfproj` file.\r\n                2.  **Consolidated Relative Path** (e.g., `Assets/guid.wav`).\r\n                3.  **Original Absolute Path** stored during the save.\r\n\r\n                If a source is still missing, it's added to the `unresolvedSources` list returned by `LoadProjectAsync`. You can then use `CompositionProjectManager.RelinkMissingMedia` to update the project with the new location of a missing file.\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing.Persistence;\r\n                using System.Threading.Tasks;\r\n\r\n                public void AttemptRelink(AudioEngine engine, ProjectSourceReference missingSource, string newFilePath, string projectDirectory)\r\n                {\r\n                    // Note: The format parameter is no longer needed in v1.3.0\r\n                    bool success = CompositionProjectManager.RelinkMissingMedia(\r\n                        engine,\r\n                        missingSource,\r\n                        newFilePath,\r\n                        projectDirectory\r\n                    );\r\n\r\n                    if (success)\r\n                    {\r\n                        Console.WriteLine($\"Successfully relinked source. You may need to reload the project or manually update segments.\");\r\n                    }\r\n                    else\r\n                    {\r\n                        Console.WriteLine($\"Failed to relink. File at new path might be invalid.\");\r\n                    }\r\n                }\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>"
  },
  {
    "id": 4.2,
    "slug": "editing-midi-tracks-and-tempo",
    "version": "1.3.0",
    "title": "Editing - MIDI Tracks & The Master Tempo",
    "description": "A deep dive into creating and editing MIDI tracks, programmatically manipulating notes and automation, and managing the master tempo track that governs all timing.",
    "navOrder": 4.2,
    "category": "Editing",
    "content": "---\r\nid: 4.2\r\ntitle: Editing - MIDI Tracks & The Master Tempo\r\ndescription: A deep dive into creating and editing MIDI tracks, programmatically manipulating notes and automation, and managing the master tempo track that governs all timing.\r\nnavOrder: 4.2\r\ncategory: Editing\r\n---\r\n\r\n# Working with MIDI Tracks & The Master Tempo\r\n\r\nSoundFlow v1.3 elevates MIDI to a first-class citizen within the editing engine, providing a comprehensive data model and powerful tools for programmatic MIDI sequencing. This guide offers a deep dive into the MIDI track system and its critical relationship with the new master `TempoTrack`.\r\n\r\n## The Master Tempo Track\r\n\r\nAt the core of all time-based operations in a v1.3 `Composition` is the **`TempoTrack`**. This is the project's central clock and rhythmic authority.\r\n\r\n*   **Structure:** The `TempoTrack` is a `List<TempoMarker>` on the `Composition` object.\r\n*   **`TempoMarker`:** A simple but crucial record struct containing:\r\n    *   `TimeSpan Time`: The absolute timeline position where the tempo change occurs.\r\n    *   `double BeatsPerMinute`: The new tempo that takes effect at that time.\r\n*   **Global Timing Authority:** The `TempoTrack` is the **single source of truth** for all conversions between real time (`TimeSpan`) and musical time (MIDI ticks). The `Sequencer`, `MidiRecorder`, `MidiQuantizer`, and all editing methods that deal with time rely on it for sample-accurate timing calculations, especially in projects with tempo changes.\r\n\r\n### Manipulating the Tempo Track\r\n\r\nYou can programmatically define a complex tempo map for your composition using the `composition.Editor`.\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.Linq;\r\n\r\n// Assuming 'composition' is an existing Composition instance.\r\n\r\n// A new composition always starts with a default tempo of 120 BPM at time zero.\r\n// Let's change the initial tempo to 110 BPM.\r\ncomposition.Editor.SetTempo(TimeSpan.Zero, 110.0);\r\n\r\n// Add a tempo change: a ritardando (slowing down) to 95 BPM at measure 16,\r\n// assuming a 4/4 time signature and the current tempo.\r\nvar timeAtMeasure16 = CalculateTimeForMeasure(\r\n    composition.TempoTrack,\r\n    ticksPerQuarterNote: composition.TicksPerQuarterNote,\r\n    timeSignatureNumerator: 4,\r\n    timeSignatureDenominator: 4,\r\n    targetMeasure: 16\r\n);\r\ncomposition.Editor.SetTempo(timeAtMeasure16, 95.0);\r\n\r\n\r\n// Add another tempo change: an accelerando (speeding up) to 140 BPM at 1 minute.\r\ncomposition.Editor.SetTempo(TimeSpan.FromMinutes(1), 140.0);\r\n\r\n// You can also query the tempo at any point in time.\r\nvar tempoAt45Seconds = composition.Editor.GetTempoAtTime(TimeSpan.FromSeconds(45));\r\nConsole.WriteLine($\"Tempo at 45s: {tempoAt45Seconds.BeatsPerMinute} BPM\"); // Outputs: 95.0\r\n\r\n\r\n/// <summary>\r\n/// A detailed helper function to calculate the exact real-time TimeSpan for the start of a given measure.\r\n/// This demonstrates how to work with the tempo track to convert musical time to absolute time.\r\n/// NOTE: A production application would also need to handle a TimeSignatureTrack for complete accuracy.\r\n/// </summary>\r\npublic static TimeSpan CalculateTimeForMeasure(\r\n    IReadOnlyList<TempoMarker> tempoTrack,\r\n    int ticksPerQuarterNote,\r\n    int timeSignatureNumerator,\r\n    int timeSignatureDenominator,\r\n    int targetMeasure)\r\n{\r\n    if (targetMeasure <= 1)\r\n    {\r\n        return TimeSpan.Zero;\r\n    }\r\n\r\n    double totalSeconds = 0;\r\n    long totalTicks = 0;\r\n    \r\n    // Calculate the number of ticks per measure for the given time signature.\r\n    var ticksPerBeat = ticksPerQuarterNote * (4.0 / timeSignatureDenominator);\r\n    var ticksPerMeasure = (long)(ticksPerBeat * timeSignatureNumerator);\r\n\r\n    var targetTicks = ticksPerMeasure * (targetMeasure - 1);\r\n\r\n    long lastTick = 0;\r\n    double currentBpm = tempoTrack.First().BeatsPerMinute;\r\n\r\n    // Iterate through the tempo map to calculate the elapsed time segment by segment.\r\n    foreach (var marker in tempoTrack)\r\n    {\r\n        // Get the tick position of the current tempo marker. This is a recursive-like problem,\r\n        // so we calculate the time of the marker itself to find its tick position.\r\n        var markerTick = MidiTimeConverter.GetTickForTimeSpan(marker.Time, ticksPerQuarterNote, tempoTrack);\r\n\r\n        if (targetTicks <= markerTick)\r\n        {\r\n            // The target measure is within the current tempo segment.\r\n            var ticksInThisSegment = targetTicks - lastTick;\r\n            var secondsPerTick = 60.0 / (currentBpm * ticksPerQuarterNote);\r\n            totalSeconds += ticksInThisSegment * secondsPerTick;\r\n            return TimeSpan.FromSeconds(totalSeconds);\r\n        }\r\n\r\n        // The target measure is after this tempo segment, so add the full duration of this segment.\r\n        var segmentDurationTicks = markerTick - lastTick;\r\n        totalSeconds += (60.0 / (currentBpm * ticksPerQuarterNote)) * segmentDurationTicks;\r\n\r\n        // Update state for the next segment.\r\n        lastTick = markerTick;\r\n        currentBpm = marker.BeatsPerMinute;\r\n    }\r\n\r\n    // If the target is after the last tempo marker, calculate the remaining time.\r\n    var remainingTicks = targetTicks - lastTick;\r\n    totalSeconds += (60.0 / (currentBpm * ticksPerQuarterNote)) * remainingTicks;\r\n\r\n    return TimeSpan.FromSeconds(totalSeconds);\r\n}\r\n```\r\n\r\n## The MIDI Data Hierarchy\r\n\r\nTo provide a powerful and non-destructive editing experience, MIDI data is organized into a clear hierarchy.\r\n\r\n1.  **`MidiTrack`:** The top-level timeline container. It holds `MidiSegment`s and routes their MIDI output.\r\n2.  **`MidiSegment`:** A \"clip\" of MIDI data on the track's timeline. It contains a single `MidiSequence`.\r\n3.  **`MidiSequence`:** A mutable, in-memory container for all the MIDI events within a segment. It synchronizes a user-friendly object model (`MidiNote`, `ControlPoint`) with the low-level event stream needed for playback.\r\n4.  **`MidiNote` & `ControlPoint`:** These are the high-level, editable objects you programmatically interact with. They represent the actual musical and automation data.\r\n\r\n### `MidiTrack`\r\n\r\nThe `MidiTrack` (`SoundFlow.Editing.MidiTrack`) is the main container for MIDI data on the timeline.\r\n\r\n*   **`Target` Property:** This is the track's most important property. It defines the `IMidiDestinationNode` where all its MIDI events will be sent. The target can be:\r\n    *   An internal `Synthesizer` component (wrapped in a `MidiTargetNode`).\r\n    *   A physical MIDI output port (wrapped in a `MidiOutputNode`).\r\n*   **`TrackSettings`:** A `MidiTrack` uses the `TrackSettings` class for mute, solo, and enable states. It also has a `MidiModifiers` list, allowing you to chain real-time MIDI effects like arpeggiators or transposer plugins.\r\n\r\n### `MidiSegment` and `MidiSequence`\r\n\r\n*   A `MidiSegment` is a container that places a block of MIDI data onto the composition's timeline. It has a `TimelineStartTime` and a `Name`.\r\n*   Its core content is the `MidiSequence`. The `MidiSequence` is a powerful data structure that holds all the notes and automation for that segment. When you edit MIDI, you are modifying the `MidiSequence`.\r\n\r\n### `MidiNote` and `ControlPoint`\r\n\r\nThese are the fundamental, editable data objects.\r\n\r\n*   **`MidiNote`:** Represents a single musical note.\r\n    *   `Id` (Guid): A unique ID for reliably targeting the note in editing operations.\r\n    *   `StartTick` (long): The note's start time in absolute MIDI ticks from the beginning of its parent `MidiSequence`.\r\n    *   `DurationTicks` (long): The note's duration in MIDI ticks.\r\n    *   `NoteNumber` (int): The MIDI pitch (0-127).\r\n    *   `Velocity` (int): The note-on velocity (1-127).\r\n*   **`ControlPoint`:** Represents a single automation point.\r\n    *   `Id` (Guid): A unique ID.\r\n    *   `Tick` (long): The absolute time of the event in MIDI ticks.\r\n    *   `Value` (int): The event's value (0-127 for CC, or 0-16383 for 14-bit Pitch Bend).\r\n\r\n## Example: Creating a MIDI Track with a Synthesizer\r\n\r\nThis complete, detailed example shows the entire process of setting up an internal synthesizer, creating a MIDI track to control it, and programmatically writing a musical phrase with notes and automation.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Enums;\r\nusing SoundFlow.Midi.Routing.Nodes;\r\nusing SoundFlow.Structs;\r\nusing SoundFlow.Synthesis;\r\nusing SoundFlow.Synthesis.Banks;\r\nusing MidiTrack = SoundFlow.Editing.MidiTrack;\r\n\r\n// The Composition is the top-level container for all tracks, segments, and timeline information.\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\nvar composition = new Composition(engine, format, \"Programmatic MIDI Demo\");\r\n\r\n// Set the project's time division; 480 is a common standard offering high editing resolution.\r\ncomposition.Editor.SetTicksPerQuarterNote(480);\r\n\r\n// Create a bank of simple, built-in synthesizer sounds.\r\nvar instrumentBank = new BasicInstrumentBank(format);\r\n// The Synthesizer component is the sound source that will generate audio from MIDI events.\r\nvar synthesizer = new Synthesizer(engine, format, instrumentBank)\r\n{\r\n    Name = \"Lead Synth\" // Give it a unique name for targeting\r\n};\r\n\r\n// Register the synthesizer as a named MIDI target within the composition for routing.\r\ncomposition.MidiTargets.Add(new MidiTargetNode(synthesizer));\r\n\r\n// A MidiTrack holds MIDI data and routes it to a specific target.\r\nvar midiTrack = new MidiTrack(\"Synth Melody\")\r\n{\r\n    // Route this track's output to the previously registered \"Lead Synth\" target.\r\n    Target = composition.MidiTargets.FirstOrDefault(t => t.Name == synthesizer.Name)\r\n};\r\n\r\n// Add the configured track to the project.\r\ncomposition.Editor.AddMidiTrack(midiTrack);\r\n\r\n// A MidiSequence contains the raw note and automation data, independent of its position on the timeline.\r\nvar sequence = new MidiSequence(composition.TicksPerQuarterNote,\r\n    [],\r\n    [],\r\n    [],\r\n    []);\r\n\r\n// A MidiSegment places a sequence onto a track at a specific start time.\r\nvar midiSegment = new MidiSegment(sequence, TimeSpan.Zero, \"Main Melody\");\r\n\r\nmidiTrack.AddSegment(midiSegment);\r\n\r\n// Use the Editor service to programmatically add MIDI events to the segment's sequence.\r\nlong ticksPerQuarter = composition.TicksPerQuarterNote;\r\nvar ticksPerEighth = ticksPerQuarter / 2;\r\n\r\n// Add notes to the segment's sequence using absolute tick positions.\r\ncomposition.Editor.AddNoteToSegment(midiSegment, 0 * ticksPerEighth, ticksPerEighth, 60, 100);\r\ncomposition.Editor.AddNoteToSegment(midiSegment, 1 * ticksPerEighth, ticksPerEighth, 63, 90);\r\ncomposition.Editor.AddNoteToSegment(midiSegment, 2 * ticksPerEighth, ticksPerEighth, 67, 110);\r\ncomposition.Editor.AddNoteToSegment(midiSegment, 3 * ticksPerEighth, ticksPerEighth, 72, 100);\r\n\r\nlong lastNoteStart = 3 * ticksPerEighth;\r\nlong lastNoteEnd = lastNoteStart + ticksPerEighth;\r\n\r\n// Add pitch bend automation; controller number -1 is reserved for pitch bend (range 0-16383, center 8192).\r\ncomposition.Editor.AddControlPointToSegment(midiSegment, -1, lastNoteStart, 8192);\r\ncomposition.Editor.AddControlPointToSegment(midiSegment, -1, lastNoteEnd, 10240);\r\n\r\n// Add automation for a standard MIDI CC controller (e.g., CC 74 for filter cutoff/brightness).\r\ncomposition.Editor.AddControlPointToSegment(midiSegment, 74, 0, 40);\r\ncomposition.Editor.AddControlPointToSegment(midiSegment, 74, ticksPerQuarter, 127);\r\ncomposition.Editor.AddControlPointToSegment(midiSegment, 74, lastNoteEnd, 60);\r\n\r\n\r\n// The full playback setup is covered in the \"Playback and Rendering\" guide. In summary:\r\n\r\n// Initialize the physical audio output device.\r\nusing var device = engine.InitializePlaybackDevice(null, format);\r\n\r\n// Add the audio-generating synthesizer to the master mixer to be heard.\r\ndevice.MasterMixer.AddComponent(synthesizer);\r\n\r\n// The composition's Renderer drives playback timing and dispatches MIDI events.\r\nvar player = new SoundPlayer(engine, format, composition.Renderer);\r\n// Add the composition renderer to the mixer to process the timeline.\r\ndevice.MasterMixer.AddComponent(player);\r\n\r\ndevice.Start();\r\nplayer.Play();\r\n\r\nConsole.WriteLine($\"Playing composition '{composition.Name}', with Duration {composition.Editor.CalculateTotalDuration()}. Press any key to stop.\");\r\nConsole.ReadKey();\r\n\r\n// Clean up\r\nConsole.WriteLine(\"\\nPlayback stopped.\");\r\nplayer.Stop();\r\ndevice.Stop();\r\ncomposition.Dispose();\r\n```"
  },
  {
    "id": 4,
    "slug": "editing-introduction",
    "version": "1.3.0",
    "title": "Editing Engine - Introduction",
    "description": "Get a high-level overview of the SoundFlow editing engine, its service-oriented architecture, and the core Composition model for audio and MIDI projects.",
    "navOrder": 4,
    "category": "Editing",
    "content": "﻿---\r\nid: 4\r\ntitle: Editing Engine - Introduction\r\ndescription: Get a high-level overview of the SoundFlow editing engine, its service-oriented architecture, and the core Composition model for audio and MIDI projects.\r\nnavOrder: 4\r\ncategory: Editing\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Card, CardBody, CardHeader} from \"@heroui/react\";\r\n\r\n# Editing Engine: Introduction & Composition Model\r\n\r\nSoundFlow features a comprehensive, non-destructive editing engine that can be thought of as a **\"DAW in a box\"** for developers. It provides a powerful, programmatic core for building applications that need to create, manipulate, and save complex multi-track timelines with both audio and MIDI.\r\n\r\n### Key Features of the Editing Engine\r\n\r\n*   **Multi-track Audio and MIDI:** Create and manage distinct timelines for both audio clips and MIDI data.\r\n*   **Non-Destructive Editing:** All operations—such as trimming, splitting, looping, and applying fades—are performed at runtime and never alter the original source files.\r\n*   **Hierarchical Effects Processing:** Apply effects (`SoundModifier`) and analyzers (`AudioAnalyzer`) at any level: on an individual clip, an entire track, or the master output.\r\n*   **Advanced Time Manipulation:** Includes high-quality, pitch-preserved time stretching (WSOLA) and classic varispeed (pitch and tempo linked).\r\n*   **Master Tempo Track:** A global timeline for defining tempo changes, which governs all MIDI and time-based calculations for perfect synchronization.\r\n*   **Robust Project Persistence:** Save and load entire projects as self-contained `.sfproj` files with sophisticated media management, including consolidation and embedding.\r\n*   **Real-time MIDI Control:** Link any parameter in your project to a physical MIDI controller for hands-on control via the MIDI Mapping system.\r\n\r\n### The New Architecture: A Service-Oriented `Composition`\r\n\r\nIn version 1.3.0, the `Composition` class was refactored to be the central **data model** of your project. The actual operations—editing, rendering, and recording—are now handled by dedicated service classes, making the architecture cleaner, more powerful, and easier to extend.\r\n\r\nWhen you create a `Composition`, you are creating the heart of your project, which then provides access to these specialized services.\r\n\r\n<Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 my-6\">\r\n    <CardHeader>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n            <h4 className=\"font-semibold text-lg\">`AudioEngine` is Now Required</h4>\r\n        </div>\r\n    </CardHeader>\r\n    <CardBody className=\"pt-0 text-sm\">\r\n        The `Composition` constructor now requires an `AudioEngine` instance. This is because its internal services (like the `Editor` for loading files or the `Recorder` for interacting with MIDI devices) need the engine context to function.\r\n    </CardBody>\r\n</Card>\r\n\r\n### The Core Services\r\n\r\nEvery `Composition` instance exposes four primary service properties. These are your main entry points for interacting with the project.\r\n\r\n1.  **`composition.Editor` (`CompositionEditor`)**\r\n    *   **Role:** The primary service for all **structural changes** to the composition.\r\n    *   **Use For:** Adding or removing tracks and segments, manipulating the timeline, cutting/splitting/joining clips, editing MIDI notes, and managing the tempo track.\r\n\r\n2.  **`composition.Renderer` (`CompositionRenderer`)**\r\n    *   **Role:** The service responsible for all **audio output**.\r\n    *   **Use For:**\r\n        *   **Live Playback:** The `Renderer` is an `ISoundDataProvider` itself, so you pass it to a `SoundPlayer` to hear your entire composition.\r\n        *   **Offline Rendering:** Use its `Render()` method to \"bounce\" your project mix to an audio buffer, which you can then save to a file.\r\n\r\n3.  **`composition.Recorder` (`CompositionRecorder`)**\r\n    *   **Role:** The service for managing the entire **MIDI recording workflow**.\r\n    *   **Use For:** Arming MIDI tracks for recording, starting and stopping the recording process, and configuring advanced features like punch-in/out and overdubbing.\r\n\r\n4.  **`composition.MappingManager` (`MidiMappingManager`)**\r\n    *   **Role:** The service that manages all **real-time MIDI control mappings**.\r\n    *   **Use For:** Creating, removing, and managing links between your physical MIDI controllers and any controllable parameter within the composition. (See the **[MIDI Control & Mapping](/docs/midi/midi-control-and-mapping)** guide for details).\r\n\r\n### Example: Creating a Composition\r\n\r\nThis example demonstrates the new way to instantiate a `Composition` and access its core services.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Structs;\r\n\r\n// 1. An AudioEngine instance is now required to create a composition.\r\n// The engine provides the context for services like file loading and MIDI device access.\r\nusing var engine = new MiniAudioEngine();\r\n\r\n// 2. Define the target audio format for the project.\r\nvar format = AudioFormat.DvdHq;\r\n\r\n// 3. Create the Composition.\r\n// It acts as the central data model for your project.\r\nvar composition = new Composition(engine, format, \"My New Project\");\r\n\r\n// 4. Access the dedicated services to interact with the composition.\r\n\r\n// Use the Editor to make structural changes.\r\nConsole.WriteLine($\"Accessing the editor: {composition.Editor.GetType().Name}\");\r\n// e.g., composition.Editor.AddTrack(new Track(\"My Audio Track\"));\r\n\r\n// Use the Renderer for playback.\r\nConsole.WriteLine($\"Accessing the renderer: {composition.Renderer.GetType().Name}\");\r\n// e.g., var player = new SoundPlayer(engine, format, composition.Renderer);\r\n\r\n// Use the Recorder to manage MIDI recording.\r\nConsole.WriteLine($\"Accessing the recorder: {composition.Recorder.GetType().Name}\");\r\n// e.g., composition.Recorder.ArmTrackForRecording(myMidiTrack, myMidiInputDevice);\r\n\r\n// Use the MappingManager to set up real-time control.\r\nConsole.WriteLine($\"Accessing the mapping manager: {composition.MappingManager.GetType().Name}\");\r\n// e.g., composition.MappingManager.AddMapping(myMidiMapping);\r\n\r\n// ... build your project using these services ...\r\n\r\n// Remember to dispose the composition to release its resources.\r\ncomposition.Dispose();\r\n```\r\n\r\nWith this foundational understanding, you can now explore the specific guides for working with audio tracks, MIDI tracks, playback, and persistence.\r\n"
  },
  {
    "id": 4.1,
    "slug": "editing-audio-tracks",
    "version": "1.3.0",
    "title": "Editing - Audio Tracks & Segments",
    "description": "Learn how to create and manipulate audio tracks and segments, and apply settings like fades, loops, time stretching, and effects.",
    "navOrder": 4.1,
    "category": "Editing",
    "content": "---\r\nid: 4.1\r\ntitle: Editing - Audio Tracks & Segments\r\ndescription: Learn how to create and manipulate audio tracks and segments, and apply settings like fades, loops, time stretching, and effects.\r\nnavOrder: 4.1\r\ncategory: Editing\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs, Card, CardBody, CardHeader} from \"@heroui/react\";\r\n\r\n# Working with Audio Tracks & Segments\r\n\r\nThis guide covers the core objects for building an audio timeline in SoundFlow: the `Track` and the `AudioSegment`. All modifications are performed through the `composition.Editor` service.\r\n\r\n## The `Track` Class\r\n\r\nA `Track` (`SoundFlow.Editing.Track`) represents a single audio timeline within a `Composition`, analogous to a track in a DAW.\r\n\r\n*   **Holds Segments:** A `Track` contains a list of `AudioSegment` objects, which are the audio clips placed on its timeline.\r\n*   **Track-Level Settings (`TrackSettings`):** Each track has its own settings object that controls its overall behavior:\r\n    *   **Basic Controls:** `Volume`, `Pan`, `IsMuted`, `IsSoloed`, `IsEnabled`.\r\n    *   **Effects Chain:** A track can have its own chain of `SoundModifier` and `AudioAnalyzer` instances that process the mixed output of all its segments.\r\n\r\n### Creating and Adding a Track\r\n\r\nYou create a `Track` instance and then add it to the composition using the `Editor` service.\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\n\r\n// Assuming 'composition' is an existing Composition instance.\r\nvar track1 = new Track(\"Lead Vocals\");\r\n\r\n// Configure track settings\r\ntrack1.Settings.Volume = 0.8f;\r\ntrack1.Settings.Pan = 0.45f; // Slightly to the left\r\n\r\n// Add a track-level effect, like a compressor\r\n// var trackCompressor = new CompressorModifier(composition.Format, -18f, 4f, 10f, 150f);\r\n// track1.Settings.AddModifier(trackCompressor);\r\n\r\n// Add the track to the composition\r\ncomposition.Editor.AddTrack(track1);\r\n```\r\n\r\n## The `AudioSegment` Class\r\n\r\nThe `AudioSegment` (`SoundFlow.Editing.AudioSegment`) is the fundamental building block for audio content. It represents a specific portion of an audio source placed at a particular time on the track's timeline.\r\n\r\n*   **Source Reference:** Points to an `ISoundDataProvider` for its audio data.\r\n*   **Timeline Placement:**\r\n    *   `SourceStartTime`: The time offset within the `ISoundDataProvider` from which this segment begins.\r\n    *   `SourceDuration`: The duration of audio to use from the `ISoundDataProvider`.\r\n    *   `TimelineStartTime`: The time at which this segment starts on the parent `Track`'s timeline.\r\n*   **Segment-Level Settings (`AudioSegmentSettings`):** Each segment has its own granular settings for ultimate control.\r\n*   **Non-Destructive:** All operations (trimming, fades, stretching) are applied at runtime and do not alter the original audio source.\r\n\r\n### Creating and Adding an Audio Segment\r\n\r\nThe recommended way to create a segment from a file is using the `composition.Editor.CreateAndAddSegmentFromFile` helper method.\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\nusing SoundFlow.Structs;\r\nusing System;\r\nusing System.IO;\r\nusing System.Threading.Tasks;\r\n\r\npublic async Task BuildAudioTrack(Composition composition, Track track)\r\n{\r\n    // This helper method reads the file, creates the provider, creates the segment,\r\n    // and adds it to the track in one step.\r\n    var vocalSegment = composition.Editor.CreateAndAddSegmentFromFile(\r\n        track,\r\n        \"path/to/vocals.wav\",\r\n        TimeSpan.FromSeconds(2) // Place the segment at 2 seconds on the timeline\r\n    );\r\n\r\n    // Now you can configure the segment's settings\r\n    vocalSegment.Settings.Volume = 0.95f;\r\n\r\n    // Apply a 200ms S-Curve fade-in\r\n    vocalSegment.Settings.FadeInDuration = TimeSpan.FromMilliseconds(200);\r\n    vocalSegment.Settings.FadeInCurve = FadeCurveType.SCurve;\r\n\r\n    // Apply a segment-level effect\r\n    // var segmentReverb = new AlgorithmicReverbModifier(composition.Format);\r\n    // vocalSegment.Settings.AddModifier(segmentReverb);\r\n}\r\n```\r\n\r\n## Advanced `AudioSegmentSettings`\r\n\r\nThe `AudioSegmentSettings` class provides powerful, non-destructive control over how a segment is played back.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Advanced Segment Settings\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"time-stretch\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:timelapse-outline' />\r\n                <span>Time Stretching (Pitch Preserved)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                This feature changes the duration of an audio segment **without affecting its pitch**. It's ideal for fitting audio to a specific time slot or for creative sound design. It is powered by a high-quality **WSOLA (Waveform Similarity Overlap-Add)** algorithm.\r\n\r\n                You can control stretching in two ways:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-2\">\r\n                    <li>\r\n                        <strong>`TimeStretchFactor` (float):</strong> A multiplier for the duration.\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li>`1.0`: No change.</li>\r\n                            <li>`2.0`: Doubles the duration (plays at half speed).</li>\r\n                            <li>`0.5`: Halves the duration (plays at double speed).</li>\r\n                        </ul>\r\n                    </li>\r\n                    <li>\r\n                        <strong>`TargetStretchDuration` (TimeSpan?):</strong> Overrides `TimeStretchFactor`. The segment will be stretched or compressed to match this exact duration. Set to `null` to disable.\r\n                    </li>\r\n                </ul>\r\n                ```csharp\r\n                // Make a segment 25% longer without changing its pitch.\r\n                mySegment.Settings.TimeStretchFactor = 1.25f;\r\n\r\n                // Or, stretch a segment to be exactly 5.5 seconds long.\r\n                mySegment.Settings.TargetStretchDuration = TimeSpan.FromSeconds(5.5);\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"varispeed\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:speedometer' />\r\n                <span>Speed Control (Varispeed)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `SpeedFactor` property provides traditional speed control, affecting both **tempo and pitch** simultaneously, like changing the speed of a tape machine.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>`1.0`: Normal speed and pitch.</li>\r\n                    <li>`2.0`: Double speed, pitch shifted up one octave.</li>\r\n                    <li>`0.5`: Half speed, pitch shifted down one octave.</li>\r\n                </ul>\r\n                <Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 my-4\">\r\n                    <CardBody>\r\n                        <div className=\"flex items-center gap-3\">\r\n                            <Icon icon=\"lucide:layers\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n                            <p className=\"text-sm\">\r\n                                <strong>Order of Operations:</strong> Time stretching is applied to the source audio <em>first</em>, and then `SpeedFactor` is applied to the time-stretched result.\r\n                            </p>\r\n                        </div>\r\n                    </CardBody>\r\n                </Card>\r\n                ```csharp\r\n                // Play a segment at half speed (and an octave lower).\r\n                mySegment.Settings.SpeedFactor = 0.5f;\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"looping\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:repeat' />\r\n                <span>Looping</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `Loop` property, an instance of `LoopSettings`, controls segment repetition.\r\n\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-2\">\r\n                    <li>\r\n                        <strong>`IsEnabled` (bool):</strong> Toggles looping on or off.\r\n                    </li>\r\n                    <li>\r\n                        <strong>`Repetitions` (int?):</strong> If set, the segment will repeat a specific number of times. The total duration on the timeline will be `EffectiveDurationOnTimeline * (Repetitions + 1)`.\r\n                    </li>\r\n                    <li>\r\n                        <strong>`FillDuration` (TimeSpan?):</strong> If set, this overrides `Repetitions`. The segment will loop as many times as needed (including partial loops) to fill the specified duration on the timeline.\r\n                    </li>\r\n                </ul>\r\n\r\n                ```csharp\r\n                // Loop a segment 3 times (will play a total of 4 times).\r\n                mySegment.Settings.Loop = new LoopSettings { IsEnabled = true, Repetitions = 3 };\r\n\r\n                // Loop a 2-second segment to fill a 10-second space on the timeline.\r\n                // It will play 5 times.\r\n                mySegment.Settings.Loop = new LoopSettings { IsEnabled = true, FillDuration = TimeSpan.FromSeconds(10) };\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n"
  },
  {
    "id": 4.5,
    "slug": "editing-advanced-editing-tools",
    "version": "1.3.0",
    "title": "Editing - Advanced Editing Tools",
    "description": "Discover advanced editing operations for both audio and MIDI, including quantization, structural editing, and file export.",
    "navOrder": 4.5,
    "category": "Editing",
    "content": "---\r\nid: 4.5\r\ntitle: Editing - Advanced Editing Tools\r\ndescription: Discover advanced editing operations for both audio and MIDI, including quantization, structural editing, and file export.\r\nnavOrder: 4.5\r\ncategory: Editing\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\n\r\n# Advanced Editing Tools\r\n\r\nThe `composition.Editor` provides a suite of powerful methods for complex, non-destructive editing of both audio and MIDI data. This guide covers utilities for quantization, structural timeline manipulation, and exporting MIDI data.\r\n\r\n## Structural Audio Editing\r\n\r\nThese operations allow you to modify the arrangement of `AudioSegment`s on a track's timeline.\r\n\r\n*   **`Editor.InsertSegment(Track track, TimeSpan insertionPoint, AudioSegment segment, bool shiftFollowing = true)`**\r\n    Inserts a segment at a specific time. If `shiftFollowing` is true, all subsequent segments on the track are pushed forward to make room.\r\n\r\n*   **`Editor.RemoveSegment(Track track, AudioSegment segment, bool shiftFollowing = true)`**\r\n    Removes a segment. If `shiftFollowing` is true, all subsequent segments are pulled back to close the gap.\r\n\r\n*   **`Editor.SilenceSegment(Track track, TimeSpan startTime, TimeSpan duration)`**\r\n    A powerful non-destructive operation that silences a region of a track. It intelligently:\r\n    *   Removes any segments fully contained within the silenced range.\r\n    *   Trims the end of any segment that overlaps the start of the range.\r\n    *   Trims the start of any segment that overlaps the end of the range.\r\n    *   Splits any segment that fully spans the silenced range into two pieces.\r\n    *   Inserts a new, silent `AudioSegment` to fill the gap, ensuring timeline integrity.\r\n\r\n## MIDI Quantization\r\n\r\nQuantization is the process of aligning MIDI notes to a specific rhythmic grid. SoundFlow provides a flexible, non-destructive quantization utility.\r\n\r\n*   **`MidiQuantizer`:** A static utility class that contains the core quantization logic.\r\n*   **`QuantizationSettings`:** A record that defines the parameters for the operation:\r\n    *   `Grid`: The rhythmic grid to align to (e.g., `SixteenthNote`, `EighthTriplet`).\r\n    *   `Strength`: A value from `0.0` to `1.0` that controls how strongly notes are pulled to the grid. `1.0` is perfect alignment.\r\n    *   `Swing`: A value from `0.0` to `1.0` that shifts every second beat on the grid forward, creating a \"swing\" feel. `0.5` is no swing.\r\n    *   `QuantizeNoteEnd`: A boolean to control whether the end of the note is also aligned to the grid.\r\n\r\n### Example: Applying Quantization\r\n\r\nYou apply quantization to a `MidiSegment` using the `composition.Editor`.\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\n\r\n// Assuming 'composition' and 'myMidiSegment' exist.\r\n\r\n// Define quantization settings:\r\n// - Align to the 16th note grid.\r\n// - Apply 85% of the correction (not perfectly rigid).\r\n// - Add a light swing feel.\r\nvar settings = new QuantizationSettings\r\n{\r\n    Grid = QuantizationSettings.GridInterval.SixteenthNote,\r\n    Strength = 0.85,\r\n    Swing = 0.55\r\n};\r\n\r\n// Apply the quantization. This modifies the note data within the segment's MidiSequence.\r\ncomposition.Editor.QuantizeSegment(myMidiSegment, settings);\r\n```\r\n\r\n## Structural MIDI Editing\r\n\r\nThe editor also provides methods for splitting and joining MIDI segments.\r\n\r\n*   **`Editor.SplitMidiSegment(MidiTrack track, MidiSegment segment, TimeSpan splitTime)`**\r\n    Splits a single `MidiSegment` into two new segments at the specified timeline position. Any notes that cross the split point are also intelligently split. The original segment is removed.\r\n\r\n*   **`Editor.JoinMidiSegments(MidiTrack track, IEnumerable<MidiSegment> segments)`**\r\n    Merges multiple `MidiSegment`s on the same track into a single new `MidiSegment`. The segments are joined based on their timeline positions, and their MIDI data is merged into a single `MidiSequence`. The original segments are removed.\r\n\r\n## MIDI File Export\r\n\r\nYou can export all MIDI data from your composition (including all tracks and the master tempo map) into a standard `.mid` file (Format 1). This is useful for interoperability with other DAWs or MIDI software.\r\n\r\nThe `composition.Editor.ExportMidiAsync()` method handles the entire process.\r\n\r\n```csharp\r\nusing SoundFlow.Editing;\r\nusing System.Threading.Tasks;\r\n\r\npublic async Task ExportMidi(Composition composition, string outputFilePath)\r\n{\r\n    // This will create a Format 1 Standard MIDI File containing all MIDI tracks\r\n    // and a conductor track with all the tempo changes from the master TempoTrack.\r\n    await composition.Editor.ExportMidiAsync(outputFilePath);\r\n\r\n    Console.WriteLine($\"MIDI data successfully exported to {outputFilePath}\");\r\n}\r\n```"
  },
  {
    "id": 3,
    "slug": "device-management",
    "version": "1.3.0",
    "title": "Device Management",
    "description": "Learn how to manage audio and MIDI devices in SoundFlow, including initialization, switching, and advanced configuration.",
    "navOrder": 3,
    "category": "Core",
    "content": "---\r\nid: 3\r\ntitle: Device Management\r\ndescription: Learn how to manage audio and MIDI devices in SoundFlow, including initialization, switching, and advanced configuration.\r\nnavOrder: 3\r\ncategory: Core\r\n---\r\n\r\nimport { Card, CardBody, Tabs, Tab, Chip } from \"@heroui/react\";\r\nimport { Icon } from \"@iconify/react\";\r\n\r\n# Device Management\r\n\r\nSoundFlow v1.3 introduced a powerful, device-centric architecture for both audio and MIDI. This guide covers how to discover, initialize, and manage devices for playback, capture, MIDI I/O, and more complex scenarios.\r\n\r\n## An Overview\r\n\r\nThe architecture separates the **`AudioEngine`** from the **`AudioDevice`**.\r\n\r\n*   **`AudioEngine`**: Acts as a central **context** or **factory**. It is responsible for interacting with low-level audio and MIDI backends (e.g., MiniAudio, PortMidi), discovering available hardware, and creating device instances.\r\n*   **`AudioDevice`**: Represents an active audio I/O stream to a specific piece of hardware. Each device has its own lifecycle (`Start`, `Stop`, `Dispose`), its own audio format (`AudioFormat`), and, in the case of playback devices, its own independent audio graph (`MasterMixer`).\r\n\r\nThis model allows for robust, multi-device applications where you can, for example, play audio to a speaker and a headphone jack simultaneously, each with different effects, while also interacting with multiple MIDI keyboards and synthesizers.\r\n\r\n## Listing Available Devices\r\n\r\nBefore initializing a device, you need to know what hardware is available. The `AudioEngine` provides this information for both audio and MIDI.\r\n\r\n<Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 mt-4\">\r\n    <CardBody>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n            <p className=\"text-sm\">\r\n                As of v1.3.0, device enumeration is split. You must call `engine.UpdateAudioDevicesInfo()` for audio hardware and `engine.UpdateMidiDevicesInfo()` for MIDI hardware. The old `UpdateDevicesInfo()` is now obsolete.\r\n            </p>\r\n        </div>\r\n    </CardBody>\r\n</Card>\r\n\r\n### Listing Audio Devices\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\n\r\n// 1. Initialize the engine context.\r\nusing var engine = new MiniAudioEngine();\r\n\r\n// 2. Refresh the list of available audio devices.\r\nengine.UpdateAudioDevicesInfo();\r\n\r\n// 3. List available playback devices.\r\nConsole.WriteLine(\"--- Audio Playback Devices ---\");\r\nif (engine.PlaybackDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No playback devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.PlaybackDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\r\n    }\r\n}\r\n\r\n\r\n// 4. List available capture devices.\r\nConsole.WriteLine(\"\\n--- Audio Capture Devices ---\");\r\nif (engine.CaptureDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No capture devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.CaptureDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\r\n    }\r\n}\r\n```\r\n\r\n### Listing MIDI Devices\r\n\r\nTo list MIDI devices, you must first enable a MIDI backend. The `SoundFlow.Midi.PortMidi` package is the recommended cross-platform solution.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Midi.PortMidi; // Import the PortMidi extension\r\n\r\n// 1. Initialize the engine context.\r\nusing var engine = new MiniAudioEngine();\r\n\r\n// 2. Enable the PortMidi backend. This is required for MIDI functionality.\r\nengine.UsePortMidi();\r\n\r\n// 3. Refresh the list of available MIDI devices.\r\nengine.UpdateMidiDevicesInfo();\r\n\r\n// 4. List available MIDI input devices.\r\nConsole.WriteLine(\"--- MIDI Input Devices ---\");\r\nif (engine.MidiInputDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No MIDI input devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.MidiInputDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name}\");\r\n    }\r\n}\r\n\r\n\r\n// 5. List available MIDI output devices.\r\nConsole.WriteLine(\"\\n--- MIDI Output Devices ---\");\r\nif (engine.MidiOutputDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No MIDI output devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.MidiOutputDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name}\");\r\n    }\r\n}\r\n```\r\n\r\n## Initializing an Audio Device\r\n\r\nOnce you have a `DeviceInfo` struct (or if you want to use the default device), you can ask the engine to initialize it.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Device initialization options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"playback\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='lucide:audio-lines' />\r\n                <span>Playback Device</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                An `AudioPlaybackDevice` is used for sending audio out to speakers or headphones.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>It has its own `MasterMixer`, which is the root of its audio graph.</li>\r\n                    <li>It must be started with `Start()` to begin processing audio.</li>\r\n                </ul>\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Structs;\r\n\r\n                using var engine = new MiniAudioEngine();\r\n\r\n                // Define the audio format for this device.\r\n                var format = AudioFormat.DvdHq; // 48kHz, 2-channel, 32-bit float\r\n\r\n                // To use the default device, pass `null` or `engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault)`\r\n                using var playbackDevice = engine.InitializePlaybackDevice(null, format);\r\n                Console.WriteLine($\"Initialized playback on: {playbackDevice.Info?.Name}\");\r\n\r\n                // Create an oscillator component with the same format.\r\n                var oscillator = new Oscillator(engine, format) { Frequency = 440f, Volume = 0.5f };\r\n\r\n                // Add the oscillator to the device's MasterMixer.\r\n                playbackDevice.MasterMixer.AddComponent(oscillator);\r\n\r\n                // Start the device's audio stream.\r\n                playbackDevice.Start();\r\n\r\n                Console.WriteLine(\"Playing a 440 Hz tone. Press any key to stop.\");\r\n                Console.ReadKey();\r\n\r\n                // Stop the device.\r\n                playbackDevice.Stop();\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"capture\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='lucide:mic' />\r\n                <span>Capture Device</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                An `AudioCaptureDevice` is used for receiving audio from a microphone or other input.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>Its primary interaction point is the `OnAudioProcessed` event.</li>\r\n                    <li>It must be started with `Start()` to begin capturing audio.</li>\r\n                </ul>\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n\r\n                using var engine = new MiniAudioEngine();\r\n                var format = new AudioFormat { SampleRate = 48000, Channels = 1, Layout = ChannelLayout.Mono, Format = SampleFormat.F32 };\r\n\r\n                // To use the default device, pass `null` or `engine.CaptureDevices.FirstOrDefault(d => d.IsDefault)`\r\n                using var captureDevice = engine.InitializeCaptureDevice(null, format);\r\n                Console.WriteLine($\"Initialized capture on: {captureDevice.Info?.Name}\");\r\n\r\n                // Subscribe to the audio data event.\r\n                captureDevice.OnAudioProcessed += (samples, capability) =>\r\n                {\r\n                    // This code block will execute on the audio thread every time a buffer is ready.\r\n                    // Be efficient here to avoid audio dropouts.\r\n                    Console.Write(\".\"); // Print a dot to show that data is flowing.\r\n                };\r\n\r\n                // Start capturing.\r\n                captureDevice.Start();\r\n\r\n                Console.WriteLine(\"Capturing audio... Press any key to stop.\");\r\n                Console.ReadKey();\r\n\r\n                // Stop capturing.\r\n                captureDevice.Stop();\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## High-Level Device Wrappers\r\n\r\nSoundFlow provides convenience wrappers for common and complex scenarios.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Device wrapper options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"fullduplex\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='carbon:arrows-horizontal' />\r\n                <span>Full-Duplex (Simultaneous I/O)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `FullDuplexDevice` simplifies scenarios like live effects processing or VoIP by managing a paired playback and capture device.\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n\r\n                using var engine = new MiniAudioEngine();\r\n                var format = AudioFormat.DvdHq;\r\n\r\n                // Getting the default devices\r\n                var playbackDevice = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                var captureDevice = engine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n\r\n                // Initialize a duplex device using the selected playback and capture hardware, or null for default.\r\n                using var duplexDevice = engine.InitializeFullDuplexDevice(playbackDevice, captureDevice, format);\r\n\r\n                Console.WriteLine($\"Passthrough active. Input: {duplexDevice.CaptureDevice.Info?.Name}, Output: {duplexDevice.PlaybackDevice.Info?.Name}\");\r\n\r\n                // Create a provider that reads from the capture device's audio stream.\r\n                using var micProvider = new MicrophoneDataProvider(duplexDevice); // MicrophoneDataProvider works with both Capture and Duplex devices\r\n\r\n                // Create a player that plays the audio from the microphone provider.\r\n                using var player = new SoundPlayer(engine, format, micProvider);\r\n\r\n                // Add the player to the duplex device's playback mixer.\r\n                duplexDevice.MasterMixer.AddComponent(player);\r\n\r\n                // Start the duplex device, provider, and player.\r\n                duplexDevice.Start();\r\n                micProvider.StartCapture();\r\n                player.Play();\r\n\r\n                Console.WriteLine(\"Live microphone passthrough is active. Press any key to stop.\");\r\n                Console.ReadKey();\r\n\r\n                // Clean up\r\n                duplexDevice.Stop();\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"loopback\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:record-rec' />\r\n                <span>Loopback Recording</span>\r\n                <Chip color=\"warning\" variant=\"flat\" size=\"sm\">Windows Only</Chip>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                Loopback allows you to capture the audio that your computer is currently playing. This is useful for recording game audio, system sounds, or audio from other applications.\r\n                <Card className=\"bg-warning-50/50 dark:bg-warning-500/10 border-1 border-warning-200/50 dark:border-warning-500/20 my-4\">\r\n                    <CardBody>\r\n                        <div className=\"flex items-center gap-3\">\r\n                            <Icon icon=\"lucide:alert-triangle\" className=\"text-warning text-2xl flex-shrink-0\" />\r\n                            <p className=\"text-sm\">\r\n                                Loopback recording is currently only supported on <strong>Windows</strong> via the WASAPI backend.\r\n                            </p>\r\n                        </div>\r\n                    </CardBody>\r\n                </Card>\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Structs;\r\n\r\n                using var engine = new MiniAudioEngine();\r\n                var format = AudioFormat.DvdHq;\r\n\r\n                // Initialize a loopback device. It internally finds the default playback\r\n                // device and sets it up for capture.\r\n                using var loopbackDevice = engine.InitializeLoopbackDevice(format);\r\n                Console.WriteLine($\"Initialized loopback capture on: {loopbackDevice.Info?.Name}\");\r\n\r\n                // Record the loopback audio to a file.\r\n                using var fileStream = new FileStream(\"system_audio.wav\", FileMode.Create);\r\n                // The Recorder now uses a string format ID.\r\n                using var recorder = new Recorder(loopbackDevice, fileStream, \"wav\");\r\n\r\n                // Start capture and recording.\r\n                loopbackDevice.Start();\r\n                recorder.StartRecording();\r\n\r\n                Console.WriteLine(\"Recording system audio... Play some sound! Press any key to stop.\");\r\n                Console.ReadKey();\r\n\r\n                // Stop and clean up.\r\n                recorder.StopRecording();\r\n                loopbackDevice.Stop();\r\n\r\n                Console.WriteLine(\"Recording stopped. Saved to system_audio.wav\");\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Backend Prioritization\r\n\r\nBy default, `MiniAudioEngine` intelligently probes the operating system to find the best available audio backend (e.g., WASAPI on Windows, CoreAudio on macOS, ALSA on Linux).\r\n\r\nTo help you decide which backends to prioritize, you can retrieve a list of all backends supported on the current platform using the static `MiniAudioEngine.AvailableBackends` property.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing System;\r\n\r\n// Print all backends supported on the current OS\r\nConsole.WriteLine(\"Available backends on this platform:\");\r\nforeach (var backend in MiniAudioEngine.AvailableBackends)\r\n{\r\n    Console.WriteLine($\"- {backend}\");\r\n}\r\n```\r\n\r\nThe available backends are determined by the operating system:\r\n*   **Windows**: `Wasapi`, `DirectSound`, `WinMm`\r\n*   **macOS / MacCatalyst**: `CoreAudio`, `Jack`\r\n*   **Linux**: `Alsa`, `PulseAudio`, `Jack`, `Oss`\r\n*   **Android**: `AAudio`, `OpenSl`\r\n*   **iOS**: `CoreAudio`\r\n*   **FreeBSD**: `Oss`, `Sndio`, `PulseAudio`\r\n\r\nHowever, in advanced scenarios, you might need to force the engine to use a specific backend—for instance, to prefer `PulseAudio` over `ALSA` on a specific Linux distribution, or to force `DirectSound` for legacy compatibility on Windows.\r\n\r\nYou can control this by passing an ordered list of `MiniAudioBackend` preferences to the engine's constructor.\r\n\r\n```csharp\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Backends.MiniAudio.Enums;\r\nusing System;\r\nusing System.Collections.Generic;\r\n\r\n// Define a strict priority list.\r\n// The engine will try to initialize WASAPI first.\r\n// If that fails, it will try DirectSound.\r\n// If that fails, initialization throws an exception.\r\nvar backendPriority = new List<MiniAudioBackend>\r\n{\r\n    MiniAudioBackend.Wasapi,\r\n    MiniAudioBackend.DirectSound\r\n};\r\n\r\nusing var engine = new MiniAudioEngine(backendPriority);\r\n\r\n// Check which backend was actually selected\r\nConsole.WriteLine($\"Active Audio Backend: {engine.ActiveBackend}\");\r\n```\r\n\r\n## Runtime Device Switching\r\n\r\nA powerful feature of SoundFlow is the ability to switch the underlying hardware device at runtime without interrupting the audio graph. The `Engine.SwitchDevice(...)` method handles this seamlessly.\r\n*   For **playback**, it moves all `SoundComponent`s from the old device's mixer to the new one.\r\n*   For **capture**, it moves all event subscribers from the old device's `OnAudioProcessed` event to the new one.\r\n*   It automatically disposes the old device instance.\r\n\r\n```csharp\r\n// ... (Setup from the Playback Device example) ...\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\n\r\n// 1. Initialize with the default device.\r\nvar playbackDevice = engine.InitializePlaybackDevice(null, format);\r\nvar oscillator = new Oscillator(engine, format) { Frequency = 440f, Volume = 0.5f };\r\nplaybackDevice.MasterMixer.AddComponent(oscillator);\r\nplaybackDevice.Start();\r\nConsole.WriteLine($\"Playing tone on: {playbackDevice.Info?.Name}\");\r\n\r\n// 2. Loop to allow switching.\r\nwhile(true)\r\n{\r\n    Console.WriteLine(\"\\nPress 's' to switch device, or 'q' to quit.\");\r\n    if (Console.ReadKey(true).Key == ConsoleKey.Q) break;\r\n\r\n    // Prompt user to select a new device from the list.\r\n    engine.UpdateAudioDevicesInfo(); // Use the new method for audio devices\r\n    Console.WriteLine(\"Select new playback device:\");\r\n    for (int i = 0; i < engine.PlaybackDevices.Length; i++)\r\n    Console.WriteLine($\"{i}: {engine.PlaybackDevices[i].Name}\");\r\n\r\n    if (int.TryParse(Console.ReadLine(), out var index) && index >= 0 && index < engine.PlaybackDevices.Length)\r\n{\r\n    var newDeviceInfo = engine.PlaybackDevices[index];\r\n    Console.WriteLine($\"Switching playback to: {newDeviceInfo.Name}...\");\r\n\r\n    // 3. THE SWITCH:\r\n    // The old device is disposed, a new one is returned, and the oscillator is moved automatically.\r\n    playbackDevice = engine.SwitchDevice(playbackDevice, newDeviceInfo);\r\n\r\n    Console.WriteLine($\"Successfully switched. Now playing on: {playbackDevice.Info?.Name}\");\r\n}\r\n}\r\n\r\nplaybackDevice.Dispose();\r\n    ```\r\n\r\n## Advanced Device Configuration\r\n\r\nFor fine-grained control over latency and backend-specific features, you can use the `MiniAudioDeviceConfig` class when initializing a device. This allows you to tune parameters like buffer sizes, sharing modes, and platform-specific settings.\r\n\r\nThis is just a glimpse of the available options. For a full list of configurable parameters, please refer to the <strong>API Reference</strong> for <code>MiniAudioDeviceConfig</code> and its nested settings classes.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Abstracts.Devices;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Backends.MiniAudio.Devices;\r\nusing SoundFlow.Backends.MiniAudio.Enums;\r\nusing SoundFlow.Structs;\r\n\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\n\r\n// Create a detailed configuration object.\r\nvar customConfig = new MiniAudioDeviceConfig\r\n{\r\n// Request a specific buffer size. 960 frames at 48kHz stereo = 10ms latency.\r\nPeriodSizeInFrames = 960,\r\n\r\n// Use shared mode for better compatibility with other applications.\r\n// For lowest latency, you could try ShareMode.Exclusive.\r\nPlayback = new DeviceSubConfig { ShareMode = ShareMode.Shared },\r\nCapture = new DeviceSubConfig { ShareMode = ShareMode.Shared },\r\n\r\n// Platform-specific settings. For Windows, use WASAPI in ProAudio mode.\r\nWasapi = new WasapiSettings{ Usage = WasapiUsage.ProAudio }\r\n};\r\n\r\n// Pass the config when initializing the device.\r\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format, customConfig);\r\n\r\nConsole.WriteLine($\"Initialized device '{playbackDevice.Info?.Name}' with custom configuration.\");\r\n// ... rest of your playback logic ...\r\n```"
  },
  {
    "id": 2,
    "slug": "core-concepts",
    "version": "1.3.0",
    "title": "Core Concepts",
    "description": "Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.",
    "navOrder": 2,
    "category": "Core",
    "content": "---\nid: 2\ntitle: Core Concepts\ndescription: Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.\nnavOrder: 2\ncategory: Core\n---\n\n# Core Concepts\n\nThis section explains the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline. Understanding these core concepts is essential for effectively using and extending the SoundFlow framework.\n\nThe v1.3.0 architecture is built around a clear separation of concerns and a highly extensible, pluggable model:\n1.  The **`AudioEngine`** acts as a central context and factory for devices, codecs, and MIDI backends.\n2.  **`AudioDevice`** instances represent the actual hardware I/O streams.\n3.  **`SoundComponent`** instances form the audio processing graph for each device.\n4.  **`MidiManager`** provides a unified interface for all MIDI operations.\n\n## Audio Engine (`AudioEngine`)\n\nThe `AudioEngine` is the top-level object in SoundFlow. It's a central context responsible for:\n\n*   **Initializing and managing audio & MIDI backends:** SoundFlow supports multiple backends (e.g., `MiniAudio` for audio, `PortMidi` for MIDI), which handle low-level interaction with the operating system's APIs. The `AudioEngine` abstracts away the backend details.\n*   **Discovering and Enumerating Devices:** The engine can list all available audio and MIDI devices.\n*   **Acting as a Factory:** The primary role of the engine is to initialize `AudioDevice` instances, create decoders/encoders via a pluggable codec system, and create MIDI devices.\n*   **Managing Global State:** It handles global features like the soloing system (`SoloComponent`/`UnsoloComponent`).\n\n> **Key Change in v1.3:** The `AudioEngine`'s role has expanded significantly. It is now the central hub for registering **codec factories** (like FFmpeg) and enabling **MIDI backends** (like PortMidi), making the entire framework highly extensible.\n\n**Key Properties:**\n\n*   `PlaybackDevices`, `CaptureDevices`: Lists of available audio devices. Must be refreshed with `UpdateAudioDevicesInfo()`.\n*   `MidiInputDevices`, `MidiOutputDevices`: Lists of available MIDI devices. Must be refreshed with `UpdateMidiDevicesInfo()` after a backend is enabled.\n*   `MidiManager`: The central hub for all MIDI device management and routing.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n\n**Key Methods:**\n\n*   `InitializePlaybackDevice(...)`: Creates and returns a new `AudioPlaybackDevice` for audio output.\n*   `InitializeCaptureDevice(...)`: Creates and returns a new `AudioCaptureDevice` for audio input.\n*   `InitializeFullDuplexDevice(...)`: A convenience method to create a paired playback and capture device.\n*   `InitializeLoopbackDevice(...)`: Creates a capture device to record system audio output (Windows only).\n*   `SwitchDevice(...)`: Switches an active device to a new physical device while preserving its state.\n*   `CreateEncoder(stream, formatId, format)`, `CreateDecoder(stream, formatId, format)`: Creates audio encoders and decoders by querying registered `ICodecFactory` implementations based on a string `formatId` (e.g., \"wav\", \"mp3\").\n*   `RegisterCodecFactory(ICodecFactory factory)`: Registers a new codec implementation with the engine.\n*   `UseMidiBackend(IMidiBackend backend)`: Enables a MIDI backend, activating all MIDI-related functionality.\n*   `UpdateAudioDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.\n*   `UpdateMidiDevicesInfo()`: Refreshes the `MidiInputDevices` and `MidiOutputDevices` lists.\n*   `Dispose()`: Releases the engine and all associated resources.\n\n**Example:**\n\n```csharp\n// 1. Initialize the engine context.\nusing var engine = new MiniAudioEngine();\n\n// 2. List available audio playback devices.\nengine.UpdateAudioDevicesInfo();\nConsole.WriteLine(\"Available Playback Devices:\");\nforeach(var device in engine.PlaybackDevices)\n{\n    Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\n}\n```\n\n## MIDI Backend & Routing (`MidiManager`)\n\nNew in v1.3, SoundFlow has a fully integrated, pluggable system for MIDI input and output.\n\n*   **`IMidiBackend` Interface:** This interface defines the contract for a MIDI backend (e.g., `PortMidiBackend`), which is responsible for communicating with the operating system's MIDI APIs.\n*   **`MidiManager`:** This core class, accessed via `engine.MidiManager`, is the central hub for all MIDI operations. It acts as a virtual patch bay, allowing you to:\n    *   Access lists of available MIDI input and output devices.\n    *   Create **routes** that connect a source (like a physical keyboard) to a destination (like a physical synthesizer module or an internal `SoundComponent` like the new `Synthesizer`).\n    *   Manage the lifecycle of MIDI devices.\n\nTo enable MIDI functionality, you must first register a backend. The `SoundFlow.Midi.PortMidi` package is the recommended implementation.\n\n**Example:**\n\n```csharp\nusing SoundFlow.Backends.MiniAudio;\nusing SoundFlow.Midi.PortMidi; // Import the PortMidi extension\n\n// 1. Initialize the engine.\nusing var engine = new MiniAudioEngine();\n\n// 2. Enable the PortMidi backend. This activates the MidiManager.\nengine.UsePortMidi();\n\n// 3. Refresh and list available MIDI devices.\nengine.UpdateMidiDevicesInfo();\nConsole.WriteLine(\"\\nAvailable MIDI Inputs:\");\nforeach(var input in engine.MidiInputDevices)\n{\n    Console.WriteLine($\"- {input.Name}\");\n}\n```\n\n## Audio Devices (`AudioDevice`)\n\nThe `AudioDevice` and its derivatives (`AudioPlaybackDevice`, `AudioCaptureDevice`) represent an active audio stream to a physical hardware device. Each device is an independent entity with its own audio format, lifecycle, and processing graph.\n\n### `AudioPlaybackDevice` (Output)\nAn `AudioPlaybackDevice` manages an audio output stream.\n\n*   **Owns a `MasterMixer`:** Each playback device has its own `MasterMixer` property. This is the root of the audio graph for that specific device. All components you want to hear on this device must be added to its mixer.\n*   **Independent `AudioFormat`:** Can be initialized with a specific sample rate, channel count, and sample format.\n*   **Lifecycle:** Must be explicitly started with `Start()` and stopped with `Stop()`. It is `IDisposable` and should be managed with a `using` statement.\n\n### `AudioCaptureDevice` (Input)\nAn `AudioCaptureDevice` manages an audio input stream.\n\n*   **`OnAudioProcessed` Event:** This event is raised whenever a new buffer of audio is captured from the hardware. You can subscribe to this event to process live microphone data.\n*   **Lifecycle:** Also has its own `Start()`, `Stop()`, and `Dispose()` methods.\n\n> For a deep dive into creating, managing, and switching devices, see the **[Device Management](./device-management)** documentation.\n\n## Sound Components (`SoundComponent`)\n\n`SoundComponent` remains the abstract base class for all audio processing units in SoundFlow. Each component represents a node in the **audio graph**.\n\n**Key Changes in v1.3:**\n\n*   **MIDI Mappable:** `SoundComponent` now implements `IMidiMappable`, giving each instance a unique `Id` property. This allows its parameters to be targeted by the new real-time MIDI mapping system.\n\n**Key Features:**\n\n*   **Modular Processing:** Components encapsulate specific audio processing logic, making the system modular and extensible.\n*   **Input and Output Connections:** Components can have zero or more input and output connections, allowing data to flow between them.\n*   **`GenerateAudio(Span<float> buffer, int channels)`:** The core processing method that derived classes must implement.\n    *   **Generate new audio samples:** For source components like oscillators or file players.\n    *   **Modify existing audio samples:** For effects, filters, or analyzers.\n*   **Properties:**\n    *   `Name`: A descriptive name for the component.\n    *   `Volume`: Controls the output gain.\n    *   `Pan`: Controls the stereo panning (0.0 for full left, 0.5 for center, 1.0 for full right).\n    *   `Enabled`: Enables or disables the component's processing.\n    *   `Solo`: Isolates the component for debugging.\n    *   `Mute`: Silences the component's output.\n    *   `Parent`: The `Mixer` to which this component belongs (if any).\n*   **Methods:**\n    *   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n    *   `DisconnectInput(SoundComponent input)`: Disconnects an input connection.\n    *   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an `AudioAnalyzer` to this component.\n    *   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an `AudioAnalyzer` from this component.\n    *   `AddModifier(SoundModifier modifier)`: Adds a `SoundModifier` to this component.\n    *   `RemoveModifier(SoundModifier modifier)`: Removes a `SoundModifier` from this component.\n\n**Example:**\n\n```csharp\n// A SineWaveGenerator component aware of its format.\npublic class SineWaveGenerator : SoundComponent\n{\n    public float Frequency { get; set; } = 440f;\n    private float _phase;\n\n    // The constructor now takes the engine and format context.\n    public SineWaveGenerator(AudioEngine engine, AudioFormat format) : base(engine, format) { }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels)\n    {\n        // Now uses the component's own Format property\n        var sampleRate = this.Format.SampleRate; \n        for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] = MathF.Sin(_phase);\n            _phase += 2 * MathF.PI * Frequency / sampleRate;\n            if (_phase > 2 * MathF.PI) _phase -= 2 * MathF.PI;\n        }\n    }\n}\n```\n\n## Mixer (`Mixer`)\n\nThe `Mixer` is a specialized `SoundComponent` that combines the output of multiple `SoundComponent` instances.\n\n**Key Features:**\n\n*   **Device-Specific `MasterMixer`:** Each `AudioPlaybackDevice` has its own `MasterMixer`. All audio to be played on a device must be routed to its `MasterMixer`.\n*   **Creating Sub-Mixers:** You can create your own `Mixer` instances (`new Mixer(engine, format)`) to group components.\n*   **Adding and Removing Components:**\n    *   `AddComponent(SoundComponent component)`: Adds a component to the mixer's inputs.\n    *   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n\t\n**Example:**\n\n```csharp\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format);\n\n// Create a SoundPlayer and an Oscillator\nusing var dataProvider = new StreamDataProvider(engine, File.OpenRead(\"audio.wav\")); // Auto-detects format\nvar player = new SoundPlayer(engine, format, dataProvider);\nvar oscillator = new Oscillator(engine, format) { Frequency = 220, Type = Oscillator.WaveformType.Square };\n\n// Add both components to the device's MasterMixer\nplaybackDevice.MasterMixer.AddComponent(player);\nplaybackDevice.MasterMixer.AddComponent(oscillator);\n\n// Start the device to enable its audio stream\nplaybackDevice.Start();\nplayer.Play();\noscillator.Play();\n// ...\n```\n\n## Sound Modifiers (`SoundModifier`)\n\n`SoundModifier` is an abstract base class for creating audio effects. Modifiers are applied to `SoundComponent` instances or to items in the editing hierarchy (`AudioSegment`, `Track`, `Composition`).\n\n**Key Changes in v1.3:**\n\n*   **MIDI Controllable:** `SoundModifier` now implements `IMidiMappable` and `IMidiControllable`. This means modifiers have a unique `Id` for mapping and can directly respond to MIDI messages via the new `ProcessMidiMessage(MidiMessage message)` method.\n\n**Key Features:**\n\n*   **`ProcessSample(float sample, int channel)`:** This is the core method that derived classes can implement to process audio on a sample-by-sample basis.\n*   **`Process(Span<float> buffer, int channels)`:** This method can be overridden for buffer-based processing, which is often more efficient for complex effects. By default, it calls `ProcessSample` for each sample.\n*   **`Enabled` Property:** Allows dynamically enabling or disabling the modifier's effect.\n*   **Chaining:** Modifiers can be chained together on `SoundComponent` instances (or `AudioSegment`, `Track`, `Composition`) to create complex effect pipelines.\n\n**Built-in Modifiers:**\n\nSoundFlow provides a variety of built-in modifiers, including:\n*   Algorithmic Reverb Modifier: Simulates reverberation.\n*   Ambient Reverb Modifier: Creates a sense of spaciousness.\n*   Bass Boost Modifier: Enhances low frequencies.\n*   Chorus Modifier: Creates a chorus effect.\n*   Compressor Modifier: Reduces dynamic range.\n*   Delay Modifier: Applies a delay effect.\n*   Frequency Band Modifier: Boosts or cuts frequency bands.\n*   Noise Reduction Modifier: Reduces noise.\n*   Parametric Equalizer: Provides precise EQ control.\n*   Stereo Chorus Modifier: Creates a stereo chorus.\n*   Treble Boost Modifier: Enhances high frequencies.\n*   Vocal Extractor Modifier: Advanced vocal isolation using spectral and spatial processing.\n*   And potentially external modifiers like `WebRtcApmModifier` via extensions.\n\n\n**Example:**\n\n```csharp\n// Create engine, format, and device\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format);\n\nusing var dataProvider = new StreamDataProvider(engine, File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(engine, format, dataProvider);\n\n// The Filter is now a modifier and its constructor no longer needs the engine.\nvar filter = new Filter(format) { CutoffFrequency = 800f, Resonance = 0.7f };\n\n// Add the filter modifier to the player\nplayer.AddModifier(filter);\n\n// Add the player to the device's MasterMixer\nplaybackDevice.MasterMixer.AddComponent(player);\n// ...\n```\n\n## Sound Players (`SoundPlayerBase`, `SoundPlayer`, `SurroundPlayer`)\n\nThese classes play audio from a data source.\n\n*   **`SoundPlayerBase`:** The abstract base class that provides common functionality for all sound playback components. It implements `ISoundPlayer` and handles:\n    *   Core playback logic: reading from an `ISoundDataProvider`, managing playback state (Play, Pause, Stop).\n    *   Playback speed adjustment via the `PlaybackSpeed` property.\n    *   Looping with `IsLooping`, `LoopStartSamples`/`Seconds`, and `LoopEndSamples`/`Seconds`.\n    *   Seeking capabilities via `Seek` methods (accepting `TimeSpan`, `float` seconds, or `int` sample offset).\n    *   Volume control (inherited from `SoundComponent`).\n    *   A `PlaybackEnded` event.\n*   **`SoundPlayer`:** The standard concrete implementation of `SoundPlayerBase` for typical mono or stereo audio playback.\n\n*   **`SurroundPlayer`:**\n    *   All features from `SoundPlayerBase` and `ISoundPlayer`.\n    *   `SpeakerConfiguration`: Allows you to define the speaker setup (e.g., Stereo, Quad, 5.1, 7.1, or a custom configuration).\n    *   `PanningMethod`: Selects the panning algorithm to use (Linear, EqualPower, or VBAP).\n    *   `ListenerPosition`: Sets the listener's position relative to the speakers.\n    *   `VbapParameters`: Provides fine-grained control over VBAP (Vector Base Amplitude Panning) settings.\n\n## Audio Recording (`Recorder`)\n\nThe `Recorder` captures audio from an `AudioCaptureDevice`, directing it to a `Stream` for file storage or a `ProcessCallback` for real-time processing. It integrates `SoundModifier` and `AudioAnalyzer` components for on-the-fly audio manipulation and analysis, and implements `IDisposable` for resource management.\n\n**Key Features:**\n*   **Dual Recording Modes:** Records to an output `Stream` (e.g., file) or processes raw samples via a `ProcessCallback`.\n*   **`StartRecording()`**: Begins audio capture.\n*   **`PauseRecording()`**: Pauses recording; no data processed during this state.\n*   **`ResumeRecording()`**: Resumes a paused recording.\n*   **`StopRecording()`**: Stops recording, finalizes output, and releases resources.\n*   **`State`**: Current recording state (`Playing`, `Paused`, `Stopped`).\n*   **`SampleFormat`**: Sample format for raw audio (e.g., `Float32`), inherited from the capture device.\n*   **`FormatId`**: String identifier for the encoding format (e.g., \"wav\", \"mp3\").\n*   **`SampleRate`**: Audio sample rate (e.g., 44100 Hz), inherited from the capture device.\n*   **`Channels`**: Number of audio channels (e.g., 1 for mono, 2 for stereo), inherited from the capture device.\n*   **`Stream`**: Output `Stream` for encoded audio when recording to file. `Stream.Null` if using `ProcessCallback`.\n*   **`ProcessCallback`**: `AudioProcessCallback` delegate invoked with raw audio samples for real-time custom processing. `null` if using `Stream` output.\n*   **`Modifiers`**: Read-only collection of `SoundModifier` components.\n    *   **`AddModifier()`**: Adds a `SoundModifier` to the pipeline.\n    *   **`RemoveModifier()`**: Removes a `SoundModifier` from the pipeline.\n*   **`Analyzers`**: Read-only collection of `AudioAnalyzer` components.\n    *   **`AddAnalyzer()`**: Adds an `AudioAnalyzer` to the pipeline.\n    *   **`RemoveAnalyzer()`**: Removes an `AudioAnalyzer` from the pipeline.\n*   **Resource Management**: Implements `IDisposable` for proper resource cleanup.\n\n**Key Changes in v1.3:**\n\n*   The `EncodingFormat` property is replaced by a string `FormatId`.\n*   `StartRecording()` now accepts an optional `SoundTags` object to write metadata upon completion.\n*   `StopRecording()` is now asynchronous (`StopRecordingAsync()`) to handle file I/O without blocking. A synchronous `StopRecording()` wrapper is also available.\n\n## Audio Providers (`ISoundDataProvider`)\n\n`ISoundDataProvider` is an interface that defines a standard way to access audio data, regardless of its source.\n\n**Key Features:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Can be `-1` for live streams.\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio.\n*   `ReadBytes(Span<float> buffer)`: Reads a chunk of audio data into the provided buffer.\n*   `Seek(int offset)`: Moves the read position to a specific offset (in samples).\n*   `EndOfStreamReached`: An event that is raised when the end of the audio data is reached.\n*   `PositionChanged`: An event that is raised when the read position changes.\n*   `Dispose()`: Implementations should release underlying resources (e.g., file streams).\n\n**Key Changes in v1.3:**\n\n*   **Automatic Format Detection:** Constructors for `StreamDataProvider` and `AssetDataProvider` no longer require an `AudioFormat`. They can now automatically detect the format by reading the stream's header.\n*   **`FormatInfo` Property:** A new property that provides detailed metadata about the audio source (codec, duration, bitrate, tags) if it was read from a file.\n*   **`RawDataProvider`:** Constructors have been simplified and no longer require `sampleRate` or `channels`.\n\n**Built-in Providers:**\n\n*   `AssetDataProvider`: Loads audio data from a byte array or `Stream` entirely into memory.\n*   `StreamDataProvider`: Reads audio data from a `Stream`, decoding on the fly.\n*   `MicrophoneDataProvider`: Provides a live stream from an `AudioCaptureDevice`.\n*   `ChunkedDataProvider`: Efficiently reads large files or streams in chunks.\n*   `NetworkDataProvider`: Reads audio data from a network source (URL, HLS).\n*   `QueueDataProvider`: A thread-safe queue for scenarios where one part of your application generates audio and another part consumes it.\n*   `RawDataProvider`: Reads audio data from a raw PCM stream or various raw array types (`float[]`, `byte[]`, `int[]`, `short[]`).\n*   `MidiDataProvider`: A new, non-audio provider that processes MIDI files into a playable, time-ordered event stream for the `Sequencer`.\n\n**Example:**\n\n```csharp\nusing var engine = new MiniAudioEngine();\n\n// The provider can now auto-detect the format from the stream for most common formats.\n// No AudioFormat object is needed here, but you can still pass it as a hint.\nusing var dataProvider = new StreamDataProvider(engine, File.OpenRead(\"audio.mp3\"));\n\n// You can inspect the discovered format\nConsole.WriteLine($\"Detected Sample Rate: {dataProvider.SampleRate}\");\nConsole.WriteLine($\"Detected Title: {dataProvider.FormatInfo?.Tags?.Title}\");\n\n// Use the provider with a player. You can create an AudioFormat from the provider's info.\nvar format = new AudioFormat { SampleRate = dataProvider.SampleRate, Channels = dataProvider.FormatInfo.ChannelCount };\nvar player = new SoundPlayer(engine, format, dataProvider);\n// ...\n```\n\n## Audio Encoding/Decoding (`ICodecFactory`, `ISoundEncoder`, `ISoundDecoder`)\n\n`ISoundEncoder` and `ISoundDecoder` are interfaces for encoding and decoding audio data to and from different formats. Both are `IDisposable`.\n\nThe encoding/decoding system is now fully pluggable.\n\n*   **`ICodecFactory`:** A new interface for creating custom codec implementations. You can register factories with the `AudioEngine`.\n*   **`ISoundEncoder` / `ISoundDecoder`:** The core interfaces for encoding and decoding.\n*   **`MiniAudioCodecFactory`:** The `MiniAudioEngine` automatically registers a factory for its built-in formats (WAV, MP3, FLAC) as a low-priority fallback.\n*   **`FFmpegCodecFactory`:** By adding the `SoundFlow.Codecs.FFMpeg` package, you can register this factory to add support for a massive range of additional formats.\n\n## Audio Analysis (`AudioAnalyzer`)\n\n`AudioAnalyzer` is an abstract base class for creating components that analyze audio data. Analyzers typically extract information from the audio stream without modifying it.\n\n**Key Features:**\n\n*   **Constructor:** Initialized with an `AudioFormat` and an optional `IVisualizer` to send data to.\n*   `Analyze(Span<float> buffer, int channels)`: An abstract method that derived classes must implement to perform their specific analysis.\n*   `Enabled`: If false, the `Analyze` step might be skipped by the `SoundComponent` it's attached to.\n*   **Integration with Visualizers:** Analyzers are often used in conjunction with `IVisualizer` implementations to display the analysis results visually.\n\n**Built-in Analyzers:**\n\n*   `LevelMeterAnalyzer`: Measures the RMS (root-mean-square) and peak levels of an audio signal.\n*   `SpectrumAnalyzer`: Computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).\n*   `VoiceActivityDetector`: Detects the presence of human voice in an audio stream.\n\n\n## Audio Visualization (`IVisualizer`)\n\n`IVisualizer` is an interface for creating components that visualize audio data. Visualizers typically don't modify the audio stream but instead render a graphical representation of the data. It implements `IDisposable`.\n\n**Key Features:**\n\n*   `Name`: A descriptive name for the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: This method is called by the audio engine to provide the visualizer with a chunk of audio data to process.\n*   `Render(IVisualizationContext context)`: This method is called to render the visualization. It receives an `IVisualizationContext` instance, which provides drawing methods.\n*   `VisualizationUpdated`: An event that is raised when the visualization needs to be redrawn (e.g., when new audio data has been processed).\n*   `Dispose()`: Releases resources held by the visualizer.\n\n## Visualization Context (`IVisualizationContext`):\n\nThis interface provides a set of drawing methods for rendering the visualization. The specific implementation of `IVisualizationContext` will depend on the UI framework you are using.\n\n**Built-in Visualizers:**\n\n*   `LevelMeterVisualizer`: Displays a level meter that shows the current RMS or peak level of the audio.\n*   `SpectrumVisualizer`: Renders a bar graph representing the frequency spectrum of the audio.\n*   `WaveformVisualizer`: Draws the waveform of the audio signal.\n\n## Editing Engine & Persistence (`SoundFlow.Editing`, `SoundFlow.Editing.Persistence`)\n\nSoundFlow features a powerful non-destructive audio editing engine.\n\n**Key Changes in v1.3:**\n\n*   **Service-Oriented `Composition`:** The `Composition` class now delegates tasks to `CompositionEditor`, `CompositionRenderer`, `CompositionRecorder`, and `MidiMappingManager`.\n*   **MIDI Support:** The editing model now includes `MidiTrack` and `MidiSegment` to handle MIDI data alongside audio.\n*   **Master Tempo Track:** Compositions now have a `TempoTrack` for defining tempo changes over time, which governs all MIDI and time-based calculations.\n*   **MIDI Mapping:** The new `MidiMappingManager` allows for real-time control of almost any parameter in the composition.\n\n**Key Concepts:**\n\n*   **`Composition`**: The main container for an audio project, holding multiple `Track`s and now also `MidiTrack`s. In v1.3, it acts as a central data model, delegating operations to service classes like `CompositionEditor` and `CompositionRenderer`.\n*   **`Track`**: Represents a single audio track within a `Composition`. Contains `AudioSegment`s and has its own settings (volume, pan, mute, solo, effects).\n*   **`MidiTrack`**: A new track type for holding `MidiSegment`s and routing MIDI data to a target (like a synthesizer).\n*   **`AudioSegment`**: A clip of audio placed on a `Track`'s timeline. It references a portion of an `ISoundDataProvider` and has its own extensive settings.\n    *   **`AudioSegmentSettings`**: Controls volume, pan, fades (with `FadeCurveType`), looping (`LoopSettings`), reverse playback, speed, and **pitch-preserved time stretching** (via `TimeStretchFactor` or `TargetStretchDuration`, powered by `WsolaTimeStretcher`).\n    *   Supports segment-level modifiers and analyzers.\n*   **Non-Destructive:** Edits do not alter the original audio source files. All operations are applied at runtime during playback or rendering.\n*   **Project Persistence (`CompositionProjectManager`)**:\n    *   Save and load entire compositions as `.sfproj` files.\n    *   **Media Consolidation**: Option to copy all external audio files into an `Assets` folder within the project.\n    *   **Embed Small Media**: Option to embed small audio files (e.g., SFX) directly into the project file.\n    *   **Relink Missing Media**: If an audio file is moved, the project can be relinked to its new location.\n    *   **New in v1.3:** The project format now saves and loads all MIDI tracks, the master tempo track, and real-time MIDI mappings.\n\nThis engine allows for programmatic creation and manipulation of complex audio timelines, effects processing at multiple levels (segment, track, master), and robust project management. For a detailed guide, please see the **[Editing Engine & Persistence](./editing-engine)** documentation."
  },
  {
    "id": 5,
    "slug": "api-reference",
    "version": "1.3.0",
    "title": "API Reference",
    "description": "A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.",
    "navOrder": 5,
    "category": "Core",
    "content": "---\nid: 5\ntitle: API Reference\ndescription: A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.\nnavOrder: 5\ncategory: Core\n---\n\n# API Reference\n\nThis section provides a detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members. It serves as a comprehensive reference for developers working with the SoundFlow library.\n\n## Namespaces\n\nSoundFlow is organized into the following namespaces:\n\n*   **`SoundFlow.Abstracts`:** Contains abstract classes and interfaces that define the core framework of SoundFlow. This includes base classes for the audio engine, audio devices, sound components, modifiers, and analyzers.\n*   **`SoundFlow.Backends`:** Provides backend-specific implementations for audio input/output.\n*   **`SoundFlow.Backends.MiniAudio`:** The primary audio backend, which uses the `miniaudio` library.\n*   **`SoundFlow.Codecs`:** Contains official codec extensions for the SoundFlow engine.\n*   **`SoundFlow.Codecs.FFMpeg`:** Provides a codec factory using the FFmpeg library to support a wide range of audio formats like MP3, AAC, OGG, Opus, and more.\n*   **`SoundFlow.Codecs.FFMpeg.Enums`:** Contains enumerations specific to the FFmpeg codec wrapper, such as `FFmpegResult`.\n*   **`SoundFlow.Codecs.FFMpeg.Exceptions`:** Contains exception classes specific to the FFmpeg codec wrapper, such as `FFmpegException`.\n*   **`SoundFlow.Components`:** Contains concrete `SoundComponent` classes that provide various audio processing functionalities, including playback, mixing, synthesis, and analysis. It also includes standalone components like the `Recorder`.\n*   **`SoundFlow.Editing`:** Contains classes for non-destructive audio editing, including `Composition`, `Track`, `MidiTrack`, `AudioSegment`, `MidiSegment`, `MidiSequence`, and their respective settings classes.\n*   **`SoundFlow.Editing.Mapping`:** Contains classes for real-time MIDI mapping and control of `IMidiMappable` objects within a `Composition`.\n*   **`SoundFlow.Editing.Persistence`:** Contains classes for saving and loading audio compositions, such as `CompositionProjectManager` and various project data DTOs.\n*   **`SoundFlow.Enums`:** Contains enumerations used throughout the SoundFlow library to represent different states, options, and capabilities. The original `EncodingFormat` enum is removed as it is superseded by the codec factory system using format strings.\n*   **`SoundFlow.Exceptions`:** Contains custom exception classes used for error handling within SoundFlow.\n*   **`SoundFlow.Interfaces`:** Contains interfaces that define contracts for various functionalities, such as audio data providers, encoders, decoders, codec factories, and MIDI backends.\n*   **`SoundFlow.Metadata`:** Contains classes for reading and writing metadata (tags, format info) from various audio file formats.\n*   **`SoundFlow.Metadata.Midi`:** Contains classes for parsing and generating Standard MIDI Files (SMF).\n*   **`SoundFlow.Midi`:** Contains core classes and abstractions for handling MIDI data and devices.\n*   **`SoundFlow.Midi.Abstracts`:** Contains base classes for MIDI devices and modifiers.\n*   **`SoundFlow.Midi.Devices`:** Contains abstract representations of MIDI input and output devices.\n*   **`SoundFlow.Midi.Enums`:** Contains enumerations for MIDI commands and related types.\n*   **`SoundFlow.Midi.Interfaces`:** Contains interfaces for MIDI-controllable components and routing graph nodes.\n*   **`SoundFlow.Midi.Modifier`:** Contains concrete MIDI modifier (effect) classes, such as `TransposeModifier`.\n*   **`SoundFlow.Midi.PortMidi`:** Provides a MIDI backend implementation using the cross-platform PortMidi library.\n*   **`SoundFlow.Midi.PortMidi.Enums`:** Contains enumerations for PortMidi errors and synchronization states.\n*   **`SoundFlow.Midi.PortMidi.Exceptions`:** Contains exception classes specific to the PortMidi backend.\n*   **`SoundFlow.Midi.Routing`:** Contains classes for managing MIDI routing, including the `MidiManager` and `MidiRoute`.\n*   **`SoundFlow.Midi.Structs`:** Contains struct types for MIDI data representation, like `MidiMessage`.\n*   **`SoundFlow.Modifiers`:** Contains concrete `SoundModifier` classes that implement various audio effects.\n*   **`SoundFlow.Providers`:** Contains classes that implement the `ISoundDataProvider` interface, providing ways to load audio data from different sources.\n*   **`SoundFlow.Structs`:** Contains custom struct types used within SoundFlow, including `AudioFormat`, `DeviceInfo`, `Result`, and a comprehensive set of error types.\n*   **`SoundFlow.Synthesis`:** Contains a polyphonic, multi-timbral synthesizer engine, including `Synthesizer`, `Sequencer`, and related classes for instrument banks and voices.\n*   **`SoundFlow.Utils`:** Contains utility classes and extension methods that provide helpful functionalities for working with audio data and performing common operations.\n*   **`SoundFlow.Visualization`:** Contains classes related to audio visualization, including analyzers and visualizers.\n*   **`SoundFlow.Extensions`:** Namespace for official extensions.\n*   **`SoundFlow.Extensions.WebRtc.Apm`:** Provides integration with the WebRTC Audio Processing Module for features like echo cancellation, noise suppression, and automatic gain control.\n*   **`SoundFlow.Extensions.WebRtc.Apm.Components`:** Contains components utilizing the WebRTC APM, like `NoiseSuppressor`.\n*   **`SoundFlow.Extensions.WebRtc.Apm.Modifiers`:** Contains modifiers utilizing the WebRTC APM, like `WebRtcApmModifier`.\n\n## Key Classes and Interfaces\n\nBelow is a summary of the key classes and interfaces in SoundFlow.\n\n### Abstracts\n\n| Class/Interface                                         | Description                                                                                                                                                                           |\n|---------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`AudioAnalyzer`](#abstracts-audioanalyzer)             | Abstract base class for audio analysis components. Implements `IMidiMappable`.                                                                                                    |\n| [`AudioEngine`](#abstracts-audioengine)                 | Abstract base class for an audio engine. Manages audio and MIDI devices, provides a pluggable codec system, and is the root context.                                                         |\n| [`AudioDevice`](#abstracts-audiodevice)                 | Abstract base class for an initialized audio device (playback or capture).                                                                                                            |\n| [`AudioPlaybackDevice`](#abstracts-audioplaybackdevice) | Abstract class representing an initialized output/playback device. Contains a `MasterMixer`.                                                                                          |\n| [`AudioCaptureDevice`](#abstracts-audiocapturedevice)   | Abstract class representing an initialized input/capture device. Exposes an `OnAudioProcessed` event.                                                                                 |\n| [`FullDuplexDevice`](#abstracts-fullduplexdevice)       | A high-level abstraction managing a paired playback and capture device for simultaneous I/O.                                                                                          |\n| [`DeviceConfig`](#abstracts-deviceconfig)               | Abstract base class for backend-specific device configuration objects.                                                                                                                |\n| [`SoundComponent`](#abstracts-soundcomponent)           | Abstract base class for all audio processing units in SoundFlow. Represents a node in the audio graph. Implements `IMidiMappable`.                                                    |\n| [`SoundModifier`](#abstracts-soundmodifier)             | Abstract base class for audio effects that modify audio samples. Implements `IMidiMappable` and `IMidiControllable`.                                                                  |\n| [`SoundPlayerBase`](#abstracts-soundplayerbase)         | Abstract base class providing common functionality for sound playback components. Inherits from `SoundComponent` and implements `ISoundPlayer`. Includes support for time-stretching. |\n\n### Backends.MiniAudio\n\n| Class/Interface                                                     | Description                                                                                                                  |\n|---------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n| [`MiniAudioEngine`](#backendsminiaudio-miniaudioengine)             | `AudioEngine` implementation that uses the `miniaudio` library for audio I/O.                                                |\n| [`MiniAudioDeviceConfig`](#backendsminiaudio-miniaudiodeviceconfig) | `DeviceConfig` implementation for MiniAudio, providing detailed, backend-specific settings for WASAPI, CoreAudio, ALSA, etc. |\n| [`MiniAudioCodecFactory`](#backendsminiaudio-miniaudiocodecfactory) | `ICodecFactory` implementation for codecs natively supported by `miniaudio` (WAV, MP3, FLAC). Registered by default at low priority. |\n| [`MiniaudioException`](#exceptions-miniaudioexception) | Exception thrown for errors originating from the MiniAudio backend.                                   |\n\n### Codecs.FFMpeg\n| Class/Interface                                          | Description                                                                                                                   |\n|----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| [`FFmpegCodecFactory`](#codecsffmpeg-ffmpegcodecfactory) | An `ICodecFactory` that uses FFmpeg to provide decoding and encoding for a wide range of formats (MP3, AAC, OGG, Opus, etc.). |\n| [`FFmpegException`](#codecsffmpeg-ffmpegexception)       | Exception thrown for errors originating from the native FFmpeg wrapper library.                                               |\n\n### Components\n\n| Class/Interface                                                | Description                                                                                                                                            |\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`EnvelopeGenerator`](#components-envelopegenerator)           | `SoundComponent` that generates an ADSR (Attack, Decay, Sustain, Release) envelope signal.                                                             |\n| [`LowFrequencyOscillator`](#components-lowfrequencyoscillator) | `SoundComponent` that generates a low-frequency oscillator (LFO) signal with various waveforms.                                                        |\n| [`Mixer`](#components-mixer)                                   | `SoundComponent` that mixes multiple audio streams together. The `Mixer.Master` property provides access to the default root mixer.                    |\n| [`Oscillator`](#components-oscillator)                         | `SoundComponent` that generates various band-limited waveforms (sine, square, sawtooth, etc.).                                                    |\n| [`Recorder`](#components-recorder)                             | A component that captures audio from a device and writes it to a file or stream, with support for metadata tagging. |\n| [`SoundPlayer`](#components-soundplayer)                       | `SoundPlayerBase` implementation that plays audio from an `ISoundDataProvider`.                                                                        |\n| [`SurroundPlayer`](#components-surroundplayer)                 | `SoundPlayerBase` implementation that extends `SoundPlayer` to support surround sound configurations with customizable speaker positions, delays, and panning methods. |\n| [`VoiceActivityDetector`](#components-voiceactivitydetector)   | An `AudioAnalyzer` that detects human voice in an audio stream, featuring configurable activation and hangover times to prevent rapid state changes.   |\n| [`WsolaTimeStretcher`](#components-wsolatimestretcher)\t | Implements WSOLA algorithm for real-time, pitch-preserved time stretching. Used internally by `AudioSegment`. |\n\n### Editing\n\n| Class/Interface                                       | Description                                                                                                                                            |\n| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`Composition`](#editing-composition)                 | Top-level container for audio and MIDI tracks, representing a complete project. Provides access to `Renderer`, `Editor`, and `Recorder` services. `IDisposable`. |\n| [`CompositionRenderer`](#editing-compositionrenderer) | Renders a `Composition` to an audio stream. Implements `ISoundDataProvider`.                                                                           |\n| [`CompositionEditor`](#editing-compositioneditor)     | Provides methods for manipulating the structure of a `Composition` (tracks, segments, tempo, etc.).                                                      |\n| [`CompositionRecorder`](#editing-compositionrecorder) | Manages the MIDI recording workflow for a `Composition`, including track arming and punch-in/out.                                                      |\n| [`Track`](#editing-track)                             | Represents a single audio track within a `Composition`, containing `AudioSegment`s and track-level settings. Implements `IMidiMappable`.             |\n| [`MidiTrack`](#editing-miditrack)                     | Represents a single MIDI track within a `Composition`, containing `MidiSegment`s and routing to a MIDI target.                                         |\n| [`AudioSegment`](#editing-audiosegment)               | Represents a single audio clip on a `Track`'s timeline, referencing a portion of an audio source and applying playback settings. `IDisposable`.      |\n| [`MidiSegment`](#editing-midisegment)                 | Represents a single MIDI clip on a `MidiTrack`, containing an editable `MidiSequence`. `IDisposable`.                                                    |\n| [`MidiSequence`](#editing-midisequence)               | A mutable container for editable MIDI data, including notes and automation, within a `MidiSegment`.                                                      |\n| [`AudioSegmentSettings`](#editing-audiosegmentsettings) | Configurable settings for an `AudioSegment` (volume, pan, fades, loop, reverse, speed, time stretch, modifiers, analyzers). Implements `IMidiMappable`. |\n| [`TrackSettings`](#editing-tracksettings)             | Configurable settings for a `Track` or `MidiTrack` (volume, pan, mute, solo, enabled, modifiers). Implements `IMidiMappable`.                                 |\n| [`LoopSettings`](#editing-loopsettings)               | (struct) Defines looping behavior for an `AudioSegment` (repetitions, target duration).                                                                |\n| [`FadeCurveType`](#editing-fadecurvetype)             | (enum) Defines curve types for fade effects (Linear, Logarithmic, SCurve).                                                                             |\n| [`MidiNote`](#editing-midinote)                       | Represents a single, editable MIDI note with start time, duration, pitch, and velocity.                                                                |\n| [`ControlPoint`](#editing-controlpoint)               | Represents a single, editable MIDI automation point for CC or Pitch Bend data.                                                                         |\n| [`TempoMarker`](#editing-tempomarker)                 | (record struct) Represents a tempo change at a specific point in the `Composition` timeline.                                                             |\n| [`QuantizationSettings`](#editing-quantizationsettings) | (record) Defines settings for MIDI note quantization (Grid, Strength, Swing, etc.).                                                                    |\n| [`MidiExporter`](#editing-midiexporter)               | Static utility for exporting a `Composition` to a Standard MIDI File (.mid).                                                                           |\n\n### Editing.Mapping\n\n| Class/Interface                                            | Description                                                                       |\n|------------------------------------------------------------|-----------------------------------------------------------------------------------|\n| [`MidiMappingManager`](#editingmapping-midimappingmanager) | Manages and executes a collection of real-time MIDI mappings for a `Composition`. |\n| [`MidiMapping`](#editingmapping-midimapping)               | Represents a complete link between a MIDI message and a controllable parameter.   |\n\n### Editing.Persistence\n\n| Class/Interface                                                        | Description                                                                                                                            |\n| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| [`CompositionProjectManager`](#editingpersistence-compositionprojectmanager) | Static class for saving and loading `Composition` projects to/from `.sfproj` files. Handles media consolidation and relinking.       |\n| [`ProjectSaveOptions`](#editingpersistence-projectsaveoptions)           | Provides configurable options for saving a project, such as media consolidation, embedding, and folder names.                          |\n| [`ProjectData`](#editingpersistence-projectdata)                         | DTO representing the root of a saved project file.                                                                                     |\n| [`ProjectTrack`](#editingpersistence-projecttrack)                       | DTO for a `Track` within a saved project.                                                                                              |\n| [`ProjectMidiTrack`](#editingpersistence-projectmiditrack)               | DTO for a `MidiTrack` within a saved project.                                                                                          |\n| [`ProjectSegment`](#editingpersistence-projectsegment)                   | DTO for an `AudioSegment` within a saved project.                                                                                      |\n| [`ProjectMidiSegment`](#editingpersistence-projectmidisegment)           | DTO for a `MidiSegment` within a saved project.                                                                                        |\n| [`ProjectAudioSegmentSettings`](#editingpersistence-projectaudiosegmentsettings) | DTO for `AudioSegmentSettings` within a saved project.                                                                                 |\n| [`ProjectTrackSettings`](#editingpersistence-projecttracksettings)       | DTO for `TrackSettings` within a saved project.                                                                                        |\n| [`ProjectSourceReference`](#editingpersistence-projectsourcereference)   | DTO representing how an audio source is referenced in a project (file path, embedded data, consolidation).                             |\n| [`ProjectEffectData`](#editingpersistence-projecteffectdata)             | DTO for serializing `SoundModifier`, `MidiModifier`, or `AudioAnalyzer` instances.                                                     |\n| [`ProjectMidiMapping`](#editingpersistence-projectmidimapping)           | DTO for serializing a `MidiMapping`.                                                                                                   |\n| [`ProjectTempoMarker`](#editingpersistence-projecttempomarker)           | DTO for serializing a `TempoMarker`.                                                                                                   |\n\n### Enums\n\n| Enum                                             | Description                                                                                                                                 |\n| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Capability`](#enums-capability)                | Specifies the capabilities of an audio device (Playback, Record, Mixed, Loopback).                                                          |\n| [`ChannelLayout`](#enums-channellayout)          | Defines the physical or logical arrangement of channels in an audio stream (Mono, Stereo, Quad, Surround51, Surround71).                     |\n| [`DeviceType`](#enums-devicetype)                | Specifies the type of audio device (Playback, Capture).                                                                                     |\n| [`LogLevel`](#enums-loglevel)                    | Defines severity levels for log messages (Debug, Info, Warning, Error).                                                                     |\n| [`MiniAudioBackend`](#enums-miniaudiobackend)    | Represents the available low-level audio backends that MiniAudio can use (e.g., WASAPI, CoreAudio, ALSA).                                     |\n| [`PlaybackState`](#enums-playbackstate)          | Specifies the current playback state of a player or recorder (Stopped, Playing, Paused).                                                      |\n| [`SampleFormat`](#enums-sampleformat)            | Specifies the format of audio samples (e.g., U8, S16, S24, S32, F32).                                                                     |\n| [`FilterType`](#enums-filtertype)                | Specifies the type of filter to use in the `Filter` and `ParametricEqualizer` components (Peaking, LowShelf, HighShelf, BandPass, Notch, LowPass, HighPass) |\n| [`EnvelopeGenerator.EnvelopeState`](#enums-envelopegenerator-envelopestate) | Specifies the current state of the envelope generator (Idle, Attack, Decay, Sustain, Release) |\n| [`EnvelopeGenerator.TriggerMode`](#enums-envelopegenerator-triggermode) | Specifies how the envelope generator is triggered (NoteOn, Gate, Trigger) |\n| [`LowFrequencyOscillator.WaveformType`](#enums-lowfrequencyoscillator-waveformtype) | Specifies the waveform type for the low-frequency oscillator (Sine, Square, Triangle, Sawtooth, ReverseSawtooth, Random, SampleAndHold) |\n| [`LowFrequencyOscillator.TriggerMode`](#enums-lowfrequencyoscillator-triggermode) | Specifies how the LFO is triggered (FreeRunning, NoteTrigger) |\n| [`Oscillator.WaveformType`](#enums-oscillator-waveformtype) | Specifies the waveform type for the oscillator (Sine, Square, Sawtooth, Triangle, Noise, Pulse) |\n| [`SurroundPlayer.SpeakerConfiguration`](#enums-surroundplayer-speakerconfiguration) | Specifies the speaker configuration for the surround player (Stereo, Quad, Surround51, Surround71, Custom) |\n| [`SurroundPlayer.PanningMethod`](#enums-surroundplayer-panningmethod) | Specifies the panning method for the surround player (Linear, EqualPower, Vbap) |\n| [`FadeCurveType`](#editing-fadecurvetype)\t   | Specifies curve types for fade effects (Linear, Logarithmic, SCurve). |\n| **MIDI Enums** (`SoundFlow.Midi...`)             |                                                                                                                                             |\n| [`MidiCommand`](#midi-enums-midicommand)         | Represents the command portion of a MIDI status byte (e.g., NoteOn, ControlChange).                                                           |\n| [`PortMidiError`](#midi-portmidi-enums-portmidierror) | Error codes returned by the PortMidi library.                                                                                               |\n| [`SyncMode`](#midi-portmidi-enums-syncmode)      | Defines the MIDI synchronization mode (Off, Master, Slave).                                                                                   |\n| [`SyncSource`](#midi-portmidi-enums-syncsource)  | Defines the source of synchronization when in Slave mode (Internal, MidiClock, MTC).                                                          |\n| [`SyncStatus`](#midi-portmidi-enums-syncstatus)  | Represents the current lock status of MIDI synchronization (Unlocked, Locked).                                                                |\n| **`SoundFlow.Extensions.WebRtc.Apm` Enums**          |                                                                                                                                             |\n| [`ApmError`](#extensions-webrtc-apm-apmerror)    | Error codes returned by the WebRTC Audio Processing Module.                                                                                |\n| [`NoiseSuppressionLevel`](#extensions-webrtc-apm-noisesuppressionlevel) | Specifies noise suppression levels (Low, Moderate, High, VeryHigh).                                                                  |\n| [`GainControlMode`](#extensions-webrtc-apm-gaincontrolmode) | Specifies gain controller modes (AdaptiveAnalog, AdaptiveDigital, FixedDigital).                                                        |\n| [`DownmixMethod`](#extensions-webrtc-apm-downmixmethod) | Specifies methods for downmixing audio channels (AverageChannels, UseFirstChannel).                                                    |\n| [`RuntimeSettingType`](#extensions-webrtc-apm-runtimesettingtype) | Specifies types of runtime settings for the WebRTC APM.                                                                        |\n\n### Exceptions\n\n| Class                                           | Description                                                                                   |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [`BackendException`](#exceptions-backendexception) | Base class for exceptions thrown by backend libraries (e.g., MiniAudio, PortMidi, FFmpeg).   |\n| [`MiniaudioException`](#exceptions-miniaudioexception) | Thrown for errors originating from the MiniAudio backend.                                   |\n| [`FFmpegException`](#exceptions-ffmpegexception)     | Thrown for errors originating from the FFmpeg codec backend.                                |\n| [`PortBackendException`](#exceptions-portbackendexception) | Thrown for errors originating from the PortMidi backend.                                  |\n\n### Extensions.WebRtc.Apm\n\n| Class/Interface                                                                 | Description                                                                                                                                       |\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AudioProcessingModule`](#extensions-webrtc-apm-audioprocessingmodule)         | Provides access to the native WebRTC Audio Processing Module for advanced audio processing like AEC, NS, AGC.                                     |\n| [`ApmConfig`](#extensions-webrtc-apm-apmconfig)                                 | Represents a configuration for the `AudioProcessingModule`, allowing enabling/disabling and setting parameters for various APM features.            |\n| [`StreamConfig`](#extensions-webrtc-apm-streamconfig)                           | Represents a stream configuration (sample rate, channels) for audio processing within the APM.                                                    |\n| [`ProcessingConfig`](#extensions-webrtc-apm-processingconfig)                   | Holds multiple `StreamConfig` instances for input, output, and reverse streams for the APM.                                                       |\n| **Components Namespace**                                                        |                                                                                                                                                   |\n| [`NoiseSuppressor`](#extensions-webrtc-apm-components-noisesuppressor)          | A component for offline/batch noise suppression using WebRTC APM, processing audio from an `ISoundDataProvider`.                                  |\n| **Modifiers Namespace**                                                         |                                                                                                                                                   |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier)       | A `SoundModifier` that applies WebRTC APM features (AEC, NS, AGC, etc.) in real-time to an audio stream within the SoundFlow graph. Configurable. |\n\n### Interfaces\n\n| Interface                                           | Description                                                                                                                                  |\n| --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ICodecFactory`](#interfaces-icodecfactory)         | Defines a factory for creating `ISoundDecoder` and `ISoundEncoder` instances, enabling a pluggable codec system.                             |\n| [`IMidiBackend`](#interfaces-imidibackend)           | Defines the contract for a pluggable MIDI backend, responsible for creating and managing MIDI devices.                                       |\n| [`IMidiMappable`](#interfaces-imidimappable)         | Marks a class as being a target for real-time MIDI mapping. Requires a unique `Guid Id`.                                                       |\n| [`IMidiControllable`](#interfaces-imidicontrollable) | Defines an interface for components that can be directly controlled by MIDI messages.                                                        |\n| [`ISoundDataProvider`](#interfaces-isounddataprovider) | Defines a standard way to access audio data from various sources. Implements `IDisposable`.                                                |\n| [`ISoundDecoder`](#interfaces-isounddecoder)         | Defines the contract for decoding audio data from a specific format into raw audio samples.                                                 |\n| [`ISoundEncoder`](#interfaces-isoundencoder)         | Defines the contract for encoding raw audio samples into a specific format.                                                                 |\n| [`ISoundPlayer`](#interfaces-isoundplayer)           | Defines the contract for controlling audio playback (Play, Pause, Stop, Seek, Looping, Speed, Volume).                                       |\n| [`IVisualizationContext`](#interfaces-ivisualizationcontext) | Provides drawing methods for rendering audio visualizations. The implementation depends on the specific UI framework used.               |\n| [`IVisualizer`](#interfaces-ivisualizer)             | Defines the contract for components that visualize audio data.                                                                            |\n\n### Metadata\n| Class/Interface                                        | Description                                                                                                  |\n|--------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n| [`SoundMetadataReader`](#metadata-soundmetadatareader) | Static class to read format information and metadata tags (artist, title, album art, etc.) from audio files. |\n| [`SoundMetadataWriter`](#metadata-soundmetadatawriter) | Static class to write or remove metadata tags from audio files.                                              |\n| [`SoundFormatInfo`](#metadata-soundformatinfo)         | Holds detailed information about an audio file's format, codec, duration, and tags.                          |\n\n### Midi\n| Class/Interface                      | Description                                                                                                                  |\n|--------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n| [`MidiManager`](#midi-midimanager)   | Central hub for managing MIDI devices, routing (`MidiRoute`), and MPE configuration. Accessed via `AudioEngine.MidiManager`. |\n| [`MidiRoute`](#midi-midiroute)       | Represents a single connection from a MIDI source to a destination, with an optional chain of `MidiModifier`s.               |\n| [`MidiMessage`](#midi-midimessage)   | (record struct) Represents a standard MIDI channel message (Note On, CC, Pitch Bend, etc.).                                  |\n| [`MidiModifier`](#midi-midimodifier) | Abstract base class for real-time MIDI processing components (MIDI effects).                                                 |\n\n### Midi.Modifier\n\n| Class                                                          | Description                                                                                                         |\n|----------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n| [`ArpeggiatorModifier`](#midimodifier-arpeggiatormodifier)     | A stateful, temporal modifier that generates rhythmic patterns from held notes. Implements `ITemporalMidiModifier`. |\n| [`ChannelFilterModifier`](#midimodifier-channelfiltermodifier) | Filters messages, allowing only those on a specific MIDI channel to pass.                                           |\n| [`HarmonizerModifier`](#midimodifier-harmonizermodifier)       | Generates chords from single notes based on a list of intervals.                                                    |\n| [`RandomizerModifier`](#midimodifier-randomizermodifier)       | Introduces randomness to note events, affecting probability, velocity, and pitch.                                   |\n| [`TransposeModifier`](#midimodifier-transposemodifier)         | Transposes the pitch of Note On/Off messages by a fixed semitone amount.                                            |\n| [`VelocityModifier`](#midimodifier-velocitymodifier)           | Reshapes velocity data using clamping, offsets, and non-linear curves.                                              |\n\n### Midi.PortMidi\n\n| Class/Interface                                        | Description                                                                                                  |\n|--------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n| [`PortMidiBackend`](#midiportmidi-portmidibackend)     | Concrete implementation of `IMidiBackend` using the PortMidi library. Provides device I/O and MIDI synchronization. |\n| [`PortMidiExtensions`](#midiportmidi-portmidiextensions) | Provides the `UsePortMidi()` extension method for easy registration with the `AudioEngine`.                 |\n\n### Modifiers\n\n| Class                                                                                                                        | Description                                                                                                                                                                                           |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`AlgorithmicReverbModifier`](#modifiers-algorithmicreverbmodifier)                                                          | Simulates reverberation using a network of comb and all-pass filters. Now supports multi-channel processing and MIDI control.                                                                         |\n| [`BassBoosterModifier`](#modifiers-bassboostmodifier)                                                                        | Enhances low-frequency content using a resonant low-pass filter. Supports MIDI control.                                                                                                               |\n| [`ChorusModifier`](#modifiers-chorusmodifier)                                                                                | Creates a chorus effect by mixing delayed and modulated copies of the signal. Supports MIDI control.                                                                                                  |\n| [`CompressorModifier`](#modifiers-compressormodifier)                                                                        | Reduces the dynamic range of the audio signal using a compressor algorithm.                                                                                                                           |\n| [`DelayModifier`](#modifiers-delaymodifier)                                                                                  | Applies a delay effect with feedback and optional low-pass filtering of the delayed signal. Supports MIDI control.                                                                                    |\n| [`Filter`](#modifiers-filter)                                                                                                | `SoundModifier` that applies a digital filter (low-pass, high-pass, band-pass, notch) to the audio signal. Supports MIDI control.                                                                     |\n| [`FrequencyBandModifier`](#modifiers-frequencybandmodifier)                                                                  | Allows boosting or cutting specific frequency bands using a combination of low-pass and high-pass filters.                                                                                            |\n| [`ParametricEqualizer`](#modifiers-parametricequalizer)                                                                      | Provides precise control over the frequency spectrum with multiple configurable bands, each of which can be set as a peaking, low-shelf, high-shelf, band-pass, notch, low-pass, or high-pass filter. |\n| [`MultiChannelChorusModifier`](#modifiers-multichannelchorusmodifier)                                                        | Creates a chorus effect with independent processing for each channel, allowing for rich spatial effects.                                                                                              |\n| [`ResamplerModifier`](#modifiers-resamplermodifier)                                                                          | A real-time resampling modifier that changes the playback speed and pitch of an audio signal.                                                                                                         |\n| [`TrebleBoostModifier`](#modifiers-trebleboostmodifier)                                                                      | Enhances high-frequency content using a high-pass filter. Supports MIDI control.                                                                                                                      |\n| [`VocalExtractorModifier`](#modifiers-vocalextractormodifier)                                                                | An advanced vocal extraction effect that isolates vocals using spatial (Mid-Side) processing for stereo pairs and spectral gating for mono channels.                                                  |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier) (from `SoundFlow.Extensions.WebRtc.Apm.Modifiers`) | Applies WebRTC APM features like echo cancellation, noise suppression, and AGC in real-time.                                                                                                          |\n\n### Providers\n\n| Class                                                         | Description                                                                                                                                  |\n| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AssetDataProvider`](#providers-assetdataprovider)           | `ISoundDataProvider` that decodes and loads an entire audio file into memory. Automatically detects format. Implements `IDisposable`.           |\n| [`StreamDataProvider`](#providers-streamdataprovider)         | `ISoundDataProvider` that reads and decodes audio from a `Stream` on-demand. Automatically detects format. Implements `IDisposable`.            |\n| [`MicrophoneDataProvider`](#providers-microphonedataprovider) | `ISoundDataProvider` that captures and provides audio data from an `AudioCaptureDevice` in real-time. Implements `IDisposable`.                 |\n| [`ChunkedDataProvider`](#providers-chunkeddataprovider)       | `ISoundDataProvider` that reads and decodes audio in chunks, improving efficiency for large files. Automatically detects format. Implements `IDisposable`. |\n| [`NetworkDataProvider`](#providers-networkdataprovider)       | `ISoundDataProvider` that provides audio data from a network source (direct URL or HLS playlist). Automatically detects format. Implements `IDisposable`. |\n| [`QueueDataProvider`](#providers-queuedataprovider)           | `ISoundDataProvider` fed by an external source in real-time, ideal for generated or procedural audio. Implements `IDisposable`.                |\n| [`RawDataProvider`](#providers-rawdataprovider)               | `ISoundDataProvider` for reading raw PCM audio data from a stream or array. Implements `IDisposable`.                                        |\n| [`MidiDataProvider`](#providers-mididataprovider)             | Processes a `MidiFile` into a single, time-ordered sequence of events for playback by a `Sequencer`.                                         |\n\n\n### Structs\n\n| Struct                                       | Description                                                                                    |\n| -------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| [`AudioFormat`](#structs-audioformat)        | (record struct) Represents the format of an audio stream (sample format, channels, layout, sample rate). |\n| [`DeviceInfo`](#structs-deviceinfo)          | Represents information about an audio device, including ID, name, and supported formats.         |\n| [`MidiDeviceInfo`](#structs-midideviceinfo)  | (record struct) Represents information about a MIDI device, including its backend-specific ID and name. |\n| [`NativeDataFormat`](#structs-nativedataformat) | Represents a native data format supported by an audio device (format, channels, sample rate). |\n| [`Result`](#structs-result)                  | (struct) Represents the outcome of an operation, which can be a success or a failure with an `IError`. |\n| [`IError`](#structs-ierror)                  | (interface) and various record implementations (`ValidationError`, `NotFoundError`, etc.) define a structured error handling system. |\n\n### Synthesis\n\n| Class/Interface | Description |\n|---|---|\n| [`Synthesizer`](#synthesis-synthesizer) | A polyphonic, multi-timbral `SoundComponent` that generates audio from MIDI messages using an `IInstrumentBank`. |\n| [`Sequencer`](#synthesis-sequencer) | A `SoundComponent` that plays back MIDI events from a `MidiDataProvider` with sample-accurate timing. |\n| [`IInstrumentBank`](#synthesis-iinstrumentbank) | Interface for a collection of instruments, accessible via MIDI bank and program numbers. |\n| [`SoundFontBank`](#synthesis-soundfontbank) | An `IInstrumentBank` implementation that loads instruments from a SoundFont 2 (SF2) file. |\n| [`BasicInstrumentBank`](#synthesis-basicinstrumentbank) | A simple, procedural `IInstrumentBank` implementation useful for testing and fallbacks. |\n| [`MultiInstrumentBank`](#synthesis-multiinstrumentbank) | A composite `IInstrumentBank` that layers multiple banks, allowing for stacked sounds and complex fallback chains. |\n\n### Synthesis.Instruments\n\n| Class                                         | Description                                                                                                                  |\n|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n| [`Instrument`](#synthesis-instruments-instrument) | Represents a single patch containing mappings that link key/velocity ranges to voice definitions.                            |\n| [`VoiceDefinition`](#synthesis-instruments-voicedefinition) | A factory blueprint for creating synthesizer voices with specific parameters (oscillator type, ADSR, sample data).           |\n| [`VoiceMapping`](#synthesis-instruments-voicemapping) | Defines the criteria (Min/Max Key, Min/Max Velocity) under which a specific `VoiceDefinition` is used.                       |\n\n### Synthesis.Interfaces\n\n| Interface                                   | Description                                                                                             |\n|---------------------------------------------|---------------------------------------------------------------------------------------------------------|\n| [`IGenerator`](#synthesis-interfaces-igenerator) | Interface for components in the voice signal chain (oscillators, samplers, envelopes) that generate audio or control signals. |\n| [`IVoice`](#synthesis-interfaces-ivoice)         | Interface representing an active, sounding voice within the synthesizer.                                |\n\n### Utils\n\n| Class                                           | Description                                                                                                                                             |\n|-------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`ChannelMixer`](#utils-channelmixer)           | High-performance static class for mixing audio channels between different layouts (e.g., stereo to mono).                                               |\n| [`Extensions`](#utils-extensions)               | Provides extension methods for working with audio data and other utility functions, including `ReadArray<T>` for reading structures from native memory. |\n| [`Log`](#utils-log)                             | Provides a centralized, decoupled logging mechanism. Subscribe to `Log.OnLog` to capture library messages.                                              |\n| [`MathHelper`](#utils-mathhelper)               | Provides mathematical functions, including optimized FFT, window functions, and a new `ResampleLinear` method.                                          |\n| [`MidiTimeConverter`](#utils-miditimeconverter) | Static utility to convert between MIDI ticks and `TimeSpan` based on a tempo map.                                                                       |\n| [`ControllableParameterAttribute`](#utils-controllableparameterattribute) | Attribute used to expose properties of `IMidiMappable` objects to the MIDI mapping system. |\n\n### Visualization\n\n| Class/Interface                                         | Description                                                                                                                                                                                      |\n| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`LevelMeterAnalyzer`](#visualization-levelmeteranalyzer) | `AudioAnalyzer` that calculates the RMS (root mean square) and peak levels of an audio signal.                                                                                                  |\n| [`LevelMeterVisualizer`](#visualization-levelmetervisualizer) | `IVisualizer` that displays a level meter showing the current RMS or peak level of the audio.                                                                                                 |\n| [`SpectrumAnalyzer`](#visualization-spectrumanalyzer)    | `AudioAnalyzer` that computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).                                                                                         |\n| [`SpectrumVisualizer`](#visualization-spectrumvisualizer)  | `IVisualizer` that renders a bar graph representing the frequency spectrum of the audio.                                                                                                        |\n| [`WaveformVisualizer`](#visualization-waveformvisualizer)  | `IVisualizer` that draws the waveform of the audio signal.                                                                                                                                     |\n\n\n## Detailed Class and Interface Documentation\n\nThis section provides more in-depth information about some of the key classes and interfaces.\n\n### Abstracts `AudioAnalyzer`\n\n```csharp\npublic abstract class AudioAnalyzer : IMidiMappable\n{\n    protected AudioAnalyzer(AudioFormat format, IVisualizer? visualizer = null);\n\n    public Guid Id { get; }\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n    public AudioFormat Format { get; }\n\n    public void Process(Span<float> buffer, int channels);\n    protected abstract void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Id`: A unique identifier for the analyzer instance, used for MIDI mapping.\n*   `Name`: The name of the analyzer.\n*   `Enabled`: Gets or sets whether the analyzer is active. If false, `Analyze` is skipped.\n*   `Format`: The audio format the analyzer is configured to process.\n\n**Methods:**\n\n*   `Process(Span<float> buffer, int channels)`: Processes the audio data, calling `Analyze` and then sending data to the attached visualizer.\n*   `Analyze(Span<float> buffer, int channels)`: Abstract method to be implemented by derived classes to perform audio analysis on the provided buffer.\n\n### Abstracts `AudioEngine`\n\n```csharp\npublic abstract class AudioEngine : IDisposable\n{\n    // Properties\n    public DeviceInfo[] PlaybackDevices { get; protected set; }\n    public DeviceInfo[] CaptureDevices { get; protected set; }\n    public MidiDeviceInfo[] MidiInputDevices { get; protected set; }\n    public MidiDeviceInfo[] MidiOutputDevices { get; protected set; }\n    public MidiManager MidiManager { get; }\n    public bool IsDisposed { get; private set; }\n\n    // Events\n    public event EventHandler<DeviceEventArgs>? DeviceStarted;\n    public event EventHandler<DeviceEventArgs>? DeviceStopped;\n    public event EventHandler<AudioFramesRenderedEventArgs>? AudioFramesRendered;\n\n    // Codec Management\n    public void RegisterCodecFactory(ICodecFactory factory);\n    public bool UnregisterCodecFactory(string factoryId);\n    public bool SetCodecPriority(string factoryId, int newPriority);\n    public IReadOnlyList<ICodecFactory> GetRegisteredCodecs(string formatId);\n    public ISoundEncoder CreateEncoder(Stream stream, string formatId, AudioFormat format);\n    public ISoundDecoder CreateDecoder(Stream stream, string formatId, AudioFormat format);\n    public ISoundDecoder CreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);\n\n    // MIDI\n    public void UseMidiBackend(IMidiBackend midiBackend);\n    public virtual void UpdateMidiDevicesInfo();\n\n    // Device Management\n    public abstract AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public abstract FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);\n    public abstract void UpdateAudioDevicesInfo();\n\n    // Component Management\n    public void SoloComponent(SoundComponent component);\n    public void UnsoloComponent(SoundComponent component);\n    public SoundComponent? GetSoloedComponent();\n\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `PlaybackDevices`: An array of available audio playback devices.\n*   `CaptureDevices`: An array of available audio capture devices.\n*   `MidiInputDevices`: An array of available MIDI input devices.\n*   `MidiOutputDevices`: An array of available MIDI output devices.\n*   `MidiManager`: The central manager for all MIDI devices and routing.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n\n**Events:**\n\n*   `DeviceStarted`: Occurs when an audio device begins processing, useful for synchronization.\n*   `DeviceStopped`: Occurs when an audio device stops processing.\n*   `AudioFramesRendered`: Occurs after a block of audio is rendered, providing a sample-accurate clock for master synchronization.\n\n**Methods:**\n\n*   `SoloComponent(SoundComponent component)`: Solos a specific component, muting all others.\n*   `UnsoloComponent(SoundComponent component)`: Removes the solo status from a component.\n*   `GetSoloedComponent()`: Returns the currently soloed component, if any.\n*   `UseMidiBackend(IMidiBackend midiBackend)`: Configures the engine to use a specific MIDI backend implementation (e.g., `PortMidiBackend`).\n*   `RegisterCodecFactory(ICodecFactory factory)`: Registers a codec factory to add support for new audio formats.\n*   `UnregisterCodecFactory(string factoryId)`: Unregisters a codec factory by its unique ID.\n*   `SetCodecPriority(string factoryId, int newPriority)`: Overrides the priority of a registered codec factory.\n*   `GetRegisteredCodecs(string formatId)`: Gets a list of registered factories for a specific format, ordered by priority.\n*   `CreateEncoder(Stream stream, string formatId, ...)`: Creates a sound encoder by querying registered factories for the specified format ID (e.g., \"wav\", \"mp3\").\n*   `CreateDecoder(Stream stream, string formatId, ...)`: Creates a sound decoder for a known format ID.\n*   `CreateDecoder(Stream stream, out AudioFormat, ...)`: Creates a sound decoder by probing the stream with all available codecs to automatically detect the format.\n*   `InitializePlaybackDevice(...)`: Initializes and returns a new playback device.\n*   `InitializeCaptureDevice(...)`: Initializes and returns a new capture device.\n*   `InitializeFullDuplexDevice(...)`: Initializes a high-level full-duplex device for simultaneous I/O.\n*   `InitializeLoopbackDevice(...)`: Initializes a device for loopback recording of system audio.\n*   `SwitchDevice(...)`: Switches an active device to a new physical device, preserving its state (components or subscribers).\n*   `UpdateAudioDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.\n*   `UpdateMidiDevicesInfo()`: Refreshes the `MidiInputDevices` and `MidiOutputDevices` lists from the configured MIDI backend.\n*   `Dispose()`: Disposes the engine and all associated resources.\n\n### Abstracts `AudioDevice`\n\n```csharp\npublic abstract class AudioDevice : IDisposable\n{\n    public AudioEngine Engine { get; }\n    public DeviceInfo? Info { get; protected init; }\n    public DeviceConfig Config { get; protected init; }\n    public Capability Capability { get; protected init; }\n    public AudioFormat Format { get; }\n    public bool IsRunning { get; protected set; }\n    public bool IsDisposed { get; protected set; }\n\n    public event EventHandler? OnDisposed;\n\n    public abstract void Start();\n    public abstract void Stop();\n    public abstract void Dispose();\n}\n```\n\n**Description:** An abstract base class representing an initialized audio device, which is managed by an `AudioEngine`. This class encapsulates the common state and behavior for both playback and capture devices, serving as the foundation for all device interactions within the framework.\n\n**Properties:**\n\n*   `Engine`: Gets the parent `AudioEngine` that manages this device instance.\n*   `Info`: Gets the informational struct (`DeviceInfo`) for the physical device being used. This contains details like the device name and ID. `null` if a default device was used without specific info.\n*   `Config`: Gets the `DeviceConfig` object used to initialize the device. This holds any backend-specific configuration settings.\n*   `Capability`: Gets the capability of this device (e.g., `Playback`, `Record`, `Loopback`).\n*   `Format`: Gets the `AudioFormat` (sample rate, channels, bit depth) that the device was initialized with.\n*   `IsRunning`: Gets a value indicating whether the device is currently started and processing audio.\n*   `IsDisposed`: Gets a value indicating whether this device instance has been disposed and can no longer be used.\n\n**Events:**\n\n*   `OnDisposed`: This event is raised when the device's `Dispose()` method is called, signaling that it is being shut down and its resources are being released.\n\n**Methods:**\n\n*   `Start()`: Abstract method that must be implemented by derived classes to start the audio stream for the device.\n*   `Stop()`: Abstract method that must be implemented by derived classes to stop the audio stream for the device.\n*   `Dispose()`: Abstract method to release all resources used by the audio device.\n\n### Abstracts `AudioPlaybackDevice`\n\n```csharp\npublic abstract class AudioPlaybackDevice : AudioDevice\n{\n    public Mixer MasterMixer { get; }\n}\n```\n\n**Description:** An abstract class that represents an initialized playback (output) audio device. It inherits from `AudioDevice` and extends it with functionality specific to audio output.\n\n**Properties:**\n\n*   `MasterMixer`: Gets the master `Mixer` for this device. All audio to be played on this device must be routed through this mixer. You can connect various `SoundComponent`s (like `SoundPlayer`, `Oscillator`, or other `Mixer`s) to this master mixer to combine them into a final output signal.\n\n### Abstracts `AudioCaptureDevice`\n\n```csharp\npublic abstract class AudioCaptureDevice : AudioDevice\n{\n    public event AudioProcessCallback? OnAudioProcessed;\n    internal Delegate[] GetEventSubscribers();\n}\n```\n\n**Description:** An abstract class that represents an initialized capture (input) audio device. It inherits from `AudioDevice` and provides the core mechanism for receiving audio data from an input source like a microphone.\n\n**Events:**\n\n*   `OnAudioProcessed`: This event is the primary way to receive audio data from the capture device. It is raised by the backend whenever a new chunk of audio samples has been captured. Subscribe to this event to process live audio input. The event delegate is `AudioProcessCallback(Span<float> samples, Capability capability)`.\n\n**Methods:**\n\n*   `GetEventSubscribers()`: (Internal) An internal method used by the `AudioEngine` to retrieve the list of subscribers to the `OnAudioProcessed` event. This is crucial for the device switching functionality, allowing event subscriptions to be preserved when moving from one physical device to another.\n\n### Abstracts `FullDuplexDevice`\n\n```csharp\npublic sealed class FullDuplexDevice : AudioDevice, IDisposable\n{\n    public AudioPlaybackDevice PlaybackDevice { get; }\n    public AudioCaptureDevice CaptureDevice { get; }\n    public Mixer MasterMixer => PlaybackDevice.MasterMixer;\n\n    public event AudioProcessCallback? OnAudioProcessed;\n\n    public override void Start();\n    public override void Stop();\n    public override void Dispose();\n}\n```\n\n**Description:** A high-level, sealed class that simplifies full-duplex (simultaneous input and output) audio operations. It internally manages a paired `AudioPlaybackDevice` and `AudioCaptureDevice`, making it ideal for applications like live effects processing, VoIP, or real-time instrument monitoring where you need to listen to an input while producing an output.\n\n**Properties:**\n\n*   `PlaybackDevice`: Gets the underlying `AudioPlaybackDevice` instance used for audio output.\n*   `CaptureDevice`: Gets the underlying `AudioCaptureDevice` instance used for audio input.\n*   `MasterMixer`: A convenient shortcut to access the `MasterMixer` of the underlying `PlaybackDevice`. This is where you should route all audio you want to play out.\n\n**Events:**\n\n*   `OnAudioProcessed`: An event that is raised when audio data is captured from the input device. This event is a direct pass-through to the `OnAudioProcessed` event of the underlying `CaptureDevice`.\n\n**Methods:**\n\n*   `Start()`: Starts both the underlying capture and playback devices simultaneously.\n*   `Stop()`: Stops both the underlying capture and playback devices simultaneously.\n*   `Dispose()`: Stops and disposes of all resources, including the underlying `PlaybackDevice` and `CaptureDevice`.\n\n### Abstracts `DeviceConfig`\n\n```csharp\npublic abstract class DeviceConfig;\n```\n**Description:** A marker base class for creating backend-specific device configuration objects. This allows passing detailed, implementation-specific settings during device initialization. See `MiniAudioDeviceConfig` for an example.\n\n### Abstracts `SoundComponent`\n\n```csharp\npublic abstract class SoundComponent : IDisposable, IMidiMappable\n{\n    protected SoundComponent(AudioEngine engine, AudioFormat format);\n\n    public Guid Id { get; }\n    public AudioEngine Engine { get; }\n    public AudioFormat Format { get; }\n    public virtual string Name { get; set; }\n    public Mixer? Parent { get; set; }\n    public virtual float Volume { get; set; }\n    public virtual float Pan { get; set; } // Range 0.0 (Left) to 1.0 (Right)\n    public virtual bool Enabled { get; set; }\n    public virtual bool Solo { get; set; }\n    public virtual bool Mute { get; set; }\n    public bool IsDisposed { get; private set; }\n\n    public IReadOnlyList<SoundComponent> Inputs { get; }\n    public IReadOnlyList<SoundModifier> Modifiers { get; }\n    public IReadOnlyList<AudioAnalyzer> Analyzers { get; }\n\n    public void ConnectInput(SoundComponent input);\n    public void DisconnectInput(SoundComponent input);\n    public void AddModifier(SoundModifier modifier);\n    public void RemoveModifier(SoundModifier modifier);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    internal void Process(Span<float> outputBuffer, int channels);\n    protected abstract void GenerateAudio(Span<float> buffer, int channels);\n    public virtual void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Id`: A unique identifier for the component instance, used for MIDI mapping.\n*   `Engine`: The engine context this component belongs to.\n*   `Format`: The audio format of this component.\n*   `Name`: The name of the component.\n*   `Parent`: The parent mixer of this component.\n*   `Inputs`: Read-only list of connected input components.\n*   `Modifiers`: Read-only list of applied modifiers.\n*   `Analyzers`: Read-only list of attached audio analyzers.\n*   `Volume`: The volume of the component's output.\n*   `Pan`: The panning of the component's output (0.0 for full left, 0.5 for center, 1.0 for full right).\n*   `Enabled`: Whether the component is enabled.\n*   `Solo`: Whether the component is soloed.\n*   `Mute`: Whether the component is muted.\n*   `IsDisposed`: Indicates whether the component has been disposed.\n\n**Methods:**\n\n*   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n*   `DisconnectInput(SoundComponent input)`: Disconnects an input from this component.\n*   `AddModifier(SoundModifier modifier)`: Adds a sound modifier to the component.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a sound modifier from the component.\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an audio analyzer to the component.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an audio analyzer from the component.\n*   `Process(Span<float> outputBuffer, int channels)`: Processes the component's audio, applying modifiers and handling input/output connections.\n*   `GenerateAudio(Span<float> buffer, int channels)`: Abstract method that derived classes must implement to generate or modify audio data.\n*   `Dispose()`: Disposes the component and disconnects it from the audio graph.\n\n### Abstracts `SoundModifier`\n\n```csharp\npublic abstract class SoundModifier : IMidiMappable, IMidiControllable\n{\n    public Guid Id { get; }\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    public virtual void ProcessMidiMessage(MidiMessage message);\n    public abstract float ProcessSample(float sample, int channel);\n    public virtual void Process(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Id`: A unique identifier for the modifier instance, used for MIDI mapping.\n*   `Name`: The name of the modifier.\n*   `Enabled`: Gets or sets whether the modifier is active and should process audio. Defaults to true.\n\n**Methods:**\n\n*   `ProcessMidiMessage(MidiMessage message)`: A virtual method that allows derived classes to be controlled by MIDI messages.\n*   `ProcessSample(float sample, int channel)`: Abstract method to be implemented by derived classes to process a single audio sample.\n*   `Process(Span<float> buffer, int channels)`: Processes a buffer of audio data. Applies `ProcessSample` to each sample if not overridden.\n\n### Abstracts `SoundPlayerBase`\n\n```csharp\npublic abstract class SoundPlayerBase : SoundComponent, ISoundPlayer\n{\n    protected SoundPlayerBase(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n\n    public float PlaybackSpeed { get; set; }\n    public PlaybackState State { get; private set; }\n    public bool IsLooping { get; set; }\n    public float Time { get; }\n    public float Duration { get; }\n    public int LoopStartSamples { get; }\n    public int LoopEndSamples { get; }\n    public float LoopStartSeconds { get; }\n    public float LoopEndSeconds { get; }\n    // Volume is inherited from SoundComponent\n\n    public event EventHandler<EventArgs>? PlaybackEnded;\n\n    protected override void GenerateAudio(Span<float> output, int channels);\n    protected virtual void HandleEndOfStream(Span<float> remainingOutputBuffer, int channels);\n    protected virtual void OnPlaybackEnded();\n\n    public void Play();\n    public void Pause();\n    public void Stop();\n    public bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    public bool Seek(float time);\n    public bool Seek(int sampleOffset);\n    public void SetLoopPoints(float startTime, float? endTime = -1f);\n    public void SetLoopPoints(int startSample, int endSample = -1);\n    public void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `PlaybackSpeed`: Gets or sets the playback speed (1.0 is normal). Values other than 1.0 use a WSOLA time stretcher for pitch preservation.\n*   `State`: Gets the current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Gets or sets whether looping is enabled.\n*   `Time`: Gets the current playback position in seconds, affected by `PlaybackSpeed`.\n*   `Duration`: Gets the total duration of the audio in seconds.\n*   `LoopStartSamples`: Gets the loop start point in samples.\n*   `LoopEndSamples`: Gets the loop end point in samples (-1 for end of audio).\n*   `LoopStartSeconds`: Gets the loop start point in seconds.\n*   `LoopEndSeconds`: Gets the loop end point in seconds (-1 for end of audio).\n*   `Volume`: (Inherited from `SoundComponent`) Gets or sets the volume of the player.\n\n**Events:**\n\n*   `PlaybackEnded`: Occurs when playback reaches the end of the audio (not raised during looping).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> output)`: (Protected Override) Core audio generation logic, handles reading from data provider, resampling for playback speed, and looping.\n*   `HandleEndOfStream(Span<float> remainingOutputBuffer)`: (Protected Virtual) Handles logic when the data provider reaches its end (looping or stopping).\n*   `OnPlaybackEnded()`: (Protected Virtual) Invokes the `PlaybackEnded` event.\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to a specific time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to a specific time in seconds. Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to a specific sample offset. Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures loop points using start/end times in seconds.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures loop points using start/end sample indices.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures loop points using `TimeSpan`.\n*   `Dispose()`: Disposes the player and its underlying data provider.\n\n### Abstracts `WsolaTimeStretcher`\n```csharp\npublic class WsolaTimeStretcher\n{\n    public WsolaTimeStretcher(int initialChannels = 2, float initialSpeed = 1.0f);\n\n    public void SetChannels(int channels);\n    public void SetSpeed(float speed);\n    public int MinInputSamplesToProcess { get; }\n    public void Reset();\n    public float GetTargetSpeed();\n    public int Process(ReadOnlySpan<float> input, Span<float> output, out int samplesConsumedFromInputBuffer, out int sourceSamplesRepresentedByOutput);\n    public int Flush(Span<float> output);\n}\n```\n**Description:** Implements the WSOLA (Waveform Similarity Overlap-Add) algorithm for real-time, pitch-preserved time stretching of audio. Allows changing playback speed without altering pitch. Used internally by `SoundPlayerBase` and `AudioSegment`.\n\n### Backends.MiniAudio `MiniAudioCodecFactory`\n```csharp\npublic sealed class MiniAudioCodecFactory : ICodecFactory\n```\n**Description:** The built-in codec factory registered by `MiniAudioEngine` with priority 0. Supports `wav`, `mp3`, and `flac` decoding via the native `miniaudio` library.\n\n### Backends.MiniAudio `MiniAudioDecoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioDecoder : ISoundDecoder\n{\n    internal MiniAudioDecoder(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n    public int Length { get; private set; } // Length can be updated after initial check\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int Decode(Span<float> samples);\n    public void Dispose();\n    public bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n* `IsDisposed`: Indicates whether the decoder has been disposed.\n* `Length`: The total length of the decoded audio data in samples. *Note: Can be updated after initial checks if the stream length was not immediately available.*\n* `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n* `EndOfStreamReached`: Occurs when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n* `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer. Internally synchronized.\n* `Dispose()`: Releases the resources used by the decoder.\n* `Seek(int offset)`: Seeks to the specified offset within the audio stream (in samples). Internally synchronized.\n\n### Backends.MiniAudio `MiniAudioEncoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioEncoder : ISoundEncoder\n{\n    public MiniAudioEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n\n    public void Dispose();\n    public int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the encoder.\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples and writes them to the output file or stream.\n\n### Backends.MiniAudio `MiniAudioEngine`\n\n```csharp\npublic class MiniAudioEngine : AudioEngine\n{\n    public MiniAudioEngine(IEnumerable<MiniAudioBackend>? backendPriority = null);\n\n    public static IReadOnlyList<MiniAudioBackend> AvailableBackends { get; }\n    public MiniAudioBackend ActiveBackend { get; }\n\n    // Inherits all public methods from AudioEngine\n    public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);\n    public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format);\n    public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format);\n    public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);\n\n    public override void UpdateAudioDevicesInfo();\n}\n```\n\n**Description:** This is the concrete implementation of the abstract `AudioEngine` class using the powerful `miniaudio` C library as its backend. It is responsible for all low-level audio I/O operations, including device discovery, initialization, and data processing callbacks. It manages the native `miniaudio` context and registers the default `MiniAudioCodecFactory` for WAV, MP3, and FLAC support. Because it handles the native interop, this class ensures that SoundFlow is cross-platform.\n\n**Constructor:**\n\n*   `MiniAudioEngine(IEnumerable<MiniAudioBackend>? backendPriority = null)`: Initializes the engine. Optionally accepts a prioritized list of backends to attempt to use.\n\n**Properties:**\n\n*   `AvailableBackends`: A static list of `MiniAudioBackend` enums that are supported on the current operating system.\n*   `ActiveBackend`: The low-level audio backend that was successfully initialized and is currently in use.\n\n**Methods:**\n\n*   `InitializePlaybackDevice(...)`: Creates and returns a playback device connected to a physical sound output device.\n*   `InitializeCaptureDevice(...)`: Creates and returns a capture device for a physical sound input device.\n*   `InitializeFullDuplexDevice(...)`: Creates and returns a `FullDuplexDevice` that manages both a playback and capture stream simultaneously.\n*   `InitializeLoopbackDevice(...)`: Creates a special capture device that records the system's audio output. On Windows, this uses the WASAPI loopback feature.\n*   `CreateEncoder(...)`: Returns a `MiniAudioEncoder` instance, currently supporting `.wav` file encoding.\n*   `CreateDecoder(...)`: Returns a `MiniAudioDecoder` instance, capable of decoding various audio formats like `.wav`, `.mp3`, and `.flac`.\n*   `SwitchDevice(...)`: Implements the logic to seamlessly switch between physical audio devices at runtime, preserving the state of the audio graph.\n*   `UpdateAudioDevicesInfo()`: Communicates with the `miniaudio` backend to refresh the lists of available playback and capture devices.\n\n---\n\n### Backends.MiniAudio `MiniAudioDeviceConfig`\n\n```csharp\npublic class MiniAudioDeviceConfig : DeviceConfig\n{\n    public uint PeriodSizeInFrames { get; set; }\n    public uint PeriodSizeInMilliseconds { get; set; }\n    public uint Periods { get; set; }\n    public bool NoPreSilencedOutputBuffer { get; set; }\n    public bool NoClip { get; set; }\n    public bool NoDisableDenormals { get; set; }\n    public bool NoFixedSizedCallback { get; set; }\n    public DeviceSubConfig Playback { get; set; }\n    public DeviceSubConfig Capture { get; set; }\n    public WasapiSettings? Wasapi { get; set; }\n    public CoreAudioSettings? CoreAudio { get; set; }\n    public AlsaSettings? Alsa { get; set; }\n    public PulseSettings? Pulse { get; set; }\n    public OpenSlSettings? OpenSL { get; set; }\n    public AAudioSettings? AAudio { get; set; }\n}\n```\n\n**Description:** A detailed configuration object that inherits from `DeviceConfig` and is specifically designed for initializing a MiniAudio device. It provides fine-grained control over buffer sizes, performance flags, and exposes nested configuration classes for OS-specific audio backends like WASAPI (Windows), CoreAudio (macOS), ALSA (Linux), and others. This allows developers to tune performance and behavior for specific platforms.\n\n**General Properties:**\n\n*   `PeriodSizeInFrames`: Gets or sets the desired size of the internal processing buffer in frames (a frame is one sample for each channel). This gives precise, sample-level control over buffer latency. Takes precedence over `PeriodSizeInMilliseconds`. Default is 0 (backend default).\n*   `PeriodSizeInMilliseconds`: Gets or sets the desired size of the internal processing buffer in milliseconds. A more intuitive way to control latency. Default is 0 (backend default).\n*   `Periods`: Gets or sets the number of periods to use for the device's buffer. Default is 0 (backend default).\n*   `NoPreSilencedOutputBuffer`: If `true`, the output buffer passed to the audio callback will contain undefined data instead of being cleared to silence. This can be a minor performance optimization if you are always filling the entire buffer. Default is `false`.\n*   `NoClip`: If `true`, the backend will not clip F32 sample values that are outside the [-1.0, 1.0] range. Default is `false`.\n*   `NoDisableDenormals`: If `true`, the backend will not attempt to disable denormal floating-point numbers, which can slightly improve precision at the cost of performance on some CPUs. Default is `false`.\n*   `NoFixedSizedCallback`: If `true`, the backend is not required to provide buffers of a fixed size in every callback. This can be an optimization if your processing logic is flexible. Default is `false`.\n\n**Sub-Configurations:**\n\n*   `Playback`: A `DeviceSubConfig` object for playback-specific settings.\n*   `Capture`: A `DeviceSubConfig` object for capture-specific settings.\n*   `Wasapi`: A `WasapiSettings` object for Windows-specific settings. Only used on Windows.\n*   `CoreAudio`: A `CoreAudioSettings` object for macOS/iOS-specific settings. Only used on Apple platforms.\n*   `Alsa`: An `AlsaSettings` object for Linux-specific settings. Only used on Linux with ALSA.\n*   `Pulse`: A `PulseSettings` object for Linux-specific settings. Only used on Linux with PulseAudio.\n*   `OpenSL`: An `OpenSlSettings` object for Android-specific settings.\n*   `AAudio`: An `AAudioSettings` object for modern Android-specific settings.\n\n---\n\n#### `DeviceSubConfig` (Nested Class)\n\n```csharp\npublic class DeviceSubConfig\n{\n    public ShareMode ShareMode { get; set; } = ShareMode.Shared;\n    internal bool IsLoopback { get; set; }\n}\n```\n\n**Description:** Contains settings for a specific direction (playback or capture).\n\n**Properties:**\n\n*   `ShareMode`: Specifies how the device is opened. `ShareMode.Shared` (default) allows multiple applications to use the device. `ShareMode.Exclusive` attempts to gain exclusive control for the lowest possible latency, but may not be supported by all devices.\n*   `IsLoopback`: (Internal) A flag used to indicate that a capture device should be initialized in loopback mode.\n\n---\n\n#### `WasapiSettings` (Nested Class)\n\n```csharp\npublic class WasapiSettings\n{\n    public WasapiUsage Usage { get; set; } = WasapiUsage.Default;\n    public bool NoAutoConvertSRC { get; set; }\n    public bool NoDefaultQualitySRC { get; set; }\n    public bool NoAutoStreamRouting { get; set; }\n    public bool NoHardwareOffloading { get; set; }\n}\n```\n**Description:** Contains settings specific to the WASAPI audio backend on Windows.\n\n**Properties:**\n\n*   `Usage`: Hints to the OS about the stream's purpose (`Default`, `Games`, `ProAudio`), which can affect system-level audio processing and prioritization.\n*   `NoAutoConvertSRC`: If `true`, disables automatic sample rate conversion by WASAPI, letting MiniAudio handle it instead.\n*   `NoDefaultQualitySRC`: If `true`, prevents WASAPI from using its default quality for sample rate conversion.\n*   `NoAutoStreamRouting`: If `true`, disables automatic stream routing by the OS.\n*   `NoHardwareOffloading`: If `true`, disables WASAPI's hardware offloading feature.\n\n---\n\n#### `CoreAudioSettings` (Nested Class)\n\n```csharp\npublic class CoreAudioSettings\n{\n    public bool AllowNominalSampleRateChange { get; set; }\n}\n```\n**Description:** Contains settings specific to the CoreAudio backend on macOS and iOS.\n\n**Properties:**\n\n*   `AllowNominalSampleRateChange`: If `true`, allows the OS to change the device's sample rate to match the stream. Typically used on desktop macOS.\n\n---\n\n#### `AlsaSettings` (Nested Class)\n\n```csharp\npublic class AlsaSettings\n{\n    public bool NoMMap { get; set; }\n    public bool NoAutoFormat { get; set; }\n    public bool NoAutoChannels { get; set; }\n    public bool NoAutoResample { get; set; }\n}\n```\n**Description:** Contains settings specific to the ALSA audio backend on Linux.\n\n**Properties:**\n\n*   `NoMMap`: If `true`, disables memory-mapped (MMap) mode for ALSA.\n*   `NoAutoFormat`: If `true`, prevents ALSA from performing automatic format conversion.\n*   `NoAutoChannels`: If `true`, prevents ALSA from performing automatic channel count conversion.\n*   `NoAutoResample`: If `true`, prevents ALSA from performing automatic resampling.\n\n---\n\n#### `PulseSettings` (Nested Class)\n\n```csharp\npublic class PulseSettings\n{\n    public string? StreamNamePlayback { get; set; }\n    public string? StreamNameCapture { get; set; }\n}\n```\n**Description:** Contains settings specific to the PulseAudio backend on Linux.\n\n**Properties:**\n\n*   `StreamNamePlayback`: Sets a custom name for the playback stream as it appears in PulseAudio volume controls.\n*   `StreamNameCapture`: Sets a custom name for the capture stream.\n\n---\n\n#### `OpenSlSettings` (Nested Class)\n\n```csharp\npublic class OpenSlSettings\n{\n    public OpenSlStreamType StreamType { get; set; }\n    public OpenSlRecordingPreset RecordingPreset { get; set; }\n}\n```\n**Description:** Contains settings specific to the OpenSL ES backend on Android.\n\n**Properties:**\n\n*   `StreamType`: Specifies the type of audio stream (e.g., `Voice`, `Media`, `Alarm`) to help Android manage audio focus and routing.\n*   `RecordingPreset`: Optimizes the microphone input for a specific scenario (e.g., `VoiceCommunication`, `Camcorder`).\n\n---\n\n#### `AAudioSettings` (Nested Class)\n\n```csharp\npublic class AAudioSettings\n{\n    public AAudioUsage Usage { get; set; }\n    public AAudioContentType ContentType { get; set; }\n    public AAudioInputPreset InputPreset { get; set; }\n    public AAudioAllowedCapturePolicy AllowedCapturePolicy { get; set; }\n}\n```\n**Description:** Contains settings specific to the modern AAudio backend on Android.\n\n**Properties:**\n\n*   `Usage`: Hints to the system about the stream's purpose (e.g., `Media`, `Game`, `Assistant`) for optimized routing and resource management.\n*   `ContentType`: Describes the type of content being played (e.g., `Music`, `Speech`, `Sonification`).\n*   `InputPreset`: Specifies a configuration for the audio input, optimizing it for scenarios like `VoiceRecognition` or `Camcorder`.\n*   `AllowedCapturePolicy`: Controls whether other applications are allowed to capture the audio from this stream.\n\n### Components `EnvelopeGenerator`\n\n```csharp\npublic class EnvelopeGenerator : SoundComponent\n{\n    public EnvelopeGenerator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public EnvelopeState State { get; }\n    public float AttackTime { get; set; }\n    public float DecayTime { get; set; }\n    public override string Name { get; set; }\n    public float ReleaseTime { get; set; }\n    public bool Retrigger { get; set; }\n    public float SustainLevel { get; set; }\n    public TriggerMode Trigger { get; set; }\n\n    public event Action<float>? LevelChanged;\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public float ProcessSample();\n    public void TriggerOff();\n    public void TriggerOn();\n}\n```\n\n**Properties:**\n\n*   `State`: Gets the current stage of the envelope (`Idle`, `Attack`, etc.).\n*   `AttackTime`: The attack time of the envelope (in seconds).\n*   `DecayTime`: The decay time of the envelope (in seconds).\n*   `Name`: The name of the envelope generator.\n*   `ReleaseTime`: The release time of the envelope (in seconds).\n*   `Retrigger`: Whether to retrigger the envelope on each new trigger.\n*   `SustainLevel`: The sustain level of the envelope.\n*   `Trigger`: The trigger mode (`NoteOn`, `Gate`, `Trigger`).\n\n**Events:**\n\n*   `LevelChanged`: Occurs when the envelope level changes.\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Fills a buffer with the envelope signal.\n*   `ProcessSample()`: Processes and returns a single sample of the envelope's output.\n*   `TriggerOff()`: Triggers the release stage of the envelope.\n*   `TriggerOn()`: Triggers the attack stage of the envelope.\n\n### Components `LowFrequencyOscillator`\n\n```csharp\npublic class LowFrequencyOscillator : SoundComponent\n{\n    public LowFrequencyOscillator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float Depth { get; set; }\n    public TriggerMode Mode { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float Rate { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public float GetLastOutput();\n    public void Trigger();\n}\n```\n\n**Properties:**\n\n*   `Depth`: The depth of the LFO's modulation.\n*   `Mode`: The trigger mode (`FreeRunning`, `NoteTrigger`).\n*   `Name`: The name of the LFO.\n*   `Phase`: The initial phase of the LFO.\n*   `Rate`: The rate (frequency) of the LFO.\n*   `Type`: The waveform type (`Sine`, `Square`, `Triangle`, `Sawtooth`, `ReverseSawtooth`, `Random`, `SampleAndHold`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the LFO signal.\n*   `GetLastOutput()`: Returns the last generated output sample.\n*   `Trigger()`: Triggers the LFO (if in `NoteTrigger` mode).\n\n### Components `Mixer`\n\n```csharp\npublic sealed class Mixer : SoundComponent\n{\n    public Mixer(AudioEngine engine, AudioFormat format, bool isMasterMixer = false);\n\n    public IReadOnlyCollection<SoundComponent> Components { get; }\n    public AudioPlaybackDevice? ParentDevice { get; internal set; }\n    public bool IsMasterMixer { get; }\n    public override string Name { get; set; }\n\n    public void AddComponent(SoundComponent component);\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public void RemoveComponent(SoundComponent component);\n    public override void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Components`: A read-only collection of sound components mixed by this mixer.\n*   `ParentDevice`: The playback device this mixer is the master for, if any.\n*   `IsMasterMixer`: A value indicating whether this is a master mixer for a device.\n*   `Name`: The name of the mixer.\n\n**Methods:**\n\n*   `AddComponent(SoundComponent component)`: Adds a component to the mixer.\n*   `GenerateAudio(Span<float> buffer, int channels)`: Mixes the audio from all connected components.\n*   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n*   `Dispose()`: Disposes the mixer and all components within it.\n\n### Components `Oscillator`\n\n```csharp\npublic class Oscillator : SoundComponent\n{\n    public Oscillator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float Amplitude { get; set; }\n    public float Frequency { get; set; }\n    public override string Name { get; set; }\n    public float PhaseOffset { get; set; }\n    public float PulseWidth { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Amplitude`: The amplitude of the oscillator.\n*   `Frequency`: The frequency of the oscillator.\n*   `Name`: The name of the oscillator.\n*   `PhaseOffset`: The phase offset of the waveform in radians.\n*   `PulseWidth`: The pulse width (for pulse waveforms).\n*   `Type`: The band-limited waveform type (`Sine`, `Square`, `Sawtooth`, `Triangle`, `Noise`, `Pulse`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the oscillator's output.\n\n### Components `Recorder`\n\n```csharp\npublic class Recorder : IDisposable\n{\n    public Recorder(AudioCaptureDevice captureDevice, string filePath, string formatId = \"wav\");\n    public Recorder(AudioCaptureDevice captureDevice, Stream stream, string formatId = \"wav\");\n    public Recorder(AudioCaptureDevice captureDevice, AudioProcessCallback callback);\n\n    public PlaybackState State { get; private set; }\n    public readonly SampleFormat SampleFormat;\n    public readonly string FormatId;\n    public readonly string? FilePath;\n    public readonly int SampleRate;\n    public readonly int Channels;\n    public readonly Stream Stream;\n    public AudioProcessCallback? ProcessCallback;\n    public ReadOnlyCollection<SoundModifier> Modifiers { get; }\n    public ReadOnlyCollection<AudioAnalyzer> Analyzers { get; }\n\n    public void StartRecording(SoundTags? tags = null);\n    public void ResumeRecording();\n    public void PauseRecording();\n    public Task StopRecordingAsync();\n    public void StopRecording();\n    public void AddModifier(SoundModifier modifier);\n    public void RemoveModifier(SoundModifier modifier);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Analyzers`: Gets a read-only collection of 'AudioAnalyzer\" components applied to the recorder. Analyzers are used to process and extract data from the audio stream during recording.\n*   `Modifiers`: Gets a read-only collection of \"SoundModifier\" components applied to the recorder. Modifiers are applied to the audio data before encoding or processing via callback, allowing for real-time audio effects during recording.\n*   `Channels`: The number of channels to record.\n*   `State`: The current recording state (`Stopped`, `Playing`, `Paused`).\n*   `FormatId`: The string identifier for the encoding format (e.g., \"wav\", \"flac\").\n*   `FilePath`: The final destination file path, if recording to a file.\n*   `Stream`: The stream to write encoded recorded audio to.\n*   `ProcessCallback`: A callback for processing recorded audio in real time.\n*   `SampleRate`: The sample rate for recording.\n*   `SampleFormat`: The sample format for recording.\n\n**Methods:**\n\n*   `StartRecording(SoundTags? tags = null)`: Starts the recording. Optionally accepts metadata tags to be written to the file upon completion.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `PauseRecording()`: Pauses the recording.\n*   `StopRecordingAsync()`: Asynchronously stops the recording, finalizes the file, and writes metadata tags if provided.\n*   `StopRecording()`: Synchronously stops the recording.\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an \"AudioAnalyzer\" to the recording pipeline. Analyzers process audio data during recording, enabling real-time analysis.\n*   `AddModifier(SoundModifier modifier)`: Adds a \"SoundModifier\" to the recording pipeline. Modifiers apply effects to the audio data in real-time as it's being recorded.\n*   `Dispose()`: Releases resources used by the recorder.\n*   `PauseRecording()`: Pauses the recording.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes a specific \"AudioAnalyzer\" from the recording pipeline.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a specific \"SoundModifier\" from the recording pipeline.\n*   `Dispose()`: Stops recording and releases all resources.\n\n### Components `SoundPlayer`\n\n```csharp\npublic sealed class SoundPlayer : SoundPlayerBase\n{\n    public SoundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n}\n```\nInherits all playback functionality, properties, and events from `SoundPlayerBase`.\n\n**Properties:**\n* `Name`: The name of the sound player component (default: \"Sound Player\").\n\n### Components `SurroundPlayer`\n\n```csharp\npublic sealed class SurroundPlayer : SoundPlayerBase\n{\n    public SurroundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n    public Vector2 ListenerPosition { get; set; }\n    public PanningMethod Panning { get; set; }\n    public VbapParameters VbapParameters { get; set; }\n    public SurroundConfiguration SurroundConfig { get; set; }\n    public SpeakerConfiguration SpeakerConfig { get; set; }\n\n    protected override void GenerateAudio(Span<float> output, int channels);\n    public void SetSpeakerConfiguration(SpeakerConfiguration config);\n    // Inherits Play, Pause, Stop, Seek, IsLooping, etc. from SoundPlayerBase\n}\n```\nInherits base playback functionality from `SoundPlayerBase` and adds surround-specific features with improved channel mapping for various source layouts (mono, stereo, 5.1, 7.1, etc.).\n\n**Properties:**\n*   `Name`: The name of the surround player component (default: \"Surround Player\").\n*   `ListenerPosition`: The position of the listener in the surround sound field (Vector2).\n*   `Panning`: Gets or sets the panning method to use for surround sound (`Linear`, `EqualPower`, `VBAP`).\n*   `SpeakerConfig`: Gets or sets the speaker configuration (`Stereo`, `Quad`, `Surround51`, `Surround71`, `Custom`).\n*   `VbapParameters`: Gets or sets parameters for Vector Base Amplitude Panning (VBAP).\n*   `SurroundConfig`: Gets or sets the custom surround configuration when `SpeakerConfig` is set to `Custom`.\n\n**Methods:**\n*   `GenerateAudio(Span<float> output, int channels)`: (Overrides `SoundPlayerBase`) Reads audio data, applies resampling, then applies surround processing (including upmixing/downmixing) and looping if enabled.\n*   `SetSpeakerConfiguration(SpeakerConfiguration config)`: Sets the speaker configuration for surround sound playback.\n*   `Seek(int sampleOffset)`: (Overrides `SoundPlayerBase`) Seeks and re-initializes delay lines for surround processing.\n\n\n### Components `VoiceActivityDetector`\n\n```csharp\npublic class VoiceActivityDetector : AudioAnalyzer\n{\n    public VoiceActivityDetector(AudioFormat format, int fftSize = 1024, float energyThreshold = 5f, IVisualizer? visualizer = null);\n\n    public bool IsVoiceActive { get; private set; }\n    public float EnergyThreshold { get; set; }\n    public float ActivationTimeMs { get; set; }\n    public float HangoverTimeMs { get; set; }\n    public int SpeechLowBand { get; set; }\n    public int SpeechHighBand { get; set; }\n\n    public override string Name { get; set; }\n\n    public event Action<bool>? SpeechDetected;\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: Gets or sets the name of the voice activity detector component, useful for identification and debugging.\n*   `IsVoiceActive`: Read-only property indicating if voice is currently detected.\n*   `EnergyThreshold`: The energy threshold for detection.\n*   `ActivationTimeMs`: Time in milliseconds the signal must be considered speech before activation. Helps prevent short noise bursts from triggering.\n*   `HangoverTimeMs`: Time in milliseconds to keep the VAD active after the last speech frame. Prevents deactivation during short pauses.\n*   `SpeechLowBand`/`SpeechHighBand`: The frequency range (in Hz) to analyze for speech.\n\n**Events:**\n\n*   `SpeechDetected`: An event that is raised whenever the voice activity state changes (i.e., when speech is detected or ceases to be detected). Listeners can subscribe to this event to respond in real-time to changes in voice activity.\n\n**Methods:**\n\n*   `VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null)`: Constructor for the VoiceActivityDetector class. Initializes a new instance of the voice activity detector with configurable FFT size, detection threshold and optional visualizer for audio analysis visualization.\n*   `fftSize`: `int` – The size of the FFT (Fast Fourier Transform) window used for spectral analysis. Must be a power of two. Larger FFT sizes provide finer frequency resolution but may increase processing latency. Default is 1024.\n*   `energyThreshold`: `float` – The sensitivity threshold for voice detection. A lower value increases sensitivity. Default is 0.01.\n*   `visualizer`: `IVisualizer?` – An optional visualizer instance that can be attached to the analyzer for visualizing audio processing data, useful for debugging and tuning. Default is `null`.\n*   `Analyze(Span<float> buffer, int channels)`: Checks the audio buffer and updates the `IsVoiceActive` property based on the detection algorithm.\n*   `buffer`: `Span<float>` – The audio buffer to analyze for voice activity. The audio data in this buffer is processed to determine if voice is present.\n\n**Remarks:**\n\n*   **Frequency Range:** The `SpeechLowBand` and `SpeechHighBand` properties allow you to customize the frequency range that the VAD focuses on for speech detection. Speech typically falls within the 300Hz to 3400Hz range, but you may need to adjust these values depending on the characteristics of your audio and the type of speech you are detecting.\n*   **Threshold Sensitivity:** The `Threshold` property is crucial for controlling the sensitivity of the voice activity detection. Adjusting this threshold may be necessary to achieve optimal performance in different environments and with varying audio input levels.\n*   **FFT Size:** The `fftSize` parameter in the constructor determines the FFT window size. A larger FFT size provides better frequency resolution, which can be beneficial in noisy environments or when detecting subtle voice activity. However, it also increases the computational cost and latency. Ensure that the FFT size is always a power of 2 for optimal performance and compatibility with FFT algorithms.\n*   **Performance Tuning:** For optimal performance, especially in real-time applications, carefully tune the `fftSize` and `Threshold` parameters. Larger FFT sizes are more computationally intensive but offer better frequency resolution. Adjust the `Threshold` based on the ambient noise level and the desired sensitivity of voice detection.\n*   **Environment Considerations:** The ideal settings for `fftSize`, `Threshold`, `SpeechLowBand`, and `SpeechHighBand` may vary depending on the environment in which the voice activity detector is used. In noisy environments, you might need to increase the `fftSize` and adjust the `Threshold` to minimize false positives.\n*   **Visualizer for Debugging:** The optional `visualizer` parameter in the constructor is highly useful for debugging and tuning the voice activity detector. By attaching a visualizer, you can visually inspect the audio data and the detector's response, which can help in understanding and adjusting the detector's parameters for optimal performance in your specific use case.\n\n### Enums `LogLevel`\n\n```csharp\npublic enum LogLevel\n{\n    Debug,\n    Info,\n    Warning,\n    Error\n}\n```\n\n**Description:** Defines the severity levels for log messages used by the `SoundFlow.Utils.Log` static class.\n\n**Values:**\n\n*   `Debug`: Detailed information intended for debugging purposes.\n*   `Info`: General informational messages about the library's operation.\n*   `Warning`: Indicates a potential issue that does not prevent the current operation from completing but may lead to unexpected behavior.\n*   `Error`: Indicates a definite error that has occurred, which may affect functionality.\n\n### Enums `MidiCommand`\n\n```csharp\npublic enum MidiCommand : byte\n{\n    NoteOff = 0x80,\n    NoteOn = 0x90,\n    PolyphonicKeyPressure = 0xA0,\n    ControlChange = 0xB0,\n    ProgramChange = 0xC0,\n    ChannelPressure = 0xD0,\n    PitchBend = 0xE0,\n    SystemExclusive = 0xF0\n}\n```\n\n**Description:** Represents the command portion of a MIDI status byte, defining the type of MIDI message. This enum covers the most common channel messages.\n\n**Values:**\n\n*   `NoteOff`: Note Off message (e.g., releasing a key).\n*   `NoteOn`: Note On message (e.g., pressing a key). A Note On with velocity 0 is often interpreted as a Note Off.\n*   `PolyphonicKeyPressure`: Polyphonic Key Pressure (Aftertouch) message.\n*   `ControlChange`: Control Change (CC) message, used for adjusting parameters like volume, pan, or effects.\n*   `ProgramChange`: Program Change message, used for changing instruments or presets.\n*   `ChannelPressure`: Channel Pressure (Aftertouch) message, which applies to the entire channel.\n*   `PitchBend`: Pitch Bend message.\n*   `SystemExclusive`: System Exclusive (SysEx) message start byte.\n\n### Enums `MiniAudioBackend`\n\n```csharp\npublic enum MiniAudioBackend\n{\n    Wasapi = 1,\n    DirectSound = 2,\n    WinMm = 3,\n    CoreAudio = 4,\n    Sndio = 5,\n    Audio4 = 6,\n    Oss = 7,\n    PulseAudio = 8,\n    Alsa = 9,\n    Jack = 10,\n    AAudio = 11,\n    OpenSl = 12,\n    WebAudio = 13,\n    Custom = 14,\n    Null = 0\n}\n```\n\n**Description:** Represents the available low-level audio backends that the `MiniAudioEngine` can use for audio I/O. The integer values correspond directly to the native `ma_backend` enum in the `miniaudio` library.\n\n**Values:**\n\n*   `Wasapi`: The modern Windows Audio Session API. (Windows Vista+)\n*   `DirectSound`: The legacy DirectSound API. (Windows)\n*   `WinMm`: The legacy Windows MultiMedia API (WaveOut). (Windows)\n*   `CoreAudio`: The standard audio API for macOS and iOS.\n*   `Sndio`: A common audio server on OpenBSD.\n*   `Oss`: The Open Sound System, common on BSD and older Linux systems.\n*   `PulseAudio`: A common sound server on modern Linux desktops.\n*   `Alsa`: The Advanced Linux Sound Architecture, the standard low-level audio API on Linux.\n*   `Jack`: The JACK Audio Connection Kit, a professional low-latency audio server for Linux and macOS.\n*   `AAudio`: The modern low-latency audio API for Android. (Android 8.0+)\n*   `OpenSl`: The legacy audio API for Android.\n*   `WebAudio`: The Web Audio API, for use in WebAssembly environments.\n*   `Null`: A silent backend that consumes and discards audio data. Used as a fallback.\n*   `Custom`: A placeholder for a user-defined backend.\n\n### Enums `Capability`\n\n```csharp\n[Flags]\npublic enum Capability\n{\n    Playback = 1,\n    Record = 2,\n    Mixed = Playback | Record,\n    Loopback = 4\n}\n```\n\n**Values:**\n\n*   `Playback`: Indicates playback capability.\n*   `Record`: Indicates recording capability.\n*   `Mixed`: Indicates both playback and recording capability.\n*   `Loopback`: Indicates loopback capability (recording system audio output).\n\n### Enums `DeviceType`\n\n```csharp\npublic enum DeviceType\n{\n    Playback,\n    Capture\n}\n```\n**Values:**\n*   `Playback`: Device used for audio playback.\n*   `Capture`: Device used for audio capture.\n\n### Enums `PlaybackState`\n\n```csharp\npublic enum PlaybackState\n{\n    Stopped,\n    Playing,\n    Paused\n}\n```\n\n**Values:**\n\n*   `Stopped`: Playback is stopped.\n*   `Playing`: Playback is currently in progress.\n*   `Paused`: Playback is paused.\n\n### Enums `MiniAudioResult`\n\n```csharp\npublic enum MiniAudioResult\n{\n    Success = 0,\n    Error = -1,\n    // ... (other error codes)\n    CrcMismatch = -100,\n    FormatNotSupported = -200,\n    // ... (other backend-specific error codes)\n    DeviceNotInitialized = -300,\n    // ... (other device-related error codes)\n    FailedToInitBackend = -400\n    // ... (other backend initialization error codes)\n}\n```\n\n**Values:**\n\n*   `Success`: The operation was successful.\n*   `Error`: A generic error occurred.\n*   `CrcMismatch`: CRC checksum mismatch.\n*   `FormatNotSupported`: The requested audio format is not supported.\n*   `DeviceNotInitialized`: The audio device is not initialized.\n*   `FailedToInitBackend`: Failed to initialize the audio backend.\n*   **(Many other error codes representing various error conditions)**\n\n### Enums `SampleFormat`\n\n```csharp\npublic enum SampleFormat\n{\n    Unknown = 0,\n    U8 = 1,\n    S16 = 2,\n    S24 = 3,\n    S32 = 4,\n    F32 = 5\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown sample format.\n*   `U8`: Unsigned 8-bit integer.\n*   `S16`: Signed 16-bit integer.\n*   `S24`: Signed 24-bit integer packed in 3 bytes.\n*   `S32`: Signed 32-bit integer.\n*   `F32`: 32-bit floating-point.\n\n### Enums `FilterType`\n\n```csharp\npublic enum FilterType\n{\n    Peaking,\n    LowShelf,\n    HighShelf,\n    BandPass,\n    Notch,\n    LowPass,\n    HighPass\n}\n```\n\n**Values:**\n\n*   `Peaking`: Peaking filter.\n*   `LowShelf`: Low-shelf filter.\n*   `HighShelf`: High-shelf filter.\n*   `BandPass`: Band-pass filter.\n*   `Notch`: Notch filter.\n*   `LowPass`: Low-pass filter.\n*   `HighPass`: High-pass filter.\n\n### Enums `EnvelopeGenerator.EnvelopeState`\n\n```csharp\npublic enum EnvelopeState\n{\n    Idle,\n    Attack,\n    Decay,\n    Sustain,\n    Release\n}\n```\n\n**Values:**\n\n*   `Idle`: The envelope is inactive.\n*   `Attack`: The attack stage of the envelope.\n*   `Decay`: The decay stage of the envelope.\n*   `Sustain`: The sustain stage of the envelope.\n*   `Release`: The release stage of the envelope.\n\n### Enums `EnvelopeGenerator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    NoteOn,\n    Gate,\n    Trigger\n}\n```\n\n**Values:**\n\n* `NoteOn`: The envelope will go directly from attack to sustain, without a decay stage.\n* `Gate`: The envelope will progress normally, and will only enter release stage when trigger is off.\n* `Trigger`: The envelope will always progress to the end, including the release stage.\n\n### Enums `LowFrequencyOscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Triangle,\n    Sawtooth,\n    ReverseSawtooth,\n    Random,\n    SampleAndHold\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Triangle`: Triangle wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `ReverseSawtooth`: Reverse sawtooth wave.\n*   `Random`: Random values.\n*   `SampleAndHold`: Sample and hold random values.\n\n### Enums `LowFrequencyOscillator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    FreeRunning,\n    NoteTrigger\n}\n```\n\n**Values:**\n\n* `FreeRunning`: The LFO will run continuously without needing a trigger.\n* `NoteTrigger`: The LFO will only start when triggered.\n\n### Enums `Oscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Sawtooth,\n    Triangle,\n    Noise,\n    Pulse\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Band-limited square wave.\n*   `Sawtooth`: Band-limited sawtooth wave.\n*   `Triangle`: Triangle wave.\n*   `Noise`: White noise.\n*   `Pulse`: Band-limited pulse wave.\n\n### Enums `SurroundPlayer.SpeakerConfiguration`\n\n```csharp\npublic enum SpeakerConfiguration\n{\n    Stereo,\n    Quad,\n    Surround51,\n    Surround71,\n    Custom\n}\n```\n\n**Values:**\n\n*   `Stereo`: Standard stereo configuration (2 speakers).\n*   `Quad`: Quadraphonic configuration (4 speakers).\n*   `Surround51`: 5.1 surround sound configuration (6 speakers).\n*   `Surround71`: 7.1 surround sound configuration (8 speakers).\n* *   `Custom`: A custom speaker configuration defined by the user.\n\n### Enums `SurroundPlayer.PanningMethod`\n\n```csharp\npublic enum PanningMethod\n{\n    Linear,\n    EqualPower,\n    Vbap\n}\n```\n\n**Values:**\n\n*   `Linear`: Linear panning.\n*   `EqualPower`: Equal power panning.\n*   `Vbap`: Vector Base Amplitude Panning (VBAP).\n\n### Editing `Composition`\n\n```csharp\npublic sealed class Composition : ISequencerContext, IDisposable, IMidiMappable\n{\n    // Service Accessors\n    public CompositionRenderer Renderer { get; }\n    public CompositionEditor Editor { get; }\n    public CompositionRecorder Recorder { get; }\n    public MidiMappingManager MappingManager { get; }\n\n    // State\n    public Guid Id { get; }\n    public string Name { get; set; }\n    public AudioFormat Format { get; }\n    public int TicksPerQuarterNote { get; set; } // Default 480\n    public float MasterVolume { get; set; }\n    public bool IsDirty { get; }\n\n    // Data Collections\n    public List<Track> Tracks { get; }\n    public List<MidiTrack> MidiTracks { get; }\n    public List<TempoMarker> TempoTrack { get; }\n    public List<IMidiDestinationNode> MidiTargets { get; } // Internal synths/effects\n    public List<SoundModifier> Modifiers { get; init; } // Master bus effects\n    public List<AudioAnalyzer> Analyzers { get; init; }\n\n    public void MarkDirty();\n    public void ClearDirtyFlag();\n    public void Dispose();\n}\n```\n**Description:** Represents a complete audio and MIDI composition, acting as the top-level container for multiple tracks. It serves as a façade, providing access to its data model and specialized services for rendering, editing, and recording. In previous versions, this class directly implemented `ISoundDataProvider`; this functionality has now been moved to the `Renderer` property.\n\n**Properties:**\n*   `Id`: A unique identifier for the composition instance, used for MIDI mapping.\n*   `Renderer`: Gets the `CompositionRenderer` service for this composition, which handles audio output generation and implements `ISoundDataProvider`.\n*   `Editor`: Gets the `CompositionEditor` service, which provides methods to manipulate tracks, segments, and other structural elements.\n*   `Recorder`: Gets the `CompositionRecorder` service, which manages the MIDI recording workflow.\n*   `MappingManager`: Gets the `MidiMappingManager` service, which manages real-time control mappings.\n*   `Format`: Gets the base audio format of the composition.\n*   `Name`: The name of the composition.\n*   `Modifiers`: A list of `SoundModifier` instances applied to the master output of the composition.\n*   `Analyzers`: A list of `AudioAnalyzer` instances that process the master output of the composition.\n*   `MidiTargets`: A list of internal MIDI-controllable components (like `Synthesizer`s) that can be targeted by `MidiTrack`s.\n*   `Tracks`: The list of audio `Track`s contained within this composition.\n*   `MidiTracks`: The list of `MidiTrack`s contained within this composition.\n*   `TempoTrack`: The master tempo track for the composition, defining tempo changes over time.\n*   `MasterVolume`: The master volume level for the entire composition (0.0 to 1.0 or higher for gain).\n*   `IsDirty`: A flag that indicates if there are unsaved changes.\n*   `SampleRate`: The target sample rate for rendering.\n*   `TargetChannels`: The target number of channels for the rendered output.\n*   `TicksPerQuarterNote`: The time division for all MIDI tracks in this composition, in ticks per quarter note (e.g., 480).\n*   `IsDisposed`: Indicates whether the composition has been disposed.\n\n**Methods:**\n*   `MarkDirty()`: Marks the composition as having unsaved changes.\n*   `ClearDirtyFlag()`: Resets the `IsDirty` flag to `false`.\n*   `Dispose()`: Disposes of all disposable resources owned by the composition and its services.\n\n### Editing `CompositionRenderer`\n\n```csharp\npublic sealed class CompositionRenderer : ISoundDataProvider\n{\n    public bool IsSyncDriven { get; set; }\n    public TimeSpan? LoopStartTime { get; set; }\n    public TimeSpan? LoopEndTime { get; set; }\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public bool IsDisposed { get; private set; }\n    // ... ISoundDataProvider implementation ...\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public float[] Render(TimeSpan startTime, TimeSpan duration);\n    public int Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer);\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n\n    // Sync Transport Controls\n    public void SyncPlay();\n    public void SyncStop();\n    public void SyncContinue();\n    public void SyncSeek(TimeSpan time);\n    public void AdvanceBySyncTicks(int tickCount);\n    public TempoMarker GetTempoAtCurrentPosition();\n}\n```\n**Description:** Renders a `Composition` into an audio stream. This class now contains the audio generation logic previously in `Composition` and implements `ISoundDataProvider`, allowing it to be used as a source for `SoundPlayer` or other components.\n\n**Properties:**\n*   `IsSyncDriven`: Gets or sets whether the renderer's transport is driven by an external sync source (e.g., MIDI Clock).\n*   `LoopStartTime`: Gets or sets the start time of the playback loop.\n*   `LoopEndTime`: Gets or sets the end time of the playback loop.\n*   `Position`: The current read position in samples.\n*   `Length`: The total length of the composition in samples.\n*   `CanSeek`: Always `true`.\n*   `IsDisposed`: Indicates whether the renderer has been disposed.\n\n**Methods:**\n*   `Render(...)`: Renders a specific time portion of the composition into a buffer, mixing audio and MIDI tracks.\n*   `ReadBytes(Span<float> buffer)`: `ISoundDataProvider` implementation for reading rendered audio.\n*   `Seek(int sampleOffset)`: `ISoundDataProvider` implementation for seeking.\n*   `SyncPlay()`, `SyncStop()`, `SyncContinue()`, `SyncSeek(...)`, `AdvanceBySyncTicks(...)`, `GetTempoAtCurrentPosition()`: Methods for controlling the transport when slaved to an external synchronization source. Intended for use by backend implementations.\n\n### Editing `CompositionEditor`\n\n```csharp\npublic sealed class CompositionEditor : IDisposable\n{\n    public AudioSegment CreateAndAddSegmentFromFile(Track track, string filePath, TimeSpan timelineStartTime, ReadOptions? options = null);\n    public AudioSegment CreateSegmentFromFile(string filePath, TimeSpan timelineStartTime, ReadOptions? options = null);\n    public void AddTrack(Track track);\n    public void AddMidiTrack(MidiTrack midiTrack);\n    public bool RemoveTrack(Track track);\n    public bool RemoveMidiTrack(MidiTrack midiTrack);\n    public TempoMarker GetTempoAtTime(TimeSpan time);\n    public void SetTempo(TimeSpan time, double newBpm);\n    public bool RemoveTempoChange(TimeSpan time);\n    public void SetTicksPerQuarterNote(int ticksPerQuarterNote);\n    public Task ExportMidiAsync(string filePath);\n    public TimeSpan CalculateTotalDuration();\n    public bool ReplaceSegment(...);\n    public bool RemoveSegment(...);\n    public AudioSegment? SilenceSegment(Track track, TimeSpan rangeStartTime, TimeSpan rangeDuration);\n    public void InsertSegment(Track track, TimeSpan insertionPoint, AudioSegment segmentToInsert, bool shiftFollowing = true);\n\n    // MIDI Editing Methods\n    public IReadOnlyCollection<MidiNote> GetNotes(MidiSegment segment);\n    public void AddNoteToSegment(MidiSegment segment, long startTick, long durationTicks, int noteNumber, int velocity);\n    public void ModifyNotesInSegment(MidiSegment segment, IEnumerable<NoteModification> modifications);\n    public void RemoveNotesFromSegment(MidiSegment segment, IEnumerable<Guid> noteIds);\n    public IReadOnlyCollection<ControlPoint> GetControlPoints(MidiSegment segment, int controllerNumber);\n    public void AddControlPointToSegment(MidiSegment segment, int controllerNumber, long tick, int value);\n    public void ModifyControlPointsInSegment(MidiSegment segment, int controllerNumber, IEnumerable<ControlPointModification> modifications);\n    public void RemoveControlPointsFromSegment(MidiSegment segment, int controllerNumber, IEnumerable<Guid> pointIds);\n    public void QuantizeSegment(MidiSegment segment, QuantizationSettings settings);\n    public (MidiSegment? part1, MidiSegment? part2)? SplitMidiSegment(MidiTrack track, MidiSegment segmentToSplit, TimeSpan splitTimeOnTimeline);\n    public MidiSegment? JoinMidiSegments(MidiTrack track, IEnumerable<MidiSegment> segmentsToJoin);\n\n    public void Dispose();\n}\n```\n**Description:** Provides high-level editing logic that modifies a `Composition` object. This class encapsulates all actions that alter the composition's structure, including track management, audio segment manipulation, and detailed MIDI editing.\n\n### Editing `CompositionRecorder`\n\n```csharp\npublic sealed class CompositionRecorder : IDisposable\n{\n    public TimeSpan? PunchInTime { get; set; }\n    public TimeSpan? PunchOutTime { get; set; }\n    public bool IsRecording { get; }\n    public bool IsWaitingForPunchIn { get; }\n\n    public void ArmTrackForRecording(MidiTrack track, MidiInputDevice inputDevice);\n    public void DisarmTrack(MidiTrack track);\n\n    // Starts recording immediately or waits for PunchInTime\n    public void StartRecording(TimeSpan startTime, RecordingMode mode = RecordingMode.Normal, MidiSegment? targetSegment = null);\n    public void StopRecording();\n}\n```\n**Description:** Orchestrates the recording process.\n*   `ArmTrackForRecording`: Links a physical input to a track.\n*   `PunchIn/Out`: Automates start/stop based on timeline position.\n*   `RecordingMode`: `Normal` (creates new segment) or `OverdubMerge` (adds notes to existing segment).\n\n### Editing `Track`\n\n```csharp\npublic class Track : IMidiMappable\n{\n    public Track(string name = \"Track\", TrackSettings? settings = null);\n\n    public Guid Id { get; }\n    public string Name { get; set; }\n    public List<AudioSegment> Segments { get; }\n    public TrackSettings Settings { get; set; }\n    internal Composition? ParentComposition { get; set; }\n\n    public void MarkDirty();\n    public void AddSegment(AudioSegment segment);\n    public bool RemoveSegment(AudioSegment segment, bool shiftSubsequent = false);\n    public void InsertSegmentAt(AudioSegment segmentToInsert, TimeSpan insertionTime, bool shiftSubsequent = true);\n    public TimeSpan CalculateDuration();\n    public int Render(TimeSpan overallStartTime, TimeSpan durationToRender, Span<float> outputBuffer, int targetSampleRate, int targetChannels);\n}\n```\n**Description:** Represents a single audio track within a `Composition`. It now implements `IMidiMappable`, allowing its `TrackSettings` (volume, pan, etc.) to be controlled via MIDI mapping.\n\n**Properties:**\n*   `Id`: A unique identifier for the track instance, used for MIDI mapping.\n\n### Editing `AudioSegment`\n\n```csharp\npublic class AudioSegment : IDisposable\n{\n    public AudioSegment(\n        AudioFormat format,\n        ISoundDataProvider sourceDataProvider,\n        TimeSpan sourceStartTime,\n        TimeSpan sourceDuration,\n        TimeSpan timelineStartTime,\n        string name = \"Segment\",\n        AudioSegmentSettings? settings = null,\n        bool ownsDataProvider = false);\n\n    public string Name { get; set; }\n    public ISoundDataProvider SourceDataProvider { get; private set; }\n    public TimeSpan SourceStartTime { get; set; }\n    public TimeSpan SourceDuration { get; set; }\n    public TimeSpan TimelineStartTime { get; set; }\n    public AudioSegmentSettings Settings { get; set; }\n    internal Track? ParentTrack { get; set; }\n\n    public TimeSpan StretchedSourceDuration { get; }\n    public TimeSpan EffectiveDurationOnTimeline { get; }\n    public TimeSpan TimelineEndTime { get; }\n    public TimeSpan GetTotalLoopedDurationOnTimeline();\n    public AudioSegment Clone(TimeSpan? newTimelineStartTime = null);\n    internal void ReplaceSource(ISoundDataProvider newSource, TimeSpan newSourceStartTime, TimeSpan newSourceDuration);\n    public int ReadProcessedSamples(TimeSpan segmentTimelineOffset, TimeSpan durationToRead, Span<float> outputBuffer, int outputBufferOffset, int targetSampleRate, int targetChannels);\n    internal void FullResetState();\n    public void Dispose();\n    public void MarkDirty();\n}\n```\n\n### Editing `AudioSegmentSettings`\n\n```csharp\npublic class AudioSegmentSettings : IMidiMappable\n{\n    public Guid Id { get; }\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public TimeSpan FadeInDuration { get; set; }\n    public FadeCurveType FadeInCurve { get; set; }\n    public TimeSpan FadeOutDuration { get; set; }\n    public FadeCurveType FadeOutCurve { get; set; }\n    public bool IsReversed { get; set; }\n    public LoopSettings Loop { get; set; }\n    public float SpeedFactor { get; set; }\n    public float TimeStretchFactor { get; set; } // Overridden by TargetStretchDuration if set\n    public TimeSpan? TargetStretchDuration { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public AudioSegmentSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `TrackSettings`\n\n```csharp\npublic class TrackSettings : IMidiMappable\n{\n    public Guid Id { get; }\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public List<MidiModifier> MidiModifiers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public bool IsMuted { get; set; }\n    public bool IsSoloed { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public TrackSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n**Description:** Configurable settings for a `Track` or `MidiTrack`. Now implements `IMidiMappable` and includes a `MidiModifiers` list for applying real-time MIDI effects to `MidiTrack`s.\n\n### Editing `LoopSettings`\n\n```csharp\npublic record struct LoopSettings\n{\n    public int Repetitions { get; }\n    public TimeSpan? TargetDuration { get; }\n    public LoopSettings(int repetitions = 0, TimeSpan? targetDuration = null);\n    public static LoopSettings PlayOnce { get; }\n}\n```\n\n### Editing `FadeCurveType`\n\n```csharp\npublic enum FadeCurveType\n{\n    Linear,\n    Logarithmic,\n    SCurve\n}\n```\n\n### Editing.Persistence.DTOs\nThese are Data Transfer Objects (DTOs) for serialization. See [Editing and Persistence Guide](./editing-engine.mdx#project-persistence) for their purpose.\n*   `CompositionProjectManager` (static class): `SaveProjectAsync`, `LoadProjectAsync`, `RelinkMissingMedia`.\n*   `ProjectSaveOptions`\n*   `ProjectData`\n*   `ProjectTrack`, `ProjectMidiTrack`\n*   `ProjectSegment`, `ProjectMidiSegment`\n*   `ProjectAudioSegmentSettings`, `ProjectTrackSettings`\n*   `ProjectSourceReference`\n*   `ProjectEffectData`\n*   `ProjectMidiMapping`\n*   `ProjectTempoMarker`\n\n### Editing `MidiSequence`\n\n```csharp\npublic class MidiSequence\n{\n    public MidiSequence(MidiFile midiFile);\n    public MidiSequence(int ticksPerQuarterNote, IEnumerable<MidiNote> notes, IEnumerable<(int ccNumber, ControlPoint point)> ccEvents, IEnumerable<ControlPoint> pitchBendEvents, IEnumerable<MidiEvent> otherEvents);\n\n    public int TicksPerQuarterNote { get; }\n    public IReadOnlyCollection<MidiNote> Notes { get; }\n    public IReadOnlyList<MidiEvent> Events { get; }\n    public long LengthTicks { get; }\n    public IReadOnlyCollection<ControlPoint> PitchBendEvents { get; }\n    public IReadOnlyDictionary<int, IReadOnlyCollection<ControlPoint>> ControlChangeEvents { get; }\n\n    public MidiNote AddNote(long startTick, long durationTicks, int noteNumber, int velocity);\n    public void RemoveNotes(IEnumerable<Guid> noteIds);\n    public void ModifyNotes(IEnumerable<NoteModification> modifications);\n    public ControlPoint AddControlPoint(int controllerNumber, long tick, int value);\n    public void RemoveControlPoints(int controllerNumber, IEnumerable<Guid> pointIds);\n    public void ModifyControlPoints(int controllerNumber, IEnumerable<ControlPointModification> modifications);\n    public MidiFile ToMidiFile();\n    public (MidiSequence part1, MidiSequence part2) Split(long splitTick);\n    public static MidiSequence Join(IEnumerable<(long tickOffset, MidiSequence sequence)> sequences);\n}\n```\n\n**Description:**\nA mutable container representing the musical content of a MIDI segment. It acts as the primary data model for MIDI editing, bridging the gap between high-level musical concepts (like notes with duration) and low-level MIDI events (like Note On/Off messages).\n\n**Constructors:**\n*   `MidiSequence(MidiFile midiFile)`: Initializes the sequence by parsing a standard `MidiFile` object. It automatically converts paired Note On/Off events into `MidiNote` objects.\n*   `MidiSequence(int ticksPerQuarterNote, ...)`: Initializes a new sequence from raw collections of notes, control points, and other events. Typically used internally during split/join operations.\n\n**Properties:**\n*   **`TicksPerQuarterNote`**: The time resolution of the sequence (e.g., 480 or 960 PPQ).\n*   **`Notes`**: A read-only collection of all `MidiNote` objects in the sequence.\n*   **`Events`**: A read-only list of all raw `MidiEvent` objects, strictly ordered by time. This list is lazily rebuilt whenever the high-level data (Notes, Control Points) is modified, ensuring it is always up-to-date for playback.\n*   **`LengthTicks`**: The total duration of the sequence in ticks, determined by the position of the last event.\n*   **`PitchBendEvents`**: A collection of automation points for Pitch Bend.\n*   **`ControlChangeEvents`**: A dictionary mapping CC numbers to their respective automation curves.\n\n**Methods:**\n*   **`AddNote(...)`**: Creates and adds a new `MidiNote`.\n*   **`RemoveNotes(...)`**: Removes notes identified by their unique GUIDs.\n*   **`ModifyNotes(...)`**: Applies batch updates to notes (moving, resizing, transposing) efficiently.\n*   **`AddControlPoint(...)`**: Adds a new automation point for a CC or Pitch Bend (controller -1).\n*   **`ToMidiFile()`**: Serializes the current state of the sequence back into a standard `MidiFile` structure, converting absolute ticks back to delta-times.\n*   **`Split(long splitTick)`**: Splits the sequence into two new sequences at the specified tick. Handles note splitting logic (truncating the first part, creating a new start for the second).\n*   **`Join(...)`**: Static method that merges multiple sequences into one, applying time offsets.\n\n---\n\n### Editing `MidiSegment`\n\n```csharp\npublic sealed class MidiSegment : IDisposable\n{\n    public MidiSegment(MidiSequence sequence, TimeSpan timelineStartTime, string name = \"MIDI Segment\");\n\n    public string Name { get; set; }\n    public MidiSequence Sequence { get; }\n    public MidiDataProvider DataProvider { get; }\n    public TimeSpan TimelineStartTime { get; set; }\n    public TimeSpan SourceDuration { get; }\n    public TimeSpan TimelineEndTime { get; }\n    public MidiTrack? ParentTrack { get; set; }\n\n    public void MarkDirty();\n    public void Dispose();\n}\n```\n\n**Description:**\nRepresents a clip of MIDI data placed on a track's timeline. It wraps a `MidiSequence` (the content) and adds placement information (`TimelineStartTime`). Crucially, it manages a cached `MidiDataProvider` optimized for real-time playback.\n\n**Properties:**\n*   **`Sequence`**: The mutable data model for this segment.\n*   **`DataProvider`**: A read-only, thread-safe view of the sequence used by the audio engine. This provider is automatically regenerated when the sequence is modified (marked dirty).\n*   **`TimelineStartTime`**: The absolute time on the composition timeline where this segment begins.\n*   **`SourceDuration`**: The duration of the segment in time, calculated from the sequence's tick length and the composition's tempo map.\n*   **`ParentTrack`**: Reference to the track containing this segment.\n\n**Methods:**\n*   **`MarkDirty()`**: Signals that the underlying `Sequence` has changed. This invalidates the cached `DataProvider`, forcing it to be rebuilt on the next access to ensure playback reflects the edits.\n\n---\n\n### Editing.Mapping `MidiMappingManager`\n\n```csharp\npublic sealed class MidiMappingManager : IDisposable\n{\n    internal MidiMappingManager(Composition composition);\n\n    public IReadOnlyList<MidiMapping> Mappings { get; }\n\n    public void AddInputDevice(MidiInputDevice device);\n    public void RemoveInputDevice(MidiInputDevice device);\n    public void AddMapping(MidiMapping mapping);\n    public bool RemoveMapping(Guid mappingId);\n    public void Dispose();\n}\n```\n\n**Description:**\nThe runtime engine responsible for executing MIDI mappings. It listens to all subscribed MIDI input devices, interprets incoming messages (including complex 14-bit NRPN sequences), and applies them to the target properties or methods within the composition.\n\n**Methods:**\n*   **`AddInputDevice(MidiInputDevice device)`**: Subscribes the manager to the specified input device. The manager attaches listeners to `OnMessageReceived` and initializes internal parsers for that device (e.g., High-Res CC parser).\n*   **`RemoveInputDevice(...)`**: Unsubscribes from the device and cleans up associated parsers.\n*   **`AddMapping(MidiMapping mapping)`**: Registers a new mapping rule.\n*   **`RemoveMapping(Guid mappingId)`**: Removes a mapping rule by its ID.\n\n**Internal Logic:**\n*   **Caching**: It maintains a cache of reflection metadata (`PropertyInfo`, `MethodInfo`) for resolved mappings to ensure high-performance execution during real-time audio processing.\n*   **High-Resolution Parsing**: It tracks the state of RPN/NRPN controller streams per channel/device. When a full 14-bit message sequence is detected, it synthesizes a `HighResolutionControlChange` event for mapping.\n\n---\n\n### Editing.Mapping `MidiMapping` & DTOs\n\n```csharp\npublic sealed class MidiMapping\n{\n    public MidiMapping(MidiInputSource source, MidiMappingTarget target, ValueTransformer transformer, MidiMappingBehavior behavior = MidiMappingBehavior.Absolute);\n\n    public Guid Id { get; }\n    public MidiInputSource Source { get; set; }\n    public MidiMappingTarget Target { get; set; }\n    public ValueTransformer Transformer { get; set; }\n    public MidiMappingBehavior Behavior { get; set; }\n    public int ActivationThreshold { get; set; } = 1;\n    public bool IsResolved { get; internal set; }\n}\n```\n\n**Description:**\nDefines a single persistent link between a MIDI source event and a destination parameter.\n\n**Properties:**\n*   **`Source`**: Defines the criteria for triggering the mapping (Device, Channel, Message Type, Parameter).\n*   **`Target`**: Defines what is being controlled (Object ID, Member Name, Type).\n*   **`Transformer`**: Defines how the input value (e.g., 0-127) maps to the target property's range (e.g., 20Hz-20kHz).\n*   **`Behavior`**:\n*   `Absolute`: Sets the value directly based on the input.\n*   `Relative`: Increments/decrements the value (for endless encoders).\n*   `Toggle`: Flips a boolean value when the input exceeds the threshold.\n*   `Trigger`: Invokes a method when the input exceeds the threshold.\n*   **`IsResolved`**: Indicates if the target object and property were successfully found in the current composition. Mappings may become unresolved if target objects are deleted.\n\n**Supporting DTOs:**\n\n*   **`MidiInputSource`**:\n*   `DeviceName`: Name of the MIDI device.\n*   `Channel`: 1-16, or 0 for Omni.\n*   `MessageType`: `ControlChange`, `NoteOn`, `NoteOff`, `PitchBend`, `HighResolutionControlChange`.\n*   `MessageParameter`: The note number or CC number.\n*   **`MidiMappingTarget`**:\n*   `TargetObjectId`: GUID of the `IMidiMappable` object.\n*   `TargetMemberName`: Name of the property or method.\n*   `TargetType`: `Property` or `Method`.\n*   **`ValueTransformer`**:\n*   `SourceMin`/`Max`: Input range (e.g., 0-127).\n*   `TargetMin`/`Max`: Output range (e.g., 0.0-1.0).\n*   `CurveType`: `Linear`, `Exponential`, `Logarithmic`.\n\n---\n\n### Midi `MidiManager`\n\n```csharp\npublic sealed class MidiManager : IDisposable\n{\n    public IReadOnlyList<MidiDeviceInfo> AvailableInputs { get; }\n    public IReadOnlyList<MidiDeviceInfo> AvailableOutputs { get; }\n    public IReadOnlyList<MidiRoute> Routes { get; }\n    public IReadOnlyList<MidiRoute> FaultedRoutes { get; }\n\n    public event Action<MidiRoute, IError?>? OnRouteFaulted;\n\n    public void ConfigureMpeZone(MidiDeviceInfo deviceInfo, MpeZone zone);\n    public void DeconfigureMpeZone(MidiDeviceInfo deviceInfo);\n\n    public MidiRoute CreateRoute(MidiDeviceInfo inputDevice, IMidiControllable target);\n    public MidiRoute CreateRoute(MidiDeviceInfo inputDevice, MidiDeviceInfo outputDevice);\n    public void AddRoute(MidiRoute route);\n    public bool RemoveRoute(MidiRoute route);\n\n    public MidiInputNode GetOrCreateInputNode(MidiDeviceInfo deviceInfo);\n    public MidiOutputNode GetOrCreateOutputNode(MidiDeviceInfo deviceInfo);\n    public void Dispose();\n}\n```\n\n**Description:**\nThe central hub for all MIDI connectivity. It acts as a \"virtual patch bay,\" managing the lifecycles of physical device connections and routing messages between them and internal software components.\n\n**Key Features:**\n*   **Device Abstraction**: Users interact with `MidiDeviceInfo` structs. The manager handles the actual instantiation (`MidiInputDevice`/`MidiOutputDevice`) and ensures devices are shared efficiently.\n*   **Routing Graph**: Manages a list of `MidiRoute` objects. Each route connects a source node to a destination node.\n*   **MPE Processing**:\n*   **`ConfigureMpeZone`**: Configures an input device to be treated as an MPE source. The manager attaches a specialized parser that intercepts messages on member channels and converts them into internal, per-note expression events (Pitch, Pressure, Timbre) before routing them to MPE-aware targets (like `Synthesizer`).\n*   This abstracts MPE complexity away from the routing logic.\n*   **Fault Handling**: Monitors routes for errors (e.g., device disconnection) and raises `OnRouteFaulted` so the application can respond (e.g., show a UI alert).\n\n---\n\n### Midi.PortMidi `PortMidiBackend`\n\n```csharp\npublic sealed class PortMidiBackend : IMidiBackend\n{\n    public PortMidiBackend();\n\n    public SyncMode CurrentSyncMode { get; }\n    public SyncStatus CurrentSyncStatus { get; }\n    public double DetectedBpm { get; }\n\n    public event Action<SyncStatus>? OnSyncStatusChanged;\n    public event Action<double>? OnBpmChanged;\n\n    public void Initialize(AudioEngine engine);\n    public MidiInputDevice CreateMidiInputDevice(MidiDeviceInfo deviceInfo);\n    public MidiOutputDevice CreateMidiOutputDevice(MidiDeviceInfo deviceInfo);\n    public void UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs);\n\n    public void ConfigureSync(SyncMode mode, SyncSource source, MidiDeviceInfo? inputDeviceInfo, MidiDeviceInfo? outputDeviceInfo, CompositionRenderer? renderer);\n    public void Dispose();\n}\n```\n\n**Description:**\nThe concrete implementation of `IMidiBackend` using the PortMidi library. Beyond basic I/O, it implements a robust synchronization engine.\n\n**Synchronization Features:**\n*   **`ConfigureSync`**: Sets up the backend to act as a Clock Master or Slave.\n*   **Slave Mode**: Listens to the specified `inputDeviceInfo` for MIDI Clock (F8) or MTC (F1/SysEx). It drives the `CompositionRenderer`'s transport, syncing playback to the external source.\n*   **Master Mode**: Uses the `CompositionRenderer`'s position and `AudioFramesRendered` events to generate a stable MIDI Clock signal sent to `outputDeviceInfo`.\n*   **`DetectedBpm`**: When in Slave mode, reports the smoothed, detected tempo of the incoming clock signal.\n*   **`SyncStatus`**: Reports whether the engine is currently locked to the external clock signal.\n\n---\n\n### Synthesis `Synthesizer`\n\n```csharp\npublic sealed class Synthesizer : SoundComponent, IMidiControllable\n{\n    public Synthesizer(AudioEngine engine, AudioFormat format, IInstrumentBank instrumentBank);\n\n    public IReadOnlyList<MidiModifier> MidiModifiers { get; }\n    public float Bpm { get; set; } = 120f;\n    public bool MpeEnabled { get; set; }\n\n    public void AddMidiModifier(MidiModifier modifier);\n    public void RemoveMidiModifier(MidiModifier modifier);\n    public void ProcessMidiMessage(MidiMessage message);\n    public void Reset();\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n}\n```\n\n**Description:**\nA high-performance, polyphonic, multi-timbral synthesizer engine.\n\n**Architecture:**\n1.  **Input Stage**: Incoming MIDI messages pass through the **MidiModifier Chain** (Arpeggiators, Transposers, etc.).\n2.  **Channel Dispatch**: Processed messages are routed to one of 16 internal `MidiChannel` objects.\n3.  **Voice Allocation**: Each channel manages a pool of `IVoice` instances. When a Note On is received, a voice is allocated from the bank's `Instrument`.\n4.  **MPE Handling**: If `MpeEnabled` is true, the standard channel dispatch is overridden. The synthesizer tracks active notes and routes per-note expression messages (Pitch Bend, Channel Pressure, CC74) directly to the specific voice instance playing that note, regardless of its assigned MIDI channel.\n\n**Properties:**\n*   **`Bpm`**: Sets the master tempo for the synthesizer. This drives any temporal modifiers (like Arpeggiators) in the input chain.\n*   **`MidiModifiers`**: The chain of MIDI effects that process input before sound generation.\n\n---\n\n### Synthesis.Banks `SoundFontBank`\n\n```csharp\npublic sealed class SoundFontBank : IInstrumentBank, IDisposable\n{\n    public SoundFontBank(string filePath, AudioFormat format);\n\n    public IReadOnlyList<PresetInfo> AvailablePresets { get; }\n\n    public Instrument GetInstrument(int bank, int program);\n    public void Dispose();\n}\n```\n\n**Description:**\nLoads and parses standard SoundFont 2 (.sf2) files. It acts as a bridge between the file format and the runtime synthesis engine.\n\n**Functionality:**\n*   **Parsing**: Reads the RIFF chunk structure of the SF2 file.\n*   **Sample Loading**: Extracts audio samples, converts them to floating-point, and resamples them to match the engine's sample rate.\n*   **Instrument Building**: Iterates through Presets, Zones, and Generators in the SF2 metadata to construct `Instrument` objects. It maps velocity layers and key ranges to specific `VoiceDefinition`s containing `SamplerGenerator`s.\n*   **`AvailablePresets`**: Exposes a list of all programs found in the file, making it easy to build UI selectors.\n\n---\n\n### Utils `ControllableParameterAttribute`\n\n```csharp\n[AttributeUsage(AttributeTargets.Property)]\npublic sealed class ControllableParameterAttribute : Attribute\n{\n    public ControllableParameterAttribute(string displayName, double minValue, double maxValue, MappingScale scale = MappingScale.Linear);\n\n    public string DisplayName { get; }\n    public double MinValue { get; }\n    public double MaxValue { get; }\n    public MappingScale Scale { get; }\n}\n```\n\n**Description:**\nA declarative attribute used to expose properties of `SoundModifier`, `TrackSettings`, or any `IMidiMappable` object to the MIDI mapping system.\n\n**Usage:**\nWhen the `MidiMappingManager` needs to map a MIDI value (0-127) to a property, it inspects this attribute to determine the valid range and scaling behavior.\n\n**Example:**\n```csharp\n[ControllableParameter(\"Cutoff\", 20.0, 20000.0, MappingScale.Logarithmic)]\npublic float CutoffFrequency { get; set; }\n```\nThis tells the system that MIDI value 0 maps to 20.0, 127 maps to 20000.0, and the mapping should follow a logarithmic curve (standard for frequency controls).\n\n---\n\n### Midi.PortMidi `PortMidiExtensions`\n\n```csharp\npublic static class PortMidiExtensions\n{\n    public static PortMidiBackend UsePortMidi(this AudioEngine engine);\n}\n```\n\n**Description:**\nProvides a fluent extension method to easily register the PortMidi backend with an `AudioEngine`.\n\n**Example:**\n```csharp\nvar engine = new MiniAudioEngine();\nvar midiBackend = engine.UsePortMidi(); // Registers and initializes backend\n// midiBackend can now be used to configure sync\n```\n\n---\n\n### Synthesis `BasicInstrumentBank`\n\n```csharp\npublic sealed class BasicInstrumentBank : IInstrumentBank\n{\n    public BasicInstrumentBank(AudioFormat format);\n    public Instrument GetInstrument(int bank, int program);\n}\n```\n\n**Description:**\nA lightweight, zero-dependency instrument bank. It contains a hardcoded set of procedural instruments (using basic waveforms like Sine, Sawtooth, Square).\n\n**Use Cases:**\n*   **Testing**: Verifying synthesis logic without needing external files.\n*   **Fallback**: Ensuring the synthesizer always has *something* to play if a requested SoundFont fails to load.\n*   **Presets**: Typically includes basic General MIDI approximations (e.g., Piano, Strings, Lead).\n\n---\n\n### Synthesis `MultiInstrumentBank`\n\n```csharp\npublic sealed class MultiInstrumentBank : IInstrumentBank, IDisposable\n{\n    public MultiInstrumentBank(Instrument masterFallbackInstrument);\n\n    public void AddBank(IInstrumentBank bank);\n    public bool RemoveBank(IInstrumentBank bank);\n    public void ClearBanks();\n    public Instrument GetInstrument(int bank, int program);\n    public void Dispose();\n}\n```\n\n**Description:**\nA composite implementation of `IInstrumentBank`. It maintains an ordered list of child banks.\n\n**Resolution Logic:**\nWhen `GetInstrument(bank, program)` is called:\n1.  It iterates through the child banks in **reverse order** (LIFO / Priority).\n2.  It calls `GetInstrument` on each child bank.\n3.  If a child bank returns a valid instrument (i.e., `!instrument.IsFallback`), that instrument is returned immediately.\n4.  If no valid instrument is found in any child bank, `masterFallbackInstrument` is returned.\n\nThis allows users to \"stack\" banks. For example, loading a Piano SF2 first, then a Synth SF2. If the Synth SF2 doesn't have a piano patch, the request falls through to the Piano SF2.\n\n---\n\n### Synthesis.Instruments `Instrument`\n\n```csharp\npublic class Instrument\n{\n    public Instrument(List<VoiceMapping> mappings, VoiceDefinition fallbackDefinition, bool isFallback = false);\n\n    public bool IsFallback { get; }\n    public bool IsEmpty { get; }\n\n    public VoiceDefinition GetVoiceDefinition(int noteNumber, int velocity);\n}\n```\n\n**Description:**\nRepresents a single patch (e.g., \"Grand Piano\"). It is essentially a collection of `VoiceMapping` rules.\n\n**Methods:**\n*   **`GetVoiceDefinition(...)`**: The core lookup logic. It scans the internal mappings to find the one that covers the requested Note Number and Velocity range. This enables:\n*   **Key Splitting**: Different sounds for different keyboard ranges (e.g., Bass on left hand, Piano on right).\n*   **Velocity Layering**: Different samples for soft vs. hard key presses.\n\n---\n\n### Synthesis.Instruments `VoiceDefinition`\n\n```csharp\npublic class VoiceDefinition\n{\n    public VoiceDefinition(AudioFormat format, Oscillator.WaveformType oscType, int unison, float detune, float attack, float decay, float sustain, float release, bool useFilter = false);\n\n    public IVoice CreateVoice(VoiceContext context);\n}\n```\n\n**Description:**\nA factory/blueprint for creating voices. It stores the static configuration for a sound (e.g., ADSR times, oscillator type, sample reference). It does *not* hold state for an active note.\n\n**Function:**\nWhen `CreateVoice` is called (on Note On), it instantiates a new `IVoice` object (either `Voice` for synth sounds or `SampleVoice` for sampled sounds), injecting the configuration defined here.\n\n---\n\n### Synthesis.Instruments `VoiceMapping`\n\n```csharp\npublic class VoiceMapping\n{\n    public VoiceMapping(VoiceDefinition definition);\n\n    public VoiceDefinition Definition { get; }\n    public int MinKey { get; set; }\n    public int MaxKey { get; set; }\n    public int MinVelocity { get; set; }\n    public int MaxVelocity { get; set; }\n\n    // Tuning & Panning Offsets\n    public float Pan { get; set; }\n    public int Tune { get; set; }\n\n    public bool IsMatch(int noteNumber, int velocity);\n}\n```\n\n**Description:**\nDefines the criteria under which a specific `VoiceDefinition` should be used. It specifies the valid range of MIDI keys and velocities.\n\n---\n\n### Synthesis.Interfaces `IGenerator`\n\n```csharp\npublic interface IGenerator\n{\n    int Generate(Span<float> buffer, VoiceContext context);\n    void Reset();\n}\n```\n\n**Description:**\nThe fundamental building block of the synthesis engine's signal chain.\n*   **Generators**: Produce audio (e.g., `OscillatorGenerator`, `SamplerGenerator`) or control signals (e.g., `AdsrGenerator`).\n*   **Context**: The `VoiceContext` passed to `Generate` provides current real-time data (frequency, sample rate) needed for signal generation.\n\n---\n\n### Synthesis.Interfaces `IVoice`\n\n```csharp\npublic interface IVoice\n{\n    int NoteNumber { get; }\n    int Velocity { get; }\n    bool IsReleasing { get; }\n    bool IsSustained { get; set; }\n    bool IsFinished { get; }\n\n    void Render(Span<float> buffer);\n    void NoteOff();\n    void Kill();\n    void ProcessMidiControl(MidiMessage message, float channelPitchBend);\n\n    void SetPerNotePitchBend(float semitones);\n    void SetPerNotePressure(float value);\n    void SetPerNoteTimbre(float value);\n}\n```\n\n**Description:**\nThe interface for an active, sounding voice.\n\n**Lifecycle:**\n1.  Created by `Synthesizer` on Note On.\n2.  `Render()` is called repeatedly to fill audio buffers.\n3.  `NoteOff()` triggers the release envelope.\n4.  `IsFinished` becomes true when the envelope completes.\n5.  Synthesizer removes/recycles the voice.\n\n**MPE Support**:\nThe `SetPerNote...` methods allow individual voices to be modulated independently of the global channel state, enabling polyphonic expression.\n\n---\n\n### Editing `MidiExporter`\n\n```csharp\npublic static class MidiExporter\n{\n    public static Task ExportAsync(Composition composition, string filePath);\n}\n```\n\n**Description:**\nA utility to export the current `Composition` state to a standard `.mid` file.\n*   Converts internal `TempoTrack` markers to MIDI Meta Events (Set Tempo).\n*   Converts all `MidiTrack` segments and their absolute timings into standard MIDI Track Chunks.\n*   Handles merging multiple segments into single continuous tracks as required by the SMF specification.\n\n---\n\n### Editing `QuantizationSettings`\n\n```csharp\npublic record QuantizationSettings\n{\n    public enum GridInterval { WholeNote, ... SixteenthNote, ... }\n\n    public GridInterval Grid { get; init; } = GridInterval.SixteenthNote;\n    public double Strength { get; init; } = 1.0;\n    public bool QuantizeNoteEnd { get; init; } = false;\n    public double Swing { get; init; } = 0.5;\n}\n```\n\n**Description:**\nParameters for the quantization algorithm.\n*   **`Strength`**: (0.0 - 1.0) Interpolates the note position between its original time and the grid time. Allows for \"soft\" quantization.\n*   **`Swing`**: (0.0 - 1.0) Adjusts the grid itself. 0.5 is straight. >0.5 delays every second grid point to create a swing/shuffle feel.\n\n---\n\n### Editing `MidiNote`\n\n```csharp\npublic sealed class MidiNote\n{\n    public MidiNote(long startTick, long durationTicks, int noteNumber, int velocity);\n\n    public Guid Id { get; }\n    public long StartTick { get; set; }\n    public long DurationTicks { get; set; }\n    public int NoteNumber { get; set; }\n    public int Velocity { get; set; }\n}\n```\n\n**Description:**\nThe high-level editing model for a note.\n*   **Abstraction**: Unlike raw MIDI (which consists of separate On/Off events), this object represents the *entire note* with a start and duration.\n*   **Editing**: Changing properties here (e.g., `StartTick`) automatically updates the underlying raw event stream in the `MidiSequence`.\n\n---\n\n### Editing `ControlPoint`\n\n```csharp\npublic sealed class ControlPoint\n{\n    public ControlPoint(long tick, int value);\n\n    public Guid Id { get; }\n    public long Tick { get; set; }\n    public int Value { get; set; }\n}\n```\n\n**Description:**\nRepresents a single node in an automation curve (CC or Pitch Bend). Used by `MidiSequence` to manage continuous controller data in an editable way.\n\n---\n\n### Editing `TempoMarker`\n\n```csharp\npublic readonly record struct TempoMarker(TimeSpan Time, double BeatsPerMinute);\n```\n\n**Description:**\nA data structure used in the `Composition.TempoTrack` list. It defines a specific BPM value taking effect at a specific absolute time. The system interpolates time between these markers to calculate MIDI ticks.\n\n---\n\n### Midi.Enums `SyncMode`\n\n```csharp\npublic enum SyncMode\n{\n    Off,\n    Master,\n    Slave\n}\n```\n*   **`Off`**: No external synchronization.\n*   **`Master`**: The engine generates MIDI Clock messages based on its internal transport.\n*   **`Slave`**: The engine's transport is driven by incoming MIDI messages.\n\n### Midi.Enums `SyncSource`\n\n```csharp\npublic enum SyncSource\n{\n    Internal,\n    MidiClock,\n    Mtc\n}\n```\n*   **`Internal`**: Standard internal clock (used when `SyncMode` is Off/Master).\n*   **`MidiClock`**: Sync to MIDI Beat Clock (0xF8) messages. Tempo is derived from the clock rate.\n*   **`Mtc`**: Sync to MIDI Time Code (quarter frame) messages. Position is absolute based on timecode.\n\n### Midi.Enums `SyncStatus`\n\n```csharp\npublic enum SyncStatus\n{\n    Unlocked,\n    Locked\n}\n```\n*   **`Unlocked`**: No valid clock signal is being received, or the signal is unstable.\n*   **`Locked`**: The engine has successfully locked onto the external clock source.\n\n### Midi.Enums `PortMidiError`\n\n```csharp\npublic enum PortMidiError\n{\n    NoError = 0,\n    HostError = -10000,\n    InvalidDeviceId,\n    InsufficientMemory,\n    BufferTooSmall,\n    BufferOverflow,\n    BadPtr,\n    BadData,\n    InternalError,\n    DeviceIsBusy\n}\n```\n**Description:**\nNative error codes returned by the PortMidi library. `PortBackendException` wraps these codes to provide descriptive .NET exceptions.\n\n### Midi.Modifier `ArpeggiatorModifier`\n\n```csharp\npublic sealed class ArpeggiatorModifier : MidiModifier, ITemporalMidiModifier\n{\n    public override string Name { get; } // \"Arpeggiator (Mode)\"\n\n    public ArpMode Mode { get; set; } = ArpMode.Up;\n    public int Octaves { get; set; } = 1;\n    public double Rate { get; set; } = 0.25;\n    public double Gate { get; set; } = 0.9;\n\n    public override IEnumerable<MidiMessage> Process(MidiMessage message);\n    public IEnumerable<MidiMessage> Tick(double deltaSeconds, double bpm);\n    public void ResetState();\n}\n```\n\n**Description:**\nA stateful, temporal MIDI modifier that transforms held chords into rhythmic arpeggio patterns. Unlike standard modifiers that output messages immediately upon input, the Arpeggiator buffers incoming `NoteOn` messages and generates a stream of new notes over time based on a clock signal.\n\nThis class implements `ITemporalMidiModifier`, meaning it requires an external clock source (typically the `Synthesizer`'s audio render loop) to call its `Tick` method to advance its internal timing logic.\n\n**Properties:**\n\n*   `Mode`: Gets or sets the arpeggiation pattern direction.\n*   `Up`: Plays notes from lowest to highest.\n*   `Down`: Plays notes from highest to lowest.\n*   `UpDown`: Plays from lowest to highest, then back down.\n*   `Random`: Plays held notes in a random order.\n*   **`Octaves`**: Gets or sets the number of octaves the pattern will span. The arpeggiator will cycle through the held notes, then repeat them transposed up by one octave, up to this limit, before wrapping back to the base octave. Default is `1`.\n*   **`Rate`**: Gets or sets the step duration in musical beats (Quarter Notes).\n*   `1.0` = Quarter Note.\n*   `0.5` = Eighth Note.\n*   `0.25` = Sixteenth Note (Default).\n*   **`Gate`**: Gets or sets the note duration as a fraction of the step `Rate`.\n*   Range: `0.0` to `1.0`.\n*   Example: A value of `0.5` means the note will sound for half the step time and be silent for the other half (staccato). A value of `1.0` creates a legato feel. Default is `0.9`.\n\n**Methods:**\n\n*   **`Process(MidiMessage message)`**: Intercepts `NoteOn` and `NoteOff` messages to update the internal list of held notes. These messages are consumed and **not** passed through to the output immediately. All other message types (CC, PitchBend) are passed through unchanged.\n*   **`Tick(double deltaSeconds, double bpm)`**: Advances the arpeggiator's internal clock.\n*   `deltaSeconds`: The time elapsed since the last tick.\n*   `bpm`: The current tempo in Beats Per Minute.\n*   **Returns**: An enumerable of generated `MidiMessage`s (new Note On/Off events) if a step boundary was crossed during this tick.\n*   **`ResetState()`**: Resets the internal playback index, octave counter, and timing accumulator to zero. Useful for re-syncing the pattern when the transport starts or when the first note of a new phrase is played.\n\n---\n\n### Midi.Modifier `ChannelFilterModifier`\n\n```csharp\npublic sealed class ChannelFilterModifier : MidiModifier\n{\n    public ChannelFilterModifier(int channel);\n\n    public override string Name { get; } // \"Channel Filter (Channel)\"\n    public int Channel { get; set; }\n\n    public override IEnumerable<MidiMessage> Process(MidiMessage message);\n}\n```\n\n**Description:**\nA utility modifier that filters the MIDI stream, allowing only messages on a specific MIDI channel to pass through. All messages on other channels are dropped. This is useful for creating keyboard splits or isolating specific controllers in a complex routing graph.\n\n**Constructor:**\n\n*   `ChannelFilterModifier(int channel)`: Initializes the filter. `channel` should be 1-16.\n\n**Properties:**\n\n*   **`Channel`**: Gets or sets the target MIDI channel (1-16) to allow.\n\n**Methods:**\n\n*   **`Process(MidiMessage message)`**: Checks the `Channel` property of the incoming message.\n*   If `message.Channel == this.Channel`, the message is returned.\n*   Otherwise, an empty collection is returned (dropping the message).\n*   If `IsEnabled` is false, the message is passed through regardless of channel.\n\n---\n\n### Midi.Modifier `HarmonizerModifier`\n\n```csharp\npublic sealed class HarmonizerModifier : MidiModifier\n{\n    public HarmonizerModifier(int[] intervals);\n\n    public override string Name { get; } // \"Harmonizer (N Notes)\"\n    public int[] Intervals { get; set; }\n\n    public override IEnumerable<MidiMessage> Process(MidiMessage message);\n}\n```\n\n**Description:**\nA modifier that automatically generates chords from single input notes. For every incoming `NoteOn` or `NoteOff` message, it generates multiple messages based on a defined list of semitone intervals relative to the root note.\n\n**Constructor:**\n\n*   `HarmonizerModifier(int[] intervals)`: Initializes the harmonizer with a set of intervals.\n*   Example: `new int[] { 0, 4, 7 }` creates a Major triad (Root, Major 3rd, Perfect 5th).\n\n**Properties:**\n\n*   **`Intervals`**: Gets or sets the array of intervals in semitones.\n*   `0` represents the original root note.\n*   Negative values generate intervals below the root.\n*   If this array is empty, the original message is passed through unchanged.\n\n**Methods:**\n\n*   **`Process(MidiMessage message)`**:\n*   For `NoteOn`/`NoteOff`: Iterates through `Intervals`. For each interval, calculates `NewNote = OriginalNote + Interval`. Resulting notes are clamped to the valid MIDI range (0-127). Returns the collection of generated chord notes.\n*   For other messages: Passes them through unchanged.\n\n---\n\n### Midi.Modifier `RandomizerModifier`\n\n```csharp\npublic sealed class RandomizerModifier : MidiModifier\n{\n    public RandomizerModifier();\n\n    public override string Name { get; } // \"Randomizer\"\n\n    public float Chance { get; set; } = 1.0f;\n    public float VelocityRandomness { get; set; } = 0.0f;\n    public float VelocityBias { get; set; } = 0.0f;\n    public float PitchRandomness { get; set; } = 0.0f;\n    public int PitchRange { get; set; } = 12;\n    public int MinNote { get; set; } = 0;\n    public int MaxNote { get; set; } = 127;\n\n    public override IEnumerable<MidiMessage> Process(MidiMessage message);\n}\n```\n\n**Description:**\nA powerful stochastic modifier that introduces controlled randomness to MIDI note data. It can be used to \"humanize\" sequences (subtle velocity changes) or for generative music (random pitch/probability). It maintains internal state to ensure that if a note's pitch is randomized on `NoteOn`, the corresponding `NoteOff` is randomized to the exact same pitch, preventing stuck notes.\n\n**Properties:**\n\n*   **`Chance`**: The probability that a note will be played (0.0 to 1.0).\n*   `1.0`: Always play.\n*   `0.5`: 50% chance to play.\n*   If a note is skipped based on chance, it is dropped completely.\n*   **`VelocityRandomness`**: The amount of random deviation applied to velocity (0.0 to 1.0).\n*   `0.0`: No change.\n*   `1.0`: Maximum deviation (full range).\n*   **`VelocityBias`**: Biases the velocity randomization towards higher or lower values (-1.0 to 1.0).\n*   `-1.0`: Tends towards lower velocities (softer).\n*   `0.0`: Balanced randomization.\n*   `1.0`: Tends towards higher velocities (louder).\n*   **`PitchRandomness`**: The probability that a note's pitch will be randomized (0.0 to 1.0).\n*   **`PitchRange`**: The maximum range in semitones (+/-) for pitch randomization.\n*   Example: `12` means the note can shift up or down by up to one octave.\n*   **`MinNote`** / **`MaxNote`**: Defines the key range (0-127) affected by this modifier. Notes outside this range are passed through unmodified.\n\n**Methods:**\n\n*   **`Process(MidiMessage message)`**: Applies randomization logic to `NoteOn` messages. Stores state for pitch-shifted notes to ensure correct `NoteOff` handling.\n\n---\n\n### Midi.Modifier `TransposeModifier`\n\n```csharp\npublic sealed class TransposeModifier : MidiModifier\n{\n    public TransposeModifier(int semitones);\n\n    public override string Name { get; } // \"Transpose (N st)\"\n    public int Semitones { get; set; }\n\n    public override IEnumerable<MidiMessage> Process(MidiMessage message);\n}\n```\n\n**Description:**\nA simple modifier that shifts the pitch of all incoming `NoteOn` and `NoteOff` messages by a fixed number of semitones.\n\n**Constructor:**\n\n*   `TransposeModifier(int semitones)`: Initializes the modifier with the specified shift.\n\n**Properties:**\n\n*   **`Semitones`**: Gets or sets the transposition amount.\n*   Positive values transpose up (e.g., `12` = up 1 octave).\n*   Negative values transpose down (e.g., `-12` = down 1 octave).\n\n**Methods:**\n\n*   **`Process(MidiMessage message)`**: Adds `Semitones` to `message.NoteNumber`. The result is clamped to the valid MIDI note range (0-127). Non-note messages are passed through unchanged.\n\n---\n\n### Midi.Modifier `VelocityModifier`\n\n```csharp\npublic sealed class VelocityModifier : MidiModifier\n{\n    public override string Name { get; } // \"Velocity\"\n\n    public float Curve { get; set; } = 0.0f;\n    public int MinVelocity { get; set; } = 1;\n    public int MaxVelocity { get; set; } = 127;\n    public int Add { get; set; } = 0;\n\n    public override IEnumerable<MidiMessage> Process(MidiMessage message);\n}\n```\n\n**Description:**\nA modifier designed to reshape the dynamic response of MIDI notes. It can compress, expand, offset, or apply non-linear curves to the velocity of incoming `NoteOn` messages.\n\n**Properties:**\n\n*   **`Curve`**: Gets or sets the velocity response curve (-1.0 to 1.0).\n*   `0.0`: Linear response (Input = Output).\n*   `-1.0` (Logarithmic): Makes it easier to play loud notes (boosts low velocities).\n*   `+1.0` (Exponential): Makes it harder to play loud notes (attenuates low velocities), offering more dynamic control for expressive playing.\n*   *Math:* The curve uses an exponent derived from $2^{-c}$.\n*   **`MinVelocity`**: Clamps the minimum output velocity. Input velocities that would map below this value are set to this value. Range 1-127.\n*   **`MaxVelocity`**: Clamps the maximum output velocity. Range 1-127.\n*   **`Add`**: A fixed integer value added to the velocity after the curve and range calculations. Can be positive (boost) or negative (attenuate).\n\n**Methods:**\n\n*   **`Process(MidiMessage message)`**:\n1.  Normalizes input velocity (0-1).\n2.  Applies `Curve`.\n3.  Remaps the result to the range [`MinVelocity`, `MaxVelocity`].\n4.  Adds `Add`.\n5.  Clamps final result to 1-127.\n6.  Returns the modified message.\n\n### Extensions.WebRtc.Apm\n\n#### `AudioProcessingModule` (Class)\n```csharp\npublic class AudioProcessingModule : IDisposable\n{\n    public AudioProcessingModule();\n    public ApmError ApplyConfig(ApmConfig config);\n    public ApmError Initialize();\n    public ApmError ProcessStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    public ApmError ProcessReverseStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    // ... other methods for setting delays, levels, runtime settings, getting info, AEC dump ...\n    public static int GetFrameSize(int sampleRateHz);\n    public void Dispose();\n}\n```\n**Description:** Provides low-level access to the WebRTC Audio Processing Module. It's responsible for initializing the APM, applying configurations, and processing audio frames. Generally used internally by `WebRtcApmModifier` and `NoiseSuppressor`.\n\n#### `ApmConfig` (Class)\n```csharp\npublic class ApmConfig : IDisposable\n{\n    public ApmConfig();\n    public void SetEchoCanceller(bool enabled, bool mobileMode);\n    public void SetNoiseSuppression(bool enabled, NoiseSuppressionLevel level);\n    public void SetGainController1(bool enabled, GainControlMode mode, int targetLevelDbfs, int compressionGainDb, bool enableLimiter);\n    public void SetGainController2(bool enabled);\n    public void SetHighPassFilter(bool enabled);\n    public void SetPreAmplifier(bool enabled, float fixedGainFactor);\n    public void SetPipeline(int maxInternalRate, bool multiChannelRender, bool multiChannelCapture, DownmixMethod downmixMethod);\n    public void Dispose();\n}\n```\n**Description:** Used to configure the features of the `AudioProcessingModule` such as echo cancellation, noise suppression, gain control, etc.\n\n#### `StreamConfig` (Class)\n```csharp\npublic class StreamConfig : IDisposable\n{\n    public StreamConfig(int sampleRateHz, int numChannels);\n    public int SampleRateHz { get; }\n    public int NumChannels { get; }\n    public void Dispose();\n}\n```\n**Description:** Defines the properties (sample rate, number of channels) of an audio stream being processed by the APM.\n\n#### `ProcessingConfig` (Class)\nThis class holds multiple `StreamConfig` instances for different parts of the APM pipeline (input, output, reverse input, reverse output).\n\n#### `NoiseSuppressor` (Component - `SoundFlow.Extensions.WebRtc.Apm.Components`)\n```csharp\npublic class NoiseSuppressor : IDisposable\n{\n    public NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High, bool useMultichannelProcessing = false);\n    public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;\n    public float[] ProcessAll();\n    public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null);\n    public void Dispose();\n}\n```\n**Description:** A component for offline/batch noise suppression using WebRTC APM. It takes an `ISoundDataProvider`, processes its audio, and outputs the cleaned audio either as a whole or in chunks.\n**Key Members:**\n*   `NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, ...)`: Constructor.\n*   `OnAudioChunkProcessed` (event): Raised when a chunk of audio is processed.\n*   `ProcessAll()`: Processes the entire audio stream and returns it.\n*   `ProcessChunks()`: Processes audio in chunks, raising `OnAudioChunkProcessed`.\n\n#### `WebRtcApmModifier` (Modifier - `SoundFlow.Extensions.WebRtc.Apm.Modifiers`)\n```csharp\npublic sealed class WebRtcApmModifier : SoundModifier, IDisposable\n{\n    public WebRtcApmModifier(\n        bool aecEnabled = false, bool aecMobileMode = false, int aecLatencyMs = 40,\n        bool nsEnabled = false, NoiseSuppressionLevel nsLevel = NoiseSuppressionLevel.High,\n        // ... other AGC, HPF, PreAmp, Pipeline settings ...\n    );\n\n    public override string Name { get; set; }\n    public EchoCancellationSettings EchoCancellation { get; }\n    public NoiseSuppressionSettings NoiseSuppression { get; }\n    public AutomaticGainControlSettings AutomaticGainControl { get; }\n    public ProcessingPipelineSettings ProcessingPipeline { get; }\n    public bool HighPassFilterEnabled { get; set; }\n    public bool PreAmplifierEnabled { get; set; }\n    public float PreAmplifierGainFactor { get; set; }\n    public float PostProcessGain { get; set; }\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n    public void Dispose();\n}\n```\n\n**Description:** A `SoundModifier` that applies various WebRTC APM features (AEC, NS, AGC, HPF, PreAmp) to an audio stream in real-time.\n\n**Key Members:**\n*   Constructor with detailed initial settings.\n*   Properties for configuring each APM feature (`EchoCancellation`, `NoiseSuppression`, `AutomaticGainControl`, `ProcessingPipeline`, `HighPassFilterEnabled`, etc.).\n*   `Process(Span<float> buffer, int channels)`: Core processing logic.\n*   `Dispose()`: Releases native APM resources.\n\n#### Enums for WebRTC APM\n*   `ApmError`: Error codes.\n*   `NoiseSuppressionLevel`: Low, Moderate, High, VeryHigh.\n*   `GainControlMode`: AdaptiveAnalog, AdaptiveDigital, FixedDigital.\n*   `DownmixMethod`: AverageChannels, UseFirstChannel.\n*   `RuntimeSettingType`: Types for runtime APM settings.\n\n### Codecs.FFMpeg `FFmpegCodecFactory`\n\n```csharp\npublic sealed class FFmpegCodecFactory : ICodecFactory\n{\n    public string FactoryId { get; }\n    public IReadOnlyCollection<string> SupportedFormatIds { get; }\n    public int Priority { get; }\n\n    public ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format);\n    public ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);\n    public ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format);\n}\n```\n**Description:** An implementation of `ICodecFactory` that uses a native FFmpeg wrapper to provide robust decoding and encoding for a broad spectrum of audio formats, including popular lossy (MP3, AAC, OGG, Opus) and lossless (FLAC, ALAC) codecs. To enable these formats, an instance of this factory must be registered with the `AudioEngine` using `engine.RegisterCodecFactory(new FFmpegCodecFactory())`.\n\n**Properties:**\n\n*   `FactoryId`: A unique string identifier for this factory, which is `\"SoundFlow.Codecs.FFMpeg\"`.\n*   `SupportedFormatIds`: A read-only collection of supported format strings derived from the specific FFmpeg build configuration. This list includes, but is not limited to:\n*   **Lossless:** `\"wav\"`, `\"aiff\"`, `\"flac\"`, `\"alac\"`, `\"ape\"`, `\"wv\"`, `\"tta\"`, `\"shn\"`\n*   **Lossy:** `\"mp3\"`, `\"mp2\"`, `\"ogg\"`, `\"opus\"`, `\"aac\"`, `\"m4a\"`, `\"wma\"`, `\"ac3\"`\n*   **Container/Other:** `\"mka\"`, `\"mpc\"`, `\"tak\"`, `\"ra\"`, `\"dsf\"`, `\"au\"`, `\"gsm\"`\n*   `Priority`: The priority of this factory, which is `100`. This high value ensures it will be tried before lower-priority built-in codecs (like the `MiniAudioCodecFactory`) for any formats they might both support.\n\n**Methods:**\n\n*   `CreateDecoder(Stream stream, string formatId, AudioFormat format)`: Creates an internal `FFmpegDecoder` instance for the specified format if it is in the `SupportedFormatIds` list.\n*   `TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null)`: Probes the input stream using the native FFmpeg format detection capabilities to determine the audio format and create a decoder. On success, `detectedFormat` is populated with the precise format discovered in the stream.\n*   `CreateEncoder(Stream stream, string formatId, AudioFormat format)`: Creates an internal `FFmpegEncoder` instance for the specified format. It includes special logic to handle container formats like `\"m4a\"`, automatically selecting the `alac` encoder for high-bit-depth lossless audio or `aac` for standard lossy audio.\n\n### Codecs.FFMpeg `FFmpegException`\n\n```csharp\npublic sealed class FFmpegException : BackendException\n{\n    public FFmpegException(FFmpegResult result, string? message);\n\n    public FFmpegResult Result { get; }\n}\n```\n\n**Description:** Represents an error that occurred within the native `soundflow-ffmpeg` wrapper library. It inherits from `BackendException` and provides a strongly-typed `FFmpegResult` enum value that corresponds to a specific error code from the native C API, making it easier to diagnose issues.\n\n**Constructor:**\n\n*   `FFmpegException(FFmpegResult result, string? message)`: Initializes the exception with the native result code and an optional custom message. If the message is null, a descriptive error message is generated from the `result` code.\n\n**Properties:**\n\n*   `Result`: Gets the detailed `FFmpegResult` enum value that caused the exception. This provides specific insight into what failed (e.g., `DecoderErrorCodecNotFound`, `EncoderErrorWriteHeader`).\n\n\n### Interfaces `ICodecFactory`\n\n```csharp\npublic interface ICodecFactory\n{\n    string FactoryId { get; }\n    IReadOnlyCollection<string> SupportedFormatIds { get; }\n    int Priority { get; }\n\n    ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format);\n    ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);\n    ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format);\n}\n```\n\n**Description:** Defines a factory for creating custom `ISoundDecoder` and `ISoundEncoder` instances. This allows developers to extend the audio engine with support for new formats or alternative implementations (e.g., using a platform-specific library like FFmpeg).\n\n**Properties:**\n\n*   `FactoryId`: A unique string identifier for the factory.\n*   `SupportedFormatIds`: A collection of lowercase string identifiers for the audio formats this factory supports (e.g., \"mp3\", \"flac\").\n*   `Priority`: The priority level. Higher numbers are tried first when searching for a codec.\n\n**Methods:**\n\n*   `CreateDecoder(...)`: Creates a decoder instance for a known format ID.\n*   `TryCreateDecoder(...)`: Attempts to create a decoder by probing an unknown stream. This method should detect the format and return the resulting `AudioFormat`.\n*   `CreateEncoder(...)`: Creates an encoder instance for a specific format ID.\n\n### Interfaces `IMidiBackend`\n\n```csharp\npublic interface IMidiBackend : IDisposable\n{\n    void Initialize(AudioEngine engine);\n    MidiInputDevice CreateMidiInputDevice(MidiDeviceInfo deviceInfo);\n    MidiOutputDevice CreateMidiOutputDevice(MidiDeviceInfo deviceInfo);\n    void UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs);\n}\n```\n\n**Description:** Defines the contract for a pluggable MIDI backend, responsible for creating and managing MIDI devices.\n\n**Methods:**\n\n*   `Initialize(AudioEngine engine)`: Initializes the backend with a reference to the parent engine.\n*   `CreateMidiInputDevice(MidiDeviceInfo deviceInfo)`: Creates a platform-specific MIDI input device instance.\n*   `CreateMidiOutputDevice(MidiDeviceInfo deviceInfo)`: Creates a platform-specific MIDI output device instance.\n*   `UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs)`: Retrieves the list of available MIDI input and output devices.\n\n### Interfaces `IMidiMappable`\n\n```csharp\npublic interface IMidiMappable\n{\n    Guid Id { get; }\n}\n```\n\n**Description:** Defines an interface for objects that can be a target for MIDI mapping (e.g., a `SoundModifier` or `TrackSettings`). It ensures that any mappable object has a stable, unique identifier.\n\n### Interfaces `IMidiControllable`\n\n```csharp\npublic interface IMidiControllable\n{\n    void ProcessMidiMessage(MidiMessage message);\n}\n```\n\n**Description:** Defines an interface for components that can be directly controlled by incoming MIDI messages (e.g., a `Synthesizer`).\n\n### Interfaces `ISoundDataProvider`\n\n```csharp\npublic interface ISoundDataProvider : IDisposable\n{\n    int Position { get; }\n    int Length { get; }\n    bool CanSeek { get; }\n    SampleFormat SampleFormat { get; }\n    int SampleRate { get; }\n    bool IsDisposed { get; }\n    SoundFormatInfo? FormatInfo { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n    event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    int ReadBytes(Span<float> buffer);\n    void Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Can be -1 if length is unknown.\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n*   `IsDisposed`: Indicates whether the provider has been disposed.\n*   `FormatInfo`: Detailed format and tag information read from the source file. Can be `null`.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio data.\n*   `Dispose()`: Releases resources held by the data provider.\n\n### Interfaces `ISoundDecoder`\n\n```csharp\npublic interface ISoundDecoder : IDisposable\n{\n    bool IsDisposed { get; }\n    int Length { get; }\n    SampleFormat SampleFormat { get; }\n    int Channels { get; }\n    int SampleRate { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n\n    int Decode(Span<float> samples);\n    bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the decoder has been disposed.\n*   `Length`: The total length of the decoded audio data (in samples).\n*   `SampleFormat`: The sample format of the decoded audio data.\n*   `Channels`: The number of channels in the decoded audio data.\n*   `SampleRate`: The sample rate of the decoded audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n*   `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio stream.\n*   `Dispose()`: Releases the resources used by the decoder.\n\n### Interfaces `ISoundEncoder`\n\n```csharp\npublic interface ISoundEncoder : IDisposable\n{\n    bool IsDisposed { get; }\n\n    int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples.\n*   `Dispose()`: Releases the resources used by the encoder.\n\n\n### Interfaces `ISoundPlayer`\n\n```csharp\npublic interface ISoundPlayer\n{\n    PlaybackState State { get; }\n    bool IsLooping { get; set; }\n    float PlaybackSpeed { get; set; }\n    float Volume { get; set; }\n    float Time { get; }\n    float Duration { get; }\n    float LoopStartSeconds { get; }\n    float LoopEndSeconds { get; }\n    int LoopStartSamples { get; }\n    int LoopEndSamples { get; }\n\n    void Play();\n    void Pause();\n    void Stop();\n    bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    bool Seek(float time);\n    bool Seek(int sampleOffset);\n    void SetLoopPoints(float startTime, float? endTime = -1f);\n    void SetLoopPoints(int startSample, int endSample = -1);\n    void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `State`: The current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Whether looping is enabled or disabled (`get`, `set`).\n*   `PlaybackSpeed`: Gets or sets the playback speed. 1.0 is normal speed.\n*   `Volume`: Gets or sets the volume of the sound player (0.0 to 1.0 or higher for gain).\n*   `Time`: The current playback position (in seconds).\n*   `Duration`: The total duration of the audio (in seconds).\n*   `LoopStartSeconds`: Gets the configured loop start point in seconds.\n*   `LoopEndSeconds`: Gets the configured loop end point in seconds.  -1 indicates looping to the natural end.\n*   `LoopStartSamples`: Gets the configured loop start point in samples.\n*   `LoopEndSamples`: Gets the configured loop end point in samples. -1 indicates looping to the natural end.\n\n**Methods:**\n\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to the specified time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to the specified time (in seconds). Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset (in samples). Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures custom loop points using start and end times in seconds. `endTime` is optional;  use -1 or `null` to loop to the natural end.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures custom loop points using start and end sample indices. `endSample` is optional; use -1 to loop to the natural end.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures custom loop points using `TimeSpan`.\n\n### Interfaces `IVisualizationContext`\n\n```csharp\npublic interface IVisualizationContext\n{\n    void Clear();\n    void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f);\n    void DrawRectangle(float x, float y, float width, float height, Color color);\n}\n```\n\n**Methods:**\n\n*   `Clear()`: Clears the drawing surface.\n*   `DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)`: Draws a line from (`x1`, `y1`) to (`x2`, `y2`) with the specified color and thickness.\n*   `DrawRectangle(float x, float y, float width, float height, Color color)`: Draws a rectangle with the specified position, dimensions, and color.\n\n### Interfaces `IVisualizer`\n\n```csharp\npublic interface IVisualizer : IDisposable\n{\n    string Name { get; }\n\n    event EventHandler VisualizationUpdated;\n\n    void ProcessOnAudioData(Span<float> audioData);\n    void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes a chunk of audio data for visualization.\n*   `Render(IVisualizationContext context)`: Renders the visualization using the provided `IVisualizationContext`.\n*   `Dispose()`: Releases the resources used by the visualizer.\n\n### Metadata `SoundMetadataReader`\n\n```csharp\npublic static class SoundMetadataReader\n{\n    public static Result<SoundFormatInfo> Read(string filePath, ReadOptions? options = null);\n    public static Task<Result<SoundFormatInfo>> ReadAsync(string filePath, ReadOptions? options = null);\n    public static Result<SoundFormatInfo> Read(Stream stream, ReadOptions? options = null);\n    public static Task<Result<SoundFormatInfo>> ReadAsync(Stream stream, ReadOptions? options = null, bool leaveOpen = false);\n}\n```\n\n**Description:** A static class that provides a unified API for reading format information and metadata tags from various audio file formats. It automatically identifies the file type and uses the appropriate internal parser.\n\n**Methods:**\n\n*   `Read(string filePath, ...)`: Synchronously reads metadata from a file at the given path.\n*   `ReadAsync(string filePath, ...)`: Asynchronously reads metadata from a file at the given path.\n*   `Read(Stream stream, ...)`: Synchronously reads metadata from a stream. The stream must be readable and seekable.\n*   `ReadAsync(Stream stream, ...)`: Asynchronously reads metadata from a stream.\n\nAll methods return a `Result<SoundFormatInfo>`. On success, the `Value` property contains a `SoundFormatInfo` object with the file's details. On failure, the `Error` property contains an `IError` object describing the issue (e.g., `UnsupportedFormatError`, `HeaderNotFoundError`).\n\n### Metadata `SoundMetadataWriter`\n\n```csharp\npublic static class SoundMetadataWriter\n{\n    public static Result WriteTags(string filePath, SoundTags tags);\n    public static Task<Result> WriteTagsAsync(string filePath, SoundTags? tags);\n    public static Result RemoveTags(string filePath);\n    public static Task<Result> RemoveTagsAsync(string filePath);\n}\n```\n\n**Description:** A static class for writing or removing metadata tags from audio files. These operations are destructive and rewrite the file. The process is atomic: a temporary file is created, and only upon successful writing is the original file replaced.\n\n**Methods:**\n\n*   `WriteTags(string filePath, SoundTags tags)`: Synchronously writes the provided tags to the file, overwriting any existing tags.\n*   `WriteTagsAsync(string filePath, SoundTags? tags)`: Asynchronously writes the provided tags to the file.\n*   `RemoveTags(string filePath)`: Synchronously removes all recognizable metadata tags from the file.\n*   `RemoveTagsAsync(string filePath)`: Asynchronously removes all recognizable metadata tags from the file.\n\nAll methods return a `Result` object. `IsSuccess` will be `true` if the operation completed successfully.\n\n### Metadata `SoundFormatInfo` (and supporting models)\n\n```csharp\npublic class SoundFormatInfo\n{\n    public string FormatName { get; set; }\n    public string FormatIdentifier { get; set; }\n    public string CodecName { get; set; }\n    public string ContainerVersion { get; set; }\n    public TimeSpan Duration { get; set; }\n    public int ChannelCount { get; set; }\n    public int SampleRate { get; set; }\n    public int BitsPerSample { get; set; }\n    public int Bitrate { get; set; }\n    public BitrateMode BitrateMode { get; set; }\n    public bool IsLossless { get; set; }\n    public SoundTags? Tags { get; set; }\n    public CueSheet? Cues { get; set; }\n}\n\npublic class SoundTags\n{\n    public string Title { get; internal set; }\n    public string Artist { get; internal set; }\n    public string Album { get; internal set; }\n    public string Genre { get; internal set; }\n    public uint? Year { get; internal set; }\n    public uint? TrackNumber { get; internal set; }\n    public byte[]? AlbumArt { get; internal set; }\n    public string? Lyrics { get; internal set; }\n}\n\npublic class ReadOptions\n{\n    public bool ReadTags { get; set; } = true;\n    public bool ReadAlbumArt { get; set; }\n    public bool ReadCueSheet { get; set; }\n    public DurationAccuracy DurationAccuracy { get; set; } = DurationAccuracy.AccurateScan;\n}\n```\n\n**Description:** A set of classes that model the metadata of an audio file.\n\n*   **`SoundFormatInfo`**: The main container holding all discovered information about a file, including its format, duration, sample rate, and any tags or cue sheets that were read.\n*   **`SoundTags`**: Holds common metadata tags like artist, title, album, genre, year, track number, embedded album art, and lyrics.\n*   **`ReadOptions`**: A configuration object passed to `SoundMetadataReader` to control what data should be parsed (e.g., whether to read album art or scan for accurate duration).\n\n### Modifiers `AlgorithmicReverbModifier`\n\n```csharp\npublic sealed class AlgorithmicReverbModifier : SoundModifier\n{\n    public AlgorithmicReverbModifier(AudioFormat format);\n\n    public override string Name { get; set; }\n    [ControllableParameter(\"Wet\", 0.0, 1.0)] public float Wet { get; set; }\n    [ControllableParameter(\"Room Size\", 0.0, 1.0)] public float RoomSize { get; set; }\n    [ControllableParameter(\"Damping\", 0.0, 1.0)] public float Damp { get; set; }\n    [ControllableParameter(\"Width\", 0.0, 1.0)] public float Width { get; set; }\n    [ControllableParameter(\"Pre-Delay\", 0.0, 100.0)] public float PreDelay { get; set; }\n    [ControllableParameter(\"Mix\", 0.0, 1.0)] public float Mix { get; set; }\n\n    public override void ProcessMidiMessage(MidiMessage message);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Description:** An algorithmic reverb modifier based on the Freeverb algorithm. Now supports multi-channel processing, applying distinct delay lines and modulation to create a wider spatial effect. Can be controlled via MIDI CC messages.\n\n**Constructor:** `AlgorithmicReverbModifier(AudioFormat format)`: Initializes the modifier for the specified audio format.\n\n**Properties:**\n\n*   `Damp`: The damping factor of the reverb.\n*   `Name`: The name of the modifier.\n*   `PreDelay`: The pre-delay time (in milliseconds).\n*   `RoomSize`: The simulated room size.\n*   `Wet`: The wet mix amount.\n*   `Width`: The stereo width of the reverb.\n*   `Mix`: The wet/dry mix level of the reverb.\n\n**Methods:**\n\n*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control reverb parameters.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the reverb effect.\n\n### Modifiers `BassBoostModifier`\n\n```csharp\npublic class BassBoosterModifier : SoundModifier\n{\n    public BassBoosterModifier(AudioFormat format, float cutoff = 150f, float boostGainDb = 6f);\n\n    [ControllableParameter(\"Cutoff\", 20.0, 1000.0, MappingScale.Logarithmic)]\n    public float Cutoff { get; set; }\n    [ControllableParameter(\"Boost\", 0.0, 24.0)]\n    public float BoostGainDb { get; set; }\n\n    public override void ProcessMidiMessage(MidiMessage message);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Cutoff`: The cutoff frequency below which the bass boost is applied.\n*   `BoostGainDb`: The gain applied to the bass boost in dB.\n\n**Methods:**\n\n*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control boost parameters.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the bass boost effect.\n\n### Modifiers `ChorusModifier`\n\n```csharp\npublic sealed class ChorusModifier : SoundModifier\n{\n    public ChorusModifier(AudioFormat format, float depthMs = 2f, float rateHz = 0.5f, float feedback = 0.7f, float wetDryMix = 0.5f, float maxDelayMs = 50f);\n\n    [ControllableParameter(\"Depth\", 0.1, 8.0)] public float DepthMs { get; set; }\n    [ControllableParameter(\"Rate\", 0.05, 5.0)] public float RateHz { get; set; }\n    [ControllableParameter(\"Feedback\", 0.0, 0.95)] public float Feedback { get; } // Read-only\n    [ControllableParameter(\"Mix\", 0.0, 1.0)] public float WetDryMix { get; set; }\n\n    public override void ProcessMidiMessage(MidiMessage message);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `DepthMs`: The depth of the chorus effect in milliseconds.\n*   `RateHz`: The rate of the LFO modulation in Hz.\n*   `Feedback`: The feedback amount of the chorus effect.\n*   `WetDryMix`: The wet/dry mix of the chorus effect.\n\n**Methods:**\n\n*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control chorus parameters.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the chorus effect.\n\n### Modifiers `CompressorModifier`\n\n```csharp\npublic class CompressorModifier : SoundModifier\n{\n    public CompressorModifier(AudioFormat format, float thresholdDb, float ratio, float attackMs, float releaseMs, float kneeDb = 0, float makeupGainDb = 0);\n\n    [ControllableParameter(\"Threshold\", -60.0, 0.0)] public float ThresholdDb { get; set; }\n    [ControllableParameter(\"Ratio\", 1.0, 20.0)] public float Ratio { get; set; }\n    [ControllableParameter(\"Attack\", 0.1, 200.0, MappingScale.Logarithmic)] public float AttackMs { get; set; }\n    [ControllableParameter(\"Release\", 5.0, 2000.0, MappingScale.Logarithmic)] public float ReleaseMs { get; set; }\n    [ControllableParameter(\"Knee\", 0.0, 12.0)] public float KneeDb { get; set; }\n    [ControllableParameter(\"Makeup Gain\", 0.0, 24.0)] public float MakeupGainDb { get; set; }\n\n    public void UpdateParameters();\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `ThresholdDb`: The threshold (in dB) above which compression is applied.\n*   `Ratio`: The compression ratio.\n*   `AttackMs`: The attack time (in milliseconds).\n*   `ReleaseMs`: The release time (in milliseconds).\n*   `KneeDb`: The knee width (in dB).\n*   `MakeupGainDb`: The amount of makeup gain to apply after compression.\n\n**Methods:**\n\n*   `UpdateParameters()`: Recalculates internal coefficients after changing properties.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the compression effect.\n\n### Modifiers `DelayModifier`\n\n```csharp\npublic sealed class DelayModifier : SoundModifier\n{\n    public DelayModifier(int delaySamples, float feedback, float wetMix, float cutoff = 10000);\n\n    [ControllableParameter(\"Feedback\", 0.0, 0.99)] public float Feedback { get; set; }\n    [ControllableParameter(\"Mix\", 0.0, 1.0)] public float WetMix { get; set; }\n    [ControllableParameter(\"Cutoff\", 100.0, 20000.0, MappingScale.Logarithmic)] public float Cutoff { get; set; }\n\n    public override void ProcessMidiMessage(MidiMessage message);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Feedback`: The feedback amount of the delay.\n*   `WetMix`: The wet/dry mix of the delay (0 = dry, 1 = wet).\n*   `Cutoff`: The cutoff frequency for the low-pass filter applied to the delayed signal.\n\n**Methods:**\n\n*   `ProcessMidiMessage(MidiMessage message)`: Handles MIDI CC messages to control delay parameters.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the delay effect.\n\n### Modifiers `Filter`\n\n```csharp\npublic class Filter : SoundModifier\n{\n    public Filter(AudioFormat format);\n\n    public AudioFormat Format { get; set; }\n    [ControllableParameter(\"Filter Type\", 0, 3)] public FilterType Type { get; set; }\n    [ControllableParameter(\"Cutoff\", 20.0, 20000.0, MappingScale.Logarithmic)] public float CutoffFrequency { get; set; }\n    [ControllableParameter(\"Resonance\", 0.0, 1.0)] public float Resonance { get; set; }\n    public override string Name { get; set; } = \"Filter\";\n\n    public override void ProcessMidiMessage(MidiMessage message);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Description:** A `SoundModifier` that applies a digital biquad filter (low-pass, high-pass, band-pass, notch) to the audio signal and can be controlled via MIDI.\n\n**Properties:**\n\n*   `Format`: The audio format of the filter.\n*   `Type`: The filter type (`LowPass`, `HighPass`, `BandPass`, `Notch`).\n*   `CutoffFrequency`: The cutoff frequency of the filter.\n*   `Resonance`: The resonance of the filter.\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `ProcessMidiMessage(MidiMessage message)`: Handles MIDI CC messages to control cutoff and resonance.\n*   `ProcessSample(float sample, int channel)`: Applies the filter to a single audio sample.\n\n### Modifiers `FrequencyBandModifier`\n\n```csharp\npublic class FrequencyBandModifier : SoundModifier\n{\n    public FrequencyBandModifier(AudioFormat format, float lowCutoff, float highCutoff);\n\n    [ControllableParameter(\"High Cut\", 20.0, 20000.0, MappingScale.Logarithmic)]\n    public float HighCutoffFrequency { get; set; }\n    [ControllableParameter(\"Low Cut\", 20.0, 20000.0, MappingScale.Logarithmic)]\n    public float LowCutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `HighCutoffFrequency`: The high cutoff frequency of the frequency band.\n*   `LowCutoffFrequency`: The low cutoff frequency of the frequency band.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the frequency band modification.\n\n### Modifiers `NoiseReductionModifier`\n\n```csharp\npublic class NoiseReductionModifier : SoundModifier\n{\n    public NoiseReductionModifier(int fftSize = 2048, float alpha = 3f, float beta = 0.001f, float smoothingFactor = 0.9f, float gain = 1.5f, int noiseFrames = 5, VoiceActivityDetector? vad = null);\n\n    public override string Name { get; set; }\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `Process(Span<float> buffer, int channels)`: Processes an entire buffer of audio, applying noise reduction.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `NoiseReductionModifier` operates on buffers, not individual samples.\n\n### Modifiers `ParametricEqualizer`\n\n```csharp\npublic class ParametricEqualizer : SoundModifier\n{\n    public ParametricEqualizer(int channels);\n\n    public override string Name { get; set; }\n    \n    public void AddBand(EqualizerBand band);\n    public void AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth);\n    public void AddBands(IEnumerable<EqualizerBand> bands);\n    public void ClearBands();\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel);\n    public void RemoveBand(EqualizerBand band);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `AddBand(EqualizerBand band)`: Adds an `EqualizerBand` to the equalizer.\n*   `AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth)`: Adds an equalizer band with the specified parameters.\n*   `AddBands(IEnumerable<EqualizerBand> bands)`: Adds multiple equalizer bands.\n*   `ClearBands()`: Removes all equalizer bands.\n*   `Process(Span<float> buffer, int channels)`: Processes an entire buffer of audio, applying equalization.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `ParametricEqualizer` operates on buffers, not individual samples.\n*   `RemoveBand(EqualizerBand band)`: Removes a specific equalizer band.\n\n\n### Modifiers `MultiChannelChorusModifier`\n\n```csharp\npublic class MultiChannelChorusModifier : SoundModifier\n{\n    public MultiChannelChorusModifier(\n        AudioFormat format,\n        float wetMix,\n        int maxDelay,\n        params (float depth, float rate, float feedback)[] channelParameters);\n        \n    [ControllableParameter(\"Mix\", 0.0, 1.0)]\n    public float WetMix { get; set; }\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n}\n```\n**Description:** A sound modifier that implements a multi-channel chorus effect, allowing for different chorus parameters (depth, rate, feedback) for each audio channel. This is useful for creating rich, spatial chorus effects.\n\n### Modifiers `ResamplerModifier`\n\n```csharp\npublic class ResamplerModifier : SoundModifier\n{\n    public ResamplerModifier(float resampleFactor = 1.0f);\n    public ResamplerModifier(int sourceRate, int targetRate);\n\n    [ControllableParameter(\"Factor\", 0.1, 4.0)]\n    public float ResampleFactor { get; set; }\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n}\n```\n**Description:** A real-time resampling modifier that changes the playback speed and pitch of an audio signal by modulating the sample consumption rate.\n\n**Constructors:**\n*   `ResamplerModifier(float resampleFactor = 1.0f)`: Initializes with a fixed speed factor.\n*   `ResamplerModifier(int sourceRate, int targetRate)`: Calculates the factor based on source and target rates.\n\n**Properties:**\n*   `ResampleFactor`: Gets or sets the resampling factor. > 1.0 speeds up, < 1.0 slows down.\n\n**Methods:**\n*   `Process(Span<float> buffer, int channels)`: Processes and resamples the audio block.\n*   `ProcessSample(float sample, int channel)`: **Throws `NotSupportedException`**. Resampling must be done on blocks for interpolation.\n\n### Modifiers `TrebleBoostModifier`\n\n```csharp\npublic class TrebleBoosterModifier : SoundModifier\n{\n    public TrebleBoosterModifier(AudioFormat format, float cutoff = 4000f, float boostGainDb = 6f);\n\n    [ControllableParameter(\"Boost\", 0.0, 24.0)]\n    public float BoostGainDb { get; set; }\n    [ControllableParameter(\"Cutoff\", 1000.0, 20000.0, MappingScale.Logarithmic)]\n    public float Cutoff { get; set; }\n\n    public override void ProcessMidiMessage(MidiMessage message);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `BoostGainDb`: The gain of the treble boost in decibels.\n*   `Cutoff`: The cutoff frequency above which the treble boost is applied.\n\n**Methods:**\n*   `ProcessMidiMessage(MidiMessage message)`: Handles incoming MIDI CC messages to control boost parameters.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the treble boost effect.\n\n### Modifiers `VocalExtractorModifier`\n\n```csharp\npublic class VocalExtractorModifier : SoundModifier\n{\n    public VocalExtractorModifier(int sampleRate, float minFrequency = 100f, float maxFrequency = 10000f, int fftSize = 4096, int hopSize = 1024);\n\n    public override string Name { get; set; }\n    public float MinFrequency { get; set; }\n    public float MaxFrequency { get; set; }\n    public int FftSize { get; }\n    public int HopSize { get; }\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n}\n```\n\n**Description:** An advanced audio modifier designed to isolate vocals from a mixed track. It employs a hybrid approach:\n*   **Stereo/Quad Sources:** Uses Spatial Isolation (Mid-Side subtraction) to remove instruments panned to the sides, preserving center-panned vocals.\n*   **Mono/Center Sources:** Falls back to Spectral Gating and Bandpass filtering to suppress noise and non-vocal frequencies based on energy thresholds.\n\n**Constructor:**\n*   `VocalExtractorModifier(int sampleRate, float minFrequency = 100f, float maxFrequency = 10000f, int fftSize = 4096, int hopSize = 1024)`:\n*   `sampleRate`: The sample rate of the audio (must match the engine/source).\n*   `minFrequency` / `maxFrequency`: The frequency range to preserve. Defaults to 100Hz-10kHz to capture the human voice while cutting mud and high hiss.\n*   `fftSize`: The size of the FFT window. Defaults to 4096, which provides better frequency resolution for isolating bass/low-mid frequencies compared to smaller windows.\n*   `hopSize`: The overlap size for the STFT process. Defaults to 1024 (25% overlap with 4096 window).\n\n**Properties:**\n*   `MinFrequency`: The lower bound of the frequency range to preserve (in Hz). Setting this avoids processing sub-bass rumble.\n*   `MaxFrequency`: The upper bound of the frequency range to preserve (in Hz).\n*   `FftSize`: (Read-only) The configured FFT size.\n*   `HopSize`: (Read-only) The configured hop size.\n\n**Methods:**\n*   `Process(Span<float> buffer, int channels)`: Processes the audio buffer. It handles channel management, pairing L/R channels for spatial processing and treating others as mono.\n*   `ProcessSample(float sample, int channel)`: **Throws `NotSupportedException`**. This modifier relies on FFT-based block processing and cannot operate on single samples.\n\n### Providers `AssetDataProvider`\n\n```csharp\npublic sealed class AssetDataProvider : ISoundDataProvider\n{\n    public AssetDataProvider(AudioEngine engine, Stream stream, ReadOptions? options = null);\n    public AssetDataProvider(AudioEngine engine, AudioFormat format, Stream stream);\n    public AssetDataProvider(AudioEngine engine, byte[] data, ReadOptions? options = null);\n\n    public bool CanSeek { get; } = true;\n    public int Length { get; }\n    public int Position { get; }\n    public int SampleRate { get; }\n    public SampleFormat SampleFormat { get; }\n    public SoundFormatInfo? FormatInfo { get; }\n    public bool IsDisposed { get; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public void Dispose();\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n}\n```\n\n**Description:** An `ISoundDataProvider` that decodes and loads an entire audio file into an in-memory float array. It automatically detects the audio format by reading metadata or probing with codecs. Ideal for small to medium-sized files where seeking performance is critical.\n\n**Constructors:**\n\n*   `AssetDataProvider(AudioEngine engine, Stream stream, ...)`: Initializes by reading from a stream, automatically detecting the format.\n*   `AssetDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes by reading from a stream with a specified target `AudioFormat`.\n*   `AssetDataProvider(AudioEngine engine, byte[] data, ...)`: Initializes from a byte array containing the full audio file data.\n\n**Properties:**\n\n*   `FormatInfo`: Contains detailed format and tag information read from the source file.\n*   `CanSeek`: Indicates whether seeking is supported (always true for `AssetDataProvider`).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the provider.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio data.\n\n### Providers `StreamDataProvider`\n\n```csharp\npublic sealed class StreamDataProvider : ISoundDataProvider\n{\n    public StreamDataProvider(AudioEngine engine, Stream stream, ReadOptions? options = null);\n    public StreamDataProvider(AudioEngine engine, AudioFormat format, Stream stream);\n    \n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; private set; }\n    public int SampleRate { get; }\n    public SampleFormat SampleFormat { get; }\n    public SoundFormatInfo? FormatInfo { get; }\n    public bool IsDisposed { get; private set; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Description:** An `ISoundDataProvider` that reads and decodes audio data directly from a `Stream` on demand. It supports seeking if the underlying stream is seekable. This provider automatically detects the audio format by reading metadata or probing with available codecs.\n\n**Constructors:**\n\n*   `StreamDataProvider(AudioEngine engine, Stream stream, ...)`: Initializes by wrapping a stream, automatically detecting the format.\n*   `StreamDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes by wrapping a stream with a specified target `AudioFormat`.\n\n**Properties:**\n\n*   `FormatInfo`: Contains detailed format and tag information read from the source file.\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the stream into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio stream (if supported).\n*   `Dispose()`: Releases resources used by the provider.\n\n### Providers `MicrophoneDataProvider`\n\n```csharp\npublic class MicrophoneDataProvider : ISoundDataProvider\n{\n    public MicrophoneDataProvider(AudioCaptureDevice captureDevice, int bufferSize = 8);\n\n    public int Position { get; private set; }\n    public int Length { get; } = -1;\n    public bool CanSeek { get; } = false;\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n    public SoundFormatInfo? FormatInfo { get; } = null;\n    public bool IsDisposed { get; private set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void StartCapture();\n    public void StopCapture();    \n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset); // Throws NotSupportedException\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the captured audio data (in samples).\n*   `Length`: Returns -1.\n*   `CanSeek`: Returns `false`.\n*   `SampleFormat`: The sample format of the captured audio data.\n*   `SampleRate`: The sample rate of the captured audio data.\n*   `FormatInfo`: Always `null`.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when `StopCapture()` is called, signaling the end of the microphone input stream.\n*   `PositionChanged`: Raised after reading data, indicating that the read position has changed.\n\n**Methods:**\n\n*   `MicrophoneDataProvider(AudioCaptureDevice captureDevice, int bufferSize = 8)`: Constructor.\n* `bufferSize`: The number of audio sample arrays to hold in internal queue. Higher values will lead to higher latency but will be more resilient to performance spikes.\n*   `StartCapture()`: Starts capturing audio data from the microphone.\n*   `StopCapture()`: Stops capturing audio data and raises the `EndOfStreamReached` event.\n*   `ReadBytes(Span<float> buffer)`: Reads captured audio samples into the provided buffer. If not enough data is available in the queue it will fill the rest of the buffer with silence.\n*   `Seek(int offset)`: Throws `NotSupportedException` because seeking is not supported for live microphone input.\n*   `Dispose()`: Releases resources used by the `MicrophoneDataProvider`, unsubscribes from the `AudioEngine.OnAudioProcessed` event, and clears the internal buffer queue.\n\n### Providers `ChunkedDataProvider`\n\n```csharp\npublic sealed class ChunkedDataProvider : ISoundDataProvider\n{\n    public ChunkedDataProvider(AudioEngine engine, Stream stream, ReadOptions? options = null, int chunkSize = 220500);\n    public ChunkedDataProvider(AudioEngine engine, AudioFormat format, Stream stream, int chunkSize = 220500);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n    public SoundFormatInfo? FormatInfo { get; }\n    public bool IsDisposed { get; private set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Description:** An `ISoundDataProvider` designed for efficient handling of large audio files by reading and decoding them in manageable chunks. It automatically detects the audio format by reading metadata or probing with codecs.\n\n**Constructors:**\n\n*   `ChunkedDataProvider(AudioEngine engine, Stream stream, ...)`: Initializes by reading from a stream, automatically detecting the format.\n*   `ChunkedDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes by reading from a stream with a specified target `AudioFormat`.\n\n**Properties:**\n\n*   `FormatInfo`: Contains detailed format and tag information read from the source file.\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data in samples.\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a `Stream`. It creates an `ISoundDecoder` to decode the stream, sets the default chunk size (220500 samples per channel, which is 10 seconds at 44.1 kHz), and starts prefetching data.\n*   `ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a file path. It opens a `FileStream` and calls the other constructor.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is prefilled with decoded audio. If the buffer runs out, it decodes another chunk from the stream.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported by the stream and decoder). It disposes of the current decoder, creates a new one, and seeks within the stream. Then, it clears the internal buffer and refills it from the new position.\n*   `Dispose()`: Releases the resources used by the `ChunkedDataProvider`, including the decoder and the stream.\n\n**Remarks:**\n\nThe `ChunkedDataProvider` is designed to handle large audio files efficiently by reading and decoding them in chunks. This prevents the entire file from being loaded into memory at once. The default chunk size is set to 10 seconds of audio at 44.1 kHz, but you can adjust this value in the constructor. The class uses an internal buffer (`Queue<float>`) to store decoded audio samples and prefetches data in the background.\n\n### Providers `QueueDataProvider`\n\n```csharp\npublic enum QueueFullBehavior\n{\n    /// <summary>\n    ///     Throw an <see cref=\"InvalidOperationException\"/> when the queue is full. This is the default behavior.\n    /// </summary>\n    Throw,\n\n    /// <summary>\n    ///     Block the calling thread until space becomes available in the queue.\n    /// </summary>\n    Block,\n\n    /// <summary>\n    ///     Silently drop the incoming samples and return immediately.\n    /// </summary>\n    Drop\n}\n\npublic class QueueDataProvider : ISoundDataProvider\n{\n    public QueueDataProvider(AudioFormat format, int? maxSamples = null, QueueFullBehavior fullBehavior = QueueFullBehavior.Throw);\n\n    public int SamplesAvailable { get; }\n    public long TotalSamplesEnqueued { get; }\n    public bool IsDisposed { get; }\n\n    public int Position { get; }\n    public int Length { get; } = -1;\n    public bool CanSeek { get; } = false;\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n    public SoundFormatInfo? FormatInfo { get; } = null;\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void AddSamples(ReadOnlySpan<float> samples);\n    public void Reset();\n    public void CompleteAdding();\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from an in-memory queue that is fed samples externally. This provider is ideal for scenarios where audio data is generated or received in chunks. It supports configurable behavior for when the queue becomes full, specified by the `QueueFullBehavior` enum.\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: Always returns -1, indicating the total length is unknown as it's a dynamic queue.\n*   `CanSeek`: Always returns `false`, as seeking is not supported by this provider.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n*   `SamplesAvailable`: Gets the number of samples currently available in the queue.\n*   `TotalSamplesEnqueued`: Gets the total number of samples that have been enqueued into the provider since its creation or last reset.\n*   `IsDisposed`: Gets a value indicating whether the provider has been disposed.\n*   `FormatInfo`: Always `null`.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached, which occurs when `CompleteAdding()` has been called and all samples have been read from the queue.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `AddSamples(ReadOnlySpan<float> samples)`: Adds audio samples to the internal queue. The behavior when the queue is full is determined by the `QueueFullBehavior` set during construction:\n*   If `QueueFullBehavior.Throw` (default), an `InvalidOperationException` is thrown if adding samples would exceed the maximum size.\n*   If `QueueFullBehavior.Block`, the calling thread will block until space becomes available in the queue for the entire block of samples.\n*   If `QueueFullBehavior.Drop`, the incoming samples are silently discarded.\n*   Throws `InvalidOperationException` if called after `CompleteAdding()` has been invoked.\n*   `Reset()`: Resets the provider to its initial state, clearing the sample queue, resetting the position, and allowing samples to be added again. Any threads previously blocked in `AddSamples` (if `QueueFullBehavior.Block` was active) will be unblocked.\n*   `CompleteAdding()`: Marks that no more samples will be added to the queue. Once called, subsequent calls to `AddSamples` will throw an `InvalidOperationException`. The `EndOfStreamReached` event will be raised when all remaining samples in the queue have been read.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the internal queue into the provided buffer. Returns the number of samples read. If the queue is empty, it returns 0. If `CompleteAdding()` has been called and the queue becomes empty, the `EndOfStreamReached` event will be fired.\n*   `Seek(int sampleOffset)`: Throws an `InvalidOperationException` because seeking is not supported by this queue-based provider.\n*   `Dispose()`: Releases the resources used by the `QueueDataProvider`, clears the sample queue, and unblocks any waiting threads.\n\n### Providers `NetworkDataProvider`\n\n```csharp\npublic sealed class NetworkDataProvider : ISoundDataProvider\n{\n    public NetworkDataProvider(AudioEngine engine, string url, ReadOptions? options = null);\n    public NetworkDataProvider(AudioEngine engine, AudioFormat format, string url, string? hlsSegmentFormatId = null);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; private set; }\n    public SampleFormat SampleFormat { get; private set; }\n    public int SampleRate { get; private set; }\n    public SoundFormatInfo? FormatInfo { get; }\n    public bool IsDisposed { get; private set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Description:** An `ISoundDataProvider` that provides audio data from a network source, handling both direct URLs and HLS playlists. It automatically detects the format of direct streams.\n\n**Constructors:**\n\n*   `NetworkDataProvider(AudioEngine engine, string url, ...)`: Initializes for a direct stream, automatically detecting the format. Not recommended for HLS.\n*   `NetworkDataProvider(AudioEngine engine, AudioFormat format, ...)`: Initializes with a specific target format. Required for HLS streams where the segment format must be known.\n\n**Properties:**\n\n*   `Length`: The total length of the audio data (in samples). Returns -1 for live HLS streams.\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleRate`: The sample rate of the audio data.\n*   `FormatInfo`: Contains detailed format and tag information read from the source stream.\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleFormat`: The format of the audio samples. Determined after the initial connection to the stream.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `NetworkDataProvider(string url, int? sampleRate = null)`: Constructor that initializes the `NetworkDataProvider` with a network URL. It determines whether the URL points to a direct audio file or an HLS playlist.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is filled asynchronously.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported). The behavior differs for direct URLs and HLS streams:\n*   **Direct URLs:** Performs an HTTP range request to fetch data starting from the desired offset.\n*   **HLS Streams:** Locates the HLS segment containing the desired time offset and starts downloading from that segment.\n*   `Dispose()`: Releases the resources used by the `NetworkDataProvider`, including the `HttpClient`, decoder, and stream.\n\n**Remarks:**\n\nThe `NetworkDataProvider` can handle both direct audio URLs and HLS (HTTP Live Streaming) playlists. It automatically detects the stream type and behaves accordingly.\n\n**Direct Audio URLs:**\n\n*   It uses `HttpClient` to make requests to the URL.\n*   It supports seeking if the server responds with an \"Accept-Ranges: bytes\" header.\n*   It creates an `ISoundDecoder` to decode the audio stream.\n*   It buffers audio data asynchronously in a background thread.\n\n**HLS Playlists:**\n\n*   It downloads and parses the M3U(8) playlist file.\n*   It identifies the individual media segments (e.g., `.ts` files).\n*   It downloads and decodes segments sequentially.\n*   It refreshes the playlist periodically for live streams.\n*   It supports seeking by selecting the appropriate segment based on the desired time offset.\n*   It determines whether the playlist has a defined end by checking for the `#EXT-X-ENDLIST` tag, which affects whether `Length` is known and `CanSeek` is true.\n\nThe class uses an internal `Queue<float>` to buffer audio samples. The `ReadBytes` method waits for data to become available in the buffer if it's empty.\n\n### Providers `RawDataProvider`\n\n```csharp\npublic sealed class RawDataProvider : ISoundDataProvider\n{\n    public RawDataProvider(float[] rawSamples);\n    public RawDataProvider(Stream pcmStream, SampleFormat sampleFormat);\n    public RawDataProvider(byte[] rawBytes, SampleFormat sampleFormat);\n    public RawDataProvider(int[] rawSamples);\n    public RawDataProvider(short[] rawSamples);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; } = true;\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; } = 48000;\n    public SoundFormatInfo? FormatInfo { get; } = null;\n    public bool IsDisposed { get; private set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data directly from raw PCM arrays or streams. Assumes a sample rate of 48000 Hz.\n\n**Properties:**\n*   `Position`: The current read position in samples.\n*   `Length`: The total length of the stream in samples.\n*   `CanSeek`: Indicates if the underlying stream is seekable.\n*   `SampleFormat`: The sample format of the raw audio data.\n*   `SampleRate`: The sample rate of the raw audio data.\n**Events:**\n*   `EndOfStreamReached`: Raised when the end of the stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n**Methods:**\n*   `ReadBytes(Span<float> buffer)`: Reads raw PCM data from the stream and converts it to `float` if necessary.\n*   `Seek(int offset)`: Seeks to the specified offset in the underlying stream if `CanSeek` is true.\n*   `Dispose()`: Disposes the underlying stream.\n\n### Providers `MidiDataProvider`\n\n```csharp\npublic sealed class MidiDataProvider\n{\n    public MidiDataProvider(MidiFile midiFile);\n    public MidiDataProvider(Stream stream);\n    public MidiDataProvider(MidiSequence sequence);\n\n    public IReadOnlyList<TimedMidiEvent> Events { get; }\n    public int TicksPerQuarterNote { get; }\n    public long LengthTicks { get; }\n    public TimeSpan Duration { get; }\n\n    public IEnumerable<TimedMidiEvent> GetEvents(long startTick, long endTick);\n    public TimeSpan GetTimeSpanForTick(long tick);\n    public long GetTickForTimeSpan(TimeSpan time);\n    public MidiFile ToMidiFile();\n}\n```\n\n**Description:** A specialized provider that processes raw MIDI data into a time-ordered, linear sequence of events suitable for playback. It acts as the data source for the `Sequencer` component.\n\n**Key Capabilities:**\n*   **Time Ordering:** It merges events from multiple tracks in a `MidiFile` into a single chronological list, simplifying playback logic.\n*   **Tempo Mapping:** It scans the MIDI file for tempo change events to build an internal tempo map.\n*   **Time Conversion:** It provides crucial methods (`GetTimeSpanForTick` and `GetTickForTimeSpan`) that use the internal tempo map to accurately convert between MIDI ticks and real-world time (`TimeSpan`). This handles complex variable tempo changes seamlessly.\n\n**Constructors:**\n*   `MidiDataProvider(MidiFile midiFile)`: Creates a provider from a parsed `MidiFile`.\n*   `MidiDataProvider(Stream stream)`: Parses a MIDI file directly from a stream.\n*   `MidiDataProvider(MidiSequence sequence)`: Creates a provider from an editable `MidiSequence` (used by the editing engine).\n\n**Methods:**\n*   `GetEvents(long startTick, long endTick)`: Efficiently retrieves all events occurring within a specific tick range. Used by the `Sequencer` during the audio render loop.\n*   `ToMidiFile()`: Converts the processed sequence back into a standard `MidiFile` object.\n\n### Structs\n\n### `AudioFormat`\n```csharp\npublic record struct AudioFormat\n{\n    public SampleFormat Format;\n    public int Channels;\n    public ChannelLayout Layout;\n    public int SampleRate;\n    public float InverseSampleRate { get; }\n\n    public static readonly AudioFormat Cd;\n    public static readonly AudioFormat Dvd;\n    public static readonly AudioFormat DvdHq;\n    public static readonly AudioFormat Surround51;\n    public static readonly AudioFormat Surround71;\n    public static readonly AudioFormat Studio;\n    public static readonly AudioFormat StudioHq;\n    public static readonly AudioFormat Broadcast;\n    public static readonly AudioFormat Telephony;\n\n    public static AudioFormat? GetFormatFromStream(Stream stream);\n    public static ChannelLayout GetLayoutFromChannels(int channels);\n}\n```\n**Description:** A record struct representing the format of an audio stream, including sample format, channel count, channel layout, and sample rate.\n\n**Fields:**\n\n*   `Format`: The sample format.\n*   `Channels`: The number of audio channels.\n*   `Layout`: The physical or logical arrangement of channels.\n*   `SampleRate`: The sample rate in Hertz.\n\n**Properties:**\n\n*   `InverseSampleRate`: Gets the inverse of the sample rate, useful for calculations involving sample duration.\n\n**Static Presets:**\n\n*   `Cd`: Standard Compact Disc (CD) audio format (S16, 2 Channels, 44100 Hz).\n*   `Dvd`: Standard DVD-Video audio format (S16, 2 Channels, 48000 Hz).\n*   `DvdHq`: High-quality DVD-Video audio format (F32, 2 Channels, 48000 Hz).\n*   `Studio`: Common studio recording format (S24, 2 Channels, 96000 Hz).\n*   `StudioHq`: High-quality studio recording format (F32, 2 Channels, 96000 Hz).\n*   `Broadcast`: Standard broadcast audio format (S16, 1 Channel, 48000 Hz).\n*   `Telephony`: Telephony and VoIP audio format (U8, 1 Channel, 8000 Hz).\n\n**Static Methods:**\n\n*   `GetFormatFromStream(Stream stream)`: Infers an `AudioFormat` by reading a stream's metadata.\n*   `GetLayoutFromChannels(int channels)`: Infers a `ChannelLayout` from a channel count.\n\n### Codecs.FFMpeg `FFmpegCodecFactory`\n\n```csharp\npublic sealed class FFmpegCodecFactory : ICodecFactory\n{\n    public string FactoryId { get; } = \"SoundFlow.Codecs.FFMpeg\";\n    public IReadOnlyCollection<string> SupportedFormatIds { get; }\n    public int Priority { get; } = 100;\n\n    public ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format);\n    public ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null);\n    public ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format);\n}\n```\n\n**Description:** An implementation of `ICodecFactory` that uses a native FFmpeg wrapper to provide robust decoding and encoding for a broad spectrum of audio formats, including popular lossy (MP3, AAC, OGG, Opus) and lossless (FLAC, ALAC) codecs. This factory is designed to be registered manually with the `AudioEngine`.\n\n**Properties:**\n\n*   `FactoryId`: The unique identifier, `\"SoundFlow.Codecs.FFMpeg\"`.\n*   `SupportedFormatIds`: A read-only list of supported format strings, including: `\"wav\"`, `\"aiff\"`, `\"flac\"`, `\"alac\"`, `\"ape\"`, `\"wv\"`, `\"tta\"`, `\"shn\"`, `\"mp3\"`, `\"mp2\"`, `\"ogg\"`, `\"opus\"`, `\"aac\"`, `\"m4a\"`, `\"wma\"`, `\"ac3\"`, `\"mka\"`, `\"mpc\"`, `\"tak\"`, `\"ra\"`, `\"dsf\"`, `\"au\"`, `\"gsm\"`.\n*   `Priority`: Set to `100` to ensure it takes precedence over lower-priority built-in or default codecs (like the `MiniAudioCodecFactory`).\n\n**Methods:**\n\n*   `CreateDecoder(...)`: Creates an `FFmpegDecoder` instance for the specified format.\n*   `TryCreateDecoder(...)`: Probes the input stream using the native FFmpeg format detection capabilities to determine the format and create a decoder.\n*   `CreateEncoder(...)`: Creates an `FFmpegEncoder` instance. Handles logic for formats like `\"m4a\"` (which may map to `alac` or `aac` depending on the input quality).\n\n### Codecs.FFMpeg `FFmpegException`\n\n```csharp\npublic sealed class FFmpegException : BackendException\n{\n    public FFmpegException(FFmpegResult result, string? message);\n\n    public FFmpegResult Result { get; }\n}\n```\n\n**Description:** Represents an error that occurred within the native FFmpeg Codecs library. It inherits from `BackendException` and carries the specific `FFmpegResult` enum value returned by the native function.\n\n**Constructor:**\n\n*   `FFmpegException(FFmpegResult result, string? message)`: Initializes the exception with the native result code and an optional custom message.\n\n**Properties:**\n\n*   `Result`: Gets the detailed result code from the native FFmpeg library.\n\n### Structs `DeviceInfo`\n```csharp\npublic record struct DeviceInfo\n{\n    public IntPtr Id;\n    public bool IsDefault;\n    public uint NativeDataFormatCount;\n    public IntPtr NativeDataFormats; // Pointer to an array of NativeDataFormat\n\n    public string Name { get; }\n    public NativeDataFormat[] SupportedDataFormats { get; }\n}\n```\n**Description:** Represents information about an audio device, including its native ID, name, whether it's the default system device, and a count/pointer to its supported native data formats. The `Name` is now a property that correctly decodes the UTF-8 name from the native byte array.\n\n#### `MidiDeviceInfo`\n```csharp\npublic readonly record struct MidiDeviceInfo\n{\n    public int Id { get; init; }\n    public string Name { get; init; }\n}\n```\n**Description:** Represents informational details for a MIDI device, unique to its configured backend.\n\n### Structs `NativeDataFormat`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct NativeDataFormat\n{\n    public SampleFormat Format;\n    public uint Channels;\n    public uint SampleRate;\n    public uint Flags;\n}\n```\n**Description:** Represents a specific native data format (sample format, channels, sample rate) supported by an audio device. Accessed via the `NativeDataFormats` pointer in `DeviceInfo`.\n\n### Structs `Result`\n```csharp\npublic readonly struct Result\n{\n    public bool IsSuccess { get; }\n    public bool IsFailure { get; }\n    public IError? Error { get; }\n\n    public static Result Ok();\n    public static Result Fail(IError error);\n}\n\npublic readonly struct Result<T>\n{\n    public bool IsSuccess { get; }\n    public bool IsFailure { get; }\n    public T? Value { get; }\n    public IError? Error { get; }\n\n    public static Result<T> Ok(T value);\n    public static Result<T> Fail(IError error);\n}\n```\n**Description:** Implements the Railway-Oriented Programming (ROP) pattern using lightweight record structs, representing the outcome of an operation. This pattern encourages explicit error handling via the contained `IError` instead of relying on exceptions for expected failure paths.\n\n### Structs `IError`\n\nThe `IError` interface and its various record-based implementations form a structured, railway-oriented error handling system. Instead of throwing exceptions for predictable failures, methods return a `Result` struct containing an object that implements `IError`. This allows for robust and explicit error handling.\n\n```csharp\npublic interface IError\n{\n    string Message { get; }\n    Exception? InnerException { get; }\n}\n```\n\n**Implementations:**\n\nThe framework provides a comprehensive set of specific error types to clearly identify the nature of a failure.\n\n*   **`Error(string Message, Exception? InnerException = null)`**\nA base record implementation for a generic error.\n\n*   **`ValidationError(string Message)`**\nRepresents errors related to invalid input arguments or preconditions.\n\n*   **`NotFoundError(string ResourceName, string Message)`**\nRepresents an error when a required resource (like a file) is not found.\n\n*   **`FileFormatError(string Message, Exception? InnerException = null)`**\nAn abstract base record for errors that occur during the parsing or writing of a specific file format.\n\n*   **`UnsupportedFormatError(string FormatDetails)`**\nThe error that occurs when attempting to read an audio format that is not supported by the library.\n\n*   **`HeaderNotFoundError(string HeaderDescription)`**\nThe error that occurs when a mandatory header, marker, or chunk is missing from a file (e.g., \"RIFF chunk\", \"ID3v2 tag\").\n\n*   **`CorruptChunkError(string ChunkId, string Reason, Exception? InnerException = null)`**\nThe error that occurs when a recognized audio file's structural component (chunk, atom, etc.) is malformed.\n\n*   **`CorruptFrameError(string FrameDescription, string Reason, Exception? InnerException = null)`**\nThe error that occurs when a specific frame within a stream-based format (like MP3 or AAC) is malformed.\n\n*   **`ObjectDisposedError(string ObjectDescription)`**\nThe error that occurs when an operation is attempted on an object that has already been disposed.\n\n*   **`DeviceError(string Message, Exception? InnerException = null)`**\nAn abstract base record for errors related to an audio or MIDI device.\n\n*   **`DeviceStateError(string Reason)`**\nThe error that occurs when an operation is performed on a device that is in an invalid state for that operation.\n\n*   **`DeviceOperationError(string Operation, string Reason, Exception? InnerException = null)`**\nThe error that occurs when a core device operation (like open, start, or stop) fails.\n\n*   **`DeviceNotFoundError(string DeviceIdentifier)`**\nThe error that occurs when a requested audio device cannot be found.\n\n*   **`BackendNotFoundError(string? BackendName = null)`**\nThe error that occurs when a required audio backend (like WASAPI, CoreAudio, etc.) is not available or enabled.\n\n*   **`ResourceBusyError(string ResourceName)`**\nThe error that occurs when an operation attempts to use a resource that is already in use.\n\n*   **`OutOfMemoryError()`**\nThe error that occurs when an attempt to allocate memory fails.\n\n*   **`InvalidOperationError(string Message)`**\nThe error that occurs when a method call is invalid for the object's current state, analogous to `System.InvalidOperationException`.\n\n*   **`NotImplementedError(string FeatureName)`**\nThe error that occurs when a requested feature or method is not implemented.\n\n*   **`HostError(string Reason, Exception? InnerException = null)`**\nRepresents a generic error reported by the underlying operating system or host environment.\n\n*   **`InternalLibraryError(string LibraryName, string Reason, Exception? InnerException = null)`**\nRepresents an unexpected error within a wrapped native library (e.g., \"PortMidi\", \"miniaudio\").\n\n*   **`IOError(string OperationDescription, Exception? InnerException = null)`**\nRepresents errors that occur during an I/O operation (e.g., \"reading from file stream\").\n\n*   **`TimeoutError(string OperationDescription)`**\nThe error that occurs when an operation times out.\n\n*   **`AccessDeniedError(string ResourceIdentifier)`**\nThe error that occurs when access to a requested resource is denied.\n\n### Utils `Extensions`\n\n```csharp\npublic static class Extensions\n{\n    public static int GetBytesPerSample(this SampleFormat sampleFormat);\n    public static SampleFormat GetSampleFormatFromBitsPerSample(this int bitsPerSample);\n    public static unsafe Span<T> GetSpan<T>(nint ptr, int length) where T : unmanaged;\n    public static T[] ReadArray<T>(this nint pointer, int count) where T : struct;\n}\n```\n**Methods:**\n*   `GetBytesPerSample(this SampleFormat sampleFormat)`: Returns the number of bytes per sample for a given sample format.\n*   `GetSampleFormatFromBitsPerSample(this int bitsPerSample)`: Converts a bit depth to the corresponding `SampleFormat`.\n*   `GetSpan<T>(nint ptr, int length)`: Creates a span of type `T` from a native memory pointer.\n*   `ReadArray<T>(this nint pointer, int count) where T : struct`: Reads an array of structures of type `T` from a native memory pointer.\n\n### Utils `Log`\n\n```csharp\npublic static class Log\n{\n    public static event Action<LogLevel, string>? OnLog;\n\n    public static void Debug(string message);\n    public static void Info(string message);\n    public static void Warning(string message);\n    public static void Error(string message);\n}\n```\n\n**Description:** Provides a centralized, decoupled logging mechanism for the entire SoundFlow library. Your application can subscribe to the static `OnLog` event to capture and handle all log messages generated by the library, allowing you to route them to a console, file, UI, or any other logging framework.\n\n**Event:**\n\n*   `OnLog`: A static event that is raised whenever a log message is generated. The delegate provides the `LogLevel` and the `string` message.\n\n**Static Methods:**\n\n*   `Debug(string message)`: Invokes the `OnLog` event with a `Debug` level message.\n*   `Info(string message)`: Invokes the `OnLog` event with an `Info` level message.\n*   `Warning(string message)`: Invokes the `OnLog` event with a `Warning` level message.\n*   `Error(string message)`: Invokes the `OnLog` event with an `Error` level message.\n\n**Example Usage (in your application):**\n```csharp\nLog.OnLog += (level, message) => {\n    Console.WriteLine($\"[{level}] {message}\");\n};\n```\n\n### `MathHelper`\n```csharp\npublic static class MathHelper\n{\n    public static bool EnableAvx { get; set; }\n    public static bool EnableSse { get; set; }\n    \n    public static float[] ResampleLinear(float[] inputData, int channels, int sourceRate, int targetRate);\n    public static void InverseFft(Complex[] data);\n    public static void Fft(Complex[] data);\n\n    public static float[] HammingWindow(int size);\n    public static float[] HanningWindow(int size);\n\n    public static float Lerp(float a, float b, float t);\n    public static bool IsPowerOfTwo(long n);\n    public static double Mod(this double x, double y);\n    public static float PrincipalAngle(float angle);\n}\n```\n**Description:** Provides static helper methods for common mathematical operations, with a particular focus on signal processing functions like Fast Fourier Transforms (FFT/IFFT) and windowing functions. These methods leverage SIMD (Single Instruction, Multiple Data) instructions (AVX and SSE) for optimized performance on compatible hardware, with fallbacks to scalar implementations.\n\n**Properties:**\n\n*   `EnableAvx`: Gets or sets a value indicating whether to use AVX (Advanced Vector Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of AVX, forcing a fallback to SSE or scalar code.\n*   `EnableSse`: Gets or sets a value indicating whether to use SSE (Streaming SIMD Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of SSE, forcing a fallback to scalar code. This also affects AVX routines that may utilize SSE for specific operations.\n\n**Methods:**\n\n*   `ResampleLinear(...)`: Resamples an array of audio data using linear interpolation, affecting speed and pitch.\n*   `InverseFft(Complex[] data)`: Computes the Inverse Fast Fourier Transform (IFFT) of a complex array.\n*   `Fft(Complex[] data)`: Computes the Fast Fourier Transform (FFT) of a complex array.\n*   `HammingWindow(int size)`: Generates a Hamming window of the specified `size`.\n*   `HanningWindow(int size)`: Generates a Hanning window of the specified `size`.\n*   `Lerp(float a, float b, float t)`: Performs linear interpolation between two float values `a` and `b`.\n*   `IsPowerOfTwo(long n)`: Checks if a given long integer `n` is a power of two.\n*   `Mod(this double x, double y)`: An extension method for `double` that returns the remainder of the division of `x` by `y`.\n*   `PrincipalAngle(float angle)`: Calculates the principal angle for a given `angle` in radians.\n\n### Utils `ChannelMixer`\n```csharp\npublic static class ChannelMixer\n{\n    public static bool EnableAvx { get; set; } = true;\n    public static bool EnableSse { get; set; } = true;\n    public static float[] Mix(float[] samples, int sourceChannels, int targetChannels);\n}\n```\n**Description:** Highly optimized SIMD (AVX/SSE) utility for upmixing (mono->stereo/surround) and downmixing (stereo->mono) interleaved audio buffers.\n\n**Properties:**\n*   `EnableAvx`: Gets or sets a value indicating whether to use AVX (Advanced Vector Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of AVX, forcing a fallback to SSE or scalar code.\n*   `EnableSse`: Gets or sets a value indicating whether to use SSE (Streaming SIMD Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of SSE, forcing a fallback to scalar code. This also affects AVX routines that may utilize SSE for specific operations.\n\n**Methods:**\n*   `Mix(...)`: Mixes audio samples from `sourceChannels` to `targetChannels` using SIMD instructions.\n\n### Utils `MidiTimeConverter`\n\n```csharp\npublic static class MidiTimeConverter\n{\n    public static TimeSpan GetTimeSpanForTick(long tick, int ticksPerQuarterNote, IReadOnlyList<TempoMarker> tempoTrack);\n    public static long GetTickForTimeSpan(TimeSpan time, int ticksPerQuarterNote, IReadOnlyList<TempoMarker> tempoTrack);\n}\n```\n**Description:** A static utility class providing essential methods to convert between MIDI ticks and real time (`TimeSpan`). These conversions are tempo-aware, using a composition's tempo track to accurately calculate timing across tempo changes.\n\n**Methods:**\n*   `GetTimeSpanForTick(...)`: Converts an absolute time in MIDI ticks to a `TimeSpan`.\n*   `GetTickForTimeSpan(...)`: Converts a `TimeSpan` to an absolute time in MIDI ticks.\n\n### Visualization `LevelMeterAnalyzer`\n\n```csharp\npublic class LevelMeterAnalyzer : AudioAnalyzer\n{\n    public LevelMeterAnalyzer(AudioFormat format, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public float Peak { get; }\n    public float Rms { get; }\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Peak`: The peak level of the audio signal.\n*   `Rms`: The RMS (root mean square) level of the audio signal.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to calculate the RMS and peak levels.\n\n### Visualization `LevelMeterVisualizer`\n\n```csharp\npublic class LevelMeterVisualizer : IVisualizer\n{\n    public LevelMeterVisualizer(LevelMeterAnalyzer levelMeterAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public Color PeakHoldColor { get; set; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the level meter bar.\n*   `Name`: The name of the visualizer.\n*   `PeakHoldColor`: The color of the peak hold indicator.\n*   `Size`: The size of the level meter.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the level meter.\n*   `Render(IVisualizationContext context)`: Renders the level meter visualization.\n\n### Visualization `SpectrumAnalyzer`\n\n```csharp\npublic class SpectrumAnalyzer : AudioAnalyzer\n{\n    public SpectrumAnalyzer(AudioFormat format, int fftSize, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public ReadOnlySpan<float> SpectrumData { get; }\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `SpectrumData`: The calculated frequency spectrum data.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to compute the frequency spectrum using an FFT.\n\n### Visualization `SpectrumVisualizer`\n\n```csharp\npublic class SpectrumVisualizer : IVisualizer\n{\n    public SpectrumVisualizer(SpectrumAnalyzer spectrumAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the spectrum bars.\n*   `Name`: The name of the visualizer.\n*   `Size`: The size of the spectrum visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the spectrum.\n*   `Render(IVisualizationContext context)`: Renders the spectrum visualization.\n\n### Visualization `WaveformVisualizer`\n\n```csharp\npublic class WaveformVisualizer : IVisualizer\n{\n    public WaveformVisualizer();\n\n    public string Name { get; }\n    public List<float> Waveform { get; }\n    public Color WaveformColor { get; set; }\n    public Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n*   `Waveform`: The waveform data.\n*   `WaveformColor`: The color of the waveform.\n*   `Size`: The size of the waveform visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the waveform.\n*   `Render(IVisualizationContext context)`: Renders the waveform visualization."
  },
  {
    "id": 6,
    "slug": "advanced-topics",
    "version": "1.3.0",
    "title": "Advanced Topics",
    "description": "Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.",
    "navOrder": 6,
    "category": "Core",
    "content": "---\nid: 6\ntitle: Advanced Topics\ndescription: Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.\nnavOrder: 6\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs, Card, CardBody, CardHeader} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\nThis section delves into more advanced topics related to SoundFlow, including extending the engine with custom components, optimizing performance, and understanding threading considerations.\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Advanced Topics\">\n    <Tab\n        key=\"extending\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ph:puzzle-piece-bold' />\n                <span>Extending SoundFlow</span>\n            </div>\n        }\n    >\n        ## Extending SoundFlow\n\n        One of SoundFlow's key strengths is its extensibility. You can tailor the engine to your specific needs by creating custom:\n\n        *   **Codec Factories (`ICodecFactory`)**: Add support for new audio formats (e.g., Ogg, AAC) by creating extensions like the official `SoundFlow.Codecs.FFMpeg` package.\n        *   **MIDI Backends (`IMidiBackend`)**: Integrate different MIDI APIs (e.g., CoreMIDI on macOS) by creating extensions like the official `SoundFlow.Midi.PortMidi` package.\n        *   **Sound Components (`SoundComponent`)**: Create unique audio generators or processors like synthesizers or custom mixers.\n        *   **Sound Modifiers (`SoundModifier`)**: Implement custom audio effects like distortion, phasers, or unique filters.\n        *   **MIDI Modifiers (`MidiModifier`)**: Implement custom real-time MIDI effects like arpeggiators, chord generators, or velocity curves.\n        *   **Audio Analyzers (`AudioAnalyzer`)** and **Visualizers (`IVisualizer`)**.\n        *   **Audio Backends (`AudioEngine`)**: Add support for entirely new low-level audio APIs like ASIO or JACK.\n        *   **Sound Data Providers (`ISoundDataProvider`)**.\n        *   **Extensions (e.g., for specific DSP libraries)**: SoundFlow supports integration with external audio processing libraries. For instance, the `SoundFlow.Extensions.WebRtc.Apm` package provides features like noise suppression and echo cancellation by wrapping the WebRTC Audio Processing Module. You can create similar extensions for other libraries.\n\n        ### Custom Codec Factories (`ICodecFactory`)\n\n        New in v1.3, you can add support for new audio formats by implementing the `ICodecFactory` interface. The engine maintains a list of registered factories and queries them based on priority when a decoder or encoder is requested for a specific format ID.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement ICodecFactory\" icon='ph:plugs-connected-bold'>\n                Create a class that implements `ICodecFactory`.\n            </Step>\n            <Step title=\"Define Properties\" icon='material-symbols:settings-outline'>\n                *   `FactoryId`: A unique string for your factory (e.g., \"mycompany.mycodecfactory\").\n                *   `SupportedFormatIds`: A collection of lowercase string identifiers for the formats you support (e.g., `\"ogg\"`, `\"opus\"`).\n                *   `Priority`: An integer indicating the factory's priority. Higher numbers are tried first. The built-in `MiniAudioCodecFactory` has a priority of 0.\n            </Step>\n            <Step title=\"Implement `Create` Methods\" icon='mdi:file-code-outline'>\n                Implement `CreateDecoder`, `TryCreateDecoder`, and `CreateEncoder`.\n                *   `CreateDecoder`: Creates a decoder when the format is known.\n                *   `TryCreateDecoder`: Probes a stream to see if your factory can handle it, returning a decoder and the detected `AudioFormat` if successful.\n                *   `CreateEncoder`: Creates an encoder for writing to a specific format.\n            </Step>\n            <Step title=\"Register with Engine\" icon='ph:sign-in-bold'>\n                Register an instance of your factory with the `AudioEngine` using `engine.RegisterCodecFactory(new MyCodecFactory())`.\n            </Step>\n        </Steps>\n\n        **Example (Skeleton):**\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Structs;\n\n        public class MyOggCodecFactory : ICodecFactory\n        {\n            public string FactoryId => \"mycompany.oggfactory\";\n            public IReadOnlyCollection<string> SupportedFormatIds => new[] { \"ogg\" };\n            public int Priority => 10; // Higher than the default MiniAudio factory\n\n            public ISoundDecoder? CreateDecoder(Stream stream, string formatId, AudioFormat format)\n            {\n                if (formatId.ToLowerInvariant() == \"ogg\")\n                {\n                    // return new MyOggDecoder(stream, format);\n                }\n                return null;\n            }\n\n            public ISoundDecoder? TryCreateDecoder(Stream stream, out AudioFormat detectedFormat, AudioFormat? hintFormat = null)\n            {\n                // Logic to check the stream's header for \"OggS\" magic bytes.\n                // If it's an Ogg stream, parse the format, create the decoder, and return it.\n                // Otherwise, return null.\n                detectedFormat = default;\n                // ... probing logic ...\n                return null;\n            }\n            \n            public ISoundEncoder? CreateEncoder(Stream stream, string formatId, AudioFormat format)\n            {\n                // Not implemented in this example\n                return null;\n            }\n        }\n        ```\n\n        ### Custom MIDI Backends (`IMidiBackend`)\n\n        New in v1.3, you can integrate different MIDI APIs (e.g., Windows MIDI, CoreMIDI) by implementing the `IMidiBackend` interface.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement IMidiBackend\" icon='ph:plugs-connected-bold'>\n                Create a class that implements `IMidiBackend` and `IDisposable`.\n            </Step>\n            <Step title=\"Implement Interface Methods\" icon='material-symbols:function'>\n                *   `Initialize(AudioEngine engine)`: Called by the engine to provide a reference to itself. Used for subscribing to sync events.\n                *   `CreateMidiInputDevice(MidiDeviceInfo info)` and `CreateMidiOutputDevice(MidiDeviceInfo info)`: Return your backend-specific implementations that inherit from `MidiInputDevice` and `MidiOutputDevice`.\n                *   `UpdateMidiDevicesInfo(out MidiDeviceInfo[] inputs, out MidiDeviceInfo[] outputs)`: Enumerate the MIDI devices available through your backend's API.\n                *   `Dispose()`: Clean up all native resources.\n            </Step>\n            <Step title=\"Enable in Engine\" icon='ph:sign-in-bold'>\n                Enable your backend by calling `engine.UseMidiBackend(new MyMidiBackend())`.\n            </Step>\n        </Steps>\n\n        ### Custom Sound Components\n\n        Creating custom `SoundComponent` classes allows you to implement unique audio processing logic and integrate it seamlessly into the SoundFlow audio graph.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundComponent\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundComponent` class. Your constructor must accept `AudioEngine` and `AudioFormat` parameters and pass them to the base constructor.\n            </Step>\n            <Step title=\"Implement GenerateAudio\" icon='lucide:audio-lines'>\n                Override the `GenerateAudio(Span<float> buffer, int channels)` method. This is where you'll write the core audio processing code for your component. The `buffer` passed to this method already contains the mixed output from all of the component's inputs.\n                *   If your component **generates** new audio (e.g., an oscillator), it should add its generated samples to the `buffer`.\n                *   If your component **modifies** incoming audio, it should process the samples within the `buffer` in-place.\n            </Step>\n            <Step title=\"Override other methods (optional)\" icon='icon-park-outline:switch-one'>\n                You can override methods like `ConnectInput`, `AddAnalyzer`, `AddModifier`, etc., to customize how your component interacts with the audio graph.\n            </Step>\n            <Step title=\"Add Controllable Parameters (Optional)\" icon='material-symbols:settings-outline'>\n                Add public properties to your component. To expose them to the MIDI mapping system, implement `IMidiMappable` (which `SoundComponent` already does) and decorate the properties with the `[ControllableParameter]` attribute.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Structs;\n        using System;\n\n        public class CustomGainComponent : SoundComponent\n        {\n            [ControllableParameter(\"Gain\", 0.0, 2.0)]\n            public float Gain { get; set; } = 1.0f;\n\n            public override string Name { get; set; } = \"Custom Gain\";\n            \n            public CustomGainComponent(AudioEngine engine, AudioFormat format) : base(engine, format) { }\n\n            protected override void GenerateAudio(Span<float> buffer, int channels)\n            {\n                for (int i = 0; i < buffer.Length; i++)\n                {\n                    buffer[i] *= Gain;\n                }\n            }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System.Linq;\n        \n        // 1. Create an instance of an audio engine.\n        using var engine = new MiniAudioEngine();\n\n        // 2. Define the desired audio format.\n        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\n\n        // 3. Get the default playback device info from the engine.\n        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\n\n        // 4. Initialize a playback device.\n        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);\n\n        // 5. Create the player and your custom component, passing the engine and format.\n        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(engine, audioFormat, dataProvider);\n        var gainComponent = new CustomGainComponent(engine, audioFormat) { Gain = 0.5f };\n\n        // 6. Connect the player as an input to the gain component.\n        gainComponent.ConnectInput(player);\n        \n        // 7. Add the final component in the chain to the device's master mixer.\n        device.MasterMixer.AddComponent(gainComponent);\n        \n        // 8. Start the device and play the sound.\n        device.Start();\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Sound Modifiers\n\n        Custom `SoundModifier` classes allow you to implement your own audio effects.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundModifier\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundModifier` class.\n            </Step>\n            <Step title=\"Implement ProcessSample\" icon='icon-park-outline:sound-wave'>\n                Implement `ProcessSample(float sample, int channel)` (or override `Process` for buffer-level):\n                *   `ProcessSample(float sample, int channel)`: This method takes a single audio sample and the channel index as input and returns the modified sample.\n                *   `Process(Span<float> buffer, int channels)`: Override this for more complex effects that operate on entire buffers (e.g., FFT-based effects). By default, it calls `ProcessSample` for each sample.\n            </Step>\n            <Step title=\"Use 'Enabled' property\" icon='material-symbols:toggle-on-outline'>\n                Your modifier will have an `Enabled` property (defaulting to `true`) to allow toggling its effect.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your modifier to expose configurable parameters.\n            </Step>\n            <Step title=\"Implement MIDI Control (Optional)\" icon='cbi:midi'>\n                Override `ProcessMidiMessage(MidiMessage message)` to allow your modifier's parameters to be controlled directly by MIDI CCs or other messages.\n            </Step>\n            <Step title=\"Add Controllable Parameters (Optional)\" icon='material-symbols:settings-outline'>\n                Decorate public properties with `[ControllableParameter]` to expose them to the MIDI mapping system for UI-driven mapping.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Interfaces;\n        using System;\n\n        public class CustomDistortionModifier : SoundModifier\n        {\n            [ControllableParameter(\"Threshold\", 0.01, 1.0)]\n            public float Threshold { get; set; } = 0.5f;\n\n            public override string Name { get; set; } = \"Custom Distortion\";\n\n            public override float ProcessSample(float sample, int channel)\n            {\n                // Simple hard clipping distortion\n                return Math.Clamp(sample, -Threshold, Threshold);\n            }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System.Linq;\n\n        // 1. Create an engine and define the audio format.\n        using var engine = new MiniAudioEngine();\n        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\n        \n        // 2. Get device info and initialize a playback device.\n        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\n        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);\n        \n        // 3. Create the data provider and player.\n        using var dataProvider = new StreamDataProvider(engine, File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(engine, audioFormat, dataProvider);\n\n        // 4. Create an instance of your custom modifier.\n        var distortion = new CustomDistortionModifier { Threshold = 0.7f };\n        // distortion.Enabled = false; // To disable it\n\n        // 5. Add the modifier to a SoundComponent (like a player).\n        player.AddModifier(distortion);\n        \n        // 6. Add the player to the device's master mixer.\n        device.MasterMixer.AddComponent(player);\n        \n        // 7. Start the device and play.\n        device.Start();\n        player.Play();\n        // ...\n        ```\n        \n        ### Custom MIDI Modifiers\n\n        New in v1.3, `MidiModifier` classes allow you to implement real-time MIDI effects like arpeggiators, chord generators, or velocity curves.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from MidiModifier\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `MidiModifier` class.\n            </Step>\n            <Step title=\"Implement Process\" icon='carbon:message-queue'>\n                Override the `Process(MidiMessage message)` method. This method returns an `IEnumerable<MidiMessage>`, allowing you to:\n                *   **Filter:** Return an empty collection to drop the message.\n                *   **Transform:** Return a collection with one modified message.\n                *   **Generate:** Return a collection with multiple new messages.\n            </Step>\n            <Step title=\"Add to a Track\" icon='lucide:audio-lines'>\n                Add your modifier to a `MidiTrack`'s settings (`track.Settings.AddMidiModifier(myModifier)`). It will be applied to all MIDI events played by that track.\n            </Step>\n        </Steps>\n\n        **Example (Transpose):**\n        ```csharp\n        using SoundFlow.Midi.Abstracts;\n        using SoundFlow.Midi.Enums;\n        using SoundFlow.Midi.Structs;\n        using System;\n        using System.Collections.Generic;\n\n        public class TransposeModifier : MidiModifier\n        {\n            public int Semitones { get; set; }\n\n            public TransposeModifier(int semitones) => Semitones = semitones;\n\n            public override IEnumerable<MidiMessage> Process(MidiMessage message)\n            {\n                if (message.Command is MidiCommand.NoteOn or MidiCommand.NoteOff)\n                {\n                    var newNote = Math.Clamp(message.NoteNumber + Semitones, 0, 127);\n                    yield return message with { Data1 = (byte)newNote };\n                }\n                else\n                {\n                    yield return message; // Pass through other messages\n                }\n            }\n        }\n        ```\n\n        ### Custom Visualizers\n\n        Custom `IVisualizer` classes allow you to create unique visual representations of audio data.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement IVisualizer\" icon='ph:plugs-connected-bold'>\n                Create a new class that implements the `IVisualizer` interface.\n            </Step>\n            <Step title=\"Implement ProcessOnAudioData\" icon='carbon:data-vis-4'>\n                This method receives a `Span<float>` containing audio data. You should process this data and store the relevant information needed for rendering.\n            </Step>\n            <Step title=\"Implement Render\" icon='material-symbols:draw-outline'>\n                This method receives an `IVisualizationContext`. Use the drawing methods provided by the context (e.g., `DrawLine`, `DrawRectangle`) to render your visualization.\n            </Step>\n            <Step title=\"Raise VisualizationUpdated\" icon='mdi:bell-ring-outline'>\n                When the visualization data changes (e.g., after processing new audio data), raise the `VisualizationUpdated` event to notify the UI to update the display.\n            </Step>\n            <Step title=\"Implement Dispose\" icon='material-symbols:delete-outline'>\n                Release any unmanaged resources or unsubscribe from events.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Interfaces;\n        using System;\n        using System.Numerics;\n\n        public class CustomBarGraphVisualizer : IVisualizer\n        {\n            private float _level;\n\n            public string Name => \"Custom Bar Graph\";\n\n            public event EventHandler? VisualizationUpdated;\n\n            public void ProcessOnAudioData(Span<float> audioData)\n            {\n                if (audioData.IsEmpty) return;\n                // Calculate the average level (simplified for this example)\n                float sum = 0;\n                for (int i = 0; i < audioData.Length; i++)\n                {\n                    sum += Math.Abs(audioData[i]);\n                }\n                _level = sum / audioData.Length;\n\n                // Notify that the visualization needs to be updated\n                VisualizationUpdated?.Invoke(this, EventArgs.Empty);\n            }\n\n            public void Render(IVisualizationContext context)\n            {\n                // Clear the drawing area\n                context.Clear();\n\n                // Draw a simple bar graph based on the calculated level\n                float barHeight = _level * 200; // Scale the level for visualization\n                context.DrawRectangle(10, 200 - barHeight, 30, barHeight, new Color(0, 1, 0));\n            }\n\n            public void Dispose()\n            {\n                // Unsubscribe from events, release resources if any\n                VisualizationUpdated = null;\n            }\n        }\n        ```\n\n        ### Adding Audio Backends\n\n        SoundFlow is designed to support multiple audio backends. Currently, it includes a `MiniAudio` backend. You can add support for other audio APIs (e.g., WASAPI, ASIO, CoreAudio) by creating a new backend.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from AudioEngine\" icon='ph:engine-bold'>\n                Create a new class that inherits from the abstract `AudioEngine`. This class will manage the entire lifecycle of your custom backend.\n            </Step>\n            <Step title=\"Implement Abstract Methods\" icon='material-symbols:function'>\n                Implement all the abstract methods from `AudioEngine`:\n                *   `InitializeBackend()` and `CleanupBackend()`: Handle global setup/teardown of your native backend's context.\n                *   `InitializePlaybackDevice()`, `InitializeCaptureDevice()`, `InitializeFullDuplexDevice()`, `InitializeLoopbackDevice()`: These methods are the core of device management. You will implement the logic to initialize a device using your backend's API and return an instance of a class that inherits from the appropriate abstract device class.\n                *   `SwitchDevice(...)` (3 overloads): Implement logic to tear down an old device and initialize a new one while preserving the state (audio graph, event listeners\n                *   `CreateEncoder(...)` and `CreateDecoder(...)`: Return your backend-specific implementations of the `ISoundEncoder` and `ISoundDecoder` interfaces.).\n                *   `UpdateAudioDevicesInfo()`: Implement logic to enumerate playback and capture devices using the backend's API, populating `PlaybackDevices` and `CaptureDevices`.\n            </Step>\n            <Step title=\"Create Device Wrappers\" icon='ph:package-bold'>\n                Create concrete classes inheriting from `AudioPlaybackDevice` and `AudioCaptureDevice`. These will wrap the native device handles and logic specific to your backend, including the audio callback that drives the processing graph.\n            </Step>\n            <Step title=\"Implement Codec Interfaces\" icon='mdi:file-code-outline'>\n                Create classes that implement `ISoundEncoder` and `ISoundDecoder` to handle audio encoding and decoding for your chosen backend.\n            </Step>\n            <Step title=\"Implement a Codec Factory\" icon='mdi:file-code-outline'>\n                Create a class that implements `ICodecFactory` to handle audio encoding and decoding for any formats your backend natively supports. Register this factory in your engine's constructor.\n            </Step>\n        </Steps>\n\n        **Example (Skeleton):**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Abstracts.Devices;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Structs;\n        using System;\n        using System.IO;\n        \n        // Custom Backend Engine\n        public class MyNewAudioEngine : AudioEngine\n        {\n            private nint _context; // Example native context handle\n            \n            public MyNewAudioEngine() { }\n\n            protected override void InitializeBackend()\n            {\n                // _context = NativeApi.InitContext();\n                // UpdateAudioDevicesInfo();\n                // RegisterCodecFactory(new MyBackendCodecFactory());\n                Console.WriteLine(\"MyNewAudioEngine: Backend initialized.\");\n            }\n\n            protected override void CleanupBackend()\n            {\n                // NativeApi.UninitContext(_context);\n                Console.WriteLine(\"MyNewAudioEngine: Backend cleaned up.\");\n            }\n            \n            public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)\n            {\n                return new MyNewPlaybackDevice(this, _context, deviceInfo, format, config);\n            }\n\n            // Implement other abstract methods...\n            public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format) => throw new NotImplementedException();\n            public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format) => throw new NotImplementedException();\n            public override void UpdateAudioDevicesInfo() { /* NativeApi.GetDevices(...); */ }\n            public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n        }\n\n        // Custom Playback Device Wrapper\n        public class MyNewPlaybackDevice : AudioPlaybackDevice\n        {\n            private nint _deviceHandle;\n            \n            public MyNewPlaybackDevice(AudioEngine engine, nint context, DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config)\n                : base(engine, format, config ?? new MyDeviceConfig())\n            {\n                // _deviceHandle = NativeApi.InitDevice(context, deviceInfo?.Id, OnAudioCallback);\n                // this.Info = ... // Populate DeviceInfo from native API\n                this.Capability = Capability.Playback;\n            }\n\n            public override void Start() \n            { \n                // NativeApi.StartDevice(_deviceHandle); \n                IsRunning = true; \n                Engine.RaiseDeviceStarted(this); // Fire start event for sync\n            }\n            public override void Stop() \n            { \n                // NativeApi.StopDevice(_deviceHandle); \n                IsRunning = false; \n                Engine.RaiseDeviceStopped(this); // Fire stop event for sync\n            }\n            public override void Dispose() \n            {\n                if (IsDisposed) return;\n                Stop();\n                // NativeApi.UninitDevice(_deviceHandle);\n                OnDisposedHandler();\n                IsDisposed = true;\n            }\n            \n            // This callback is invoked by the native backend on the audio thread.\n            private void OnAudioCallback(Span<float> buffer, int frameCount)\n            {\n                // Fire the render event before processing for master clock sync\n                Engine.RaiseAudioFramesRendered(this, frameCount);\n                \n                // Process the audio graph\n                var soloed = Engine.GetSoloedComponent();\n                if (soloed != null)\n                    soloed.Process(buffer, Format.Channels);\n                else\n                    MasterMixer.Process(buffer, Format.Channels);\n            }\n        }\n        \n        // Custom Device Configuration (Optional)\n        public class MyDeviceConfig : DeviceConfig { /* ... */ }\n        ```\n    </Tab>\n\n    <Tab\n        key=\"performance\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ic:round-speed' />\n                <span>Performance Optimization</span>\n            </div>\n        }\n    >\n        ## Performance Optimization\n\n        Here are some tips for optimizing the performance of your SoundFlow applications:\n\n        *   **Buffer Sizes:** Choose appropriate buffer sizes for your use case. Smaller buffers reduce latency but increase CPU overhead. Larger buffers can improve efficiency but may introduce latency. With the `MiniAudio` backend, you can specify this via `MiniAudioDeviceConfig` when initializing a device.\n        *   **SIMD:** SoundFlow uses SIMD instructions (when available) in critical paths like the `Mixer`, `SoundComponent` (for volume and panning), `MathHelper` (for FFTs and windowing), the new `ChannelMixer` utility, and in the `DeviceBufferHelper` for audio format conversions. Ensure your target platform supports SIMD for the best performance.\n        *   **`ChannelMixer` Utility:** New in v1.3, the static `SoundFlow.Utils.ChannelMixer` class provides highly optimized, SIMD-accelerated methods for upmixing and downmixing audio channels (e.g., mono to stereo, stereo to mono, 5.1 to stereo). Use this for efficient channel manipulation.\n        *   **Profiling:** Use a profiler (like the one built into Visual Studio) to identify performance bottlenecks in your audio processing pipeline.\n        *   **Asynchronous Operations:** For long-running operations (e.g., loading large files, network requests in `NetworkDataProvider`, project saving/loading), use asynchronous programming (`async` and `await`) to avoid blocking the main thread or the audio thread.\n        *   **Avoid Allocations:** Minimize memory allocations within the `GenerateAudio` method of `SoundComponent` and the `ProcessSample` or `Process` method of `SoundModifier`. SoundFlow internally uses `ArrayPool<T>.Shared` for many temporary buffers to reduce GC pressure.\n        *   **Efficient Algorithms:** Use efficient algorithms for audio processing, especially in performance-critical sections.\n        *   **Modifier Overhead:** Each `SoundModifier` added to a component or to the editing hierarchy (`AudioSegment`, `Track`, `Composition`) introduces some overhead. For very simple operations, integrating them directly into a custom `SoundComponent` might be more performant than using many tiny modifiers. However, modifiers offer better reusability and modularity.\n        *   **Effect Toggling:** Use the `Enabled` property on `SoundModifier`, `AudioAnalyzer`, `SoundComponent`, `AudioSegmentSettings`, and `TrackSettings` to non-destructively disable effects or entire processing paths instead of removing and re-adding them, which is more efficient.\n    </Tab>\n\n    <Tab\n        key=\"threading\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='carbon:thread' />\n                <span>Threading Considerations</span>\n            </div>\n        }\n    >\n        ## Threading Considerations\n\n        SoundFlow uses a dedicated, high-priority thread (managed by the audio backend, e.g., MiniAudio) for audio processing. This ensures that audio is processed in real time and minimizes the risk of glitches or dropouts.\n\n        **Key Considerations:**\n\n        *   **Audio Thread:** The audio processing logic is executed on a dedicated audio thread. This thread is responsible for calling the audio callback provided to the backend. In SoundFlow's `MiniAudio` implementation, this callback triggers the `Process` method on the appropriate `MiniAudioPlaybackDevice` or `MiniAudioCaptureDevice`, which in turn traverses the `SoundComponent` graph (e.g., calling `MasterMixer.Process(...)`). Therefore, all code within `GenerateAudio` (for `SoundComponent`) and `Process`/`ProcessSample` (for `SoundModifier`) runs on this critical audio thread. Avoid performing any long-running or blocking operations (like I/O, complex non-audio computations, or direct UI updates) on this thread.\n        *   **UI Thread:** Never perform audio processing directly on the UI thread. This can lead to unresponsiveness and glitches. Use the `AudioEngine`'s audio thread for all audio-related operations. For UI updates based on audio events (e.g., from an `IVisualizer`'s `VisualizationUpdated` event), you must marshal the calls to the UI thread (e.g., using `Dispatcher.Invoke` in WPF/Avalonia, or `Control.Invoke` in WinForms).\n        *   **Thread Safety:** If you need to access or modify shared data from both the audio thread and another thread (e.g., the UI thread updating a `SoundModifier`'s property), use appropriate synchronization mechanisms (like `lock`, `Monitor`, or thread-safe collections) to ensure data integrity and prevent race conditions. Many properties on `SoundComponent` and `SoundModifier` are internally locked for thread-safe access from different threads.\n        *   **Engine Synchronization Events (New in v1.3):** The `AudioEngine` now exposes events that are crucial for building synchronized systems (like a MIDI backend that needs to send MIDI Clock).\n            *   `DeviceStarted`: Raised when an `AudioDevice`'s `Start()` method is called. Fired on the calling thread.\n            *   `DeviceStopped`: Raised when an `AudioDevice`'s `Stop()` method is called. Fired on the calling thread.\n            *   `AudioFramesRendered`: Raised from within the audio callback on the high-priority audio thread *before* the audio graph is processed. It provides a sample-accurate timing source for driving master synchronization logic.\n\n        <Card className=\"bg-warning-50/50 dark:bg-warning-500/10 border-1 border-warning-200/50 dark:border-warning-500/20 my-6\">\n            <CardHeader>\n                <div className=\"flex items-center gap-3\">\n                    <Icon icon=\"lucide:alert-triangle\" className=\"text-warning text-2xl flex-shrink-0\" />\n                    <h4 className=\"font-semibold text-lg\">Important Note for WPF & Avalonia Users</h4>\n                </div>\n            </CardHeader>\n            <CardBody className=\"pt-0\">\n                <div className=\"text-sm\">\n                    A subtle but critical conflict can occur when initializing audio on the main UI thread in modern Windows UI frameworks. This can cause features like Drag & Drop, the clipboard, and IMEs (Input Method Editors) to stop working.\n                </div>\n                <h5 className=\"font-semibold mt-4 mb-2\">The Core Issue: COM Thread State Conflict</h5>\n                <ul className=\"list-disc pl-5 space-y-2 text-sm\">\n                    <li>\n                        <strong>UI Frameworks (WPF/Avalonia) Require STA:</strong> These frameworks need the main UI thread to be in a **Single-Threaded Apartment (STA)** state for many core OS services to function correctly.\n                    </li>\n                    <li>\n                        <strong>Audio Backends (WASAPI) Default to MTA:</strong> For performance, low-level audio APIs often initialize the Component Object Model (COM) in a **Multi-Threaded Apartment (MTA)** state.\n                    </li>\n                    <li>\n                        <strong>The Conflict:</strong> A thread's COM state can only be set **once**. If you call `engine.InitializePlaybackDevice()` on the UI thread, the audio backend may set the state to MTA, permanently breaking STA-dependent UI features.\n                    </li>\n                </ul>\n                <h5 className=\"font-semibold mt-4 mb-2\">The Solution: Initialize on a Background Thread</h5>\n                <div className=\"text-sm mb-2\">\n                    The correct architectural pattern is to perform all SoundFlow initialization and long-running audio operations on a background thread. This keeps the UI thread responsive and in its required STA state.\n                </div>\n                ```csharp\n                // In your Avalonia or WPF application:\n                private async void OnPlayButtonClicked(object sender, RoutedEventArgs e)\n                {\n                    // INCORRECT: This blocks the UI thread and causes the COM conflict.\n                    // var engine = new MiniAudioEngine();\n                    // var device = engine.InitializePlaybackDevice(null, AudioFormat.DvdHq);\n                    // ...\n\n                    // CORRECT: Offload all audio work to a background thread.\n                    await Task.Run(() =>\n                    {\n                        using var engine = new MiniAudioEngine();\n                        using var device = engine.InitializePlaybackDevice(null, AudioFormat.DvdHq);\n                        using var dataProvider = new StreamDataProvider(engine, new FileStream(\"audio.mp3\", FileMode.Open));\n                        var player = new SoundPlayer(engine, device.Format, dataProvider);\n\n                        device.MasterMixer.AddComponent(player);\n                        device.Start();\n                        player.Play();\n\n                        // Keep the background task alive while playing.\n                        // In a real app, you would manage the engine and device lifecycle\n                        // in a more persistent way (e.g., in a service class).\n                        Thread.Sleep(5000); \n                    });\n                }\n                ```\n            </CardBody>\n        </Card>\n    </Tab>\n</Tabs>"
  },
  {
    "id": 11,
    "slug": "webrtc-apm",
    "version": "1.2.0",
    "title": "WebRTC Audio Processing Module (APM) Extension",
    "description": "Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.",
    "navOrder": 11,
    "category": "Extensions",
    "content": "﻿---\r\nid: 11\r\ntitle: WebRTC Audio Processing Module (APM) Extension\r\ndescription: Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.\r\nnavOrder: 11\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# WebRTC Audio Processing Module (APM) Extension for SoundFlow\r\n\r\nThe `SoundFlow.Extensions.WebRtc.Apm` package integrates a native library based on the high-quality WebRTC Audio Processing Module (APM) into the SoundFlow audio engine. This extension provides advanced voice processing features essential for real-time communication and audio enhancement.\r\n\r\n## Features\r\n\r\nThe WebRTC APM extension offers several key audio processing features:\r\n\r\n*   **Acoustic Echo Cancellation (AEC):** Reduces or eliminates echoes that occur when audio played through speakers is picked up by the microphone.\r\n*   **Noise Suppression (NS):** Attenuates steady-state background noise (e.g., fans, hums) to improve speech clarity. Multiple suppression levels are available.\r\n*   **Automatic Gain Control (AGC):** Dynamically adjusts the microphone input volume to maintain a consistent audio level, preventing clipping or overly quiet audio. Supports different modes and target levels.\r\n*   **High Pass Filter (HPF):** Removes low-frequency components (typically below 80Hz) to reduce rumble and DC offset.\r\n*   **Pre-Amplifier:** Applies a configurable fixed gain to the audio signal before other APM processing steps.\r\n*   **Multi-channel Processing Configuration:** Allows specifying how multi-channel audio is handled and downmixed.\r\n\r\nThese features can be configured and applied primarily through the `WebRtcApmModifier` for real-time processing within the SoundFlow audio graph, or using the `NoiseSuppressor` component for offline batch processing.\r\n\r\n**Important Note on Sample Rates:** The WebRTC APM native library primarily supports specific sample rates: **8000 Hz, 16000 Hz, 32000 Hz, and 48000 Hz**. Ensure your SoundFlow `AudioEngine` is initialized with one of these sample rates when using this extension for optimal performance and compatibility.\r\n\r\n## Installation\r\n\r\nTo use this extension, you need to have the core `SoundFlow` library installed. Then, add the `SoundFlow.Extensions.WebRtc.Apm` package to your project:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\nThis package includes the necessary C# wrapper and the native WebRTC APM binaries for supported platforms.\r\n\r\n## Usage\r\n\r\n### Real-time Processing with `WebRtcApmModifier`\r\n\r\nThe `WebRtcApmModifier` is a `SoundModifier` that can be added to any `SoundComponent` to process its audio output in real-time. This is ideal for applications like voice chat, live audio input processing, or any scenario requiring real-time audio enhancement. The following steps demonstrate a typical full-duplex setup for echo cancellation.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine & Devices\" description=\"Create the engine and a full-duplex device\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine` and Devices\r\n        First, create an instance of the `AudioEngine`. Then, define an `AudioFormat` compatible with WebRTC (e.g., 48kHz mono). Finally, initialize a `FullDuplexDevice`, which manages both a capture (microphone) and a playback (speaker) device, making it perfect for AEC scenarios.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Abstracts.Devices;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Structs;\r\n\r\n        // Initialize the audio engine\r\n        var audioEngine = new MiniAudioEngine();\r\n\r\n        // Define a WebRTC APM compatible audio format\r\n        var audioFormat = new AudioFormat\r\n        {\r\n            SampleRate = 48000,\r\n            Channels = 1, // Mono for typical voice processing\r\n            Format = SampleFormat.F32\r\n        };\r\n\r\n        // Initialize a full-duplex device using the system's default microphone and speakers\r\n        var fullDuplexDevice = audioEngine.InitializeFullDuplexDevice(\r\n            playbackDeviceInfo: audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault),\r\n            captureDeviceInfo: audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault),\r\n            format: audioFormat\r\n        );\r\n\r\n        // Start the devices so they are ready for processing\r\n        fullDuplexDevice.Start();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create SoundComponent for Mic\" description=\"Set up a data provider and player for the mic\" icon='ph:microphone-bold'>\r\n        ### 2. Set Up the Microphone Input Component\r\n        Create a `MicrophoneDataProvider` to receive live audio from the capture device, and a `SoundPlayer` to process this data through the SoundFlow graph. This `SoundPlayer` will act as our microphone source component.\r\n\r\n        ```csharp\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Providers;\r\n\r\n        // Create a data provider that reads from our duplex device\r\n        var microphoneDataProvider = new MicrophoneDataProvider(fullDuplexDevice);\r\n\r\n        // Create a sound player to process the live mic data\r\n        // We will add the APM modifier to this player\r\n        var micAudioComponent = new SoundPlayer(audioEngine, audioFormat, microphoneDataProvider);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Configure APM Modifier\" description=\"Instantiate and set up the modifier\" icon='material-symbols:settings-outline'>\r\n        ### 3. Instantiate and Configure `WebRtcApmModifier`\r\n        The modifier's constructor allows setting initial states for all features. You can also adjust them dynamically via its public properties.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Modifiers;\r\n\r\n        var apmModifier = new WebRtcApmModifier(\r\n            // Echo Cancellation (AEC) settings\r\n            aecEnabled: true,\r\n            aecMobileMode: false, // Desktop mode is generally more robust\r\n            aecLatencyMs: 40,     // Estimated system latency for AEC (tune this)\r\n\r\n            // Noise Suppression (NS) settings\r\n            nsEnabled: true,\r\n            nsLevel: NoiseSuppressionLevel.High,\r\n\r\n            // Automatic Gain Control (AGC) - Version 1 (legacy)\r\n            agc1Enabled: true,\r\n            agcMode: GainControlMode.AdaptiveDigital,\r\n            agcTargetLevel: -3,   // Target level in dBFS (0 is max, typical is -3 to -18)\r\n            agcCompressionGain: 9, // Only for FixedDigital mode\r\n            agcLimiter: true,\r\n\r\n            // Automatic Gain Control (AGC) - Version 2 (newer, often preferred)\r\n            agc2Enabled: false, // Set to true to use AGC2, potentially disable AGC1\r\n\r\n            // High Pass Filter (HPF)\r\n            hpfEnabled: true,\r\n\r\n            // Pre-Amplifier\r\n            preAmpEnabled: false,\r\n            preAmpGain: 1.0f,\r\n\r\n            // Pipeline settings for multi-channel audio (if numChannels > 1)\r\n            useMultichannelCapture: false, // Process capture (mic) as mono/stereo as configured by AudioEngine\r\n            useMultichannelRender: false,  // Process render (playback for AEC) as mono/stereo\r\n            downmixMethod: DownmixMethod.AverageChannels // Method if downmixing is needed\r\n        );\r\n\r\n        // Example of changing a setting dynamically:\r\n        // apmModifier.NoiseSuppression.Level = NoiseSuppressionLevel.VeryHigh;\r\n        ```\r\n    </Step>\r\n    <Step title=\"Add Modifier\" description=\"Attach the modifier to the mic component\" icon='ic:baseline-plus'>\r\n        ### 4. Add the Modifier to the Microphone Component\r\n\r\n        ```csharp\r\n        micAudioComponent.AddModifier(apmModifier);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Start Processing\" description=\"Add to mixer and start playback\" icon='mdi:play-box-outline'>\r\n        ### 5. Add the Component to the Mixer and Start Processing\r\n        To complete the loop for testing, add the processed microphone component to the playback device's `MasterMixer`. This will route the cleaned microphone audio to the speakers.\r\n\r\n        ```csharp\r\n        // Add the processed mic component to the playback device's master mixer\r\n        fullDuplexDevice.MasterMixer.AddComponent(micAudioComponent);\r\n\r\n        // Start the microphone provider and the player component\r\n        microphoneDataProvider.StartCapture();\r\n        micAudioComponent.Play();\r\n\r\n        Console.WriteLine(\"WebRTC APM processing microphone input. Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        // Cleanup\r\n        fullDuplexDevice.Dispose(); // Disposes underlying devices and their components\r\n        apmModifier.Dispose();      // Important to release native resources\r\n        microphoneDataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n**AEC Far-End (Playback) Signal:** For Acoustic Echo Cancellation to work, the processor needs both the microphone signal (near-end) and the speaker signal (far-end). The `WebRtcApmModifier` is designed to integrate deeply with SoundFlow's architecture. It automatically detects and captures the audio being processed by the master mixer of any active playback device within the same `AudioEngine` context. This allows it to correlate the audio being sent to the speakers with the audio coming from the microphone to perform echo cancellation seamlessly.\r\n\r\n### Offline Processing with `NoiseSuppressor`\r\n\r\nThe `NoiseSuppressor` component is designed for batch processing of audio from an `ISoundDataProvider` (e.g., an audio file). It applies only the WebRTC Noise Suppression feature.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Required for encoding/decoding\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        The engine is required for its decoding and encoding capabilities, even if not playing audio back.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Structs;\r\n\r\n        var audioEngine = new MiniAudioEngine();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create Data Provider\" description=\"Load your noisy audio file\" icon='mdi:file-music-outline'>\r\n        ### 2. Create an `ISoundDataProvider` for your noisy audio file\r\n        Use a `StreamDataProvider` to read from an audio file.\r\n\r\n        ```csharp\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Define the format of the source file. This is needed by the data provider.\r\n        var fileFormat = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        string noisyFilePath = \"path/to/your/noisy_audio.wav\";\r\n        var dataProvider = new StreamDataProvider(audioEngine, fileFormat, File.OpenRead(noisyFilePath));\r\n        ```\r\n    </Step>\r\n    <Step title=\"Instantiate NoiseSuppressor\" description=\"Set up the offline processor\" icon='icon-park-outline:sound-wave'>\r\n        ### 3. Instantiate `NoiseSuppressor`\r\n        Provide the data source and its audio parameters. These MUST match the actual properties of the audio from the data provider.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Components;\r\n\r\n        // Parameters: dataProvider, sampleRate, numChannels, suppressionLevel\r\n        var noiseSuppressor = new NoiseSuppressor(dataProvider, 48000, 1, NoiseSuppressionLevel.VeryHigh);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Process the Audio\" description=\"Process all at once or in chunks\" icon='carbon:cics-transaction-server-zos'>\r\n        ### 4. Process the audio\r\n        You can process all audio at once (for smaller files) or chunk by chunk.\r\n\r\n        **Option A: Process All (returns `float[]`)**\r\n        ```csharp\r\n        float[] cleanedAudio = noiseSuppressor.ProcessAll();\r\n        // Now 'cleanedAudio' contains the noise-suppressed audio data.\r\n        \r\n        // You can save it using an ISoundEncoder:\r\n        var outputFormat = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        using var fileStream = new FileStream(\"cleaned_audio.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        using var encoder = audioEngine.CreateEncoder(fileStream, EncodingFormat.Wav, outputFormat);\r\n        encoder.Encode(cleanedAudio.AsSpan());\r\n        ```\r\n\r\n        **Option B: Process Chunks (via event or direct handler)**\r\n        ```csharp\r\n        var outputFormatChunked = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n        using var chunkFileStream = new FileStream(\"cleaned_audio_chunked.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        using var chunkEncoder = audioEngine.CreateEncoder(chunkFileStream, EncodingFormat.Wav, outputFormatChunked);\r\n\r\n        noiseSuppressor.OnAudioChunkProcessed += (processedChunk) =>\r\n        {\r\n            if (!chunkEncoder.IsDisposed)\r\n            {\r\n                chunkEncoder.Encode(processedChunk.ToArray());\r\n            }\r\n        };\r\n\r\n        // ProcessChunks is a blocking call until the entire provider is processed.\r\n        noiseSuppressor.ProcessChunks();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Dispose Resources\" description=\"Clean up all IDisposable objects\" icon='material-symbols:delete-outline'>\r\n        ### 5. Dispose resources\r\n\r\n        ```csharp\r\n        noiseSuppressor.Dispose();\r\n        dataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n## Configuration Details\r\n\r\n### `WebRtcApmModifier` Properties:\r\n\r\n*   **`Enabled` (bool):** Enables/disables the entire APM modifier.\r\n*   **`EchoCancellation` (`EchoCancellationSettings`):**\r\n*   `Enabled` (bool): Enables/disables AEC.\r\n*   `MobileMode` (bool): Toggles between desktop (false) and mobile (true) AEC modes.\r\n*   `LatencyMs` (int): Estimated system audio latency in milliseconds. Crucial for AEC performance. Tune this value for your setup.\r\n*   **`NoiseSuppression` (`NoiseSuppressionSettings`):**\r\n*   `Enabled` (bool): Enables/disables NS.\r\n*   `Level` (`NoiseSuppressionLevel`): Sets the aggressiveness (Low, Moderate, High, VeryHigh).\r\n*   **`AutomaticGainControl` (`AutomaticGainControlSettings`):**\r\n*   `Agc1Enabled` (bool): Enables/disables the legacy AGC1.\r\n*   `Mode` (`GainControlMode`): Sets the mode for AGC1 (AdaptiveAnalog, AdaptiveDigital, FixedDigital).\r\n*   `TargetLevelDbfs` (int): Target level for AGC1 AdaptiveDigital mode (-31 to 0 dBFS).\r\n*   `CompressionGainDb` (int): Gain for AGC1 FixedDigital mode (0 to 90 dB).\r\n*   `LimiterEnabled` (bool): Enables/disables the limiter for AGC1.\r\n*   `Agc2Enabled` (bool): Enables/disables the newer AGC2.\r\n*   **`HighPassFilterEnabled` (bool):** Enables/disables the HPF.\r\n*   **`PreAmplifierEnabled` (bool):** Enables/disables the pre-amplifier.\r\n*   **`PreAmplifierGainFactor` (float):** Gain factor for the pre-amplifier (e.g., 1.0 is no change, 2.0 is +6dB).\r\n*   **`ProcessingPipeline` (`ProcessingPipelineSettings`):**\r\n*   `UseMultichannelCapture` (bool): If true and input is multi-channel, APM processes it as such. Otherwise, it might downmix.\r\n*   `UseMultichannelRender` (bool): Similar to capture, but for the far-end/render signal for AEC.\r\n*   `DownmixMethod` (`DownmixMethod`): Specifies how to downmix if multi-channel processing is disabled for a stream (AverageChannels, UseFirstChannel).\r\n*   **`PostProcessGain` (float):** A final gain applied after all APM processing (default 1.0f).\r\n\r\n### `NoiseSuppressor` Constructor:\r\n\r\n*   `dataProvider` (`ISoundDataProvider`): The audio source.\r\n*   `sampleRate` (int): Sample rate of the source audio (must be 8k, 16k, 32k, or 48k).\r\n*   `numChannels` (int): Number of channels in the source audio.\r\n*   `suppressionLevel` (`NoiseSuppressionLevel`): Desired noise suppression level.\r\n*   `useMultichannelProcessing` (bool): If true and `numChannels > 1`, attempts to process channels independently.\r\n\r\n## Licensing\r\n\r\n*   The C# code (`SoundFlow.Extensions.WebRtc.Apm` wrapper and components) is licensed under the **MIT License**.\r\n*   The native `webrtc-apm` library used by this extension is based on the WebRTC Audio Processing Module, which is typically licensed under the **BSD 3-Clause \"New\" or \"Revised\" License**. The specific version included is derived from the [PulseAudio project's extraction](https://gitlab.freedesktop.org/pulseaudio/webrtc-audio-processing).\r\n\r\n**Users must comply with the terms of both licenses.** This generally involves including the copyright notice and license text of the WebRTC code if distributing applications using this extension. Please consult the native library's specific distribution for exact requirements.\r\n\r\n## Troubleshooting\r\n\r\n*   **No effect or poor quality:**\r\n*   Verify the `AudioEngine` sample rate (set in the `AudioFormat` struct) matches one supported by WebRTC APM (8k, 16k, 32k, 48k Hz).\r\n*   For AEC, ensure `aecLatencyMs` is tuned appropriately for your system. Too low or too high values can degrade performance.\r\n*   Ensure the far-end signal is correctly being captured if AEC is enabled (usually is handled automatically by the modifier, which monitors active playback devices in the same engine context).\r\n*   **Errors during initialization:** Check the console output for any specific error messages from the native APM library. Ensure the native binaries are correctly deployed with your application.\r\n*   **Performance issues:** While WebRTC APM is optimized, processing many channels or enabling all features at very high settings can be CPU intensive. Monitor performance and adjust settings if needed."
  },
  {
    "id": 8,
    "slug": "tutorials-recording",
    "version": "1.2.0",
    "title": "Recording Audio",
    "description": "Comprehensive tutorials for recording audio from devices and processing it in real-time with SoundFlow.",
    "navOrder": 8,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 8\r\ntitle: Recording Audio\r\ndescription: Comprehensive tutorials for recording audio from devices and processing it in real-time with SoundFlow.\r\nnavOrder: 8\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Recording with SoundFlow\r\n\r\nWelcome to the SoundFlow audio recording tutorials! This guide will walk you through the essential steps to integrate audio recording capabilities into your .NET applications using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to record from the default device, perform custom real-time audio processing, or monitor microphone input, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Recording tutorials\">\r\n    <Tab key=\"basic-recording\" title=\"Basic Recording\">\r\n        This tutorial demonstrates how to record audio from the default recording device and save it to a WAV\r\n        file.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o BasicRecording\r\n                cd BasicRecording\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the basic recorder\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace BasicRecording;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default capture (recording) device.\r\n                        var defaultCaptureDevice = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultCaptureDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default capture device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define the audio format for recording. The backend will capture in this format.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 1 // Mono recording\r\n                        };\r\n                        \r\n                        // Initialize the capture device.\r\n                        using var device = audioEngine.InitializeCaptureDevice(defaultCaptureDevice, audioFormat);\r\n                        \r\n                        // Set up the output file stream.\r\n                        string outputFilePath = Path.Combine(Directory.GetCurrentDirectory(), \"output.wav\");\r\n                        using var fileStream = new FileStream(outputFilePath, FileMode.Create, FileAccess.Write, FileShare.None);\r\n                        \r\n                        // Create a recorder, linking it to the capture device and the output stream.\r\n                        using var recorder = new Recorder(device, fileStream, EncodingFormat.Wav);\r\n\r\n                        Console.WriteLine(\"Recording... Press any key to stop.\");\r\n                        device.Start(); // Start the device to begin capturing data.\r\n                        recorder.StartRecording(); // Start the recorder to begin encoding and writing.\r\n                        \r\n                        Console.ReadKey();\r\n                        \r\n                        recorder.StopRecording();\r\n                        device.Stop();\r\n\r\n                        Console.WriteLine($\"Recording stopped. Saved to {outputFilePath}\");\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        First, an `AudioEngine` is initialized. We find the `default capture device` from the engine's list of available recording devices. An `AudioFormat` is specified for the recording session (e.g., 48kHz, mono, 32-bit float).\r\n        The default capture device is then initialized with this format, creating an `AudioCaptureDevice`. This `device` instance is now the source of our audio data.\r\n        A `Recorder` is created, taking the `device` and an output `FileStream` as arguments. The recorder listens to the audio data processed by the device.\r\n        To begin, `device.Start()` is called to activate the hardware, and `recorder.StartRecording()` is called to start encoding the incoming audio from the device and writing it to the specified WAV file. After the user presses a key, both the recorder and the device are stopped, and all resources are automatically disposed by their `using` statements.\r\n    </Tab>\r\n\r\n    <Tab key=\"custom-processing\" title=\"Custom Processing\">\r\n        This tutorial demonstrates using a callback to process recorded audio in real-time.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o CustomProcessing\r\n                cd CustomProcessing\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement real-time processing\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n\r\n                namespace CustomProcessing;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultCaptureDevice = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultCaptureDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default capture device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 1\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializeCaptureDevice(defaultCaptureDevice, audioFormat);\r\n                        \r\n                        // Create a recorder that uses a callback instead of a file stream.\r\n                        using var recorder = new Recorder(device, ProcessAudio);\r\n\r\n                        Console.WriteLine(\"Recording with custom processing... Press any key to stop.\");\r\n                        device.Start();\r\n                        recorder.StartRecording();\r\n                        \r\n                        Console.ReadKey();\r\n                        \r\n                        recorder.StopRecording();\r\n                        device.Stop();\r\n                        Console.WriteLine(\"Recording stopped.\");\r\n                    }\r\n\r\n                    // This method will be called for each chunk of recorded audio.\r\n                    private static void ProcessAudio(Span<float> samples, Capability capability)\r\n                    {\r\n                        // Perform custom processing on the audio samples.\r\n                        // For example, calculate the average level:\r\n                        float sum = 0;\r\n                        for (int i = 0; i < samples.Length; i++)\r\n                        {\r\n                            sum += Math.Abs(samples[i]);\r\n                        }\r\n                        float averageLevel = sum / samples.Length;\r\n\r\n                        Console.WriteLine($\"Average level: {averageLevel:F4}\");\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This example demonstrates how to process audio data in real-time as it's being recorded. After the standard setup of initializing the `AudioEngine` and the default `AudioCaptureDevice`, a `Recorder` is created.\r\n        Instead of providing a file stream, this `Recorder` is given a callback method, `ProcessAudio`. The `Recorder` subscribes to the `device`'s `OnAudioProcessed` event. When the recorder is started, it will invoke our `ProcessAudio` method every time the device provides a new chunk of audio data.\r\n        The `ProcessAudio` method receives a `Span<float>` containing the latest audio samples, allowing for immediate analysis or processing, such as calculating the average level shown in this example. This approach is ideal for applications that need to react to live audio input without writing to a file, like voice activity detection, real-time visualizations, or triggering events based on sound.\r\n    </Tab>\r\n\r\n    <Tab key=\"mic-playback\" title=\"Mic Playback (Monitor)\">\r\n        This tutorial demonstrates capturing microphone audio and playing it back in real-time.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o MicrophonePlayback\r\n                cd MicrophonePlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement microphone loopback\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n\r\n                namespace MicrophonePlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback and capture devices.\r\n                        var defaultPlayback = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        var defaultCapture = audioEngine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlayback.Id == IntPtr.Zero || defaultCapture.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"Default playback and/or capture device not found.\");\r\n                            return;\r\n                        }\r\n                        \r\n                        // Define a common audio format for both input and output.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2 // Stereo for playback\r\n                        };\r\n\r\n                        // Use FullDuplexDevice for simplified simultaneous input and output.\r\n                        using var fullDuplexDevice = audioEngine.InitializeFullDuplexDevice(defaultPlayback, defaultCapture, audioFormat);\r\n                        \r\n                        // Create a data provider that reads from the microphone.\r\n                        // It subscribes to the capture device's audio events.\r\n                        using var microphoneDataProvider = new MicrophoneDataProvider(fullDuplexDevice.CaptureDevice);\r\n                        \r\n                        // Create a SoundPlayer to play back the microphone data.\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, microphoneDataProvider);\r\n\r\n                        // Add the player to the playback device's master mixer.\r\n                        fullDuplexDevice.PlaybackDevice.MasterMixer.AddComponent(player);\r\n\r\n                        // Start capturing and playing.\r\n                        fullDuplexDevice.Start();\r\n                        microphoneDataProvider.StartCapture();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing live microphone audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop everything.\r\n                        fullDuplexDevice.Stop();\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This example creates a real-time microphone monitoring system. The key component is the `FullDuplexDevice`, a high-level abstraction that simplifies managing simultaneous input and output.\r\n        After initializing the `AudioEngine`, we find the default playback and capture devices and define a common `AudioFormat`. `audioEngine.InitializeFullDuplexDevice` is then called to create and manage a paired `AudioCaptureDevice` and `AudioPlaybackDevice`.\r\n        A `MicrophoneDataProvider` is created and linked to the capture part of the full duplex device (`fullDuplexDevice.CaptureDevice`). This provider listens for incoming audio data.\r\n        A `SoundPlayer` is instantiated with the `MicrophoneDataProvider` as its source. This player is then added to the `MasterMixer` of the playback part of the full duplex device (`fullDuplexDevice.PlaybackDevice.MasterMixer`).\r\n        Starting the `fullDuplexDevice` activates both the capture and playback hardware streams. `microphoneDataProvider.StartCapture()` begins queuing the incoming audio, and `player.Play()` starts reading from that queue and sending the audio to the output, creating a live monitoring effect. All resources are managed with `using` statements for automatic cleanup.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for recording audio with SoundFlow!\r\nNext, explore using audio effects with SoundFlow in our [Modifiers Tutorial](./tutorials-modifiers)."
  },
  {
    "id": 7,
    "slug": "tutorials-playback",
    "version": "1.2.0",
    "title": "Playback Fundamentals",
    "description": "Comprehensive tutorials for playing audio files, streams, and controlling playback with the SoundFlow C# audio library.",
    "navOrder": 7,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 7\r\ntitle: Playback Fundamentals\r\ndescription: Comprehensive tutorials for playing audio files, streams, and controlling playback with the SoundFlow C# audio library.\r\nnavOrder: 7\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Playback with SoundFlow\r\n\r\nWelcome to the SoundFlow audio playback tutorials! This guide will walk you through the essential steps to integrate audio playback into your .NET applications using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to play local files, stream from the web, control playback dynamics, implement looping, experiment with surround sound, or efficiently handle large audio assets, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Playback tutorials\">\r\n    <Tab key=\"basic-playback\" title=\"Basic Playback\">\r\n        This tutorial demonstrates how to play an audio file from disk using `SoundPlayer` and\r\n        `StreamDataProvider`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application:\r\n                ```bash\r\n                dotnet new console -o BasicPlayback\r\n                cd BasicPlayback\r\n                ```\r\n            </Step>\r\n            <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                ### 2. Install the SoundFlow NuGet package:\r\n                ```bash\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the basic player\" icon='ph:code-bold'>\r\n                ### 3. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace BasicPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine with the MiniAudio backend.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback device.\r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // The audio format for processing. We'll use 32-bit float, which is standard for processing.\r\n                        // The data provider will handle decoding the source file to this format.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        // Initialize the playback device. This manages the connection to the physical audio hardware.\r\n                        // The 'using' statement ensures it's properly disposed of.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a data provider for the audio file.\r\n                        // Replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n\r\n                        // Create a SoundPlayer, linking the engine, format, and data provider.\r\n                        // The player is also IDisposable.\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Add the player to the device's master mixer to route its audio for playback.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device. This opens the audio stream to the hardware.\r\n                        device.Start();\r\n\r\n                        // Start playback.\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until playback finishes or the user presses a key.\r\n                        Console.WriteLine(\"Playing audio... Press any key to exit.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop the device. This stops the audio stream.\r\n                        device.Stop();\r\n\r\n                        // The `using` statements for `audioEngine`, `device`, `dataProvider`, and `player`\r\n                        // will automatically handle disposal and resource cleanup.\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file on your\r\n                computer.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 4. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        First, a `MiniAudioEngine` is initialized to manage audio operations. We then identify the `default playback device` from the engine's available devices. An `AudioFormat` struct is defined, specifying the desired internal processing format (e.g., 32-bit float, 48kHz sample rate, 2 channels).\r\n        The chosen device is then initialized using `audioEngine.InitializePlaybackDevice`, creating an `AudioPlaybackDevice`. This device handles the interaction with the audio hardware and exposes a `MasterMixer` where `SoundFlow` components can be added.\r\n        A `StreamDataProvider` is created to load the audio file. Crucially, both `StreamDataProvider` and `SoundPlayer` are now instantiated with `audioEngine` and `audioFormat` to provide them with the necessary context for decoding and processing audio in the specified format. The `player` is added to the `device.MasterMixer`.\r\n        Finally, `device.Start()` is called to open the audio stream to the hardware, enabling sound output. The `player.Play()` then begins playback. The program waits for user input, after which the `using` statements automatically handle the proper disposal and cleanup of all allocated resources (`audioEngine`, `device`, `dataProvider`, and `player`).\r\n    </Tab>\r\n\r\n    <Tab key=\"web-playback\" title=\"Web Playback\">\r\n        This tutorial demonstrates how to play an audio stream from a URL using `SoundPlayer` and\r\n        `NetworkDataProvider`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application:\r\n                ```bash\r\n                dotnet new console -o WebPlayback\r\n                cd WebPlayback\r\n                ```\r\n            </Step>\r\n            <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                ### 2. Install the SoundFlow NuGet package:\r\n                ```bash\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the network player\" icon='ph:code-bold'>\r\n                ### 3. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Linq;\r\n                using System.Threading.Tasks;\r\n\r\n                namespace WebPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static async Task Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n\r\n                        // Find the default playback device.\r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define the audio format for processing.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        // Initialize the default playback device.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a NetworkDataProvider. Replace \"your-audio-stream-url\"\r\n                        // with the actual URL (direct audio file or HLS .m3u8 playlist).\r\n                        using var dataProvider = new NetworkDataProvider(audioEngine, audioFormat, \"your-audio-stream-url\");\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing stream... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop the device.\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"your-audio-stream-url\"` with the actual URL of an audio stream (e.g., direct\r\n                MP3/WAV or an HLS .m3u8 playlist).***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 4. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example follows the same foundational setup as the basic playback tutorial: initializing an `AudioEngine`, identifying a `default playback device`, defining an `AudioFormat`, and initializing the `AudioPlaybackDevice`.\r\n        The key distinction lies in using `NetworkDataProvider` instead of `StreamDataProvider`. `NetworkDataProvider` is designed to stream and decode audio directly from a web URL. Like other data providers and players in the new API, its constructor now requires both the `audioEngine` and `audioFormat`. It intelligently handles various web audio sources, including direct MP3/WAV files and HLS `.m3u8` playlists.\r\n        After the `SoundPlayer` is created and added to the `device.MasterMixer`, `device.Start()` and `player.Play()` initiate the audio stream. The `using` statements ensure all resources are properly managed and disposed upon program exit.\r\n    </Tab>\r\n\r\n    <Tab key=\"playback-control\" title=\"Playback Control\">\r\n        This tutorial demonstrates how to control audio playback using `Play`, `Pause`, `Stop`, `Seek`, and\r\n        `PlaybackSpeed`.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o PlaybackControl\r\n                cd PlaybackControl\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the interactive player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace PlaybackControl;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider) { Volume = 0.8f }; // Example: set initial volume\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n                        Console.WriteLine(\"Playing audio... (p: pause/play, s: seek, +/-: speed, v/m: volume, any other: stop)\");\r\n\r\n                        // Handle user input for playback control.\r\n                        while (player.State != PlaybackState.Stopped)\r\n                        {\r\n                            var keyInfo = Console.ReadKey(true);\r\n                            switch (keyInfo.Key)\r\n                            {\r\n                                case ConsoleKey.P:\r\n                                    if (player.State == PlaybackState.Playing)\r\n                                        player.Pause();\r\n                                    else\r\n                                        player.Play();\r\n                                    Console.WriteLine(player.State == PlaybackState.Paused ? \"Paused\" : \"Playing\");\r\n                                    break;\r\n                                case ConsoleKey.S:\r\n                                    Console.Write(\"Enter seek time (in seconds, e.g., 10.5): \");\r\n                                    if (float.TryParse(Console.ReadLine(), out var seekTimeSeconds))\r\n                                    {\r\n                                        if (player.Seek(TimeSpan.FromSeconds(seekTimeSeconds)))\r\n                                            Console.WriteLine($\"Seeked to {seekTimeSeconds:F1}s. Current time: {player.Time:F1}s\");\r\n                                        else\r\n                                            Console.WriteLine(\"Seek failed.\");\r\n                                    }\r\n                                    else\r\n                                        Console.WriteLine(\"Invalid seek time.\");\r\n                                    break;\r\n                                case ConsoleKey.OemPlus:\r\n                                case ConsoleKey.Add:\r\n                                    player.PlaybackSpeed = Math.Min(2.0f, player.PlaybackSpeed + 0.1f);\r\n                                    Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                case ConsoleKey.OemMinus:\r\n                                case ConsoleKey.Subtract:\r\n                                    player.PlaybackSpeed = Math.Max(0.1f, player.PlaybackSpeed - 0.1f);\r\n                                    Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                case ConsoleKey.V:\r\n                                    player.Volume = Math.Min(1.5f, player.Volume + 0.1f); // Allow gain up to 150%\r\n                                    Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                case ConsoleKey.M:\r\n                                    player.Volume = Math.Max(0.0f, player.Volume - 0.1f);\r\n                                    Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                default:\r\n                                    player.Stop();\r\n                                    Console.WriteLine(\"Stopped\");\r\n                                    break;\r\n                            }\r\n                        }\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates interactive control over a `SoundPlayer` by extending the basic playback setup. After initializing the `AudioEngine`, selecting the `default playback device`, defining the `AudioFormat`, and initializing the `AudioPlaybackDevice`, a `StreamDataProvider` loads the audio file, and a `SoundPlayer` is instantiated (again, with `audioEngine` and `audioFormat`). The player is then added to the `device.MasterMixer`.\r\n        Once `device.Start()` and `player.Play()` are called, the application enters a loop to process user input from the console. The `SoundFlow` API provides intuitive properties and methods for common playback controls:\r\n        *   `P` toggles between `Play()` and `Pause()`.\r\n        *   `S` prompts for a time and calls `player.Seek(TimeSpan)`. The `Seek` method returns a boolean, indicating if the seek operation was successful (which depends on the underlying data provider's `CanSeek` capability).\r\n        *   `+` and `-` keys adjust the `player.PlaybackSpeed` property.\r\n        *   `V` and `M` keys control the `player.Volume` property, allowing for dynamic gain adjustment.\r\n        *   Any other key press calls `player.Stop()`, terminating playback and exiting the loop.\r\n        The `using` statements guarantee all resources are correctly released when the program concludes.\r\n    </Tab>\r\n\r\n    <Tab key=\"looping\" title=\"Looping\">\r\n        This tutorial demonstrates how to enable looping for a `SoundPlayer` and how to set custom loop points.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LoopingPlayback\r\n                cd LoopingPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the looping player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LoopingPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Enable looping.\r\n                        player.IsLooping = true;\r\n\r\n                        // Optional: Set custom loop points\r\n\r\n                        // Example 1: Loop from 2.5 seconds to 7.0 seconds (using float seconds)\r\n                        // player.SetLoopPoints(2.5f, 7.0f);\r\n\r\n                        // Example 2: Loop from sample 110250 to sample 308700 (using samples)\r\n                        // player.SetLoopPoints(110250, 308700); // Assuming 44.1kHz stereo, these are example values\r\n\r\n                        // Example 3: Loop from 1.5 seconds to the natural end of the audio (using TimeSpan, end point is optional)\r\n                        player.SetLoopPoints(TimeSpan.FromSeconds(1.5));\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio in a loop... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code extends the fundamental playback example to showcase audio looping capabilities. After the standard initialization of the `AudioEngine`, `AudioPlaybackDevice`, and `SoundPlayer` (including passing `audioEngine` and `audioFormat` to the data provider and player), the player's looping behavior is configured:\r\n        *   `player.IsLooping = true;`: This boolean property is the primary control to enable continuous looping. When set to `true`, upon reaching the end of the audio (or the defined loop end point), the player will automatically reset its position to the loop start point and continue playing.\r\n        *   `player.SetLoopPoints(...)`:: This method offers precise control over the section of audio that will loop. It provides multiple overloads, allowing you to specify the loop start and end points using `float` seconds, `int` samples, or `TimeSpan` values, catering to different precision requirements. If the `endTime` (or equivalent) parameter is omitted or set to a default indicating \"to end,\" the loop will extend to the natural conclusion of the audio data.\r\n        The player is added to the `device.MasterMixer`, and after starting the `device` and `player`, the application enters a wait state until the user presses a key, after which resources are automatically cleaned up.\r\n    </Tab>\r\n\r\n    <Tab key=\"surround-sound\" title=\"Surround Sound\">\r\n        This tutorial demonstrates how to use `SurroundPlayer` to play audio with surround sound configurations.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SurroundPlayback\r\n                cd SurroundPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the surround player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n                using System.Numerics;\r\n\r\n                namespace SurroundPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        // Define an audio format with 8 channels for 7.1 surround sound.\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 8\r\n                        };\r\n\r\n                        // Initialize the playback device with the 8-channel format.\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n\r\n                        // Create a SurroundPlayer. It will upmix mono/stereo sources.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SurroundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Configure the SurroundPlayer for 7.1 surround sound.\r\n                        player.SpeakerConfig = SurroundPlayer.SpeakerConfiguration.Surround71;\r\n\r\n                        // Set the panning method (VBAP is often good for surround).\r\n                        player.Panning = SurroundPlayer.PanningMethod.Vbap;\r\n\r\n                        // Set the listener position (optional, (0,0) is center).\r\n                        player.ListenerPosition = new Vector2(0, 0);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing surround sound audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file (mono, stereo,\r\n                or multi-channel).***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example illustrates how to configure `SoundFlow` for surround sound playback.\r\n        The crucial first step, after initializing the `AudioEngine`, is to define an `AudioFormat` that specifies the desired number of output channels for your surround setup (e.g., 8 channels for 7.1 surround sound). The `AudioPlaybackDevice` is then initialized using this multi-channel `AudioFormat`, ensuring the audio hardware is configured to output to all relevant speakers.\r\n        Instead of `SoundPlayer`, a `SurroundPlayer` is instantiated, taking the `audioEngine`, `audioFormat`, and `dataProvider` as arguments. The `SurroundPlayer` is specialized for spatial audio. If the input `audiofile.wav` is mono or stereo, the `SurroundPlayer` will intelligently upmix and pan the audio across the configured speaker layout.\r\n        Key properties for configuring surround playback include:\r\n        *   `player.SpeakerConfig`: Set this to one of the predefined speaker layouts (e.g., `SurroundPlayer.SpeakerConfiguration.Surround71`).\r\n        *   `player.Panning`: Choose a panning algorithm, such as `SurroundPlayer.PanningMethod.Vbap` (Vector-Based Amplitude Panning), which often provides excellent spatialization for surround sound.\r\n        *   `player.ListenerPosition`: (Optional) Adjust the virtual listener's position within the soundfield using a `Vector2`.\r\n        The `SurroundPlayer` is added to the `device.MasterMixer`, and after `device.Start()` and `player.Play()`, the application will output sound through the configured surround channels. Resources are automatically managed by `using` statements.\r\n    </Tab>\r\n\r\n    <Tab key=\"chunked-data\" title=\"Chunked Data\">\r\n        This tutorial demonstrates how to use the `ChunkedDataProvider` for efficient playback of large audio\r\n        files.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ChunkedPlayback\r\n                cd ChunkedPlayback\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the chunked data player\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ChunkedPlayback;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultPlaybackDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultPlaybackDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultPlaybackDevice, audioFormat);\r\n                        \r\n                        // Create a ChunkedDataProvider and load a large audio file.\r\n                        // Replace \"path/to/your/large/audiofile.wav\" with the actual path.\r\n                        using var dataProvider = new ChunkedDataProvider(audioEngine, audioFormat, \"path/to/your/large/audiofile.wav\");\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio with ChunkedDataProvider... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/large/audiofile.wav\"` with the path to a large audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        This tutorial showcases the `ChunkedDataProvider`, a specialized data provider designed for efficient playback of very large audio files. Unlike `StreamDataProvider` or `AssetDataProvider` which might load an entire file into memory or process it sequentially without optimized chunking, `ChunkedDataProvider` reads and decodes audio data in smaller, manageable chunks. This significantly reduces memory footprint and improves responsiveness, especially for long recordings or high-resolution audio.\r\n        The setup follows the familiar pattern: initialize `AudioEngine`, select `default playback device`, define `AudioFormat`, and initialize `AudioPlaybackDevice`. The `ChunkedDataProvider` is then instantiated, requiring `audioEngine`, `audioFormat`, and the path to the large audio file. A `SoundPlayer` is created with this provider, added to the `device.MasterMixer`, and playback begins.\r\n        The integration is seamless; simply swap `StreamDataProvider` (or `AssetDataProvider`) with `ChunkedDataProvider` in your setup. The underlying chunking mechanism works transparently, ensuring smooth playback while optimizing resource usage. All resources are released automatically via `using` statements.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for playing audio with SoundFlow!\r\nNext, explore recording audio with SoundFlow in our [Recording Audio Tutorial](./tutorials-recording)."
  },
  {
    "id": 9,
    "slug": "tutorials-modifiers",
    "version": "1.2.0",
    "title": "Audio Modifiers & Effects",
    "description": "Comprehensive tutorials for applying various audio effects and modifiers (reverb, EQ, compression, etc.) with SoundFlow.",
    "navOrder": 9,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 9\r\ntitle: Audio Modifiers & Effects\r\ndescription: Comprehensive tutorials for applying various audio effects and modifiers (reverb, EQ, compression, etc.) with SoundFlow.\r\nnavOrder: 9\r\ncategory: Tutorials and Examples\r\n---\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Applying Audio Modifiers & Effects with SoundFlow\r\n\r\nWelcome to the SoundFlow audio modifiers and effects tutorials! This guide will walk you through applying various digital audio effects to your playback streams using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're looking to add reverb, equalize frequencies, compress dynamics, or mix multiple sources, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Modifier tutorials\">\r\n    <Tab key=\"reverb\" title=\"Reverb\">\r\n        Demonstrates how to apply a reverb effect using the `AlgorithmicReverbModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ReverbEffect\r\n                cd ReverbEffect\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with reverb\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ReverbEffect;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create an AlgorithmicReverbModifier. It now needs the audio format.\r\n                        var reverb = new AlgorithmicReverbModifier(audioFormat)\r\n                        {\r\n                            RoomSize = 0.8f,\r\n                            Damp = 0.5f,\r\n                            Wet = 0.3f, // Wet mix (0=dry, 1=fully wet)\r\n                            Width = 1f,\r\n                            PreDelay = 20f // Pre-delay in milliseconds\r\n                        };\r\n\r\n                        // Add the reverb modifier to the player.\r\n                        player.AddModifier(reverb);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start the device and player.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with reverb... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        After initializing the `AudioEngine` and the `AudioPlaybackDevice`, an `AlgorithmicReverbModifier` is created. In the new API, the modifier's constructor requires an `AudioFormat` instance to correctly configure its internal delay lines and filters based on the sample rate and channel count. The `Wet` property now controls the blend between the original (dry) and processed (wet) signals, where 0 is fully dry and 1 is fully wet. We add this `reverb` modifier to the `player`, which is then added to the `device.MasterMixer` for playback. Experiment with `RoomSize`, `Damp`, `Wet`, `Width`, and `PreDelay` to shape the reverb's character.\r\n    </Tab>\r\n\r\n    <Tab key=\"equalization\" title=\"Equalization\">\r\n        Demonstrates how to use the `ParametricEqualizer` to adjust frequency balance.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Equalization\r\n                cd Equalization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with EQ\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.Collections.Generic;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Equalization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a ParametricEqualizer, which now requires the audio format.\r\n                        var equalizer = new ParametricEqualizer(audioFormat);\r\n\r\n                        // Add some equalizer bands.\r\n                        var bands = new List<EqualizerBand>\r\n                        {\r\n                            // Boost low frequencies (bass)\r\n                            new(FilterType.LowShelf, 100, 6, 0.7f),\r\n                            // Cut mid frequencies\r\n                            new(FilterType.Peaking, 1000, -4, 2f),\r\n                            // Boost high frequencies (treble)\r\n                            new(FilterType.HighShelf, 10000, 5, 0.7f)\r\n                        };\r\n                        equalizer.AddBands(bands);\r\n\r\n                        // Add the equalizer to the player.\r\n                        player.AddModifier(equalizer);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with equalization... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust bands\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        Following the new API structure, this code sets up the `AudioEngine` and an `AudioPlaybackDevice`. A `ParametricEqualizer` is instantiated, and like the reverb modifier, its constructor now requires an `AudioFormat` to correctly calculate filter coefficients based on the sample rate.\r\n        We define a `List<EqualizerBand>` to configure our EQ settings: a low-shelf filter to boost bass, a peaking filter to cut a specific mid-range frequency, and a high-shelf filter to boost treble. The `equalizer.AddBands()` method applies these settings. The `equalizer` is then added as a modifier to the `player`, which is subsequently added to the `device.MasterMixer`. You will hear the audio with the equalization applied. Experiment with different `FilterType` enums, frequencies, gain values, and Q factors to shape the sound to your liking.\r\n    </Tab>\r\n\r\n    <Tab key=\"chorus-delay\" title=\"Chorus & Delay\">\r\n        Demonstrates how to apply chorus and delay effects using `ChorusModifier` and `DelayModifier`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o ChorusDelay\r\n                cd ChorusDelay\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with effects\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace ChorusDelay;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a ChorusModifier.\r\n                        var chorus = new ChorusModifier(\r\n                            audioFormat,\r\n                            depthMs: 2f,           // Depth (in milliseconds)\r\n                            rateHz: 0.8f,          // LFO Rate (in Hz)\r\n                            feedback: 0.5f,      // Feedback amount\r\n                            wetDryMix: 0.5f      // Wet/dry mix (0 = dry, 1 = wet)\r\n                        );\r\n\r\n                        // Create a DelayModifier.\r\n                        var delaySamples = (int)(audioFormat.SampleRate * 0.5); // 500ms delay\r\n                        var delay = new DelayModifier(\r\n                            audioFormat,\r\n                            delaySamples: delaySamples, \r\n                            feedback: 0.6f,      \r\n                            wetMix: 0.4f,       \r\n                            cutoff: 4000f \r\n                        );\r\n\r\n                        // Add the chorus and delay modifiers to the player.\r\n                        player.AddModifier(chorus);\r\n                        player.AddModifier(delay);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with chorus and delay... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust effects\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates chaining multiple effects. After the standard `AudioEngine` and `AudioPlaybackDevice` setup, we instantiate `ChorusModifier` and `DelayModifier`. Both modifiers now require an `AudioFormat` object in their constructors to configure internal buffers and timing calculations correctly based on the sample rate. The delay length for the `DelayModifier` is now specified in samples instead of milliseconds.\r\n        Both `chorus` and `delay` modifiers are added to the `player` instance. They will be processed in the order they were added: the audio will first pass through the chorus effect, and the output of the chorus will then be fed into the delay effect. This chain is then added to the `device.MasterMixer` for playback.\r\n    </Tab>\r\n\r\n    <Tab key=\"compression\" title=\"Compression\">\r\n        Demonstrates how to use the `CompressorModifier` to reduce the dynamic range of an audio stream.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Compression\r\n                cd Compression\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with compressor\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Modifiers;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Compression;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a CompressorModifier.\r\n                        var compressor = new CompressorModifier(\r\n                            audioFormat,\r\n                            thresholdDb: -20f, // Threshold (in dB)\r\n                            ratio: 4f,        // Compression ratio\r\n                            attackMs: 10f,      // Attack time (in milliseconds)\r\n                            releaseMs: 100f,    // Release time (in milliseconds)\r\n                            kneeDb: 5f,         // Knee width (in dB)\r\n                            makeupGainDb: 6f   // Makeup gain (in dB)\r\n                        );\r\n\r\n                        // Add the compressor to the player.\r\n                        player.AddModifier(compressor);\r\n                        \r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n                        \r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio with compression... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code showcases dynamic range compression. After initializing the `AudioEngine` and an `AudioPlaybackDevice`, a `CompressorModifier` is created. Its constructor now requires an `AudioFormat` to accurately time its attack and release stages based on the sample rate. The compressor is configured with a -20dB threshold, a 4:1 ratio, a soft knee of 5dB, and 6dB of makeup gain to compensate for the volume reduction. This modifier is then added to the `player`, which is routed to the `device.MasterMixer`. When played, the audio's dynamic range will be reduced, making quiet parts louder and loud parts quieter, resulting in a more consistent overall volume level.\r\n    </Tab>\r\n\r\n    <Tab key=\"mixing\" title=\"Mixing\">\r\n        Demonstrates how to use the `Mixer` to combine multiple audio sources.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o Mixing\r\n                cd Mixing\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Combine multiple audio sources\" icon='ph:code-bold'>\r\n                ### 2. Replace the contents of `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace Mixing;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create two SoundPlayer instances and load different audio files.\r\n                        using var dataProvider1 = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile1.wav\"));\r\n                        using var player1 = new SoundPlayer(audioEngine, audioFormat, dataProvider1);\r\n\r\n                        using var dataProvider2 = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile2.wav\"));\r\n                        using var player2 = new SoundPlayer(audioEngine, audioFormat, dataProvider2);\r\n\r\n                        // Create an Oscillator that generates a sine wave.\r\n                        using var oscillator = new Oscillator(audioEngine, audioFormat)\r\n                        {\r\n                            Frequency = 440, // 440 Hz (A4 note)\r\n                            Amplitude = 0.5f,\r\n                            Type = Oscillator.WaveformType.Sine\r\n                        };\r\n\r\n                        // Add the players and the oscillator to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player1);\r\n                        device.MasterMixer.AddComponent(player2);\r\n                        device.MasterMixer.AddComponent(oscillator);\r\n\r\n                        // Start playback for both players.\r\n                        device.Start();\r\n                        player1.Play();\r\n                        player2.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing mixed audio... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile1.wav\"` and `\"path/to/your/audiofile2.wav\"` with the actual\r\n                paths to two different audio files.***\r\n            </Step>\r\n            <Step title=\"Run & Experiment\" description=\"Run the app and adjust levels\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the core mixing capability of SoundFlow. After initializing the `AudioEngine` and an `AudioPlaybackDevice`, we create multiple `SoundComponent` instances: two `SoundPlayer`s for audio files and one `Oscillator` for a synthesized sine wave. Note that the `Oscillator` now also requires the `audioEngine` and `audioFormat` in its constructor.\r\n        Instead of a static master mixer, all components are added directly to the `device.MasterMixer`. The mixer automatically sums the audio output from all its added components. When the device is started, you will hear both audio files and the sine wave playing simultaneously, mixed together. You can control the individual contribution of each component to the mix by adjusting its `Volume` and `Pan` properties.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for applying audio effects with SoundFlow!\r\nNext, explore using audio analyzers with SoundFlow in our [Analysis Tutorial](./tutorials-analysis)."
  },
  {
    "id": 10,
    "slug": "tutorials-analysis",
    "version": "1.2.0",
    "title": "Audio Analysis & Visualization",
    "description": "Comprehensive tutorials for analyzing audio streams (level, spectrum, VAD) and visualizing data with SoundFlow.",
    "navOrder": 10,
    "category": "Tutorials and Examples",
    "content": "﻿---\r\nid: 10\r\ntitle: Audio Analysis & Visualization\r\ndescription: Comprehensive tutorials for analyzing audio streams (level, spectrum, VAD) and visualizing data with SoundFlow.\r\nnavOrder: 10\r\ncategory: Tutorials and Examples\r\n---\r\nimport { Icon } from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Audio Analysis & Visualization with SoundFlow\r\n\r\nWelcome to the SoundFlow audio analysis and visualization tutorials! This guide will walk you through extracting valuable data from audio streams, such as level information, frequency spectrum, and voice activity, as well as visualizing this data, using the powerful SoundFlow C# audio library.\r\n\r\nWhether you're building a custom meter, a real-time spectrum display, or a smart voice assistant, these examples have you covered.\r\n\r\n<Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Audio Analysis and Visualization tutorials\">\r\n    <Tab key=\"level-metering\" title=\"Level Metering\">\r\n        This tutorial demonstrates how to use the `LevelMeterAnalyzer` to measure the RMS and peak levels of an\r\n        audio stream.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LevelMetering\r\n                cd LevelMetering\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LevelMetering;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a LevelMeterAnalyzer, passing the audio format.\r\n                        var levelMeter = new LevelMeterAnalyzer(audioFormat);\r\n\r\n                        // Attach the analyzer to the player.\r\n                        player.AddAnalyzer(levelMeter);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Create a timer to periodically display the RMS and peak levels.\r\n                        var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            Console.WriteLine($\"RMS Level: {levelMeter.Rms:F4}, Peak Level: {levelMeter.Peak:F4}\");\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop playback and clean up.\r\n                        timer.Stop();\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        After setting up the `AudioEngine` and initializing an `AudioPlaybackDevice`, a `SoundPlayer` is created. A `LevelMeterAnalyzer` is then instantiated, requiring an `AudioFormat` in its constructor. The analyzer is attached directly to the `player` using `player.AddAnalyzer(levelMeter)`.\r\n        When the `player` processes audio, it automatically passes its audio data to the attached `levelMeter`. A timer then periodically reads the `Rms` and `Peak` properties from the analyzer and displays them in the console, providing a real-time level readout.\r\n    </Tab>\r\n\r\n    <Tab key=\"spectrum-analysis\" title=\"Spectrum Analysis\">\r\n        This tutorial demonstrates how to use the `SpectrumAnalyzer` to analyze frequency content using FFT.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SpectrumAnalysis\r\n                cd SpectrumAnalysis\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace SpectrumAnalysis;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                        var spectrumAnalyzer = new SpectrumAnalyzer(audioFormat, fftSize: 2048);\r\n\r\n                        // Attach the spectrum analyzer to the player.\r\n                        player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Create a timer to periodically display the spectrum data.\r\n                        var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            // Get the spectrum data from the analyzer.\r\n                            var spectrumData = spectrumAnalyzer.SpectrumData;\r\n\r\n                            // Print the magnitude of the first few frequency bins.\r\n                            if (spectrumData.Length > 0)\r\n                            {\r\n                                Console.Write(\"Spectrum: \");\r\n                                for (int i = 0; i < Math.Min(10, spectrumData.Length); i++)\r\n                                {\r\n                                    Console.Write($\"{spectrumData[i]:F2} \");\r\n                                }\r\n                                Console.WriteLine();\r\n                            }\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying spectrum data... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        // Stop playback and clean up.\r\n                        timer.Stop();\r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code follows a similar pattern to level metering. A `SpectrumAnalyzer` is created, requiring the `audioFormat` and an `fftSize` (which must be a power of two). It is then attached to the `player` using `AddAnalyzer`. As the `player` processes audio, it feeds the data to the `spectrumAnalyzer`, which performs a Fast Fourier Transform (FFT).\r\n        A timer periodically accesses the `spectrumAnalyzer.SpectrumData` property, which contains the magnitudes of the frequency bins, and prints the first few values to the console for a simple real-time display of the frequency content.\r\n    </Tab>\r\n\r\n    <Tab key=\"vad\" title=\"Voice Activity Detection\">\r\n        This tutorial demonstrates how to use the `VoiceActivityDetector` to detect the presence of human voice.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o VoiceActivityDetection\r\n                cd VoiceActivityDetection\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement VAD on a source\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace VoiceActivityDetection;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize the audio engine.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        \r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if (defaultDevice.Id == IntPtr.Zero)\r\n                        {\r\n                            Console.WriteLine(\"No default playback device found.\");\r\n                            return;\r\n                        }\r\n\r\n                        var audioFormat = new AudioFormat\r\n                        {\r\n                            Format = SampleFormat.F32,\r\n                            SampleRate = 48000,\r\n                            Channels = 2\r\n                        };\r\n                        \r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a SoundPlayer and load an audio file with speech.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/speechfile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a VoiceActivityDetector.\r\n                        var vad = new VoiceActivityDetector(audioFormat);\r\n\r\n                        // Attach the VAD as an analyzer to the player's output.\r\n                        player.AddAnalyzer(vad);\r\n\r\n                        // Subscribe to the SpeechDetected event.\r\n                        vad.SpeechDetected += isDetected => Console.WriteLine($\"Speech detected: {isDetected}\");\r\n\r\n                        // Add the player to the device's master mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Analyzing audio for voice activity... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n                        \r\n                        device.Stop();\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/speechfile.wav\"` with the path to an audio file containing speech.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates how to detect voice in an audio stream. After initializing the `AudioEngine`, `AudioPlaybackDevice`, and `SoundPlayer`, a `VoiceActivityDetector` is created, passing the `audioFormat` to its constructor. It is then attached to the `player` with `AddAnalyzer`.\r\n        The key part of this example is subscribing to the `vad.SpeechDetected` event. This event fires whenever the VAD's state changes (from silence to speech, or vice versa), providing a boolean value. The event handler simply prints the new state to the console. This event-driven approach is efficient for building applications that need to react to the presence or absence of speech.\r\n    </Tab>\r\n\r\n    <Tab key=\"level-meter-viz\" title=\"Console Level Meter\">\r\n        Demonstrates creating a console-based level meter using the `LevelMeterAnalyzer` and\r\n        `LevelMeterVisualizer`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o LevelMeterVisualization\r\n                cd LevelMeterVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement player with visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Interfaces;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace LevelMeterVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard engine and device setup.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        // Create a player for an audio file.\r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the LevelMeterAnalyzer and LevelMeterVisualizer.\r\n                        // The visualizer is linked to the analyzer.\r\n                        var levelMeterAnalyzer = new LevelMeterAnalyzer(audioFormat);\r\n                        var levelMeterVisualizer = new LevelMeterVisualizer(levelMeterAnalyzer);\r\n                        \r\n                        // Attach the analyzer to the player. The analyzer will automatically\r\n                        // pass its data to the linked visualizer.\r\n                        player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                        // Add the player to the mixer.\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        // Start playback.\r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        levelMeterVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawLevelMeter(levelMeterAnalyzer.Rms, levelMeterAnalyzer.Peak);\r\n                        };\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            levelMeterVisualizer.ProcessOnAudioData(System.Array.Empty<float>());\r\n                            levelMeterVisualizer.Render(new ConsoleVisualizationContext());\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        levelMeterVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based level meter.\r\n                    private static void DrawLevelMeter(float rms, float peak)\r\n                    {\r\n                        int barLength = (int)(rms * 40); \r\n                        int peakMarkerPos = (int)(peak * 40);\r\n\r\n                        Console.SetCursorPosition(0, 0);\r\n                        Console.Write(\"RMS:  [\");\r\n                        Console.Write(new string('#', barLength));\r\n                        Console.Write(new string(' ', 40 - barLength));\r\n                        Console.Write(\"]\\n\");\r\n\r\n                        Console.SetCursorPosition(0, 1);\r\n                        Console.Write(\"Peak: [\");\r\n                        Console.Write(new string(' ', 40));\r\n                        Console.Write(\"]\\r\"); // Carriage return to move back\r\n                        Console.Write(\"Peak: [\");\r\n                        if(peakMarkerPos < 40) Console.SetCursorPosition(7 + peakMarkerPos, 1);\r\n                        Console.Write(\"|\");\r\n                        \r\n                        Console.SetCursorPosition(0, 3);\r\n                    }\r\n                }\r\n\r\n                // Simple IVisualizationContext implementation for console output.\r\n                public class ConsoleVisualizationContext : IVisualizationContext\r\n                {\r\n                    public void Clear() { }\r\n                    public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1) { }\r\n                    public void DrawRectangle(float x, float y, float width, float height, Color color) { }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This example demonstrates the analyzer-visualizer pattern. A `LevelMeterAnalyzer` is created to process the audio, and a `LevelMeterVisualizer` is created, linked to the analyzer. When `player.AddAnalyzer(levelMeterAnalyzer)` is called, the analyzer is attached to the player's audio stream. Inside its processing logic, the analyzer automatically calls its linked visualizer's `ProcessOnAudioData` method.\r\n        We subscribe to the `levelMeterVisualizer.VisualizationUpdated` event, which is fired by the visualizer whenever it receives new data. The event handler calls our `DrawLevelMeter` helper function to render a simple text-based meter in the console, providing a direct visual representation of the audio levels calculated by the analyzer.\r\n    </Tab>\r\n\r\n    <Tab key=\"waveform-viz\" title=\"Console Waveform\">\r\n        Demonstrates using `WaveformVisualizer` to display an audio waveform.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o WaveformVisualization\r\n                cd WaveformVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement waveform visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.Collections.Generic;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace WaveformVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard setup\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create a LevelMeterAnalyzer or any analyzer you want.\r\n                        var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                        // Create a WaveformVisualizer.\r\n                        var waveformVisualizer = new WaveformVisualizer();\r\n\r\n                        // Add the player to the master mixer.\r\n                        Mixer.Master.AddComponent(player);\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        waveformVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawWaveform(waveformVisualizer.Waveform);\r\n                        };\r\n\r\n                        // Connect the player's output to the level meter analyzer's input.\r\n                        player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                        // Add the player to the master mixer.\r\n                        Mixer.Master.AddComponent(player);\r\n\r\n                        // Start playback.\r\n                        player.Play();\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            waveformVisualizer.Render(new ConsoleVisualizationContext()); // ConsoleVisualizationContext is just a placeholder\r\n                        };\r\n                        timer.Start();\r\n\r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        Console.WriteLine(\"Playing audio and displaying waveform... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        waveformVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based waveform.\r\n                    private static void DrawWaveform(IReadOnlyList<float> waveform)\r\n                    {\r\n                        Console.Clear();\r\n                        int consoleWidth = Console.WindowWidth;\r\n                        int consoleHeight = Console.WindowHeight;\r\n\r\n                        if (waveform.Count == 0) return;\r\n\r\n                        for (int i = 0; i < consoleWidth; i++)\r\n                        {\r\n                            int waveformIndex = (int)(i * (waveform.Count / (float)consoleWidth));\r\n                            waveformIndex = Math.Clamp(waveformIndex, 0, waveform.Count - 1);\r\n\r\n                            float sampleValue = waveform[waveformIndex];\r\n                            int consoleY = (int)((sampleValue + 1) * 0.5 * (consoleHeight - 1));\r\n                            consoleY = Math.Clamp(consoleY, 0, consoleHeight - 1);\r\n\r\n                            if (i < consoleWidth && (consoleHeight - consoleY - 1) < consoleHeight)\r\n                            {\r\n                                Console.SetCursorPosition(i, consoleHeight - consoleY - 1);\r\n                                Console.Write(\"*\");\r\n                            }\r\n                        }\r\n                        Console.SetCursorPosition(0, consoleHeight - 1);\r\n                    }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        In this example, the `WaveformVisualizer` is used to directly visualize the audio samples. While it implements `IVisualizer`, it doesn't need a separate `AudioAnalyzer`. It is attached directly to the `player` using `AddAnalyzer`. When the player processes its audio buffer, it passes a copy of that buffer to the `WaveformVisualizer`, which stores it.\r\n        We subscribe to the `VisualizationUpdated` event, which the visualizer raises after receiving a new buffer. The event handler calls `DrawWaveform` to render a simple ASCII representation of the waveform to the console, providing a real-time oscilloscope-like view of the audio signal.\r\n    </Tab>\r\n\r\n    <Tab key=\"spectrum-viz\" title=\"Console Spectrum\">\r\n        Demonstrates creating a console-based spectrum analyzer using the `SpectrumAnalyzer` and\r\n        `SpectrumVisualizer`.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow:\r\n                ```bash\r\n                dotnet new console -o SpectrumAnalyzerVisualization\r\n                cd SpectrumAnalyzerVisualization\r\n                dotnet add package SoundFlow\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement spectrum visualizer\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs` with the following code:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Abstracts.Devices;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Interfaces;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using SoundFlow.Visualization;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace SpectrumAnalyzerVisualization;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Standard setup.\r\n                        using var audioEngine = new MiniAudioEngine();\r\n                        var defaultDevice = audioEngine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                        if(defaultDevice.Id == IntPtr.Zero) return;\r\n                        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\r\n                        using var device = audioEngine.InitializePlaybackDevice(defaultDevice, audioFormat);\r\n                        \r\n                        using var dataProvider = new StreamDataProvider(audioEngine, audioFormat, File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                        using var player = new SoundPlayer(audioEngine, audioFormat, dataProvider);\r\n\r\n                        // Create the SpectrumAnalyzer and SpectrumVisualizer.\r\n                        var spectrumAnalyzer = new SpectrumAnalyzer(audioFormat, fftSize: 2048);\r\n                        var spectrumVisualizer = new SpectrumVisualizer(spectrumAnalyzer);\r\n\r\n                        // Attach the analyzer to the player.\r\n                        player.AddAnalyzer(spectrumAnalyzer);\r\n                        \r\n                        device.MasterMixer.AddComponent(player);\r\n                        \r\n                        device.Start();\r\n                        player.Play();\r\n\r\n                        // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                        spectrumVisualizer.VisualizationUpdated += (sender, e) =>\r\n                        {\r\n                            DrawSpectrum(spectrumAnalyzer.SpectrumData);\r\n                        };\r\n\r\n                        // Start a timer to update the visualization.\r\n                        var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                        timer.Elapsed += (sender, e) =>\r\n                        {\r\n                            spectrumVisualizer.ProcessOnAudioData(Array.Empty<float>());\r\n                            spectrumVisualizer.Render(new ConsoleVisualizationContext());\r\n                        };\r\n                        timer.Start();\r\n\r\n                        // Keep the console application running until the user presses a key.\r\n                        Console.WriteLine(\"Playing audio and displaying spectrum analyzer... Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        device.Stop();\r\n                        spectrumVisualizer.Dispose();\r\n                    }\r\n\r\n                    // Helper method to draw a simple console-based spectrum analyzer.\r\n                    private static void DrawSpectrum(ReadOnlySpan<float> spectrumData)\r\n                    {\r\n                        Console.Clear();\r\n                        int consoleWidth = Console.WindowWidth;\r\n                        int consoleHeight = Console.WindowHeight -1;\r\n\r\n                        if (spectrumData.IsEmpty) return;\r\n\r\n                        for (int i = 0; i < consoleWidth; i++)\r\n                        {\r\n                            // Logarithmic mapping of frequency bins to console columns for better visualization\r\n                            double logIndex = Math.Log10(1 + 9 * ((double)i / consoleWidth));\r\n                            int spectrumIndex = (int)(logIndex * (spectrumData.Length - 1));\r\n                            \r\n                            float magnitude = spectrumData[spectrumIndex];\r\n                            int barHeight = (int)(magnitude * consoleHeight);\r\n                            barHeight = Math.Clamp(barHeight, 0, consoleHeight);\r\n\r\n                            for (int j = 0; j < barHeight; j++)\r\n                            {\r\n                                Console.SetCursorPosition(i, consoleHeight - 1 - j);\r\n                                Console.Write(\"█\");\r\n                            }\r\n                        }\r\n                        Console.SetCursorPosition(0, consoleHeight - 1);\r\n                    }\r\n                }\r\n\r\n                // Simple IVisualizationContext implementation for console output.\r\n                public class ConsoleVisualizationContext : IVisualizationContext\r\n                {\r\n                    public void Clear() { }\r\n                    public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f) { }\r\n                    public void DrawRectangle(float x, float y, float width, float height, Color color) { }\r\n                }\r\n                ```\r\n                ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n            </Step>\r\n            <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                ### 3. Build and run the application:\r\n                ```bash\r\n                dotnet run\r\n                ```\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n\r\n        This code demonstrates the analyzer-visualizer pattern for frequency analysis. A `SpectrumAnalyzer` is created to perform the FFT, and a `SpectrumVisualizer` is linked to it. The analyzer is attached to the `player`, which feeds it audio data. The analyzer processes this data and automatically informs the visualizer.\r\n        We subscribe to the `spectrumVisualizer.VisualizationUpdated` event. In the handler, `DrawSpectrum` is called, which reads the `spectrumAnalyzer.SpectrumData` and renders a simple bar chart of the frequency magnitudes to the console. The drawing logic uses a logarithmic scale for the frequency axis, which provides a more musically intuitive display of the spectrum.\r\n    </Tab>\r\n\t\r\n\t<Tab key=\"ui-integration\" title={<div className=\"flex items-center gap-2\">\r\n\t\t<Icon icon=\"lucide:layout-template\"/><span>UI Integration</span></div>}>\r\n        These examples use basic console output for simplicity. To integrate SoundFlow's visualizers with a GUI\r\n        framework (like WPF, WinForms, Avalonia, or MAUI), you'll need to:\r\n        <Steps layout='vertical'>\r\n            <Step title=\"Implement IVisualizationContext\" description=\"Wrap your UI framework's drawing primitives\"\r\n                  icon='material-symbols:draw-outline'>\r\n                This class will wrap the drawing primitives of your chosen UI framework. For example, in WPF, you might\r\n                use `DrawingContext` methods to draw shapes on a `Canvas`.\r\n            </Step>\r\n            <Step title=\"Update UI from Event\" description=\"Trigger a redraw on the UI thread\" icon='mdi:update'>\r\n                In the `VisualizationUpdated` event handler, trigger a redraw of your UI element that hosts the\r\n                visualization. Make sure to marshal the update to the UI thread using `Dispatcher.Invoke` or a similar\r\n                mechanism if the event is raised from a different thread.\r\n            </Step>\r\n            <Step title=\"Call Render Method\" description=\"Pass your context to the visualizer\" icon='lucide:render'>\r\n                In your UI's rendering logic, call the `Render` method of the visualizer, passing your\r\n                `IVisualizationContext` implementation.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Example (Conceptual WPF):**\r\n\r\n        ```csharp\r\n        // In your XAML:\r\n        // <Canvas x:Name=\"VisualizationCanvas\"/>\r\n\r\n        // In your code-behind:\r\n        public partial class MainWindow : Window\r\n        {\r\n            private readonly WaveformVisualizer _visualizer;\r\n\r\n            public MainWindow()\r\n            {\r\n                InitializeComponent();\r\n\r\n                // ... Initialize AudioEngine, SoundPlayer, etc. ...\r\n\r\n                _visualizer = new WaveformVisualizer();\r\n                _visualizer.VisualizationUpdated += OnVisualizationUpdated;\r\n\r\n                // ...\r\n            }\r\n\r\n            private void OnVisualizationUpdated(object? sender, EventArgs e)\r\n            {\r\n                // Marshal the update to the UI thread\r\n                Dispatcher.Invoke(() =>\r\n                {\r\n                    VisualizationCanvas.Children.Clear(); // Clear previous drawing\r\n\r\n                    // Create a custom IVisualizationContext that wraps the Canvas\r\n                    var context = new WpfVisualizationContext(VisualizationCanvas);\r\n\r\n                    // Render the visualization\r\n                    _visualizer.Render(context);\r\n                });\r\n            }\r\n\r\n            // ...\r\n        }\r\n\r\n        // IVisualizationContext implementation for WPF\r\n        public class WpfVisualizationContext : IVisualizationContext\r\n        {\r\n            private readonly Canvas _canvas;\r\n\r\n            public WpfVisualizationContext(Canvas canvas)\r\n            {\r\n                _canvas = canvas;\r\n            }\r\n\r\n            public void Clear()\r\n            {\r\n                _canvas.Children.Clear();\r\n            }\r\n\r\n            public void DrawLine(float x1, float y1, float x2, float y2, SoundFlow.Interfaces.Color color, float thickness = 1f)\r\n            {\r\n                var line = new Line\r\n                {\r\n                    X1 = x1,\r\n                    Y1 = y1,\r\n                    X2 = x2,\r\n                    Y2 = y2,\r\n                    Stroke = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255))),\r\n                    StrokeThickness = thickness\r\n                };\r\n                _canvas.Children.Add(line);\r\n            }\r\n\r\n            public void DrawRectangle(float x, float y, float width, float height, SoundFlow.Interfaces.Color color)\r\n            {\r\n                var rect = new Rectangle\r\n                {\r\n                    Width = width,\r\n                    Height = height,\r\n                    Fill = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255)))\r\n                };\r\n                Canvas.SetLeft(rect, x);\r\n                Canvas.SetTop(rect, y);\r\n                _canvas.Children.Add(rect);\r\n            }\r\n        }\r\n        ```\r\n\r\n        Remember to adapt this conceptual example to your specific UI framework and project structure.\r\n    </Tab>\r\n</Tabs>\r\n\r\nWe hope these tutorials have provided a solid foundation for audio analysis and visualization with SoundFlow!"
  },
  {
    "id": 1,
    "slug": "getting-started",
    "version": "1.2.0",
    "title": "Getting Started with SoundFlow",
    "description": "Learn how to install the library, set up your development environment, and write your first SoundFlow application.",
    "navOrder": 1,
    "category": "Core",
    "content": "---\nid: 1\ntitle: Getting Started with SoundFlow\ndescription: Learn how to install the library, set up your development environment, and write your first SoundFlow application.\nnavOrder: 1\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\n# Getting Started with SoundFlow\n\nThis guide will help you get up and running with SoundFlow quickly. You'll learn how to install the library, set up your development environment, and write your first SoundFlow application.\n\n## Prerequisites\n\nBefore you begin, make sure you have the following installed:\n\n*   **[.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0) or later:** SoundFlow is built on .NET 8.0, so you'll need the corresponding SDK to build and run SoundFlow projects.\n*   **An IDE or code editor:** You can use any IDE or code editor that supports .NET development. Popular choices include:\n    *   [Visual Studio](https://visualstudio.microsoft.com/) (Recommended for Windows)\n    *   [Visual Studio Code](https://code.visualstudio.com/) (Cross-platform)\n    *   [JetBrains Rider](https://www.jetbrains.com/rider/) (Cross-platform)\n*   **Basic knowledge of C# and .NET:** Familiarity with C# programming and .NET concepts will be helpful.\n\n**Supported Operating Systems:**\n\n*   Windows\n*   macOS\n*   Linux\n*   Android\n*   iOS\n*   FreeBSD\n\n## Installation\n\nYou can install SoundFlow in several ways:\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\n    <Tab\n        key=\"nuget\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:nuget' />\n                <span>NuGet Package Manager</span>\n            </div>\n        }\n    >\n        ### Option 1: Using the NuGet Package Manager (Recommended)\n\n        This is the easiest and recommended way to add SoundFlow to your .NET projects.\n\n        1. **Open the NuGet Package Manager Console:** In Visual Studio, go to `Tools` > `NuGet Package Manager` > `Package Manager Console`.\n        2. **Run the installation command:**\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        This command will download and install the latest version of SoundFlow and its dependencies into your current project.\n    </Tab>\n\n    <Tab\n        key=\"cli\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:dotnet' />\n                <span>.NET CLI</span>\n            </div>\n        }\n    >\n        ### Option 2: Using the .NET CLI\n\n        1. **Open your terminal or command prompt.**\n        2. **Navigate to your project directory.**\n        3. **Run the following command:**\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n\n        This command will add a reference to the SoundFlow package in your project file (`.csproj`).\n    </Tab>\n\n    <Tab\n        key=\"source\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:git' />\n                <span>Building from Source</span>\n            </div>\n        }\n    >\n        ### Option 3: Building from Source\n\n        If you want to use the latest development version of SoundFlow or contribute to the project, you can build it from source:\n\n        1. **Clone the SoundFlow repository:**\n\n        ```bash\n        git clone https://github.com/LSXPrime/SoundFlow.git\n        ```\n\n        2. **Navigate to the cloned directory:**\n\n        ```bash\n        cd SoundFlow\n        ```\n\n        3. **Build the project using the .NET CLI:**\n\n        ```bash\n        dotnet build\n        ```\n    </Tab>\n</Tabs>\n\n## Basic Usage Example\n\nLet's create a simple console application that plays an audio file using SoundFlow.\n\n<Steps layout='horizontal' nextLabel='Got it, Next' finishLabel='All Done!' resetLabel='Start Again' summaryMessage=\"Congratulations! You've built and run your first audio application with SoundFlow. Feel free to experiment with the code or start over.\">\n    <Step title=\"Create Project\" description=\"Use VS or .NET CLI\" icon='ic:outline-create-new-folder'>\n        ### 1. Create a new console application:\n        *   In Visual Studio, go to `File` > `New` > `Project`. Select `Console App` and give it a name (e.g., `SoundFlowExample`).\n        *   Or, use the .NET CLI:\n\n        ```bash\n        dotnet new console -o SoundFlowExample\n        cd SoundFlowExample\n        ```\n    </Step>\n\n    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\n        ### 2. Install the SoundFlow NuGet package: (If you haven't already)\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n    </Step>\n\n    <Step title=\"Write Code\" description=\"Implement the audio player\" icon='ph:code-bold'>\n        ### 3. Replace the contents of `Program.cs` with the following code:\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Abstracts.Devices;\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n\n        namespace SoundFlowExample;\n\n        internal static class Program\n        {\n            private static void Main(string[] args)\n            {\n                // 1. Initialize the engine context. It no longer starts a device directly.\n                using var engine = new MiniAudioEngine();\n\n                // 2. Define the audio format for playback.\n                var format = new AudioFormat\n                {\n                    SampleRate = 48000,\n                    Channels = 2,\n                    Format = SampleFormat.F32\n                };\n                \n                // 3. Initialize a specific playback device. Passing `null` will use the system default too.\n                // The `using` statement ensures the device is properly disposed.\n                var defaultDevice = engine.PlaybackDevices.FirstOrDefault(x => x.IsDefault);\n                using var playbackDevice = engine.InitializePlaybackDevice(defaultDevice, format);\n\n                // 4. Create a SoundPlayer, passing the engine and format context.\n                // Make sure you replace \"path/to/your/audiofile.wav\" with the actual path.\n                using var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"path/to/your/audiofile.wav\"));\n                var player = new SoundPlayer(engine, format, dataProvider);\n\n                // 5. Add the player to the device's master mixer.\n                playbackDevice.MasterMixer.AddComponent(player);\n\n                // 6. Start the device to begin the audio stream.\n                playbackDevice.Start();\n                \n                // 7. Start the player.\n                player.Play();\n\n                Console.WriteLine($\"Playing audio on '{playbackDevice.Info?.Name}'... Press any key to stop.\");\n                Console.ReadKey();\n\n                // 8. Stop the device, which also stops the audio stream.\n                playbackDevice.Stop();\n            }\n        }\n        ```\n        ***Replace `path/to/your/audiofile.wav` with the actual path to an audio file on your computer.***\n    </Step>\n\n    <Step title=\"Run\" description=\"Build and run the app\" icon='lucide:audio-lines'>\n        ### 4. Build and run the application:\n        *   In Visual Studio, press `F5` or go to `Debug` > `Start Debugging`.\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        *  use the .NET CLI:\n\n        ```bash\n        dotnet run\n        ```\n    </Step>\n</Steps>\n\nYou should now hear the audio file playing through your default audio output device.\n\n**Code Explanation:**\n\n*   `using SoundFlow...`: These lines import the necessary namespaces from the SoundFlow library.\n*   `using var engine = new MiniAudioEngine()`: This creates an instance of the `AudioEngine`. In the new architecture, the engine acts as a central **context** or **factory** for creating and managing audio devices. It no longer represents a single device itself.\n*   `var format = new AudioFormat { ... }`: We define the desired audio format using the `AudioFormat` struct. This bundles the sample rate, channel count, and sample format, and is passed to devices and components to ensure they operate in the correct context.\n*   `var defaultDevice = engine.PlaybackDevices.FirstOrDefault(...);` Getting the first device from the available playback devices.\n*   `using var playbackDevice = engine.InitializePlaybackDevice(defaultDevice, format)`: This is the new way to initialize a playback device. We ask the engine to initialize one for us, passing `null` to select the system's default device. The device itself is `IDisposable` and manages its own lifecycle.\n*   `using var dataProvider = ...`, `var player = new SoundPlayer(...)`: Components like `ISoundDataProvider` and `SoundPlayer` now require the `engine` and `format` contexts in their constructors.\n*   `playbackDevice.MasterMixer.AddComponent(player)`: This adds the `SoundPlayer` to the device-specific `Master` mixer, Each `AudioPlaybackDevice` has its own `MasterMixer` property. The `Master` mixer is the root of the audio graph and represents the final output of the audio device solely.\n*   `playbackDevice.Start()`: You must now explicitly start the device to begin its audio processing thread.\n*   `player.Play()`: This starts the player's internal logic, causing it to generate audio when its `Process` method is called by the device's mixer.\n*   `playbackDevice.Stop()`: This stops the device's audio thread, gracefully halting all audio output from it.\n\n## Troubleshooting\n\n*   **\"Could not load file or assembly 'SoundFlow'...\"**: Make sure you have installed the SoundFlow NuGet package or added a reference to the SoundFlow library if you built it from source.\n*   **No audio output**:\n    *   Verify that your audio device is properly configured and selected as the default output device in your operating system's sound settings.\n    *   Check the volume levels in your operating system and in the SoundFlow application.\n    *   Ensure that the audio file you are trying to play is in a supported format and is not corrupted.\n*   **Errors during installation**: If you encounter errors while installing the NuGet package, try clearing your NuGet cache (`dotnet nuget locals all --clear`) and try again.\n\nIf you encounter any other issues, please open an issue on the [GitHub repository](https://github.com/LSXPrime/SoundFlow).\n\n## Next Steps\n\nNow that you have successfully set up SoundFlow and played your first audio file, you can explore the more advanced features and concepts:\n\n*   [Core Concepts](./core-concepts): Learn more about the fundamental building blocks of SoundFlow.\n*   [Device Management](./device-management): Understand the new device-centric architecture, including multi-device playback, capture, and device switching.\n*   [Editing Engine & Persistence](./editing-engine): Discover the powerful non-destructive editing and project saving capabilities.\n*   [API Reference](./api-reference): Dive into the detailed documentation for each class and interface.\n*   [Tutorials and Examples](./tutorials-and-examples): Get hands-on experience with various SoundFlow features.\n\nHappy coding!"
  },
  {
    "id": 4,
    "slug": "editing-engine",
    "version": "1.2.0",
    "title": "Editing Engine & Persistence",
    "description": "Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.",
    "navOrder": 4,
    "category": "Core",
    "content": "---\r\nid: 4\r\ntitle: Editing Engine & Persistence\r\ndescription: Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.\r\nnavOrder: 4\r\ncategory: Core\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\nimport { Card, CardHeader, CardBody } from \"@heroui/react\";\r\n\r\n\r\n# SoundFlow Editing Engine & Persistence\r\n\r\nSoundFlow features a comprehensive, non-destructive audio editing engine and a robust project persistence system. This allows developers to programmatically build, manipulate, and save complex audio timelines, complete with effects, advanced timing controls, and media management.\r\n\r\n## Core Editing Concepts\r\n\r\nThe editing engine revolves around a few key classes:\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Composition\" description=\"The top-level project container\" icon='ph:stack-bold'>\r\n        ### `Composition` (`SoundFlow.Editing.Composition`)\r\n\r\n        The `Composition` is the top-level container for an audio project. Think of it as the main \"session\" or \"project file\" in a Digital Audio Workstation (DAW).\r\n\r\n        *   **Holds Tracks:** A `Composition` contains one or more `Track` objects.\r\n        *   **Master Settings:** It has master volume control (`MasterVolume`) and can have master effects (modifiers and analyzers) applied to the final mix.\r\n        *   **Renderable:** A `Composition` itself implements `ISoundDataProvider`, meaning the entire composed project can be played back directly using a `SoundPlayer` or rendered to an audio file.\r\n        *   **Project Properties:** Stores overall project settings like `Name`, `TargetSampleRate`, and `TargetChannels`.\r\n        *   **Dirty Flag:** Tracks unsaved changes via an `IsDirty` property.\r\n        *   **IDisposable:** Manages the disposal of resources within its scope.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Abstracts.Devices;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using SoundFlow.Structs;\r\n\r\n        // Define the format for the composition\r\n        var compositionFormat = new AudioFormat\r\n        {\r\n            SampleRate = 48000,\r\n            Channels = 2,\r\n            Format = SampleFormat.F32\r\n        };\r\n\r\n        // Create a new composition with the specified format\r\n        var composition = new Composition(compositionFormat, \"My Awesome Project\")\r\n        {\r\n            MasterVolume = 0.9f\r\n        };\r\n\r\n        // Add master effects (optional)\r\n        // composition.AddModifier(new SomeMasterReverb(compositionFormat));\r\n\r\n        // ... (add tracks and segments) ...\r\n\r\n        // To play the composition:\r\n        // 1. Create an engine context\r\n        using var engine = new MiniAudioEngine();\r\n\r\n        // 2. Initialize a playback device using the composition's format\r\n        using var playbackDevice = engine.InitializePlaybackDevice(null, composition.Format);\r\n\r\n        // 3. Create a player for the composition. The composition itself is the data provider.\r\n        var player = new SoundPlayer(engine, composition.Format, composition);\r\n\r\n        // 4. Add the player to the device's MasterMixer\r\n        playbackDevice.MasterMixer.AddComponent(player);\r\n\r\n        // 5. Start the device and the player\r\n        playbackDevice.Start();\r\n        player.Play();\r\n\r\n        // To render the composition to a float array:\r\n        // float[] renderedAudio = composition.Render(TimeSpan.Zero, composition.CalculateTotalDuration());\r\n        ```\r\n    </Step>\r\n    <Step title=\"Track\" description=\"A single audio timeline\" icon='lucide:audio-lines'>\r\n        ### `Track` (`SoundFlow.Editing.Track`)\r\n\r\n        A `Track` represents a single audio track within a `Composition`, similar to a track in a DAW.\r\n\r\n        *   **Holds Segments:** A `Track` contains a list of `AudioSegment` objects, which are the actual audio clips placed on the track's timeline.\r\n        *   **Track-Level Settings (`TrackSettings`):** Each track has its own settings:\r\n            *   `Volume`, `Pan`\r\n            *   `IsMuted`, `IsSoloed`, `IsEnabled`\r\n            *   Track-specific `Modifiers` and `Analyzers`.\r\n        *   **Timeline Management:** Tracks manage the arrangement of their segments.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n\r\n        // Assuming 'composition' from previous example\r\n        var track1 = new Track(\"Lead Vocals\");\r\n        track1.Settings.Volume = 0.8f;\r\n        track1.Settings.Pan = -0.1f; // Slightly to the left\r\n\r\n        var track2 = new Track(\"Background Music\");\r\n        track2.Settings.Volume = 0.5f;\r\n        track2.Settings.IsMuted = true; // Mute this track for now\r\n\r\n        composition.AddTrack(track1);\r\n        composition.AddTrack(track2);\r\n        ```\r\n    </Step>\r\n    <Step title=\"AudioSegment\" description=\"The fundamental audio clip\" icon='mdi:content-cut'>\r\n        ### `AudioSegment` (`SoundFlow.Editing.AudioSegment`)\r\n\r\n        The `AudioSegment` is the fundamental building block for audio content on a `Track`. It represents a specific portion of an audio source placed at a particular time on the track's timeline.\r\n\r\n        *   **Source Reference:** Points to an `ISoundDataProvider` for its audio data.\r\n        *   **Timeline Placement:**\r\n            *   `SourceStartTime`: The time offset within the `ISoundDataProvider` from which this segment begins.\r\n            *   `SourceDuration`: The duration of audio to use from the `ISoundDataProvider`.\r\n            *   `TimelineStartTime`: The time at which this segment starts on the parent `Track`'s timeline.\r\n        *   **Segment-Level Settings (`AudioSegmentSettings`):** Each segment has incredibly granular control:\r\n            *   `Volume`, `Pan`\r\n            *   `IsEnabled`\r\n            *   `IsReversed`: Play the segment's audio backward.\r\n            *   `Loop` (`LoopSettings`): Control repetitions or loop to fill a target duration.\r\n            *   `FadeInDuration`, `FadeInCurve`, `FadeOutDuration`, `FadeOutCurve`: Apply various fade shapes (`Linear`, `Logarithmic`, `S-Curve`).\r\n            *   `SpeedFactor`: Classic varispeed, affects pitch and tempo.\r\n        *   **Pitch-Preserved Time Stretching:**\r\n            *   `TimeStretchFactor`: Lengthen or shorten the segment without changing pitch (e.g., 0.5 for half duration, 2.0 for double duration).\r\n            *   `TargetStretchDuration`: Stretch the segment to fit a specific duration, preserving pitch.\r\n        *   Segment-specific `Modifiers` and `Analyzers`.\r\n        *   **Non-Destructive:** All operations (trimming, fades, stretching) are applied at runtime and do not alter the original audio source.\r\n        *   **IDisposable:** Can own and dispose its `ISoundDataProvider` if specified.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Providers;\r\n        using SoundFlow.Structs;\r\n        using System.IO;\r\n\r\n        // Assuming 'engine', 'track1' and 'compositionFormat' from previous examples\r\n        // And an audio file \"vocals.wav\" exists.\r\n        using var vocalProvider = new StreamDataProvider(engine, compositionFormat, File.OpenRead(\"vocals.wav\"));\r\n\r\n        // Create a segment: use 10 seconds of \"vocals.wav\" starting from 5s into the file,\r\n        // and place it at 2 seconds on track1's timeline.\r\n        var vocalSegment = new AudioSegment(\r\n            format: compositionFormat,\r\n            sourceDataProvider: vocalProvider,\r\n            sourceStartTime: TimeSpan.FromSeconds(5),\r\n            sourceDuration: TimeSpan.FromSeconds(10),\r\n            timelineStartTime: TimeSpan.FromSeconds(2),\r\n            name: \"Verse 1 Vocals\",\r\n            ownsDataProvider: false // vocalProvider is managed by 'using' here\r\n        );\r\n\r\n        // Apply settings\r\n        vocalSegment.Settings.Volume = 0.95f;\r\n        vocalSegment.Settings.FadeInDuration = TimeSpan.FromMilliseconds(200);\r\n        vocalSegment.Settings.FadeInCurve = FadeCurveType.SCurve;\r\n        vocalSegment.Settings.TimeStretchFactor = 1.1f; // Make it 10% longer without pitch change\r\n\r\n        track1.AddSegment(vocalSegment);\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n### Duration Calculations\r\n\r\n*   `AudioSegment.StretchedSourceDuration`: The duration of the segment's content *after* pitch-preserved time stretching is applied (but before `SpeedFactor`).\r\n*   `AudioSegment.EffectiveDurationOnTimeline`: The duration a single instance of the segment takes on the timeline, considering both `StretchedSourceDuration` and `SpeedFactor`.\r\n*   `AudioSegment.GetTotalLoopedDurationOnTimeline()`: The total duration the segment occupies on the timeline, including all loops.\r\n*   `AudioSegment.TimelineEndTime`: `TimelineStartTime + GetTotalLoopedDurationOnTimeline()`.\r\n*   `Track.CalculateDuration()`: The time of the latest `TimelineEndTime` among all its segments.\r\n*   `Composition.CalculateTotalDuration()`: The time of the latest `TimelineEndTime` among all its tracks.\r\n\r\n## Time Manipulation\r\n\r\nSoundFlow's editing engine offers sophisticated time manipulation capabilities for `AudioSegment`s:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Time manipulation options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"time-stretch\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:timelapse-outline' />\r\n                <span>Pitch-Preserved Time Stretching</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                This feature allows you to change the duration of an audio segment without affecting its pitch. It's ideal for:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>Fitting dialogue or music to a specific time slot.</li>\r\n                    <li>Creative sound design by drastically stretching or compressing audio.</li>\r\n                </ul>\r\n\r\n                It's controlled by two properties in `AudioSegmentSettings`:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>\r\n                        <strong><code>TimeStretchFactor</code> (float):</strong>\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li><code>1.0</code>: No stretching.</li>\r\n                            <li><code>&gt; 1.0</code>: Makes the segment longer (e.g., <code>2.0</code> doubles the duration).</li>\r\n                            <li><code>&lt; 1.0</code> and <code>&gt; 0.0</code>: Makes the segment shorter (e.g., <code>0.5</code> halves the duration).</li>\r\n                        </ul>\r\n                    </li>\r\n                    <li>\r\n                        <strong><code>TargetStretchDuration</code> (TimeSpan?):</strong>\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li>If set, this overrides `TimeStretchFactor`. The segment will be stretched or compressed to match this exact duration.</li>\r\n                            <li>Set to `null` to use `TimeStretchFactor` instead.</li>\r\n                        </ul>\r\n                    </li>\r\n                </ul>\r\n                <p className=\"mt-2\">Internally, SoundFlow uses a high-quality <strong>WSOLA (Waveform Similarity Overlap-Add)</strong> algorithm implemented in the <code>WsolaTimeStretcher</code> class.</p>\r\n                ```csharp\r\n                // Make a segment 50% shorter while preserving pitch\r\n                mySegment.Settings.TimeStretchFactor = 0.5f;\r\n\r\n                // Make a segment exactly 3.75 seconds long, preserving pitch\r\n                mySegment.Settings.TargetStretchDuration = TimeSpan.FromSeconds(3.75);\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"varispeed\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:speedometer' />\r\n                <span>Classic Speed Control (Varispeed)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The <code>SpeedFactor</code> property in `AudioSegmentSettings` provides traditional speed control, affecting both the tempo and the pitch of the audio, similar to changing the playback speed of a tape machine.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>\r\n                        <strong><code>SpeedFactor</code> (float):</strong>\r\n                        <ul className=\"list-disc pl-5 mt-1\">\r\n                            <li><code>1.0</code>: Normal speed and pitch.</li>\r\n                            <li><code>&gt; 1.0</code>: Faster playback, higher pitch.</li>\r\n                            <li><code>&lt; 1.0</code> and <code>&gt; 0.0</code>: Slower playback, lower pitch.</li>\r\n                        </ul>\r\n                    </li>\r\n                </ul>\r\n                <p className=\"mt-2\"><strong>Interaction:</strong> Time stretching is applied to the source audio <em>first</em>, and then the <code>SpeedFactor</code> is applied to the time-stretched result.</p>\r\n                ```csharp\r\n                // Play segment at double speed (and an octave higher)\r\n                mySegment.Settings.SpeedFactor = 2.0f;\r\n\r\n                // Play segment at half speed (and an octave lower)\r\n                mySegment.Settings.SpeedFactor = 0.5f;\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Project Persistence (`SoundFlow.Editing.Persistence`)\r\n\r\nThe `CompositionProjectManager` class provides static methods for saving and loading your `Composition` objects. Projects are saved in a JSON-based format with the `.sfproj` extension.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Project persistence options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"save\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:save-outline' />\r\n                <span>Saving a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                ### Saving a Project\r\n\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using System.Threading.Tasks;\r\n\r\n                public async Task SaveMyProject(AudioEngine engine, Composition composition, string filePath)\r\n                {\r\n                    await CompositionProjectManager.SaveProjectAsync(\r\n                        engine,                 // The engine is now required for consolidation\r\n                        composition,\r\n                        filePath,\r\n                        consolidateMedia: true,  // Recommended for portability\r\n                        embedSmallMedia: true   // Embeds small audio files directly\r\n                    );\r\n                    Console.WriteLine($\"Project saved to {filePath}\");\r\n                }\r\n                ```\r\n\r\n                **Saving Options:**\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-2\">\r\n                    <li>\r\n                        <strong><code>consolidateMedia</code> (bool):</strong> If <code>true</code> (default), SoundFlow will attempt to copy all unique external audio files referenced by segments into an <code>Assets</code> subfolder next to your <code>.sfproj</code> file. This makes the project self-contained and portable. In-memory <code>ISoundDataProvider</code>s (like <code>RawDataProvider</code> from generated audio) will also be saved as WAV files in the <code>Assets</code> folder if <code>consolidateMedia</code> is true. The project file will then store relative paths to these consolidated assets.\r\n                    </li>\r\n                    <li>\r\n                        <strong><code>embedSmallMedia</code> (bool):</strong> If <code>true</code> (default), audio sources smaller than a certain threshold (currently 1MB) will be embedded directly into the <code>.sfproj</code> file as Base64-encoded strings. This is useful for short sound effects or jingles, avoiding the need for separate files. Embedding takes precedence over consolidation for small files.\r\n                    </li>\r\n                </ul>\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"load\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:folder-open-outline' />\r\n                <span>Loading a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                ### Loading a Project\r\n\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using SoundFlow.Structs;\r\n                using System.Threading.Tasks;\r\n                using System.Linq;\r\n                using System.Collections.Generic; \r\n\r\n                public async Task<(Composition?, List<ProjectSourceReference>)> LoadMyProject(AudioEngine engine, string filePath)\r\n                {\r\n                    if (!File.Exists(filePath))\r\n                    {\r\n                        Console.WriteLine($\"Project file not found: {filePath}\");\r\n                        return (null, new List<ProjectSourceReference>());\r\n                    }\r\n\r\n                    // NOTE: The loaded composition will have its own format. For playback, you'll\r\n                    // need to initialize a device with that specific format. Here we'll just\r\n                    // use a default format for the load operation itself.\r\n                    var loadingFormat = AudioFormat.DvdHq;\r\n\r\n                    var (loadedComposition, unresolvedSources) = await CompositionProjectManager.LoadProjectAsync(engine, loadingFormat, filePath);\r\n\r\n                    if (unresolvedSources.Any())\r\n                    {\r\n                        Console.WriteLine(\"Warning: Some media sources could not be found:\");\r\n                        foreach (var missing in unresolvedSources)\r\n                        {\r\n                            Console.WriteLine($\" - Missing ID: {missing.Id}, Original Path: {missing.OriginalAbsolutePath ?? \"N/A\"}\");\r\n                            // Here you could trigger a UI for relinking\r\n                        }\r\n                    }\r\n\r\n                    Console.WriteLine($\"Project '{loadedComposition.Name}' loaded successfully!\");\r\n                    return (loadedComposition, unresolvedSources);\r\n                }\r\n                ```\r\n\r\n                When loading, `LoadProjectAsync` returns a tuple:\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>The loaded <code>Composition</code> object.</li>\r\n                    <li>A <code>List&lt;ProjectSourceReference&gt;</code> detailing any audio sources that could not be found (based on embedded data, consolidated paths, or original absolute paths).</li>\r\n                </ul>\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"relink\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='ph:link-bold' />\r\n                <span>Media Management & Relinking</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                ### Media Management & Relinking\r\n\r\n                SoundFlow's persistence system attempts to locate media in this order:\r\n                1.  **Embedded Data:** If the `ProjectSourceReference` indicates embedded data, it's decoded.\r\n                2.  **Consolidated Relative Path:** If not embedded, it looks for the file in the `Assets` folder relative to the project file.\r\n                3.  **Original Absolute Path:** If still not found, it tries the original absolute path stored during the save.\r\n\r\n                If a source is still missing, it's added to the `unresolvedSources` list. You can then use `CompositionProjectManager.RelinkMissingMediaAsync` to update the project with the new location of a missing file:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Editing;\r\n                using SoundFlow.Editing.Persistence;\r\n                using SoundFlow.Structs;\r\n                using System.Threading.Tasks;\r\n\r\n                public async Task AttemptRelink(AudioEngine engine, AudioFormat format, ProjectSourceReference missingSource, string newFilePath, string projectDirectory)\r\n                {\r\n                    bool success = CompositionProjectManager.RelinkMissingMediaAsync(\r\n                        engine,\r\n                        format,\r\n                        missingSource,\r\n                        newFilePath,\r\n                        projectDirectory\r\n                    );\r\n\r\n                    if (success)\r\n                    {\r\n                        Console.WriteLine($\"Successfully relinked '{missingSource.Id}' to '{newFilePath}'.\");\r\n                        // You might need to re-resolve or update segments in your loaded composition\r\n                        // that use this missingSourceReference. One way is to reload the project:\r\n                        // (var reloadedComposition, var newMissing) = await CompositionProjectManager.LoadProjectAsync(engine, format, projectFilePath);\r\n                        // Or, manually update ISoundDataProvider instances in affected AudioSegments.\r\n                    }\r\n                    else\r\n                    {\r\n                        Console.WriteLine($\"Failed to relink '{missingSource.Id}'. File at new path might be invalid or inaccessible.\");\r\n                    }\r\n                }\r\n                ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n<Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 mt-6\">\r\n    <CardHeader>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:lightbulb\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n            <h4 className=\"font-semibold text-lg\">Note on <code>ownsDataProvider</code></h4>\r\n        </div>\r\n    </CardHeader>\r\n    <CardBody className=\"pt-0\">\r\n        <ul className=\"list-disc pl-5 space-y-2 text-sm\">\r\n            <li>\r\n                When you create <code>AudioSegment</code>s manually for a new composition, you manage the lifecycle of their <code>ISoundDataProvider</code>s. If you pass <code>ownsDataProvider: true</code>, the segment will dispose of the provider when the segment itself (or its parent <code>Composition</code>) is disposed.\r\n            </li>\r\n            <li>\r\n                When a <code>Composition</code> is loaded from a project file, the <code>AudioSegment</code>s created during loading will typically have <code>ownsDataProvider: true</code> set for the <code>ISoundDataProvider</code>s that were resolved (from file, embedded, or consolidated assets), as the loading process instantiates these providers.\r\n            </li>\r\n        </ul>\r\n    </CardBody>\r\n</Card>\r\n\r\n## Dirty Flag (`IsDirty`)\r\n\r\n`Composition`, `Track`, and `AudioSegment` (via its `Settings`) have an `IsDirty` property.\r\n*   This flag is automatically set to `true` when any significant property that affects playback or persistence is changed.\r\n*   `CompositionProjectManager.SaveProjectAsync` calls `composition.ClearDirtyFlag()` internally upon successful save.\r\n*   You can use this flag to prompt users to save changes before closing an application, for example.\r\n\r\n## Basic Composition Example\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Components;\r\nusing SoundFlow.Enums;\r\nusing SoundFlow.Providers;\r\nusing SoundFlow.Editing; // New namespace\r\nusing System;\r\nusing System.IO;\r\nusing System.Threading.Tasks;\r\n\r\nnamespace BasicComposition;\r\n\r\ninternal static class Program\r\n{\r\n    private static async Task Main(string[] args)\r\n    {\r\n        // Initialize the audio engine.\r\n        // It's good practice to set format and channels to match your composition's target.\r\n        using var audioEngine = new MiniAudioEngine(48000, Capability.Playback, channels: 1);\r\n\r\n        // Create a composition\r\n        var composition = new Composition(\"My First Song\") { SampleRate = 48000, TargetChannels = 1 };\r\n\r\n        // Create a track\r\n        var track1 = new Track(\"Vocals\");\r\n        composition.AddTrack(track1);\r\n\r\n        // Load an audio file for a segment\r\n        // IMPORTANT: Replace with an actual path to a WAV file on your system\r\n        string audioPath = \"path/to/your/audio.wav\";\r\n        if (!File.Exists(audioPath))\r\n        {\r\n            Console.WriteLine($\"Audio file not found: {audioPath}\");\r\n            Console.WriteLine(\"Please update 'audioPath' in the example code to a valid WAV file on your system.\");\r\n            return;\r\n        }\r\n\r\n        // Create a StreamDataProvider from the audio file.\r\n        // The compositionFormat (or a compatible format) is needed for the provider.\r\n        var provider = new StreamDataProvider(audioEngine, composition.Format, File.OpenRead(audioPath));\r\n\r\n        // Create an audio segment\r\n        // Play from 0s of source, for 5s duration, place at 1s on timeline\r\n        var segment1 = new AudioSegment(\r\n            format: composition.Format, // Use the composition's format for the segment\r\n            sourceDataProvider: provider,\r\n            sourceStartTime: TimeSpan.Zero,\r\n            sourceDuration: TimeSpan.FromSeconds(5),\r\n            timelineStartTime: TimeSpan.FromSeconds(1),\r\n            name: \"Intro Vocal Clip\",\r\n            ownsDataProvider: true // The segment will dispose of the provider when the segment/composition is disposed\r\n        );\r\n\r\n        // Optionally, modify segment settings\r\n        segment1.Settings.Volume = 0.9f;\r\n        segment1.Settings.FadeInDuration = TimeSpan.FromMilliseconds(500);\r\n\r\n        track1.AddSegment(segment1);\r\n\r\n        // Create a SoundPlayer for the composition\r\n        // The composition itself acts as an ISoundDataProvider\r\n        var compositionPlayer = new SoundPlayer(audioEngine, composition.Format, composition);\r\n        audioEngine.MasterMixer.AddComponent(compositionPlayer); // Add player to the engine's master mixer\r\n        compositionPlayer.Play();\r\n\r\n        Console.WriteLine($\"Playing composition '{composition.Name}' for {composition.CalculateTotalDuration().TotalSeconds:F1}s... Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        compositionPlayer.Stop();\r\n        audioEngine.MasterMixer.RemoveComponent(compositionPlayer);\r\n\r\n        // Dispose the composition and engine to release resources\r\n        composition.Dispose();\r\n    }\r\n}\r\n```\r\n\r\n## Examples in Action\r\n\r\nThe `SoundFlow.Samples.EditingMixer` project in the SoundFlow GitHub repository provides extensive, runnable examples demonstrating:\r\n*   Building compositions with dialogue and generated audio.\r\n*   Using various `AudioSegmentSettings` like fades, loops, reverse, speed, and time stretching.\r\n*   Saving projects with different media handling strategies (consolidation, embedding).\r\n*   Loading projects and handling missing media by relinking.\r\n\r\nExploring this sample project is highly recommended to see these concepts applied in practical scenarios."
  },
  {
    "id": 3,
    "slug": "device-management",
    "version": "1.2.0",
    "title": "Device Management",
    "description": "Learn how to manage audio devices in SoundFlow, including initialization, switching, and advanced configuration.",
    "navOrder": 3,
    "category": "Core",
    "content": "---\r\nid: 3\r\ntitle: Device Management\r\ndescription: Learn how to manage audio devices in SoundFlow, including initialization, switching, and advanced configuration.\r\nnavOrder: 3\r\ncategory: Core\r\n---\r\n\r\nimport { Card, CardBody, Tabs, Tab, Chip } from \"@heroui/react\";\r\nimport { Icon } from \"@iconify/react\";\r\n\r\n# Device Management\r\n\r\nSoundFlow v1.2 introduced a powerful, device-centric architecture. This guide covers how to discover, initialize, and manage audio devices for playback, capture, and more complex scenarios.\r\n\r\n## An Overview\r\n\r\nThe most significant change in v1.2 is the separation of the **`AudioEngine`** from the **`AudioDevice`**.\r\n\r\n*   **`AudioEngine`**: Acts as a central **context** or **factory**. It is responsible for interacting with the low-level audio backend (e.g., MiniAudio), discovering available hardware, and creating device instances.\r\n*   **`AudioDevice`**: Represents an active audio I/O stream to a specific piece of hardware. Each device has its own lifecycle (`Start`, `Stop`, `Dispose`), its own audio format (`AudioFormat`), and, in the case of playback devices, its own independent audio graph (`MasterMixer`).\r\n\r\nThis model allows for robust, multi-device applications where you can, for example, play audio to a speaker and a headphone jack simultaneously, each with different effects.\r\n\r\n## Listing Available Devices\r\n\r\nBefore initializing a device, you need to know what hardware is available. The `AudioEngine` provides this information.\r\n\r\n<Card className=\"bg-primary-50/50 dark:bg-primary-500/10 border-1 border-primary-200/50 dark:border-primary-500/20 mt-4\">\r\n    <CardBody>\r\n        <div className=\"flex items-center gap-3\">\r\n            <Icon icon=\"lucide:info\" className=\"text-primary text-2xl flex-shrink-0\" />\r\n            <p className=\"text-sm\">\r\n                You must call `engine.UpdateDevicesInfo()` before accessing the device lists to ensure they are up-to-date.\r\n            </p>\r\n        </div>\r\n    </CardBody>\r\n</Card>\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Backends.MiniAudio;\r\n\r\n// 1. Initialize the engine context.\r\nusing var engine = new MiniAudioEngine();\r\n\r\n// 2. Refresh the list of available devices.\r\nengine.UpdateDevicesInfo();\r\n\r\n// 3. List available playback devices.\r\nConsole.WriteLine(\"--- Playback Devices ---\");\r\nif (engine.PlaybackDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No playback devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.PlaybackDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\r\n    }\r\n}\r\n\r\n\r\n// 4. List available capture devices.\r\nConsole.WriteLine(\"\\n--- Capture Devices ---\");\r\nif (engine.CaptureDevices.Length == 0)\r\n{\r\n    Console.WriteLine(\"No capture devices found.\");\r\n}\r\nelse\r\n{\r\n    foreach (var device in engine.CaptureDevices)\r\n    {\r\n        Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\r\n    }\r\n}\r\n```\r\n\r\n## Initializing an Audio Device\r\n\r\nOnce you have a `DeviceInfo` struct (or if you want to use the default device), you can ask the engine to initialize it.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Device initialization options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"playback\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='lucide:audio-lines' />\r\n                <span>Playback Device</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                An `AudioPlaybackDevice` is used for sending audio out to speakers or headphones.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>It has its own `MasterMixer`, which is the root of its audio graph.</li>\r\n                    <li>It must be started with `Start()` to begin processing audio.</li>\r\n                </ul>\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Abstracts.Devices;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Components;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n\r\n                    // Define the audio format for this device.\r\n                    var format = AudioFormat.DvdHq; // 48kHz, 2-channel, 32-bit float\r\n\r\n                    // To use the default device, pass `null` or `engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault)`\r\n                    using var playbackDevice = engine.InitializePlaybackDevice(null, format);\r\n                    Console.WriteLine($\"Initialized playback on: {playbackDevice.Info?.Name}\");\r\n\r\n                    // Create an oscillator component with the same format.\r\n                    var oscillator = new Oscillator(engine, format) { Frequency = 440f, Volume = 0.5f };\r\n\r\n                    // Add the oscillator to the device's MasterMixer.\r\n                    playbackDevice.MasterMixer.AddComponent(oscillator);\r\n\r\n                    // Start the device's audio stream.\r\n                    playbackDevice.Start();\r\n\r\n                    Console.WriteLine(\"Playing a 440 Hz tone. Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Stop the device.\r\n                    playbackDevice.Stop();\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"capture\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='lucide:mic' />\r\n                <span>Capture Device</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                An `AudioCaptureDevice` is used for receiving audio from a microphone or other input.\r\n                <ul className=\"list-disc pl-5 mt-2 space-y-1\">\r\n                    <li>Its primary interaction point is the `OnAudioProcessed` event.</li>\r\n                    <li>It must be started with `Start()` to begin capturing audio.</li>\r\n                </ul>\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Abstracts.Devices;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Enums;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n                    var format = new AudioFormat { SampleRate = 48000, Channels = 1, Format = SampleFormat.F32 };\r\n\r\n                    // To use the default device, pass `null` or `engine.CaptureDevices.FirstOrDefault(d => d.IsDefault)`\r\n                    using var captureDevice = engine.InitializeCaptureDevice(null, format);\r\n                    Console.WriteLine($\"Initialized capture on: {captureDevice.Info?.Name}\");\r\n\r\n                    // Subscribe to the audio data event.\r\n                    captureDevice.OnAudioProcessed += (samples, capability) =>\r\n                    {\r\n                        // This code block will execute on the audio thread every time a buffer is ready.\r\n                        // Be efficient here to avoid audio dropouts.\r\n                        Console.Write(\".\"); // Print a dot to show that data is flowing.\r\n                    };\r\n\r\n                    // Start capturing.\r\n                    captureDevice.Start();\r\n\r\n                    Console.WriteLine(\"Capturing audio... Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Stop capturing.\r\n                    captureDevice.Stop();\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## High-Level Device Wrappers\r\n\r\nSoundFlow provides convenience wrappers for common and complex scenarios.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Device wrapper options\" className=\"mt-4\">\r\n    <Tab\r\n        key=\"fullduplex\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='carbon:arrows-horizontal' />\r\n                <span>Full-Duplex (Simultaneous I/O)</span>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                The `FullDuplexDevice` simplifies scenarios like live effects processing or VoIP by managing a paired playback and capture device.\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Components;\r\n                    using SoundFlow.Providers;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n                    var format = AudioFormat.DvdHq;\r\n\r\n                    // Getting the default devices\r\n                    var playbackDevice = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\r\n                    var captureDevice = engine.CaptureDevices.FirstOrDefault(d => d.IsDefault);\r\n\r\n                    // Initialize a duplex device using the selected playback and capture hardware, or null for default.\r\n                    using var duplexDevice = engine.InitializeFullDuplexDevice(playbackDevice, captureDevice, format);\r\n\r\n                    Console.WriteLine($\"Passthrough active. Input: {duplexDevice.CaptureDevice.Info?.Name}, Output: {duplexDevice.PlaybackDevice.Info?.Name}\");\r\n\r\n                    // Create a provider that reads from the capture device's audio stream.\r\n                    using var micProvider = new MicrophoneDataProvider(duplexDevice); // MicrophoneDataProvider works with both Capture and Duplex devices\r\n\r\n                    // Create a player that plays the audio from the microphone provider.\r\n                    using var player = new SoundPlayer(engine, format, micProvider);\r\n\r\n                    // Add the player to the duplex device's playback mixer.\r\n                    duplexDevice.MasterMixer.AddComponent(player);\r\n\r\n                    // Start the duplex device, provider, and player.\r\n                    duplexDevice.Start();\r\n                    micProvider.StartCapture();\r\n                    player.Play();\r\n\r\n                    Console.WriteLine(\"Live microphone passthrough is active. Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Clean up\r\n                    duplexDevice.Stop();\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n    <Tab\r\n        key=\"loopback\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:record-rec' />\r\n                <span>Loopback Recording</span>\r\n                <Chip color=\"warning\" variant=\"flat\" size=\"sm\">Windows Only</Chip>\r\n            </div>\r\n        }\r\n    >\r\n        <Card flat className=\"bg-transparent\">\r\n            <CardBody>\r\n                Loopback allows you to capture the audio that your computer is currently playing. This is useful for recording game audio, system sounds, or audio from other applications.\r\n                <Card className=\"bg-warning-50/50 dark:bg-warning-500/10 border-1 border-warning-200/50 dark:border-warning-500/20 my-4\">\r\n                    <CardBody>\r\n                        <div className=\"flex items-center gap-3\">\r\n                            <Icon icon=\"lucide:alert-triangle\" className=\"text-warning text-2xl flex-shrink-0\" />\r\n                            <p className=\"text-sm\">\r\n                                Loopback recording is currently only supported on <strong>Windows</strong> via the WASAPI backend.\r\n                            </p>\r\n                        </div>\r\n                    </CardBody>\r\n                </Card>\r\n                    ```csharp\r\n                    using SoundFlow.Abstracts;\r\n                    using SoundFlow.Backends.MiniAudio;\r\n                    using SoundFlow.Components;\r\n                    using SoundFlow.Structs;\r\n\r\n                    using var engine = new MiniAudioEngine();\r\n                    var format = AudioFormat.DvdHq;\r\n\r\n                    // Initialize a loopback device. It internally finds the default playback\r\n                    // device and sets it up for capture.\r\n                    using var loopbackDevice = engine.InitializeLoopbackDevice(format);\r\n                    Console.WriteLine($\"Initialized loopback capture on: {loopbackDevice.Info?.Name}\");\r\n\r\n                    // Record the loopback audio to a file.\r\n                    using var fileStream = new FileStream(\"system_audio.wav\", FileMode.Create);\r\n                    using var recorder = new Recorder(loopbackDevice, fileStream);\r\n\r\n                    // Start capture and recording.\r\n                    loopbackDevice.Start();\r\n                    recorder.StartRecording();\r\n\r\n                    Console.WriteLine(\"Recording system audio... Play some sound! Press any key to stop.\");\r\n                    Console.ReadKey();\r\n\r\n                    // Stop and clean up.\r\n                    recorder.StopRecording();\r\n                    loopbackDevice.Stop();\r\n\r\n                    Console.WriteLine(\"Recording stopped. Saved to system_audio.wav\");\r\n                    ```\r\n            </CardBody>\r\n        </Card>\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Runtime Device Switching\r\n\r\nA powerful feature of SoundFlow is the ability to switch the underlying hardware device at runtime without interrupting the audio graph. The `Engine.SwitchDevice(...)` method handles this seamlessly.\r\n*   For **playback**, it moves all `SoundComponent`s from the old device's mixer to the new one.\r\n*   For **capture**, it moves all event subscribers from the old device's `OnAudioProcessed` event to the new one.\r\n*   It automatically disposes the old device instance.\r\n\r\n    ```csharp\r\n    // ... (Setup from the Playback Device example) ...\r\n    using var engine = new MiniAudioEngine();\r\n    var format = AudioFormat.DvdHq;\r\n\r\n    // 1. Initialize with the default device.\r\n    var playbackDevice = engine.InitializePlaybackDevice(null, format);\r\n    var oscillator = new Oscillator(engine, format) { Frequency = 440f, Volume = 0.5f };\r\n    playbackDevice.MasterMixer.AddComponent(oscillator);\r\n    playbackDevice.Start();\r\n    Console.WriteLine($\"Playing tone on: {playbackDevice.Info?.Name}\");\r\n\r\n    // 2. Loop to allow switching.\r\n    while(true)\r\n    {\r\n        Console.WriteLine(\"\\nPress 's' to switch device, or 'q' to quit.\");\r\n        if (Console.ReadKey(true).Key == ConsoleKey.Q) break;\r\n\r\n        // Prompt user to select a new device from the list.\r\n        engine.UpdateDevicesInfo();\r\n        Console.WriteLine(\"Select new playback device:\");\r\n        for (int i = 0; i < engine.PlaybackDevices.Length; i++)\r\n        Console.WriteLine($\"{i}: {engine.PlaybackDevices[i].Name}\");\r\n\r\n        if (int.TryParse(Console.ReadLine(), out var index) && index >= 0 && index < engine.PlaybackDevices.Length)\r\n    {\r\n        var newDeviceInfo = engine.PlaybackDevices[index];\r\n        Console.WriteLine($\"Switching playback to: {newDeviceInfo.Name}...\");\r\n\r\n        // 3. THE SWITCH:\r\n        // The old device is disposed, a new one is returned, and the oscillator is moved automatically.\r\n        playbackDevice = engine.SwitchDevice(playbackDevice, newDeviceInfo);\r\n\r\n        Console.WriteLine($\"Successfully switched. Now playing on: {playbackDevice.Info?.Name}\");\r\n    }\r\n    }\r\n\r\n    playbackDevice.Dispose();\r\n    ```\r\n\r\n## Advanced Device Configuration\r\n\r\nFor fine-grained control over latency and backend-specific features, you can use the `MiniAudioDeviceConfig` class when initializing a device. This allows you to tune parameters like buffer sizes, sharing modes, and platform-specific settings.\r\n\r\nThis is just a glimpse of the available options. For a full list of configurable parameters, please refer to the <strong>API Reference</strong> for <code>MiniAudioDeviceConfig</code> and its nested settings classes.\r\n\r\n```csharp\r\nusing SoundFlow.Abstracts;\r\nusing SoundFlow.Abstracts.Devices;\r\nusing SoundFlow.Backends.MiniAudio;\r\nusing SoundFlow.Backends.MiniAudio.Devices;\r\nusing SoundFlow.Backends.MiniAudio.Enums;\r\nusing SoundFlow.Structs;\r\n\r\nusing var engine = new MiniAudioEngine();\r\nvar format = AudioFormat.DvdHq;\r\n\r\n// Create a detailed configuration object.\r\nvar customConfig = new MiniAudioDeviceConfig\r\n{\r\n// Request a specific buffer size. 960 frames at 48kHz stereo = 10ms latency.\r\nPeriodSizeInFrames = 960,\r\n\r\n// Use shared mode for better compatibility with other applications.\r\n// For lowest latency, you could try ShareMode.Exclusive.\r\nPlayback = new DeviceSubConfig { ShareMode = ShareMode.Shared },\r\nCapture = new DeviceSubConfig { ShareMode = ShareMode.Shared },\r\n\r\n// Platform-specific settings. For Windows, use WASAPI in ProAudio mode.\r\nWasapi = new WasapiSettings{ Usage = WasapiUsage.ProAudio }\r\n};\r\n\r\n// Pass the config when initializing the device.\r\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format, customConfig);\r\n\r\nConsole.WriteLine($\"Initialized device '{playbackDevice.Info?.Name}' with custom configuration.\");\r\n// ... rest of your playback logic ...\r\n```"
  },
  {
    "id": 2,
    "slug": "core-concepts",
    "version": "1.2.0",
    "title": "Core Concepts",
    "description": "Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.",
    "navOrder": 2,
    "category": "Core",
    "content": "---\nid: 2\ntitle: Core Concepts\ndescription: Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.\nnavOrder: 2\ncategory: Core\n---\n\n# Core Concepts\n\nThis section explains the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline. Understanding these core concepts is essential for effectively using and extending the SoundFlow framework.\n\nThe v1.2.0 architecture is built around a clear separation of concerns:\n1.  The **`AudioEngine`** acts as a central context and device factory.\n2.  **`AudioDevice`** instances represent the actual hardware I/O streams.\n3.  **`SoundComponent`** instances form the audio processing graph for each device.\n\n## Audio Engine (`AudioEngine`)\n\nThe `AudioEngine` is the top-level object in SoundFlow. It's a central context responsible for:\n\n*   **Initializing and managing the audio backend:** SoundFlow supports multiple audio backends (e.g., `MiniAudio`), which handle the low-level interaction with the operating system's audio API. The `AudioEngine` abstracts away the backend details.\n*   **Discovering and Enumerating Audio Devices:** The engine can list all available playback and capture devices and allows switching between them during runtime.\n*   **Acting as a Device Factory:** The primary role of the engine is to initialize `AudioDevice` instances (e.g., `AudioPlaybackDevice`, `AudioCaptureDevice`) which you will use for I/O.\n*   **Managing Global State:** It handles global features like the soloing system (`SoloComponent`/`UnsoloComponent`).\n\n> **Key Change in v1.2:** The `AudioEngine` is no longer a device. You initialize it once without parameters like sample rate or channels. These are now defined per-device.\n\n**Key Properties:**\n\n*   `PlaybackDevices`, `CaptureDevices`: Lists of available audio devices. Must be refreshed with `UpdateDevicesInfo()`.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n\n**Key Methods:**\n\n*   `InitializePlaybackDevice(...)`: Creates and returns a new `AudioPlaybackDevice` for audio output.\n*   `InitializeCaptureDevice(...)`: Creates and returns a new `AudioCaptureDevice` for audio input.\n*   `InitializeFullDuplexDevice(...)`: A convenience method to create a paired playback and capture device for live monitoring or VoIP.\n*   `InitializeLoopbackDevice(...)`: Creates a capture device to record system audio output (Windows only).\n*   `SwitchDevice(...)`: Switches an active device to a new physical device while preserving its state.\n*   `CreateEncoder(...)`, `CreateDecoder(...)`: Creates instances of backend-specific audio encoders and decoders.\n*   `UpdateDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.\n*   `Dispose()`: Releases the engine and all associated devices.\n\n**Example:**\n\n```csharp\n// 1. Initialize the engine context. It doesn't start any devices.\nusing var engine = new MiniAudioEngine();\n\n// 2. List available playback devices.\nengine.UpdateDevicesInfo();\nConsole.WriteLine(\"Available Playback Devices:\");\nforeach(var device in engine.PlaybackDevices)\n{\n    Console.WriteLine($\"- {device.Name} {(device.IsDefault ? \"(Default)\" : \"\")}\");\n}\n```\n\n## Audio Devices (`AudioDevice`)\n\nNew in v1.2, the `AudioDevice` and its derivatives (`AudioPlaybackDevice`, `AudioCaptureDevice`) represent an active audio stream to a physical hardware device. Each device is an independent entity with its own audio format, lifecycle, and processing graph.\n\n### `AudioPlaybackDevice` (Output)\nAn `AudioPlaybackDevice` manages an audio output stream.\n\n*   **Owns a `MasterMixer`:** Each playback device has its own `MasterMixer` property. This is the root of the audio graph for that specific device. All components you want to hear on this device must be added to its mixer.\n*   **Independent `AudioFormat`:** Can be initialized with a specific sample rate, channel count, and sample format.\n*   **Lifecycle:** Must be explicitly started with `Start()` and stopped with `Stop()`. It is `IDisposable` and should be managed with a `using` statement.\n\n### `AudioCaptureDevice` (Input)\nAn `AudioCaptureDevice` manages an audio input stream.\n\n*   **`OnAudioProcessed` Event:** This event is raised whenever a new buffer of audio is captured from the hardware. You can subscribe to this event to process live microphone data.\n*   **Lifecycle:** Also has its own `Start()`, `Stop()`, and `Dispose()` methods.\n\n> For a deep dive into creating, managing, and switching devices, see the **[Device Management](./device-management)** documentation.\n\n## Sound Components (`SoundComponent`)\n\n`SoundComponent` remains the abstract base class for all audio processing units in SoundFlow (oscillators, players, filters, mixers). Each component represents a node in a directed acyclic graph (DAG), known as the **audio graph**.\n\n**Key Changes in v1.2:**\n\n*   **Explicit Context:** The constructor now requires an `AudioEngine` and an `AudioFormat`. This makes the component aware of its operating context without relying on a global static instance.\n*   **Graph per Device:** Audio graphs are now built *per device*. A component is typically part of a single device's graph via its `MasterMixer`.\n\n**Key Features:**\n\n*   **Modular Processing:** Components encapsulate specific audio processing logic, making the system modular and extensible.\n*   **Input and Output Connections:** Components can have zero or more input and output connections, allowing data to flow between them.\n*   **`GenerateAudio(Span<float> buffer, int channels)`:** The core processing method that derived classes must implement.\n    *   **Generate new audio samples:** For source components like oscillators or file players.\n    *   **Modify existing audio samples:** For effects, filters, or analyzers.\n*   **Properties:**\n    *   `Name`: A descriptive name for the component.\n    *   `Volume`: Controls the output gain.\n    *   `Pan`: Controls the stereo panning (0.0 for full left, 0.5 for center, 1.0 for full right).\n    *   `Enabled`: Enables or disables the component's processing.\n    *   `Solo`: Isolates the component for debugging.\n    *   `Mute`: Silences the component's output.\n    *   `Parent`: The `Mixer` to which this component belongs (if any).\n*   **Methods:**\n    *   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n    *   `DisconnectInput(SoundComponent input)`: Disconnects an input connection.\n    *   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an `AudioAnalyzer` to this component.\n    *   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an `AudioAnalyzer` from this component.\n    *   `AddModifier(SoundModifier modifier)`: Adds a `SoundModifier` to this component.\n    *   `RemoveModifier(SoundModifier modifier)`: Removes a `SoundModifier` from this component.\n\n**Example:**\n\n```csharp\n// A SineWaveGenerator component aware of its format.\npublic class SineWaveGenerator : SoundComponent\n{\n    public float Frequency { get; set; } = 440f;\n    private float _phase;\n\n    // The constructor now takes the engine and format context.\n    public SineWaveGenerator(AudioEngine engine, AudioFormat format) : base(engine, format) { }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels)\n    {\n        // Now uses the component's own Format property\n        var sampleRate = this.Format.SampleRate; \n        for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] = MathF.Sin(_phase);\n            _phase += 2 * MathF.PI * Frequency / sampleRate;\n            if (_phase > 2 * MathF.PI) _phase -= 2 * MathF.PI;\n        }\n    }\n}\n```\n\n## Mixer (`Mixer`)\n\nThe `Mixer` is a specialized `SoundComponent` that combines the output of multiple `SoundComponent` instances into a single audio stream.\n\n> **Breaking Change in v1.2:** The static `Mixer.Master` property has been **removed**. This is the most significant breaking change for users of previous versions.\n\n**Key Features:**\n\n*   **Device-Specific `MasterMixer`:** Each `AudioPlaybackDevice` now exposes its own `MasterMixer` property. This is the root mixer for that device. All audio that you wish to play on a device must ultimately be routed to its `MasterMixer`.\n*   **Creating Sub-Mixers:** You can still create your own `Mixer` instances (`new Mixer(engine, format)`) to group components before connecting them to a master mixer.\n*   **Adding and Removing Components:**\n    *   `AddComponent(SoundComponent component)`: Adds a component to the mixer's inputs.\n    *   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n\t\n**Example:**\n\n```csharp\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format);\n\n// Create a SoundPlayer and an Oscillator, providing the engine and format\nusing var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(engine, format, dataProvider);\nvar oscillator = new Oscillator(engine, format) { Frequency = 220, Type = Oscillator.WaveformType.Square };\n\n// Add both components to the Device'S MasterMixer\nplaybackDevice.MasterMixer.AddComponent(player);\nplaybackDevice.MasterMixer.AddComponent(oscillator);\n\n// Start the device to enable its audio stream\nplaybackDevice.Start();\nplayer.Play();\noscillator.Play();\n// ...\n```\n\n## Sound Modifiers (`SoundModifier`)\n\n`SoundModifier` is an abstract base class for creating audio effects that modify the audio stream. Modifiers are applied to `SoundComponent` instances or to `AudioSegment`, `Track`, `Composition` and process the audio data.\n\n**Key Features:**\n\n*   **`ProcessSample(float sample, int channel)`:** This is the core method that derived classes can implement to process audio on a sample-by-sample basis.\n*   **`Process(Span<float> buffer, int channels)`:** This method can be overridden for buffer-based processing, which is often more efficient for complex effects. By default, it calls `ProcessSample` for each sample.\n*   **`Enabled` Property:** Allows dynamically enabling or disabling the modifier's effect.\n*   **Chaining:** Modifiers can be chained together on `SoundComponent` instances (or `AudioSegment`, `Track`, `Composition`) to create complex effect pipelines.\n\n**Built-in Modifiers:**\n\nSoundFlow provides a variety of built-in modifiers, including:\n*   Algorithmic Reverb Modifier: Simulates reverberation.\n*   Ambient Reverb Modifier: Creates a sense of spaciousness.\n*   Bass Boost Modifier: Enhances low frequencies.\n*   Chorus Modifier: Creates a chorus effect.\n*   Compressor Modifier: Reduces dynamic range.\n*   Delay Modifier: Applies a delay effect.\n*   Frequency Band Modifier: Boosts or cuts frequency bands.\n*   Noise Reduction Modifier: Reduces noise.\n*   Parametric Equalizer: Provides precise EQ control.\n*   Stereo Chorus Modifier: Creates a stereo chorus.\n*   Treble Boost Modifier: Enhances high frequencies.\n*   And potentially external modifiers like `WebRtcApmModifier` via extensions.\n\n\n**Example:**\n\n```csharp\n// Create engine, format, and device\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\nusing var playbackDevice = engine.InitializePlaybackDevice(null, format);\n\n// Create a SoundPlayer\nusing var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(engine, format, dataProvider);\n\n// Creating a modifier now requires the format context\nvar reverb = new AlgorithmicReverbModifier(format) { RoomSize = 0.8f, Wet = 0.2f };\n\n// Add the reverb modifier to the player\nplayer.AddModifier(reverb);\n\n// Add the player to the device's MasterMixer\nplaybackDevice.MasterMixer.AddComponent(player);\n// ...\n```\n\n## Sound Players (`SoundPlayerBase`, `SoundPlayer`, `SurroundPlayer`)\n\nThese classes provide the logic for playing audio from a data source. Their initialization now requires the `engine` and `format` context.\n\n*   **`SoundPlayerBase`:** The abstract base class that provides common functionality for all sound playback components. It implements `ISoundPlayer` and handles:\n    *   Core playback logic: reading from an `ISoundDataProvider`, managing playback state (Play, Pause, Stop).\n    *   Playback speed adjustment via the `PlaybackSpeed` property.\n    *   Looping with `IsLooping`, `LoopStartSamples`/`Seconds`, and `LoopEndSamples`/`Seconds`.\n    *   Seeking capabilities via `Seek` methods (accepting `TimeSpan`, `float` seconds, or `int` sample offset).\n    *   Volume control (inherited from `SoundComponent`).\n    *   A `PlaybackEnded` event.\n\n*   **`SoundPlayer`:** The standard concrete implementation of `SoundPlayerBase` for typical mono or stereo audio playback.\n\n*   **`SurroundPlayer`:**\n    *   All features from `SoundPlayerBase` and `ISoundPlayer`.\n    *   `SpeakerConfiguration`: Allows you to define the speaker setup (e.g., Stereo, Quad, 5.1, 7.1, or a custom configuration).\n    *   `PanningMethod`: Selects the panning algorithm to use (Linear, EqualPower, or VBAP).\n    *   `ListenerPosition`: Sets the listener's position relative to the speakers.\n    *   `VbapParameters`: Provides fine-grained control over VBAP (Vector Base Amplitude Panning) settings.\n\n\n## Audio Recording (`Recorder`)\n\nThe `Recorder` captures audio from an `AudioCaptureDevice`, directing it to a `Stream` for file storage or a `ProcessCallback` for real-time processing. It integrates `SoundModifier` and `AudioAnalyzer` components for on-the-fly audio manipulation and analysis, and implements `IDisposable` for resource management.\n\n**Key Features:**\n*   **Dual Recording Modes:** Records to an output `Stream` (e.g., file) or processes raw samples via a `ProcessCallback`.\n*   **`StartRecording()`**: Begins audio capture.\n*   **`PauseRecording()`**: Pauses recording; no data processed during this state.\n*   **`ResumeRecording()`**: Resumes a paused recording.\n*   **`StopRecording()`**: Stops recording, finalizes output, and releases resources.\n*   **`State`**: Current recording state (`Playing`, `Paused`, `Stopped`).\n*   **`SampleFormat`**: Sample format for raw audio (e.g., `Float32`), inherited from the capture device.\n*   **`EncodingFormat`**: Encoding format for stream output (e.g., `Wav`). Limited backend support.\n*   **`SampleRate`**: Audio sample rate (e.g., 44100 Hz), inherited from the capture device.\n*   **`Channels`**: Number of audio channels (e.g., 1 for mono, 2 for stereo), inherited from the capture device.\n*   **`Stream`**: Output `Stream` for encoded audio when recording to file. `Stream.Null` if using `ProcessCallback`.\n*   **`ProcessCallback`**: `AudioProcessCallback` delegate invoked with raw audio samples for real-time custom processing. `null` if using `Stream` output.\n*   **`Modifiers`**: Read-only collection of `SoundModifier` components.\n    *   **`AddModifier()`**: Adds a `SoundModifier` to the pipeline.\n    *   **`RemoveModifier()`**: Removes a `SoundModifier` from the pipeline.\n*   **`Analyzers`**: Read-only collection of `AudioAnalyzer` components.\n    *   **`AddAnalyzer()`**: Adds an `AudioAnalyzer` to the pipeline.\n    *   **`RemoveAnalyzer()`**: Removes an `AudioAnalyzer` from the pipeline.\n*   **Resource Management**: Implements `IDisposable` for proper resource cleanup.\n\n\n## Audio Providers (`ISoundDataProvider`)\n\n`ISoundDataProvider` is an interface that defines a standard way to access audio data, regardless of its source. This interface is stable, but its implementations now require an `engine` and `format` context in their constructors to correctly create internal decoders.\n\n**Key Features:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Can be `-1` for live streams.\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio.\n*   `ReadBytes(Span<float> buffer)`: Reads a chunk of audio data into the provided buffer.\n*   `Seek(int offset)`: Moves the read position to a specific offset (in samples).\n*   `EndOfStreamReached`: An event that is raised when the end of the audio data is reached.\n*   `PositionChanged`: An event that is raised when the read position changes.\n*   `Dispose()`: Implementations should release underlying resources (e.g., file streams).\n\n**Built-in Providers:**\n\n*   `AssetDataProvider`: Loads audio data from a byte array or `Stream` entirely into memory.\n*   `StreamDataProvider`: Reads audio data from a `Stream`, decoding on the fly.\n*   `MicrophoneDataProvider`: Provides a live stream from an `AudioCaptureDevice`.\n*   `ChunkedDataProvider`: Efficiently reads large files or streams in chunks.\n*   `NetworkDataProvider`: Reads audio data from a network source (URL, HLS).\n*   `QueueDataProvider`: A thread-safe queue for scenarios where one part of your application generates audio and another part consumes it.\n*   `RawDataProvider`: Reads audio data from a raw PCM stream or various raw array types (`float[]`, `byte[]`, `int[]`, `short[]`).\n\nIt's good practice to dispose of `ISoundDataProvider` instances when they are no longer needed, for example, using a `using` statement.\n\n```csharp\nusing var engine = new MiniAudioEngine();\nvar format = AudioFormat.DvdHq;\n\n// The provider needs the engine and format to create an internal decoder.\nusing var dataProvider = new StreamDataProvider(engine, format, File.OpenRead(\"audio.wav\"));\n// Use dataProvider\n```\n\n## Audio Encoding/Decoding (`ISoundEncoder`, `ISoundDecoder`)\n\n`ISoundEncoder` and `ISoundDecoder` are interfaces for encoding and decoding audio data to and from different formats. Both are `IDisposable`.\n\n*   **`ISoundEncoder`:** Encodes raw audio samples into a specific format (e.g., WAV, FLAC, MP3). Currently only WAV supported by miniaudio backend.\n*   **`ISoundDecoder`:** Decodes audio data from a specific format into raw audio samples.\n\n**`MiniAudio` Backend:**\n\nThe `MiniAudio` backend provides implementations of these interfaces using the `miniaudio` library:\n\n*   `MiniAudioEncoder`\n*   `MiniAudioDecoder`\n\n\n## Audio Analysis (`AudioAnalyzer`)\n\n`AudioAnalyzer` is an abstract base class for creating components that analyze audio data. Analyzers typically extract information from the audio stream without modifying it.\n\n**Key Features:**\n\n*   **Constructor:** Initialized with an `AudioFormat` and an optional `IVisualizer` to send data to.\n*   `Analyze(Span<float> buffer, int channels)`: An abstract method that derived classes must implement to perform their specific analysis.\n*   `Enabled`: If false, the `Analyze` step might be skipped by the `SoundComponent` it's attached to.\n*   **Integration with Visualizers:** Analyzers are often used in conjunction with `IVisualizer` implementations to display the analysis results visually.\n\n**Built-in Analyzers:**\n\n*   `LevelMeterAnalyzer`: Measures the RMS (root-mean-square) and peak levels of an audio signal.\n*   `SpectrumAnalyzer`: Computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).\n*   `VoiceActivityDetector`: Detects the presence of human voice in an audio stream.\n\n\n## Audio Visualization (`IVisualizer`)\n\n`IVisualizer` is an interface for creating components that visualize audio data. Visualizers typically don't modify the audio stream but instead render a graphical representation of the data. It implements `IDisposable`.\n\n**Key Features:**\n\n*   `Name`: A descriptive name for the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: This method is called by the audio engine to provide the visualizer with a chunk of audio data to process.\n*   `Render(IVisualizationContext context)`: This method is called to render the visualization. It receives an `IVisualizationContext` instance, which provides drawing methods.\n*   `VisualizationUpdated`: An event that is raised when the visualization needs to be redrawn (e.g., when new audio data has been processed).\n*   `Dispose()`: Releases resources held by the visualizer.\n\n## Visualization Context (`IVisualizationContext`):\n\nThis interface provides a set of drawing methods for rendering the visualization. The specific implementation of `IVisualizationContext` will depend on the UI framework you are using.\n\n**Built-in Visualizers:**\n\n*   `LevelMeterVisualizer`: Displays a level meter that shows the current RMS or peak level of the audio.\n*   `SpectrumVisualizer`: Renders a bar graph representing the frequency spectrum of the audio.\n*   `WaveformVisualizer`: Draws the waveform of the audio signal.\n\n## Editing Engine & Persistence (`SoundFlow.Editing`, `SoundFlow.Editing.Persistence`)\n\nSoundFlow features a powerful non-destructive audio editing engine. For a detailed guide, please see the [Editing Engine & Persistence](./editing-engine) documentation.\n\n**Key Concepts:**\n\n*   **`Composition`**: The main container for an audio project, holding multiple `Track`s. It can be treated as an `ISoundDataProvider` and can be rendered or played back.\n*   **`Track`**: Represents a single audio track within a `Composition`. Contains `AudioSegment`s and has its own settings (volume, pan, mute, solo, effects).\n*   **`AudioSegment`**: A clip of audio placed on a `Track`'s timeline. It references a portion of an `ISoundDataProvider` and has its own extensive settings.\n    *   **`AudioSegmentSettings`**: Controls volume, pan, fades (with `FadeCurveType`), looping (`LoopSettings`), reverse playback, speed, and **pitch-preserved time stretching** (via `TimeStretchFactor` or `TargetStretchDuration`, powered by `WsolaTimeStretcher`).\n    *   Supports segment-level modifiers and analyzers.\n*   **Non-Destructive:** Edits do not alter the original audio source files. All operations are applied at runtime during playback or rendering.\n*   **Project Persistence (`CompositionProjectManager`)**:\n    *   Save and load entire compositions as `.sfproj` files.\n    *   **Media Consolidation**: Option to copy all external audio files into an `Assets` folder within the project.\n    *   **Embed Small Media**: Option to embed small audio files (e.g., SFX) directly into the project file.\n    *   **Relink Missing Media**: If an audio file is moved, the project can be relinked to its new location.\n\nThis new engine allows for programmatic creation and manipulation of complex audio timelines, effects processing at multiple levels (segment, track, master), and robust project management."
  },
  {
    "id": 5,
    "slug": "api-reference",
    "version": "1.2.0",
    "title": "API Reference",
    "description": "A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.",
    "navOrder": 5,
    "category": "Core",
    "content": "---\nid: 5\ntitle: API Reference\ndescription: A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.\nnavOrder: 5\ncategory: Core\n---\n\n# API Reference\n\nThis section provides a detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members. It serves as a comprehensive reference for developers working with the SoundFlow library.\n\n## Namespaces\n\nSoundFlow is organized into the following namespaces:\n\n*   **`SoundFlow.Abstracts`:** Contains abstract classes and interfaces that define the core framework of SoundFlow. This includes base classes for the audio engine, audio devices, sound components, modifiers, and analyzers.\n*   **`SoundFlow.Backends`:** Provides backend-specific implementations for audio input/output. The primary backend currently supported is `SoundFlow.Backends.MiniAudio`, which uses the `miniaudio` library.\n*   **`SoundFlow.Components`:** Contains concrete `SoundComponent` classes that provide various audio processing functionalities, including playback, mixing, synthesis, and analysis. It also includes standalone components like the `Recorder`.\n*   **`SoundFlow.Editing`:** Contains classes for non-destructive audio editing, including `Composition`, `Track`, `AudioSegment`, `AudioSegmentSettings`, `LoopSettings`, and `FadeCurveType`.\n*   **`SoundFlow.Editing.Persistence`:** Contains classes for saving and loading audio compositions, such as `CompositionProjectManager` and various project data DTOs.\n*   **`SoundFlow.Enums`:** Contains enumerations used throughout the SoundFlow library to represent different states, options, and capabilities.\n*   **`SoundFlow.Exceptions`:** Contains custom exception classes used for error handling within SoundFlow.\n*   **`SoundFlow.Interfaces`:** Contains interfaces that define contracts for various functionalities, such as audio data providers, encoders, decoders, and visualizers.\n*   **`SoundFlow.Modifiers`:** Contains concrete `SoundModifier` classes that implement various audio effects.\n*   **`SoundFlow.Providers`:** Contains classes that implement the `ISoundDataProvider` interface, providing ways to load audio data from different sources.\n*   **`SoundFlow.Structs`:** Contains custom struct types used within SoundFlow, often for interop or specific data representation.\n*   **`SoundFlow.Utils`:** Contains utility classes and extension methods that provide helpful functionalities for working with audio data and performing common operations.\n*   **`SoundFlow.Visualization`:** Contains classes related to audio visualization, including analyzers and visualizers.\n*   **`SoundFlow.Extensions`:** Namespace for official extensions.\n    *   **`SoundFlow.Extensions.WebRtc.Apm`:** Provides integration with the WebRTC Audio Processing Module for features like echo cancellation, noise suppression, and automatic gain control.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Components`:** Contains components utilizing the WebRTC APM, like `NoiseSuppressor`.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Modifiers`:** Contains modifiers utilizing the WebRTC APM, like `WebRtcApmModifier`.\n\n## Key Classes and Interfaces\n\nBelow is a summary of the key classes and interfaces in SoundFlow.\n\n\n### Abstracts\n\n| Class/Interface                                         | Description                                                                                                                                                                           |\n|---------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`AudioAnalyzer`](#abstracts-audioanalyzer)             | Abstract base class for audio analysis components. Inherits from `SoundComponent`.                                                                                                    |\n| [`AudioEngine`](#abstracts-audioengine)                 | Abstract base class for an audio engine. Manages audio device lifecycle, provides encoding/decoding, and is the root context.                                                         |\n| [`AudioDevice`](#abstracts-audiodevice)                 | Abstract base class for an initialized audio device (playback or capture).                                                                                                            |\n| [`AudioPlaybackDevice`](#abstracts-audioplaybackdevice) | Abstract class representing an initialized output/playback device. Contains a `MasterMixer`.                                                                                          |\n| [`AudioCaptureDevice`](#abstracts-audiocapturedevice)   | Abstract class representing an initialized input/capture device. Exposes an `OnAudioProcessed` event.                                                                                 |\n| [`FullDuplexDevice`](#abstracts-fullduplexdevice)       | A high-level abstraction managing a paired playback and capture device for simultaneous I/O.                                                                                          |\n| [`DeviceConfig`](#abstracts-deviceconfig)               | Abstract base class for backend-specific device configuration objects.                                                                                                                |\n| [`SoundComponent`](#abstracts-soundcomponent)           | Abstract base class for all audio processing units in SoundFlow. Represents a node in the audio graph.                                                                                |\n| [`SoundModifier`](#abstracts-soundmodifier)             | Abstract base class for audio effects that modify audio samples.                                                                                                                      |\n| [`SoundPlayerBase`](#abstracts-soundplayerbase)         | Abstract base class providing common functionality for sound playback components. Inherits from `SoundComponent` and implements `ISoundPlayer`. Includes support for time-stretching. |\n\n### Backends.MiniAudio\n\n| Class/Interface                                                     | Description                                                                                                                  |\n|---------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n| [`MiniAudioEngine`](#backendsminiaudio-miniaudioengine)             | `AudioEngine` implementation that uses the `miniaudio` library for audio I/O.                                                |\n| [`MiniAudioDeviceConfig`](#backendsminiaudio-miniaudiodeviceconfig) | `DeviceConfig` implementation for MiniAudio, providing detailed, backend-specific settings for WASAPI, CoreAudio, ALSA, etc. |\n| [`MiniAudioDecoder`](#backendsminiaudio-miniaudiodecoder)           | `ISoundDecoder` implementation using the `miniaudio` library.                                                                |\n| [`MiniAudioEncoder`](#backendsminiaudio-miniaudiodecoder)           | `ISoundEncoder` implementation using the `miniaudio` library.                                                                |\n\n### Components\n\n| Class/Interface                                                | Description                                                                                                                                            |\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`EnvelopeGenerator`](#components-envelopegenerator)           | `SoundComponent` that generates an ADSR (Attack, Decay, Sustain, Release) envelope signal.                                                             |\n| [`Filter`](#components-filter)                                 | `SoundComponent` that applies a digital filter (low-pass, high-pass, band-pass, notch) to the audio signal.                                            |\n| [`LowFrequencyOscillator`](#components-lowfrequencyoscillator) | `SoundComponent` that generates a low-frequency oscillator (LFO) signal with various waveforms.                                                        |\n| [`Mixer`](#components-mixer)                                   | `SoundComponent` that mixes multiple audio streams together. The `Mixer.Master` property provides access to the default root mixer.                    |\n| [`Oscillator`](#components-oscillator)                         | `SoundComponent` that generates various waveforms (sine, square, sawtooth, triangle, noise, pulse).                                                    |\n| [`Recorder`](#components-recorder)                             | `SoundComponent` that captures audio input from a recording device and allows saving it to a stream or processing it via a callback.                     |\n| [`SoundPlayer`](#components-soundplayer)                       | `SoundPlayerBase` implementation that plays audio from an `ISoundDataProvider`.                                                                        |\n| [`SurroundPlayer`](#components-surroundplayer)                 | `SoundPlayerBase` implementation that extends `SoundPlayer` to support surround sound configurations with customizable speaker positions, delays, and panning methods. |\n| [`VoiceActivityDetector`](#components-voiceactivitydetector)   | An `AudioAnalyzer` that detects human voice in an audio stream, featuring configurable activation and hangover times to prevent rapid state changes.   |\n| [`WsolaTimeStretcher`](#components-wsolatimestretcher)\t | Implements WSOLA algorithm for real-time, pitch-preserved time stretching. Used internally by `AudioSegment`. |\n\n### Editing\n\n| Class/Interface                                       | Description                                                                                                                                            |\n| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`Composition`](#editing-composition)                 | Top-level container for audio tracks, representing a complete project. Implements `ISoundDataProvider` for rendering. `IDisposable`.                 |\n| [`Track`](#editing-track)                             | Represents a single audio track within a `Composition`, containing `AudioSegment`s and track-level settings.                                         |\n| [`AudioSegment`](#editing-audiosegment)               | Represents a single audio clip on a `Track`'s timeline, referencing a portion of an audio source and applying playback settings. `IDisposable`.      |\n| [`AudioSegmentSettings`](#editing-audiosegmentsettings) | Configurable settings for an `AudioSegment` (volume, pan, fades, loop, reverse, speed, time stretch, modifiers, analyzers).                          |\n| [`TrackSettings`](#editing-tracksettings)             | Configurable settings for a `Track` (volume, pan, mute, solo, enabled, modifiers, analyzers).                                                          |\n| [`LoopSettings`](#editing-loopsettings)               | (struct) Defines looping behavior for an `AudioSegment` (repetitions, target duration).                                                                |\n| [`FadeCurveType`](#editing-fadecurvetype)             | (enum) Defines curve types for fade effects (Linear, Logarithmic, SCurve).                                                                             |\n\n### Editing.Persistence\n\n| Class/Interface                                                        | Description                                                                                                                            |\n| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| [`CompositionProjectManager`](#editingpersistence-compositionprojectmanager) | Static class for saving and loading `Composition` projects to/from `.sfproj` files. Handles media consolidation and relinking.       |\n| [`ProjectData`](#editingpersistence-projectdata)                         | DTO representing the root of a saved project file.                                                                                     |\n| [`ProjectTrack`](#editingpersistence-projecttrack)                       | DTO for a `Track` within a saved project.                                                                                              |\n| [`ProjectSegment`](#editingpersistence-projectsegment)                   | DTO for an `AudioSegment` within a saved project.                                                                                      |\n| [`ProjectAudioSegmentSettings`](#editingpersistence-projectaudiosegmentsettings) | DTO for `AudioSegmentSettings` within a saved project.                                                                                 |\n| [`ProjectTrackSettings`](#editingpersistence-projecttracksettings)       | DTO for `TrackSettings` within a saved project.                                                                                        |\n| [`ProjectSourceReference`](#editingpersistence-projectsourcereference)   | DTO representing how an audio source is referenced in a project (file path, embedded data, consolidation).                             |\n| [`ProjectEffectData`](#editingpersistence-projecteffectdata)             | DTO for serializing `SoundModifier` or `AudioAnalyzer` instances (type name, parameters).                                              |\n\n### Enums\n\n| Enum                                             | Description                                                                                                                                 |\n| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Capability`](#enums-capability)                | Specifies the capabilities of an audio device (Playback, Record, Mixed, Loopback).                                                          |\n| [`DeviceType`](#enums-devicetype)                | Specifies the type of audio device (Playback, Capture).                                                                                     |\n| [`EncodingFormat`](#enums-encodingformat)        | Specifies the audio encoding format to use (e.g., WAV, FLAC, MP3, Vorbis). *Note: MiniAudio backend currently only supports WAV for encoding.* |\n| [`PlaybackState`](#enums-playbackstate)          | Specifies the current playback state of a player or recorder (Stopped, Playing, Paused).                                                      |\n| [`Result`](#enums-result)                        | Represents the result of a native operation, including success and various error codes.                                                     |\n| [`SampleFormat`](#enums-sampleformat)            | Specifies the format of audio samples (e.g., U8, S16, S24, S32, F32).                                                                     |\n| [`FilterType`](#enums-filtertype)                | Specifies the type of filter to use in the `Filter` and `ParametricEqualizer` components (Peaking, LowShelf, HighShelf, BandPass, Notch, LowPass, HighPass) |\n| [`EnvelopeGenerator.EnvelopeState`](#enums-envelopegenerator-envelopestate) | Specifies the current state of the envelope generator (Idle, Attack, Decay, Sustain, Release) |\n| [`EnvelopeGenerator.TriggerMode`](#enums-envelopegenerator-triggermode) | Specifies how the envelope generator is triggered (NoteOn, Gate, Trigger) |\n| [`LowFrequencyOscillator.WaveformType`](#enums-lowfrequencyoscillator-waveformtype) | Specifies the waveform type for the low-frequency oscillator (Sine, Square, Triangle, Sawtooth, ReverseSawtooth, Random, SampleAndHold) |\n| [`LowFrequencyOscillator.TriggerMode`](#enums-lowfrequencyoscillator-triggermode) | Specifies how the LFO is triggered (FreeRunning, NoteTrigger) |\n| [`Oscillator.WaveformType`](#enums-oscillator-waveformtype) | Specifies the waveform type for the oscillator (Sine, Square, Sawtooth, Triangle, Noise, Pulse) |\n| [`SurroundPlayer.SpeakerConfiguration`](#enums-surroundplayer-speakerconfiguration) | Specifies the speaker configuration for the surround player (Stereo, Quad, Surround51, Surround71, Custom) |\n| [`SurroundPlayer.PanningMethod`](#enums-surroundplayer-panningmethod) | Specifies the panning method for the surround player (Linear, EqualPower, Vbap) |\n| [`FadeCurveType`](#editing-fadecurvetype)\t   | Specifies curve types for fade effects (Linear, Logarithmic, SCurve). |\n| **`SoundFlow.Extensions.WebRtc.Apm` Enums**          |                                                                                                                                             |\n| [`ApmError`](#extensions-webrtc-apm-apmerror)    | Error codes returned by the WebRTC Audio Processing Module.                                                                                |\n| [`NoiseSuppressionLevel`](#extensions-webrtc-apm-noisesuppressionlevel) | Specifies noise suppression levels (Low, Moderate, High, VeryHigh).                                                                  |\n| [`GainControlMode`](#extensions-webrtc-apm-gaincontrolmode) | Specifies gain controller modes (AdaptiveAnalog, AdaptiveDigital, FixedDigital).                                                        |\n| [`DownmixMethod`](#extensions-webrtc-apm-downmixmethod) | Specifies methods for downmixing audio channels (AverageChannels, UseFirstChannel).                                                    |\n| [`RuntimeSettingType`](#extensions-webrtc-apm-runtimesettingtype) | Specifies types of runtime settings for the WebRTC APM.                                                                        |\n\n### Exceptions\n\n| Class                                           | Description                                                                                   |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [`BackendException`](#exceptions-backendexception) | Thrown when an error occurs in a specific audio backend.                                     |\n\n### Extensions.WebRtc.Apm\n\n| Class/Interface                                                                 | Description                                                                                                                                       |\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AudioProcessingModule`](#extensions-webrtc-apm-audioprocessingmodule)         | Provides access to the native WebRTC Audio Processing Module for advanced audio processing like AEC, NS, AGC.                                     |\n| [`ApmConfig`](#extensions-webrtc-apm-apmconfig)                                 | Represents a configuration for the `AudioProcessingModule`, allowing enabling/disabling and setting parameters for various APM features.            |\n| [`StreamConfig`](#extensions-webrtc-apm-streamconfig)                           | Represents a stream configuration (sample rate, channels) for audio processing within the APM.                                                    |\n| [`ProcessingConfig`](#extensions-webrtc-apm-processingconfig)                   | Holds multiple `StreamConfig` instances for input, output, and reverse streams for the APM.                                                       |\n| **Components Namespace**                                                        |                                                                                                                                                   |\n| [`NoiseSuppressor`](#extensions-webrtc-apm-components-noisesuppressor)          | A component for offline/batch noise suppression using WebRTC APM, processing audio from an `ISoundDataProvider`.                                  |\n| **Modifiers Namespace**                                                         |                                                                                                                                                   |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier)       | A `SoundModifier` that applies WebRTC APM features (AEC, NS, AGC, etc.) in real-time to an audio stream within the SoundFlow graph. Configurable. |\n\n### Interfaces\n\n| Interface                                           | Description                                                                                                                                  |\n| --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ISoundDataProvider`](#interfaces-isounddataprovider) | Defines a standard way to access audio data from various sources. Implements `IDisposable`.                                                |\n| [`ISoundDecoder`](#interfaces-isounddecoder)         | Defines the contract for decoding audio data from a specific format into raw audio samples.                                                 |\n| [`ISoundEncoder`](#interfaces-isoundencoder)         | Defines the contract for encoding raw audio samples into a specific format.                                                                 |\n| [`ISoundPlayer`](#interfaces-isoundplayer)           | Defines the contract for controlling audio playback (Play, Pause, Stop, Seek, Looping, Speed, Volume).                                       |\n| [`IVisualizationContext`](#interfaces-ivisualizationcontext) | Provides drawing methods for rendering audio visualizations. The implementation depends on the specific UI framework used.               |\n| [`IVisualizer`](#interfaces-ivisualizer)             | Defines the contract for components that visualize audio data.                                                                            |\n\n### Modifiers\n\n| Class                                                               | Description                                                                                                                                                                                           |\n| ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AlgorithmicReverbModifier`](#modifiers-algorithmicreverbmodifier) | Simulates reverberation using a network of comb and all-pass filters. Now supports multi-channel processing.                                                                                          |\n| [`BassBoosterModifier`](#modifiers-bassboostmodifier)               | Enhances low-frequency content using a resonant low-pass filter.                                                                                                                                      |\n| [`ChorusModifier`](#modifiers-chorusmodifier)                       | Creates a chorus effect by mixing delayed and modulated copies of the signal.                                                                                                                         |\n| [`CompressorModifier`](#modifiers-compressormodifier)               | Reduces the dynamic range of the audio signal using a compressor algorithm.                                                                                                                           |\n| [`DelayModifier`](#modifiers-delaymodifier)                         | Applies a delay effect with feedback and optional low-pass filtering of the delayed signal.                                                                                                           |\n| [`FrequencyBandModifier`](#modifiers-frequencybandmodifier)         | Allows boosting or cutting specific frequency bands using a combination of low-pass and high-pass filters.                                                                                            |\n| [`ParametricEqualizer`](#modifiers-parametricequalizer)             | Provides precise control over the frequency spectrum with multiple configurable bands, each of which can be set as a peaking, low-shelf, high-shelf, band-pass, notch, low-pass, or high-pass filter. |\n| [`MultiChannelChorusModifier`](#modifiers-multichannelchorusmodifier) | Creates a chorus effect with independent processing for each channel, allowing for rich spatial effects.                                                                                              |\n| [`TrebleBoostModifier`](#modifiers-trebleboostmodifier)             | Enhances high-frequency content using a high-pass filter.                                                                                                                                             |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier) (from `SoundFlow.Extensions.WebRtc.Apm.Modifiers`) | Applies WebRTC APM features like echo cancellation, noise suppression, and AGC in real-time. |\n\n### Providers\n\n| Class                                                         | Description                                                                                                                                  |\n| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AssetDataProvider`](#providers-assetdataprovider)           | `ISoundDataProvider` implementation that reads audio data from a byte array (useful for in-memory assets). Implements `IDisposable`.            |\n| [`StreamDataProvider`](#providers-streamdataprovider)         | `ISoundDataProvider` that reads audio data from a generic `Stream` on-demand (supports seeking if the stream is seekable). Implements `IDisposable`. |\n| [`MicrophoneDataProvider`](#providers-microphonedataprovider) | `ISoundDataProvider` that captures and provides audio data from an `AudioCaptureDevice` in real-time. Implements `IDisposable`.                 |\n| [`ChunkedDataProvider`](#providers-chunkeddataprovider)       | `ISoundDataProvider` that reads and decodes audio data from a file or stream in chunks, improving efficiency for large files. Implements `IDisposable`. |\n| [`NetworkDataProvider`](#providers-networkdataprovider)       | `ISoundDataProvider` that provides audio data from a network source (direct URL or HLS playlist). Implements `IDisposable`.                    |\n| [`QueueDataProvider`](#providers-queuedataprovider)           | `ISoundDataProvider` fed by an external source in real-time, ideal for generated or procedural audio. Implements `IDisposable`.                |\n| [`RawDataProvider`](#providers-rawdataprovider)               | `ISoundDataProvider` for reading raw PCM audio data from a stream or byte/float/int/short array. Implements `IDisposable`.                     |\n\n### Structs\n\n| Struct                                       | Description                                                                                    |\n| -------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| [`AudioFormat`](#structs-audioformat)        | (record struct) Represents the format of an audio stream (sample format, channels, sample rate). |\n| [`DeviceInfo`](#structs-deviceinfo)          | Represents information about an audio device, including ID, name, and supported formats.         |\n| [`NativeDataFormat`](#structs-nativedataformat) | Represents a native data format supported by an audio device (format, channels, sample rate). |\n\n### Utils\n\n| Class                                       | Description                                                                                                                                                                                                 |\n| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Extensions`](#utils-extensions)           | Provides extension methods for working with audio data and other utility functions, including `ReadArray<T>` for reading structures from native memory.                                                                                                                             |\n| [`MathHelper`](#utils-mathhelper)           | Provides mathematical functions and algorithms used in audio processing, including optimized FFT, window functions, `Mod`, and `PrincipalAngle`. |\n\n### Visualization\n\n| Class/Interface                                         | Description                                                                                                                                                                                      |\n| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`LevelMeterAnalyzer`](#visualization-levelmeteranalyzer) | `AudioAnalyzer` that calculates the RMS (root mean square) and peak levels of an audio signal.                                                                                                  |\n| [`LevelMeterVisualizer`](#visualization-levelmetervisualizer) | `IVisualizer` that displays a level meter showing the current RMS or peak level of the audio.                                                                                                 |\n| [`SpectrumAnalyzer`](#visualization-spectrumanalyzer)    | `AudioAnalyzer` that computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).                                                                                         |\n| [`SpectrumVisualizer`](#visualization-spectrumvisualizer)  | `IVisualizer` that renders a bar graph representing the frequency spectrum of the audio.                                                                                                        |\n| [`WaveformVisualizer`](#visualization-waveformvisualizer)  | `IVisualizer` that draws the waveform of the audio signal.                                                                                                                                     |\n\n## Detailed Class and Interface Documentation\n\nThis section provides more in-depth information about some of the key classes and interfaces.\n\n### Abstracts `AudioAnalyzer`\n\n```csharp\npublic abstract class AudioAnalyzer\n{\n    protected AudioAnalyzer(AudioFormat format, IVisualizer? visualizer = null);\n\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n    public AudioFormat Format { get; }\n\n    public void Process(Span<float> buffer, int channels);\n    protected abstract void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Enabled`: Gets or sets whether the analyzer is active. If false, `Analyze` is skipped.\n*   `Format`: The audio format the analyzer is configured to process.\n\n**Methods:**\n\n*   `Process(Span<float> buffer, int channels)`: Processes the audio data, calling `Analyze` and then sending data to the attached visualizer.\n*   `Analyze(Span<float> buffer, int channels)`: Abstract method to be implemented by derived classes to perform audio analysis on the provided buffer.\n\n### Abstracts `AudioEngine`\n\n```csharp\npublic abstract class AudioEngine : IDisposable\n{\n    protected AudioEngine();\n\n    public DeviceInfo[] PlaybackDevices { get; protected set; }\n    public DeviceInfo[] CaptureDevices { get; protected set; }\n    public bool IsDisposed { get; private set; }\n\n    ~AudioEngine();\n\n    public void SoloComponent(SoundComponent component);\n    public void UnsoloComponent(SoundComponent component);\n    public SoundComponent? GetSoloedComponent();\n    public abstract ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format);\n    public abstract ISoundDecoder CreateDecoder(Stream stream, AudioFormat format);\n    public abstract AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);\n    public abstract AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public abstract AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public abstract FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);\n    public abstract void UpdateDevicesInfo();\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `PlaybackDevices`: An array of available playback devices.\n*   `CaptureDevices`: An array of available capture devices.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n\n**Methods:**\n\n*   `SoloComponent(SoundComponent component)`: Solos a specific component, muting all others.\n*   `UnsoloComponent(SoundComponent component)`: Removes the solo status from a component.\n*   `GetSoloedComponent()`: Returns the currently soloed component, if any.\n*   `CreateEncoder(...)`: Creates a backend-specific sound encoder.\n*   `CreateDecoder(...)`: Creates a backend-specific sound decoder.\n*   `InitializePlaybackDevice(...)`: Initializes and returns a new playback device.\n*   `InitializeCaptureDevice(...)`: Initializes and returns a new capture device.\n*   `InitializeFullDuplexDevice(...)`: Initializes a high-level full-duplex device for simultaneous I/O.\n*   `InitializeLoopbackDevice(...)`: Initializes a device for loopback recording of system audio.\n*   `SwitchDevice(...)`: Switches an active device to a new physical device, preserving its state (components or subscribers).\n*   `UpdateDevicesInfo()`: Refreshes the `PlaybackDevices` and `CaptureDevices` lists.\n*   `Dispose()`: Disposes the engine and all associated resources.\n\n### Abstracts `AudioDevice`\n\n```csharp\npublic abstract class AudioDevice : IDisposable\n{\n    public AudioEngine Engine { get; }\n    public DeviceInfo? Info { get; internal init; }\n    public DeviceConfig Config { get; internal init; }\n    public Capability Capability { get; internal init; }\n    public AudioFormat Format { get; }\n    public bool IsRunning { get; protected set; }\n    public bool IsDisposed { get; protected set; }\n\n    public event EventHandler? OnDisposed;\n\n    public abstract void Start();\n    public abstract void Stop();\n    public abstract void Dispose();\n}\n```\n\n**Description:** An abstract base class representing an initialized audio device, which is managed by an `AudioEngine`. This class encapsulates the common state and behavior for both playback and capture devices, serving as the foundation for all device interactions within the framework.\n\n**Properties:**\n\n*   `Engine`: Gets the parent `AudioEngine` that manages this device instance.\n*   `Info`: Gets the informational struct (`DeviceInfo`) for the physical device being used. This contains details like the device name and ID. `null` if a default device was used without specific info.\n*   `Config`: Gets the `DeviceConfig` object used to initialize the device. This holds any backend-specific configuration settings.\n*   `Capability`: Gets the capability of this device (e.g., `Playback`, `Record`, `Loopback`).\n*   `Format`: Gets the `AudioFormat` (sample rate, channels, bit depth) that the device was initialized with.\n*   `IsRunning`: Gets a value indicating whether the device is currently started and processing audio.\n*   `IsDisposed`: Gets a value indicating whether this device instance has been disposed and can no longer be used.\n\n**Events:**\n\n*   `OnDisposed`: This event is raised when the device's `Dispose()` method is called, signaling that it is being shut down and its resources are being released.\n\n**Methods:**\n\n*   `Start()`: Abstract method that must be implemented by derived classes to start the audio stream for the device.\n*   `Stop()`: Abstract method that must be implemented by derived classes to stop the audio stream for the device.\n*   `Dispose()`: Abstract method to release all resources used by the audio device.\n\n### Abstracts `AudioPlaybackDevice`\n\n```csharp\npublic abstract class AudioPlaybackDevice : AudioDevice\n{\n    public Mixer MasterMixer { get; }\n}\n```\n\n**Description:** An abstract class that represents an initialized playback (output) audio device. It inherits from `AudioDevice` and extends it with functionality specific to audio output.\n\n**Properties:**\n\n*   `MasterMixer`: Gets the master `Mixer` for this device. All audio to be played on this device must be routed through this mixer. You can connect various `SoundComponent`s (like `SoundPlayer`, `Oscillator`, or other `Mixer`s) to this master mixer to combine them into a final output signal.\n\n### Abstracts `AudioCaptureDevice`\n\n```csharp\npublic abstract class AudioCaptureDevice : AudioDevice\n{\n    public event AudioProcessCallback? OnAudioProcessed;\n    internal Delegate[] GetEventSubscribers();\n}\n```\n\n**Description:** An abstract class that represents an initialized capture (input) audio device. It inherits from `AudioDevice` and provides the core mechanism for receiving audio data from an input source like a microphone.\n\n**Events:**\n\n*   `OnAudioProcessed`: This event is the primary way to receive audio data from the capture device. It is raised by the backend whenever a new chunk of audio samples has been captured. Subscribe to this event to process live audio input. The event delegate is `AudioProcessCallback(Span<float> samples, Capability capability)`.\n\n**Methods:**\n\n*   `GetEventSubscribers()`: (Internal) An internal method used by the `AudioEngine` to retrieve the list of subscribers to the `OnAudioProcessed` event. This is crucial for the device switching functionality, allowing event subscriptions to be preserved when moving from one physical device to another.\n\n### Abstracts `FullDuplexDevice`\n\n```csharp\npublic sealed class FullDuplexDevice : AudioDevice, IDisposable\n{\n    public AudioPlaybackDevice PlaybackDevice { get; }\n    public AudioCaptureDevice CaptureDevice { get; }\n    public Mixer MasterMixer => PlaybackDevice.MasterMixer;\n\n    public event AudioProcessCallback? OnAudioProcessed;\n\n    public override void Start();\n    public override void Stop();\n    public override void Dispose();\n}\n```\n\n**Description:** A high-level, sealed class that simplifies full-duplex (simultaneous input and output) audio operations. It internally manages a paired `AudioPlaybackDevice` and `AudioCaptureDevice`, making it ideal for applications like live effects processing, VoIP, or real-time instrument monitoring where you need to listen to an input while producing an output.\n\n**Properties:**\n\n*   `PlaybackDevice`: Gets the underlying `AudioPlaybackDevice` instance used for audio output.\n*   `CaptureDevice`: Gets the underlying `AudioCaptureDevice` instance used for audio input.\n*   `MasterMixer`: A convenient shortcut to access the `MasterMixer` of the underlying `PlaybackDevice`. This is where you should route all audio you want to play out.\n\n**Events:**\n\n*   `OnAudioProcessed`: An event that is raised when audio data is captured from the input device. This event is a direct pass-through to the `OnAudioProcessed` event of the underlying `CaptureDevice`.\n\n**Methods:**\n\n*   `Start()`: Starts both the underlying capture and playback devices simultaneously.\n*   `Stop()`: Stops both the underlying capture and playback devices simultaneously.\n*   `Dispose()`: Stops and disposes of all resources, including the underlying `PlaybackDevice` and `CaptureDevice`.\n\n### Abstracts `DeviceConfig`\n\n```csharp\npublic abstract class DeviceConfig;\n```\n**Description:** A marker base class for creating backend-specific device configuration objects. This allows passing detailed, implementation-specific settings during device initialization. See `MiniAudioDeviceConfig` for an example.\n\n### Abstracts `SoundComponent`\n\n```csharp\npublic abstract class SoundComponent : IDisposable\n{\n    protected SoundComponent(AudioEngine engine, AudioFormat format);\n\n    public AudioEngine Engine { get; }\n    public AudioFormat Format { get; }\n    public virtual string Name { get; set; }\n    public Mixer? Parent { get; set; }\n    public virtual float Volume { get; set; }\n    public virtual float Pan { get; set; } // Range 0.0 (Left) to 1.0 (Right)\n    public virtual bool Enabled { get; set; }\n    public virtual bool Solo { get; set; }\n    public virtual bool Mute { get; set; }\n    public bool IsDisposed { get; private set; }\n\n    public IReadOnlyList<SoundComponent> Inputs { get; }\n    public IReadOnlyList<SoundModifier> Modifiers { get; }\n    public IReadOnlyList<AudioAnalyzer> Analyzers { get; }\n\n    public void ConnectInput(SoundComponent input);\n    public void DisconnectInput(SoundComponent input);\n    public void AddModifier(SoundModifier modifier);\n    public void RemoveModifier(SoundModifier modifier);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    internal void Process(Span<float> outputBuffer, int channels);\n    protected abstract void GenerateAudio(Span<float> buffer, int channels);\n    public virtual void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Engine`: The engine context this component belongs to.\n*   `Format`: The audio format of this component.\n*   `Name`: The name of the component.\n*   `Parent`: The parent mixer of this component.\n*   `Inputs`: Read-only list of connected input components.\n*   `Modifiers`: Read-only list of applied modifiers.\n*   `Analyzers`: Read-only list of attached audio analyzers.\n*   `Volume`: The volume of the component's output.\n*   `Pan`: The panning of the component's output (0.0 for full left, 0.5 for center, 1.0 for full right).\n*   `Enabled`: Whether the component is enabled.\n*   `Solo`: Whether the component is soloed.\n*   `Mute`: Whether the component is muted.\n*   `IsDisposed`: Indicates whether the component has been disposed.\n\n**Methods:**\n\n*   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n*   `DisconnectInput(SoundComponent input)`: Disconnects an input from this component.\n*   `AddModifier(SoundModifier modifier)`: Adds a sound modifier to the component.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a sound modifier from the component.\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an audio analyzer to the component.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an audio analyzer from the component.\n*   `Process(Span<float> outputBuffer, int channels)`: Processes the component's audio, applying modifiers and handling input/output connections.\n*   `GenerateAudio(Span<float> buffer, int channels)`: Abstract method that derived classes must implement to generate or modify audio data.\n*   `Dispose()`: Disposes the component and disconnects it from the audio graph.\n\n### Abstracts `SoundModifier`\n\n```csharp\npublic abstract class SoundModifier\n{\n    public SoundModifier();\n\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    public abstract float ProcessSample(float sample, int channel);\n    public virtual void Process(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n*   `Enabled`: Gets or sets whether the modifier is active and should process audio. Defaults to true.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Abstract method to be implemented by derived classes to process a single audio sample.\n*   `Process(Span<float> buffer)`: Processes a buffer of audio data. Applies `ProcessSample` to each sample if not overridden.\n\n### Abstracts `SoundPlayerBase`\n\n```csharp\npublic abstract class SoundPlayerBase : SoundComponent, ISoundPlayer\n{\n    protected SoundPlayerBase(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n\n    public float PlaybackSpeed { get; set; }\n    public PlaybackState State { get; private set; }\n    public bool IsLooping { get; set; }\n    public float Time { get; }\n    public float Duration { get; }\n    public int LoopStartSamples { get; }\n    public int LoopEndSamples { get; }\n    public float LoopStartSeconds { get; }\n    public float LoopEndSeconds { get; }\n    // Volume is inherited from SoundComponent\n\n    public event EventHandler<EventArgs>? PlaybackEnded;\n\n    protected override void GenerateAudio(Span<float> output, int channels);\n    protected virtual void HandleEndOfStream(Span<float> remainingOutputBuffer, int channels);\n    protected virtual void OnPlaybackEnded();\n\n    public void Play();\n    public void Pause();\n    public void Stop();\n    public bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    public bool Seek(float time);\n    public bool Seek(int sampleOffset);\n    public void SetLoopPoints(float startTime, float? endTime = -1f);\n    public void SetLoopPoints(int startSample, int endSample = -1);\n    public void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `PlaybackSpeed`: Gets or sets the playback speed (1.0 is normal). Values other than 1.0 use a WSOLA time stretcher for pitch preservation.\n*   `State`: Gets the current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Gets or sets whether looping is enabled.\n*   `Time`: Gets the current playback position in seconds, affected by `PlaybackSpeed`.\n*   `Duration`: Gets the total duration of the audio in seconds.\n*   `LoopStartSamples`: Gets the loop start point in samples.\n*   `LoopEndSamples`: Gets the loop end point in samples (-1 for end of audio).\n*   `LoopStartSeconds`: Gets the loop start point in seconds.\n*   `LoopEndSeconds`: Gets the loop end point in seconds (-1 for end of audio).\n*   `Volume`: (Inherited from `SoundComponent`) Gets or sets the volume of the player.\n\n**Events:**\n\n*   `PlaybackEnded`: Occurs when playback reaches the end of the audio (not raised during looping).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> output)`: (Protected Override) Core audio generation logic, handles reading from data provider, resampling for playback speed, and looping.\n*   `HandleEndOfStream(Span<float> remainingOutputBuffer)`: (Protected Virtual) Handles logic when the data provider reaches its end (looping or stopping).\n*   `OnPlaybackEnded()`: (Protected Virtual) Invokes the `PlaybackEnded` event.\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to a specific time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to a specific time in seconds. Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to a specific sample offset. Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures loop points using start/end times in seconds.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures loop points using start/end sample indices.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures loop points using `TimeSpan`.\n*   `Dispose()`: Disposes the player and its underlying data provider.\n\n### Abstracts `WsolaTimeStretcher`\n```csharp\npublic class WsolaTimeStretcher\n{\n    public WsolaTimeStretcher(int initialChannels = 2, float initialSpeed = 1.0f);\n\n    public void SetChannels(int channels);\n    public void SetSpeed(float speed);\n    public int MinInputSamplesToProcess { get; }\n    public void Reset();\n    public float GetTargetSpeed();\n    public int Process(ReadOnlySpan<float> input, Span<float> output, out int samplesConsumedFromInputBuffer, out int sourceSamplesRepresentedByOutput);\n    public int Flush(Span<float> output);\n}\n```\n**Description:** Implements the WSOLA (Waveform Similarity Overlap-Add) algorithm for real-time, pitch-preserved time stretching of audio. Allows changing playback speed without altering pitch. Used internally by `SoundPlayerBase` and `AudioSegment`.\n\n\n### Backends.MiniAudio `MiniAudioDecoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioDecoder : ISoundDecoder\n{\n    internal MiniAudioDecoder(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n    public int Length { get; private set; } // Length can be updated after initial check\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int Decode(Span<float> samples);\n    public void Dispose();\n    public bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n* `IsDisposed`: Indicates whether the decoder has been disposed.\n* `Length`: The total length of the decoded audio data in samples. *Note: Can be updated after initial checks if the stream length was not immediately available.*\n* `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n* `EndOfStreamReached`: Occurs when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n* `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer. Internally synchronized.\n* `Dispose()`: Releases the resources used by the decoder.\n* `Seek(int offset)`: Seeks to the specified offset within the audio stream (in samples). Internally synchronized.\n\n### Backends.MiniAudio `MiniAudioEncoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioEncoder : ISoundEncoder\n{\n    public MiniAudioEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n\n    public void Dispose();\n    public int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the encoder.\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples and writes them to the output file or stream.\n\n### Backends.MiniAudio `MiniAudioEngine`\n\n```csharp\npublic sealed class MiniAudioEngine : AudioEngine\n{\n    public MiniAudioEngine();\n\n    // Inherits all public methods from AudioEngine\n    public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null);\n    public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null);\n    public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format);\n    public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format);\n    public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null);\n    public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null);\n    public override void UpdateDevicesInfo();\n}\n```\n\n**Description:** This is the concrete implementation of the abstract `AudioEngine` class using the powerful `miniaudio` C library as its backend. It is responsible for all low-level audio I/O operations, including device discovery, initialization, and data processing callbacks. It manages the native `miniaudio` context and serves as the factory for creating `MiniAudioDecoder` and `MiniAudioEncoder` instances. Because it handles the native interop, this class ensures that SoundFlow is cross-platform, supporting Windows, macOS, Linux, Android, and iOS.\n\n**Methods:**\n\n*   `InitializePlaybackDevice(...)`: Creates and returns a `MiniAudioPlaybackDevice`, which represents an active connection to a physical sound output device.\n*   `InitializeCaptureDevice(...)`: Creates and returns a `MiniAudioCaptureDevice` for a physical sound input device.\n*   `InitializeFullDuplexDevice(...)`: Creates and returns a `FullDuplexDevice` that manages both a playback and capture stream simultaneously.\n*   `InitializeLoopbackDevice(...)`: Creates a special capture device that records the system's audio output. On Windows, this uses the WASAPI loopback feature.\n*   `CreateEncoder(...)`: Returns a `MiniAudioEncoder` instance, currently supporting `.wav` file encoding.\n*   `CreateDecoder(...)`: Returns a `MiniAudioDecoder` instance, capable of decoding various audio formats like `.wav`, `.mp3`, and `.flac`.\n*   `SwitchDevice(...)`: Implements the logic to seamlessly switch between physical audio devices at runtime, preserving the state of the audio graph.\n*   `UpdateDevicesInfo()`: Communicates with the `miniaudio` backend to refresh the lists of available playback and capture devices.\n\n---\n\n### Backends.MiniAudio `MiniAudioDeviceConfig`\n\n```csharp\npublic class MiniAudioDeviceConfig : DeviceConfig\n{\n    public uint PeriodSizeInFrames { get; set; }\n    public uint PeriodSizeInMilliseconds { get; set; }\n    public uint Periods { get; set; }\n    public bool NoPreSilencedOutputBuffer { get; set; }\n    public bool NoClip { get; set; }\n    public bool NoDisableDenormals { get; set; }\n    public bool NoFixedSizedCallback { get; set; }\n    public DeviceSubConfig Playback { get; set; }\n    public DeviceSubConfig Capture { get; set; }\n    public WasapiSettings? Wasapi { get; set; }\n    public CoreAudioSettings? CoreAudio { get; set; }\n    public AlsaSettings? Alsa { get; set; }\n    public PulseSettings? Pulse { get; set; }\n    public OpenSlSettings? OpenSL { get; set; }\n    public AAudioSettings? AAudio { get; set; }\n}\n```\n\n**Description:** A detailed configuration object that inherits from `DeviceConfig` and is specifically designed for initializing a MiniAudio device. It provides fine-grained control over buffer sizes, performance flags, and exposes nested configuration classes for OS-specific audio backends like WASAPI (Windows), CoreAudio (macOS), ALSA (Linux), and others. This allows developers to tune performance and behavior for specific platforms.\n\n**General Properties:**\n\n*   `PeriodSizeInFrames`: Gets or sets the desired size of the internal processing buffer in frames (a frame is one sample for each channel). This gives precise, sample-level control over buffer latency. Takes precedence over `PeriodSizeInMilliseconds`. Default is 0 (backend default).\n*   `PeriodSizeInMilliseconds`: Gets or sets the desired size of the internal processing buffer in milliseconds. A more intuitive way to control latency. Default is 0 (backend default).\n*   `Periods`: Gets or sets the number of periods to use for the device's buffer. Default is 0 (backend default).\n*   `NoPreSilencedOutputBuffer`: If `true`, the output buffer passed to the audio callback will contain undefined data instead of being cleared to silence. This can be a minor performance optimization if you are always filling the entire buffer. Default is `false`.\n*   `NoClip`: If `true`, the backend will not clip F32 sample values that are outside the [-1.0, 1.0] range. Default is `false`.\n*   `NoDisableDenormals`: If `true`, the backend will not attempt to disable denormal floating-point numbers, which can slightly improve precision at the cost of performance on some CPUs. Default is `false`.\n*   `NoFixedSizedCallback`: If `true`, the backend is not required to provide buffers of a fixed size in every callback. This can be an optimization if your processing logic is flexible. Default is `false`.\n\n**Sub-Configurations:**\n\n*   `Playback`: A `DeviceSubConfig` object for playback-specific settings.\n*   `Capture`: A `DeviceSubConfig` object for capture-specific settings.\n*   `Wasapi`: A `WasapiSettings` object for Windows-specific settings. Only used on Windows.\n*   `CoreAudio`: A `CoreAudioSettings` object for macOS/iOS-specific settings. Only used on Apple platforms.\n*   `Alsa`: An `AlsaSettings` object for Linux-specific settings. Only used on Linux with ALSA.\n*   `Pulse`: A `PulseSettings` object for Linux-specific settings. Only used on Linux with PulseAudio.\n*   `OpenSL`: An `OpenSlSettings` object for Android-specific settings.\n*   `AAudio`: An `AAudioSettings` object for modern Android-specific settings.\n\n---\n\n#### `DeviceSubConfig` (Nested Class)\n\n```csharp\npublic class DeviceSubConfig\n{\n    public ShareMode ShareMode { get; set; } = ShareMode.Shared;\n    internal bool IsLoopback { get; set; }\n}\n```\n\n**Description:** Contains settings for a specific direction (playback or capture).\n\n**Properties:**\n\n*   `ShareMode`: Specifies how the device is opened. `ShareMode.Shared` (default) allows multiple applications to use the device. `ShareMode.Exclusive` attempts to gain exclusive control for the lowest possible latency, but may not be supported by all devices.\n*   `IsLoopback`: (Internal) A flag used to indicate that a capture device should be initialized in loopback mode.\n\n---\n\n#### `WasapiSettings` (Nested Class)\n\n```csharp\npublic class WasapiSettings\n{\n    public WasapiUsage Usage { get; set; } = WasapiUsage.Default;\n    public bool NoAutoConvertSRC { get; set; }\n    public bool NoDefaultQualitySRC { get; set; }\n    public bool NoAutoStreamRouting { get; set; }\n    public bool NoHardwareOffloading { get; set; }\n}\n```\n**Description:** Contains settings specific to the WASAPI audio backend on Windows.\n\n**Properties:**\n\n*   `Usage`: Hints to the OS about the stream's purpose (`Default`, `Games`, `ProAudio`), which can affect system-level audio processing and prioritization.\n*   `NoAutoConvertSRC`: If `true`, disables automatic sample rate conversion by WASAPI, letting MiniAudio handle it instead.\n*   `NoDefaultQualitySRC`: If `true`, prevents WASAPI from using its default quality for sample rate conversion.\n*   `NoAutoStreamRouting`: If `true`, disables automatic stream routing by the OS.\n*   `NoHardwareOffloading`: If `true`, disables WASAPI's hardware offloading feature.\n\n---\n\n#### `CoreAudioSettings` (Nested Class)\n\n```csharp\npublic class CoreAudioSettings\n{\n    public bool AllowNominalSampleRateChange { get; set; }\n}\n```\n**Description:** Contains settings specific to the CoreAudio backend on macOS and iOS.\n\n**Properties:**\n\n*   `AllowNominalSampleRateChange`: If `true`, allows the OS to change the device's sample rate to match the stream. Typically used on desktop macOS.\n\n---\n\n#### `AlsaSettings` (Nested Class)\n\n```csharp\npublic class AlsaSettings\n{\n    public bool NoMMap { get; set; }\n    public bool NoAutoFormat { get; set; }\n    public bool NoAutoChannels { get; set; }\n    public bool NoAutoResample { get; set; }\n}\n```\n**Description:** Contains settings specific to the ALSA audio backend on Linux.\n\n**Properties:**\n\n*   `NoMMap`: If `true`, disables memory-mapped (MMap) mode for ALSA.\n*   `NoAutoFormat`: If `true`, prevents ALSA from performing automatic format conversion.\n*   `NoAutoChannels`: If `true`, prevents ALSA from performing automatic channel count conversion.\n*   `NoAutoResample`: If `true`, prevents ALSA from performing automatic resampling.\n\n---\n\n#### `PulseSettings` (Nested Class)\n\n```csharp\npublic class PulseSettings\n{\n    public string? StreamNamePlayback { get; set; }\n    public string? StreamNameCapture { get; set; }\n}\n```\n**Description:** Contains settings specific to the PulseAudio backend on Linux.\n\n**Properties:**\n\n*   `StreamNamePlayback`: Sets a custom name for the playback stream as it appears in PulseAudio volume controls.\n*   `StreamNameCapture`: Sets a custom name for the capture stream.\n\n---\n\n#### `OpenSlSettings` (Nested Class)\n\n```csharp\npublic class OpenSlSettings\n{\n    public OpenSlStreamType StreamType { get; set; }\n    public OpenSlRecordingPreset RecordingPreset { get; set; }\n}\n```\n**Description:** Contains settings specific to the OpenSL ES backend on Android.\n\n**Properties:**\n\n*   `StreamType`: Specifies the type of audio stream (e.g., `Voice`, `Media`, `Alarm`) to help Android manage audio focus and routing.\n*   `RecordingPreset`: Optimizes the microphone input for a specific scenario (e.g., `VoiceCommunication`, `Camcorder`).\n\n---\n\n#### `AAudioSettings` (Nested Class)\n\n```csharp\npublic class AAudioSettings\n{\n    public AAudioUsage Usage { get; set; }\n    public AAudioContentType ContentType { get; set; }\n    public AAudioInputPreset InputPreset { get; set; }\n    public AAudioAllowedCapturePolicy AllowedCapturePolicy { get; set; }\n}\n```\n**Description:** Contains settings specific to the modern AAudio backend on Android.\n\n**Properties:**\n\n*   `Usage`: Hints to the system about the stream's purpose (e.g., `Media`, `Game`, `Assistant`) for optimized routing and resource management.\n*   `ContentType`: Describes the type of content being played (e.g., `Music`, `Speech`, `Sonification`).\n*   `InputPreset`: Specifies a configuration for the audio input, optimizing it for scenarios like `VoiceRecognition` or `Camcorder`.\n*   `AllowedCapturePolicy`: Controls whether other applications are allowed to capture the audio from this stream.\n\n### Components `EnvelopeGenerator`\n\n```csharp\npublic class EnvelopeGenerator : SoundComponent\n{\n    public EnvelopeGenerator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float AttackTime { get; set; }\n    public float DecayTime { get; set; }\n    public override string Name { get; set; }\n    public float ReleaseTime { get; set; }\n    public bool Retrigger { get; set; }\n    public float SustainLevel { get; set; }\n    public TriggerMode Trigger { get; set; }\n\n    public event Action<float>? LevelChanged;\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public void TriggerOff();\n    public void TriggerOn();\n}\n```\n\n**Properties:**\n\n*   `AttackTime`: The attack time of the envelope (in seconds).\n*   `DecayTime`: The decay time of the envelope (in seconds).\n*   `Name`: The name of the envelope generator.\n*   `ReleaseTime`: The release time of the envelope (in seconds).\n*   `Retrigger`: Whether to retrigger the envelope on each new trigger.\n*   `SustainLevel`: The sustain level of the envelope.\n*   `Trigger`: The trigger mode (`NoteOn`, `Gate`, `Trigger`).\n\n**Events:**\n\n*   `LevelChanged`: Occurs when the envelope level changes.\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the envelope signal.\n*   `TriggerOff()`: Triggers the release stage of the envelope (if in `Gate` mode).\n*   `TriggerOn()`: Triggers the attack stage of the envelope.\n\n### Components `Filter`\n\n```csharp\npublic class Filter : SoundComponent\n{\n    public Filter(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float CutoffFrequency { get; set; }\n    public override string Name { get; set; }\n    public float Resonance { get; set; }\n    public FilterType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency of the filter.\n*   `Name`: The name of the filter.\n*   `Resonance`: The resonance of the filter.\n*   `Type`: The filter type (`LowPass`, `HighPass`, `BandPass`, `Notch`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Applies the filter to the audio buffer.\n\n### Components `LowFrequencyOscillator`\n\n```csharp\npublic class LowFrequencyOscillator : SoundComponent\n{\n    public LowFrequencyOscillator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float Depth { get; set; }\n    public TriggerMode Mode { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float Rate { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public float GetLastOutput();\n    public void Trigger();\n}\n```\n\n**Properties:**\n\n*   `Depth`: The depth of the LFO's modulation.\n*   `Mode`: The trigger mode (`FreeRunning`, `NoteTrigger`).\n*   `Name`: The name of the LFO.\n*   `Phase`: The initial phase of the LFO.\n*   `Rate`: The rate (frequency) of the LFO.\n*   `Type`: The waveform type (`Sine`, `Square`, `Triangle`, `Sawtooth`, `ReverseSawtooth`, `Random`, `SampleAndHold`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the LFO signal.\n*   `GetLastOutput()`: Returns the last generated output sample.\n*   `Trigger()`: Triggers the LFO (if in `NoteTrigger` mode).\n\n### Components `Mixer`\n\n```csharp\npublic sealed class Mixer : SoundComponent\n{\n    public Mixer(AudioEngine engine, AudioFormat format, bool isMasterMixer = false);\n\n    public IReadOnlyCollection<SoundComponent> Components { get; }\n    public AudioPlaybackDevice? ParentDevice { get; internal set; }\n    public bool IsMasterMixer { get; }\n    public override string Name { get; set; }\n\n    public void AddComponent(SoundComponent component);\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n    public void RemoveComponent(SoundComponent component);\n    public override void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Components`: A read-only collection of sound components mixed by this mixer.\n*   `ParentDevice`: The playback device this mixer is the master for, if any.\n*   `IsMasterMixer`: A value indicating whether this is a master mixer for a device.\n*   `Name`: The name of the mixer.\n\n**Methods:**\n\n*   `AddComponent(SoundComponent component)`: Adds a component to the mixer.\n*   `GenerateAudio(Span<float> buffer, int channels)`: Mixes the audio from all connected components.\n*   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n*   `Dispose()`: Disposes the mixer and all components within it.\n\n### Components `Oscillator`\n\n```csharp\npublic class Oscillator : SoundComponent\n{\n    public Oscillator(AudioEngine engine, AudioFormat format) : base(engine, format);\n\n    public float Amplitude { get; set; }\n    public float Frequency { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float PulseWidth { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Amplitude`: The amplitude of the oscillator.\n*   `Frequency`: The frequency of the oscillator.\n*   `Name`: The name of the oscillator.\n*   `Phase`: The initial phase of the oscillator.\n*   `PulseWidth`: The pulse width (for pulse waveforms).\n*   `Type`: The waveform type (`Sine`, `Square`, `Sawtooth`, `Triangle`, `Noise`, `Pulse`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer, int channels)`: Generates the oscillator's output.\n\n### Components `Recorder`\n\n```csharp\npublic class Recorder : IDisposable\n{\n    public Recorder(AudioCaptureDevice captureDevice, Stream stream, EncodingFormat encodingFormat = EncodingFormat.Wav);\n    public Recorder(AudioCaptureDevice captureDevice, AudioProcessCallback callback);\n\n    public PlaybackState State { get; private set; }\n    public readonly SampleFormat SampleFormat;\n    public readonly EncodingFormat EncodingFormat;\n    public readonly int SampleRate;\n    public readonly int Channels;\n    public readonly Stream Stream;\n    public AudioProcessCallback? ProcessCallback;\n    public ReadOnlyCollection<SoundModifier> Modifiers { get; }\n    public ReadOnlyCollection<AudioAnalyzer> Analyzers { get; }\n\n    public void StartRecording();\n    public void ResumeRecording();\n    public void PauseRecording();\n    public void StopRecording();\n    public void AddModifier(SoundModifier modifier);\n    public void RemoveModifier(SoundModifier modifier);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Analyzers`: Gets a read-only collection of <see cref=\"AudioAnalyzer\"/> components applied to the recorder. Analyzers are used to process and extract data from the audio stream during recording.\n*   `Modifiers`: Gets a read-only collection of <see cref=\"SoundModifier\"/> components applied to the recorder. Modifiers are applied to the audio data before encoding or processing via callback, allowing for real-time audio effects during recording.\n*   `Channels`: The number of channels to record.\n*   `EncodingFormat`: The encoding format for the recorded audio.\n*   `Stream`: The stream to write encoded recorded audio to.\n*   `ProcessCallback`: A callback for processing recorded audio in real time.\n*   `SampleRate`: The sample rate for recording.\n*   `State`: The current recording state (`Stopped`, `Playing`, `Paused`).\n*   `SampleFormat`: The sample format for recording.\n\n**Methods:**\n\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an <see cref=\"AudioAnalyzer\"/> to the recording pipeline. Analyzers process audio data during recording, enabling real-time analysis.\n*   `AddModifier(SoundModifier modifier)`: Adds a <see cref=\"SoundModifier\"/> to the recording pipeline. Modifiers apply effects to the audio data in real-time as it's being recorded.\n*   `Dispose()`: Releases resources used by the recorder.\n*   `PauseRecording()`: Pauses the recording.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes a specific <see cref=\"AudioAnalyzer\"/> from the recording pipeline.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a specific <see cref=\"SoundModifier\"/> from the recording pipeline.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `StartRecording()`: Starts the recording.\n*   `StopRecording()`: Stops the recording.\n*   `Dispose()`: Stops recording and releases all resources.\n\n### Components `SoundPlayer`\n\n```csharp\npublic sealed class SoundPlayer : SoundPlayerBase\n{\n    public SoundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n}\n```\nInherits all playback functionality, properties, and events from `SoundPlayerBase`.\n\n**Properties:**\n* `Name`: The name of the sound player component (default: \"Sound Player\").\n\n### Components `SurroundPlayer`\n\n```csharp\npublic sealed class SurroundPlayer : SoundPlayerBase\n{\n    public SurroundPlayer(AudioEngine engine, AudioFormat format, ISoundDataProvider dataProvider);\n\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n    public Vector2 ListenerPosition { get; set; }\n    public PanningMethod Panning { get; set; }\n    public VbapParameters VbapParameters { get; set; }\n    public SurroundConfiguration SurroundConfig { get; set; }\n    public SpeakerConfiguration SpeakerConfig { get; set; }\n\n    protected override void GenerateAudio(Span<float> output, int channels);\n    public void SetSpeakerConfiguration(SpeakerConfiguration config);\n    // Inherits Play, Pause, Stop, Seek, IsLooping, etc. from SoundPlayerBase\n}\n```\nInherits base playback functionality from `SoundPlayerBase` and adds surround-specific features.\n\n**Properties:**\n*   `Name`: The name of the surround player component (default: \"Surround Player\").\n*   `ListenerPosition`: The position of the listener in the surround sound field (Vector2).\n*   `Panning`: Gets or sets the panning method to use for surround sound (`Linear`, `EqualPower`, `VBAP`).\n*   `SpeakerConfig`: Gets or sets the speaker configuration (`Stereo`, `Quad`, `Surround51`, `Surround71`, `Custom`).\n*   `VbapParameters`: Gets or sets parameters for Vector Base Amplitude Panning (VBAP).\n*   `SurroundConfig`: Gets or sets the custom surround configuration when `SpeakerConfig` is set to `Custom`.\n\n**Methods:**\n*   `GenerateAudio(Span<float> output, int channels)`: (Overrides `SoundPlayerBase`) Reads audio data, applies resampling, then applies surround processing and looping if enabled.\n*   `SetSpeakerConfiguration(SpeakerConfiguration config)`: Sets the speaker configuration for surround sound playback.\n*   `Seek(int sampleOffset)`: (Overrides `SoundPlayerBase`) Seeks and re-initializes delay lines for surround processing.\n\n\n### Components `VoiceActivityDetector`\n\n```csharp\npublic class VoiceActivityDetector : AudioAnalyzer\n{\n    public VoiceActivityDetector(AudioFormat format, int fftSize = 1024, float energyThreshold = 5f, IVisualizer? visualizer = null);\n\n    public bool IsVoiceActive { get; private set; }\n    public float EnergyThreshold { get; set; }\n    public float ActivationTimeMs { get; set; }\n    public float HangoverTimeMs { get; set; }\n    public int SpeechLowBand { get; set; }\n    public int SpeechHighBand { get; set; }\n\n    public override string Name { get; set; }\n\n    public event Action<bool>? SpeechDetected;\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: Gets or sets the name of the voice activity detector component, useful for identification and debugging.\n*   `IsVoiceActive`: Read-only property indicating if voice is currently detected.\n*   `EnergyThreshold`: The energy threshold for detection.\n*   `ActivationTimeMs`: Time in milliseconds the signal must be considered speech before activation. Helps prevent short noise bursts from triggering.\n*   `HangoverTimeMs`: Time in milliseconds to keep the VAD active after the last speech frame. Prevents deactivation during short pauses.\n*   `SpeechLowBand`/`SpeechHighBand`: The frequency range (in Hz) to analyze for speech.\n\n**Events:**\n\n*   `SpeechDetected`: An event that is raised whenever the voice activity state changes (i.e., when speech is detected or ceases to be detected). Listeners can subscribe to this event to respond in real-time to changes in voice activity.\n\n**Methods:**\n\n*   `VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null)`: Constructor for the VoiceActivityDetector class. Initializes a new instance of the voice activity detector with configurable FFT size, detection threshold and optional visualizer for audio analysis visualization.\n    *   `fftSize`: `int` – The size of the FFT (Fast Fourier Transform) window used for spectral analysis. Must be a power of two. Larger FFT sizes provide finer frequency resolution but may increase processing latency. Default is 1024.\n    *   `energyThreshold`: `float` – The sensitivity threshold for voice detection. A lower value increases sensitivity. Default is 0.01.\n    *   `visualizer`: `IVisualizer?` – An optional visualizer instance that can be attached to the analyzer for visualizing audio processing data, useful for debugging and tuning. Default is `null`.\n*   `Analyze(Span<float> buffer, int channels)`: перевіряє audio buffer та оновлює `IsVoiceActive` property на основі алгоритму детекції.\n    *   `buffer`: `Span<float>` – The audio buffer to analyze for voice activity. The audio data in this buffer is processed to determine if voice is present.\n\n**Remarks:**\n\n*   **Frequency Range:** The `SpeechLowBand` and `SpeechHighBand` properties allow you to customize the frequency range that the VAD focuses on for speech detection. Speech typically falls within the 300Hz to 3400Hz range, but you may need to adjust these values depending on the characteristics of your audio and the type of speech you are detecting.\n*   **Threshold Sensitivity:** The `Threshold` property is crucial for controlling the sensitivity of the voice activity detection. Adjusting this threshold may be necessary to achieve optimal performance in different environments and with varying audio input levels.\n*   **FFT Size:** The `fftSize` parameter in the constructor determines the FFT window size. A larger FFT size provides better frequency resolution, which can be beneficial in noisy environments or when detecting subtle voice activity. However, it also increases the computational cost and latency. Ensure that the FFT size is always a power of 2 for optimal performance and compatibility with FFT algorithms.\n*   **Performance Tuning:** For optimal performance, especially in real-time applications, carefully tune the `fftSize` and `Threshold` parameters. Larger FFT sizes are more computationally intensive but offer better frequency resolution. Adjust the `Threshold` based on the ambient noise level and the desired sensitivity of voice detection.\n*   **Environment Considerations:** The ideal settings for `fftSize`, `Threshold`, `SpeechLowBand`, and `SpeechHighBand` may vary depending on the environment in which the voice activity detector is used. In noisy environments, you might need to increase the `fftSize` and adjust the `Threshold` to minimize false positives.\n*   **Visualizer for Debugging:** The optional `visualizer` parameter in the constructor is highly useful for debugging and tuning the voice activity detector. By attaching a visualizer, you can visually inspect the audio data and the detector's response, which can help in understanding and adjusting the detector's parameters for optimal performance in your specific use case.\n\n### Enums `Capability`\n\n```csharp\n[Flags]\npublic enum Capability\n{\n    Playback = 1,\n    Record = 2,\n    Mixed = Playback | Record,\n    Loopback = 4\n}\n```\n\n**Values:**\n\n*   `Playback`: Indicates playback capability.\n*   `Record`: Indicates recording capability.\n*   `Mixed`: Indicates both playback and recording capability.\n*   `Loopback`: Indicates loopback capability (recording system audio output).\n\n### Enums `DeviceType`\n\n```csharp\npublic enum DeviceType\n{\n    Playback,\n    Capture\n}\n```\n**Values:**\n*   `Playback`: Device used for audio playback.\n*   `Capture`: Device used for audio capture.\n\n### Enums `EncodingFormat`\n\n```csharp\npublic enum EncodingFormat\n{\n    Unknown = 0,\n    Wav,\n    Flac,\n    Mp3,\n    Vorbis\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown encoding format.\n*   `Wav`: Waveform Audio File Format (WAV).\n*   `Flac`: Free Lossless Audio Codec (FLAC).\n*   `Mp3`: MPEG-1 Audio Layer III (MP3).\n*   `Vorbis`: Ogg Vorbis.\n\n### Enums `PlaybackState`\n\n```csharp\npublic enum PlaybackState\n{\n    Stopped,\n    Playing,\n    Paused\n}\n```\n\n**Values:**\n\n*   `Stopped`: Playback is stopped.\n*   `Playing`: Playback is currently in progress.\n*   `Paused`: Playback is paused.\n\n### Enums `Result`\n\n```csharp\npublic enum Result\n{\n    Success = 0,\n    Error = -1,\n    // ... (other error codes)\n    CrcMismatch = -100,\n    FormatNotSupported = -200,\n    // ... (other backend-specific error codes)\n    DeviceNotInitialized = -300,\n    // ... (other device-related error codes)\n    FailedToInitBackend = -400\n    // ... (other backend initialization error codes)\n}\n```\n\n**Values:**\n\n*   `Success`: The operation was successful.\n*   `Error`: A generic error occurred.\n*   `CrcMismatch`: CRC checksum mismatch.\n*   `FormatNotSupported`: The requested audio format is not supported.\n*   `DeviceNotInitialized`: The audio device is not initialized.\n*   `FailedToInitBackend`: Failed to initialize the audio backend.\n*   **(Many other error codes representing various error conditions)**\n\n### Enums `SampleFormat`\n\n```csharp\npublic enum SampleFormat\n{\n    Unknown = 0,\n    U8 = 1,\n    S16 = 2,\n    S24 = 3,\n    S32 = 4,\n    F32 = 5\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown sample format.\n*   `U8`: Unsigned 8-bit integer.\n*   `S16`: Signed 16-bit integer.\n*   `S24`: Signed 24-bit integer packed in 3 bytes.\n*   `S32`: Signed 32-bit integer.\n*   `F32`: 32-bit floating-point.\n\n### Enums `FilterType`\n\n```csharp\npublic enum FilterType\n{\n    Peaking,\n    LowShelf,\n    HighShelf,\n    BandPass,\n    Notch,\n    LowPass,\n    HighPass\n}\n```\n\n**Values:**\n\n*   `Peaking`: Peaking filter.\n*   `LowShelf`: Low-shelf filter.\n*   `HighShelf`: High-shelf filter.\n*   `BandPass`: Band-pass filter.\n*   `Notch`: Notch filter.\n*   `LowPass`: Low-pass filter.\n*   `HighPass`: High-pass filter.\n\n### Enums `EnvelopeGenerator.EnvelopeState`\n\n```csharp\npublic enum EnvelopeState\n{\n    Idle,\n    Attack,\n    Decay,\n    Sustain,\n    Release\n}\n```\n\n**Values:**\n\n*   `Idle`: The envelope is inactive.\n*   `Attack`: The attack stage of the envelope.\n*   `Decay`: The decay stage of the envelope.\n*   `Sustain`: The sustain stage of the envelope.\n*   `Release`: The release stage of the envelope.\n\n### Enums `EnvelopeGenerator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    NoteOn,\n    Gate,\n    Trigger\n}\n```\n\n**Values:**\n\n* `NoteOn`: The envelope will go directly from attack to sustain, without a decay stage.\n* `Gate`: The envelope will progress normally, and will only enter release stage when trigger is off.\n* `Trigger`: The envelope will always progress to the end, including the release stage.\n\n### Enums `LowFrequencyOscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Triangle,\n    Sawtooth,\n    ReverseSawtooth,\n    Random,\n    SampleAndHold\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Triangle`: Triangle wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `ReverseSawtooth`: Reverse sawtooth wave.\n*   `Random`: Random values.\n*   `SampleAndHold`: Sample and hold random values.\n\n### Enums `LowFrequencyOscillator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    FreeRunning,\n    NoteTrigger\n}\n```\n\n**Values:**\n\n* `FreeRunning`: The LFO will run continuously without needing a trigger.\n* `NoteTrigger`: The LFO will only start when triggered.\n\n### Enums `Oscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Sawtooth,\n    Triangle,\n    Noise,\n    Pulse\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `Triangle`: Triangle wave.\n*   `Noise`: White noise.\n*   `Pulse`: Pulse wave.\n\n### Enums `SurroundPlayer.SpeakerConfiguration`\n\n```csharp\npublic enum SpeakerConfiguration\n{\n    Stereo,\n    Quad,\n    Surround51,\n    Surround71,\n    Custom\n}\n```\n\n**Values:**\n\n*   `Stereo`: Standard stereo configuration (2 speakers).\n*   `Quad`: Quadraphonic configuration (4 speakers).\n*   `Surround51`: 5.1 surround sound configuration (6 speakers).\n*   `Surround71`: 7.1 surround sound configuration (8 speakers).\n* *   `Custom`: A custom speaker configuration defined by the user.\n\n### Enums `SurroundPlayer.PanningMethod`\n\n```csharp\npublic enum PanningMethod\n{\n    Linear,\n    EqualPower,\n    Vbap\n}\n```\n\n**Values:**\n\n*   `Linear`: Linear panning.\n*   `EqualPower`: Equal power panning.\n*   `Vbap`: Vector Base Amplitude Panning (VBAP).\n\n### Editing `Composition`\nSee [Editing and Persistence Guide](./editing-engine.mdx#composition) for details.\n```csharp\npublic class Composition : ISoundDataProvider, IDisposable\n{\n    public Composition(string name = \"Composition\", int? targetChannels = null);\n\n    public string Name { get; set; }\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public List<Track> Tracks { get; }\n    public float MasterVolume { get; set; }\n    public bool IsDirty { get; }\n    public int SampleRate { get; set; } // Target sample rate for rendering\n    public int TargetChannels { get; set; }\n\n    // ISoundDataProvider implementation\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public bool IsDisposed { get; private set; }\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    \n    public void AddTrack(Track track);\n    public bool RemoveTrack(Track track);\n    public TimeSpan CalculateTotalDuration();\n    public float[] Render(TimeSpan startTime, TimeSpan duration);\n    public int Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer);\n    public void MarkDirty();\n    internal void ClearDirtyFlag();\n    public void Dispose();\n    // ... other methods like ReplaceSegment, RemoveSegment, SilenceSegment, InsertSegment ...\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `Track`\nSee [Editing and Persistence Guide](./editing-engine.mdx#track) for details.\n```csharp\npublic class Track\n{\n    public Track(string name = \"Track\", TrackSettings? settings = null);\n\n    public string Name { get; set; }\n    public List<AudioSegment> Segments { get; }\n    public TrackSettings Settings { get; set; }\n    internal Composition? ParentComposition { get; set; }\n\n    public void MarkDirty();\n    public void AddSegment(AudioSegment segment);\n    public bool RemoveSegment(AudioSegment segment, bool shiftSubsequent = false);\n    public void InsertSegmentAt(AudioSegment segmentToInsert, TimeSpan insertionTime, bool shiftSubsequent = true);\n    public TimeSpan CalculateDuration();\n    public int Render(TimeSpan overallStartTime, TimeSpan durationToRender, Span<float> outputBuffer, int targetSampleRate, int targetChannels);\n}\n```\n\n### Editing `AudioSegment`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegment) for details.\n```csharp\npublic class AudioSegment : IDisposable\n{\n    public AudioSegment(\n        ISoundDataProvider sourceDataProvider,\n        TimeSpan sourceStartTime,\n        TimeSpan sourceDuration,\n        TimeSpan timelineStartTime,\n        string name = \"Segment\",\n        AudioSegmentSettings? settings = null,\n        bool ownsDataProvider = false);\n\n    public string Name { get; set; }\n    public ISoundDataProvider SourceDataProvider { get; private set; }\n    public TimeSpan SourceStartTime { get; set; }\n    public TimeSpan SourceDuration { get; set; }\n    public TimeSpan TimelineStartTime { get; set; }\n    public AudioSegmentSettings Settings { get; set; }\n    internal Track? ParentTrack { get; set; }\n\n    public TimeSpan StretchedSourceDuration { get; }\n    public TimeSpan EffectiveDurationOnTimeline { get; }\n    public TimeSpan TimelineEndTime { get; }\n    public TimeSpan GetTotalLoopedDurationOnTimeline();\n    public AudioSegment Clone(TimeSpan? newTimelineStartTime = null);\n    internal void ReplaceSource(ISoundDataProvider newSource, TimeSpan newSourceStartTime, TimeSpan newSourceDuration);\n    public int ReadProcessedSamples(TimeSpan segmentTimelineOffset, TimeSpan durationToRead, Span<float> outputBuffer, int outputBufferOffset, int targetSampleRate, int targetChannels);\n    internal void FullResetState();\n    public void Dispose();\n    public void MarkDirty();\n}\n```\n\n### Editing `AudioSegmentSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegmentsettings) for details.\n```csharp\npublic class AudioSegmentSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public TimeSpan FadeInDuration { get; set; }\n    public FadeCurveType FadeInCurve { get; set; }\n    public TimeSpan FadeOutDuration { get; set; }\n    public FadeCurveType FadeOutCurve { get; set; }\n    public bool IsReversed { get; set; }\n    public LoopSettings Loop { get; set; }\n    public float SpeedFactor { get; set; }\n    public float TimeStretchFactor { get; set; } // Overridden by TargetStretchDuration if set\n    public TimeSpan? TargetStretchDuration { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public AudioSegmentSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `TrackSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#tracksettings) for details.\n```csharp\npublic class TrackSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public bool IsMuted { get; set; }\n    public bool IsSoloed { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public TrackSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `LoopSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#loopsettings) for details.\n```csharp\npublic record struct LoopSettings\n{\n    public int Repetitions { get; }\n    public TimeSpan? TargetDuration { get; }\n    public LoopSettings(int repetitions = 0, TimeSpan? targetDuration = null);\n    public static LoopSettings PlayOnce { get; }\n}\n```\n\n### Editing `FadeCurveType`\nSee [Editing and Persistence Guide](./editing-engine.mdx#fadecurvetype) for details.\n```csharp\npublic enum FadeCurveType\n{\n    Linear,\n    Logarithmic,\n    SCurve\n}\n```\n\n### Editing.Persistence\nThese are Data Transfer Objects (DTOs) for serialization. See [Editing and Persistence Guide](./editing-engine.mdx#project-persistence) for their purpose.\n*   `CompositionProjectManager` (static class): `SaveProjectAsync`, `LoadProjectAsync`, `RelinkMissingMediaAsync`.\n*   `ProjectData`\n*   `ProjectTrack`\n*   `ProjectSegment`\n*   `ProjectAudioSegmentSettings`\n*   `ProjectTrackSettings`\n*   `ProjectSourceReference`\n*   `ProjectEffectData`\n\n\n### Extensions.WebRtc.Apm\n\n#### `AudioProcessingModule` (Class)\n```csharp\npublic class AudioProcessingModule : IDisposable\n{\n    public AudioProcessingModule();\n    public ApmError ApplyConfig(ApmConfig config);\n    public ApmError Initialize();\n    public ApmError ProcessStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    public ApmError ProcessReverseStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    // ... other methods for setting delays, levels, runtime settings, getting info, AEC dump ...\n    public static int GetFrameSize(int sampleRateHz);\n    public void Dispose();\n}\n```\n**Description:** Provides low-level access to the WebRTC Audio Processing Module. It's responsible for initializing the APM, applying configurations, and processing audio frames. Generally used internally by `WebRtcApmModifier` and `NoiseSuppressor`.\n\n#### `ApmConfig` (Class)\n```csharp\npublic class ApmConfig : IDisposable\n{\n    public ApmConfig();\n    public void SetEchoCanceller(bool enabled, bool mobileMode);\n    public void SetNoiseSuppression(bool enabled, NoiseSuppressionLevel level);\n    public void SetGainController1(bool enabled, GainControlMode mode, int targetLevelDbfs, int compressionGainDb, bool enableLimiter);\n    public void SetGainController2(bool enabled);\n    public void SetHighPassFilter(bool enabled);\n    public void SetPreAmplifier(bool enabled, float fixedGainFactor);\n    public void SetPipeline(int maxInternalRate, bool multiChannelRender, bool multiChannelCapture, DownmixMethod downmixMethod);\n    public void Dispose();\n}\n```\n**Description:** Used to configure the features of the `AudioProcessingModule` such as echo cancellation, noise suppression, gain control, etc.\n\n#### `StreamConfig` (Class)\n```csharp\npublic class StreamConfig : IDisposable\n{\n    public StreamConfig(int sampleRateHz, int numChannels);\n    public int SampleRateHz { get; }\n    public int NumChannels { get; }\n    public void Dispose();\n}\n```\n**Description:** Defines the properties (sample rate, number of channels) of an audio stream being processed by the APM.\n\n#### `ProcessingConfig` (Class)\nThis class holds multiple `StreamConfig` instances for different parts of the APM pipeline (input, output, reverse input, reverse output).\n\n#### `NoiseSuppressor` (Component - `SoundFlow.Extensions.WebRtc.Apm.Components`)\n```csharp\npublic class NoiseSuppressor : IDisposable\n{\n    public NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High, bool useMultichannelProcessing = false);\n    public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;\n    public float[] ProcessAll();\n    public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null);\n    public void Dispose();\n}\n```\n**Description:** A component for offline/batch noise suppression using WebRTC APM. It takes an `ISoundDataProvider`, processes its audio, and outputs the cleaned audio either as a whole or in chunks.\n**Key Members:**\n*   `NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, ...)`: Constructor.\n*   `OnAudioChunkProcessed` (event): Raised when a chunk of audio is processed.\n*   `ProcessAll()`: Processes the entire audio stream and returns it.\n*   `ProcessChunks()`: Processes audio in chunks, raising `OnAudioChunkProcessed`.\n\n#### `WebRtcApmModifier` (Modifier - `SoundFlow.Extensions.WebRtc.Apm.Modifiers`)\n```csharp\npublic sealed class WebRtcApmModifier : SoundModifier, IDisposable\n{\n    public WebRtcApmModifier(\n        bool aecEnabled = false, bool aecMobileMode = false, int aecLatencyMs = 40,\n        bool nsEnabled = false, NoiseSuppressionLevel nsLevel = NoiseSuppressionLevel.High,\n        // ... other AGC, HPF, PreAmp, Pipeline settings ...\n    );\n\n    public override string Name { get; set; }\n    public EchoCancellationSettings EchoCancellation { get; }\n    public NoiseSuppressionSettings NoiseSuppression { get; }\n    public AutomaticGainControlSettings AutomaticGainControl { get; }\n    public ProcessingPipelineSettings ProcessingPipeline { get; }\n    public bool HighPassFilterEnabled { get; set; }\n    public bool PreAmplifierEnabled { get; set; }\n    public float PreAmplifierGainFactor { get; set; }\n    public float PostProcessGain { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n    public void Dispose();\n}\n```\n**Description:** A `SoundModifier` that applies various WebRTC APM features (AEC, NS, AGC, HPF, PreAmp) to an audio stream in real-time.\n**Key Members:**\n*   Constructor with detailed initial settings.\n*   Properties for configuring each APM feature (`EchoCancellation`, `NoiseSuppression`, `AutomaticGainControl`, `ProcessingPipeline`, `HighPassFilterEnabled`, etc.).\n*   `Process(Span<float> buffer)`: Core processing logic.\n*   `Dispose()`: Releases native APM resources.\n\n#### Enums for WebRTC APM\n*   `ApmError`: Error codes.\n*   `NoiseSuppressionLevel`: Low, Moderate, High, VeryHigh.\n*   `GainControlMode`: AdaptiveAnalog, AdaptiveDigital, FixedDigital.\n*   `DownmixMethod`: AverageChannels, UseFirstChannel.\n*   `RuntimeSettingType`: Types for runtime APM settings.\n\n### Interfaces `ISoundDataProvider`\n\n```csharp\npublic interface ISoundDataProvider : IDisposable\n{\n    int Position { get; }\n    int Length { get; }\n    bool CanSeek { get; }\n    SampleFormat SampleFormat { get; }\n    int SampleRate { get; }\n    bool IsDisposed { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n    event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    int ReadBytes(Span<float> buffer);\n    void Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples).\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio data.\n*   `Dispose()`: Releases resources held by the data provider.\n\n### Interfaces `ISoundDecoder`\n\n```csharp\npublic interface ISoundDecoder : IDisposable\n{\n    bool IsDisposed { get; }\n    int Length { get; }\n    SampleFormat SampleFormat { get; }\n    int Channels { get; }\n    int SampleRate { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n\n    int Decode(Span<float> samples);\n    bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the decoder has been disposed.\n*   `Length`: The total length of the decoded audio data (in samples).\n*   `SampleFormat`: The sample format of the decoded audio data.\n*   `Channels`: The number of channels in the decoded audio data.\n*   `SampleRate`: The sample rate of the decoded audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n*   `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio stream.\n*   `Dispose()`: Releases the resources used by the decoder.\n\n### Interfaces `ISoundEncoder`\n\n```csharp\npublic interface ISoundEncoder : IDisposable\n{\n    bool IsDisposed { get; }\n\n    int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples.\n*   `Dispose()`: Releases the resources used by the encoder.\n\n\n### Interfaces `ISoundPlayer`\n\n```csharp\npublic interface ISoundPlayer\n{\n    PlaybackState State { get; }\n    bool IsLooping { get; set; }\n    float PlaybackSpeed { get; set; }\n    float Volume { get; set; }\n    float Time { get; }\n    float Duration { get; }\n    float LoopStartSeconds { get; }\n    float LoopEndSeconds { get; }\n    int LoopStartSamples { get; }\n    int LoopEndSamples { get; }\n\n    void Play();\n    void Pause();\n    void Stop();\n    bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    bool Seek(float time);\n    bool Seek(int sampleOffset);\n    void SetLoopPoints(float startTime, float? endTime = -1f);\n    void SetLoopPoints(int startSample, int endSample = -1);\n    void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `State`: The current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Whether looping is enabled or disabled (`get`, `set`).\n*   `PlaybackSpeed`: Gets or sets the playback speed. 1.0 is normal speed.\n*   `Volume`: Gets or sets the volume of the sound player (0.0 to 1.0 or higher for gain).\n*   `Time`: The current playback position (in seconds).\n*   `Duration`: The total duration of the audio (in seconds).\n*   `LoopStartSeconds`: Gets the configured loop start point in seconds.\n*   `LoopEndSeconds`: Gets the configured loop end point in seconds.  -1 indicates looping to the natural end.\n*   `LoopStartSamples`: Gets the configured loop start point in samples.\n*   `LoopEndSamples`: Gets the configured loop end point in samples. -1 indicates looping to the natural end.\n\n**Methods:**\n\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to the specified time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to the specified time (in seconds). Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset (in samples). Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures custom loop points using start and end times in seconds. `endTime` is optional;  use -1 or `null` to loop to the natural end.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures custom loop points using start and end sample indices. `endSample` is optional; use -1 to loop to the natural end.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures custom loop points using `TimeSpan`.\n\n### Interfaces `IVisualizationContext`\n\n```csharp\npublic interface IVisualizationContext\n{\n    void Clear();\n    void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f);\n    void DrawRectangle(float x, float y, float width, float height, Color color);\n}\n```\n\n**Methods:**\n\n*   `Clear()`: Clears the drawing surface.\n*   `DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)`: Draws a line from (`x1`, `y1`) to (`x2`, `y2`) with the specified color and thickness.\n*   `DrawRectangle(float x, float y, float width, float height, Color color)`: Draws a rectangle with the specified position, dimensions, and color.\n\n### Interfaces `IVisualizer`\n\n```csharp\npublic interface IVisualizer : IDisposable\n{\n    string Name { get; }\n\n    event EventHandler VisualizationUpdated;\n\n    void ProcessOnAudioData(Span<float> audioData);\n    void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes a chunk of audio data for visualization.\n*   `Render(IVisualizationContext context)`: Renders the visualization using the provided `IVisualizationContext`.\n*   `Dispose()`: Releases the resources used by the visualizer.\n\n### Modifiers `AlgorithmicReverbModifier`\n\n```csharp\npublic sealed class AlgorithmicReverbModifier : SoundModifier\n{\n    public AlgorithmicReverbModifier(AudioFormat format);\n\n    public override string Name { get; set; }\n    public float Wet { get; set; }\n    public float RoomSize { get; set; }\n    public float Damp { get; set; }\n    public float Width { get; set; }\n    public float PreDelay { get; set; }\n    public float Mix { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Description:** An algorithmic reverb modifier based on the Freeverb algorithm. Now supports multi-channel audio, applying distinct delay lines and modulation to create a wider spatial effect.\n\n**Constructor:** `AlgorithmicReverbModifier(AudioFormat format)`: Initializes the modifier for the specified audio format.\n\n**Properties:**\n\n*   `Damp`: The damping factor of the reverb.\n*   `Name`: The name of the modifier.\n*   `PreDelay`: The pre-delay time (in milliseconds).\n*   `RoomSize`: The simulated room size.\n*   `Wet`: The wet/dry mix of the reverb (0 = dry, 1 = wet).\n*   `Width`: The stereo width of the reverb.\n*   `Mix`: The mix level of the reverb.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the reverb effect.\n\n### Modifiers `BassBoostModifier`\n\n```csharp\npublic class BassBoostModifier : SoundModifier\n{\n    public BassBoostModifier(AudioFormat format, float cutoff, float boostGain);\n\n    public float Cutoff { get; set; }\n    public float BoostGain { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Cutoff`: The cutoff frequency below which the bass boost is applied.\n*   `BoostGain`: The gain applied to the bass boost in dB.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the bass boost effect.\n\n### Modifiers `ChorusModifier`\n\n```csharp\npublic sealed class ChorusModifier : SoundModifier\n{\n    public ChorusModifier(AudioFormat format, float depthMs = 2f, float rateHz = 0.5f, float feedback = 0.7f, float wetDryMix = 0.5f, float maxDelayMs = 50f);\n\n    public float DepthMs { get; set; }\n    public float RateHz { get; set; }\n    public float Feedback { get; } // Read-only\n    public float WetDryMix { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `DepthMs`: The depth of the chorus effect.\n*   `RateHz`: The rate of the chorus effect.\n*   `Feedback`: The feedback amount of the chorus effect.\n*   `WetDryMix`: The wet/dry mix of the chorus effect.\n*   `maxDelayMs`: The maximum delay length (in samples) used by the chorus effect.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the chorus effect.\n\n### Modifiers `CompressorModifier`\n\n```csharp\npublic class CompressorModifier : SoundModifier\n{\n    public CompressorModifier(AudioFormat format, float thresholdDb, float ratio, float attackMs, float releaseMs, float kneeDb = 0, float makeupGainDb = 0);\n\n    public float ThresholdDb { get; set; }\n    public float Ratio { get; set; }\n    public float AttackMs { get; set; }\n    public float ReleaseMs { get; set; }\n    public float KneeDb { get; set; }\n    public float MakeupGainDb { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `ThresholdDb`: The threshold (in dB) above which compression is applied.\n*   `Ratio`: The compression ratio.\n*   `AttackMs`: The attack time (in milliseconds).\n*   `ReleaseMs`: The release time (in milliseconds).\n*   `KneeDb`: The knee width (in dB).\n*   `MakeupGainDb`: The amount of makeup gain to apply after compression.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the compression effect.\n\n### Modifiers `DelayModifier`\n\n```csharp\npublic class DelayModifier : SoundModifier\n{\n    public DelayModifier(int delayLength, float feedback, float wetMix, float cutoffFrequency);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `delayLength`: The delay length (in samples).\n*   `feedback`: The feedback amount of the delay.\n*   `wetMix`: The wet/dry mix of the delay (0 = dry, 1 = wet).\n*   `cutoffFrequency`: The cutoff frequency for the low-pass filter applied to the delayed signal.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the delay effect.\n\n### Modifiers `FrequencyBandModifier`\n\n```csharp\npublic class FrequencyBandModifier : SoundModifier\n{\n    public FrequencyBandModifier(float lowCutoffFrequency, float highCutoffFrequency);\n\n    public float HighCutoffFrequency { get; set; }\n    public float LowCutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `HighCutoffFrequency`: The high cutoff frequency of the frequency band.\n*   `LowCutoffFrequency`: The low cutoff frequency of the frequency band.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the frequency band modification.\n\n### Modifiers `NoiseReductionModifier`\n\n```csharp\npublic class NoiseReductionModifier : SoundModifier\n{\n    public NoiseReductionModifier(int fftSize = 2048, float alpha = 3f, float beta = 0.001f, float smoothingFactor = 0.9f, float gain = 1.5f, int noiseFrames = 5, VoiceActivityDetector? vad = null);\n\n    public override string Name { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying noise reduction.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `NoiseReductionModifier` operates on buffers, not individual samples.\n\n### Modifiers `ParametricEqualizer`\n\n```csharp\npublic class ParametricEqualizer : SoundModifier\n{\n    public ParametricEqualizer(int channels);\n\n    public override string Name { get; set; }\n    \n    public void AddBand(EqualizerBand band);\n    public void AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth);\n    public void AddBands(IEnumerable<EqualizerBand> bands);\n    public void ClearBands();\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n    public void RemoveBand(EqualizerBand band);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `AddBand(EqualizerBand band)`: Adds an `EqualizerBand` to the equalizer.\n*   `AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth)`: Adds an equalizer band with the specified parameters.\n*   `AddBands(IEnumerable<EqualizerBand> bands)`: Adds multiple equalizer bands.\n*   `ClearBands()`: Removes all equalizer bands.\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying equalization.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `ParametricEqualizer` operates on buffers, not individual samples.\n*   `RemoveBand(EqualizerBand band)`: Removes a specific equalizer band.\n\n\n### Modifiers `MultiChannelChorusModifier`\n\n```csharp\npublic class MultiChannelChorusModifier : SoundModifier\n{\n    public MultiChannelChorusModifier(\n        AudioFormat format,\n        float wetMix,\n        int maxDelay,\n        params (float depth, float rate, float feedback)[] channelParameters);\n\n    public override void Process(Span<float> buffer, int channels);\n    public override float ProcessSample(float sample, int channel); // Throws NotImplementedException\n}\n```\n**Description:** A sound modifier that implements a multi-channel chorus effect, allowing for different chorus parameters (depth, rate, feedback) for each audio channel. This is useful for creating rich, spatial chorus effects.\n\n### Modifiers `TrebleBoostModifier`\n\n```csharp\npublic class TrebleBoostModifier : SoundModifier\n{\n    public TrebleBoostModifier(float cutoffFrequency);\n\n    public float CutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency above which the treble boost is applied.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the treble boost effect.\n\n### Providers `AssetDataProvider`\n\n```csharp\npublic sealed class AssetDataProvider : ISoundDataProvider\n{\n    public AssetDataProvider(AudioEngine engine, AudioFormat format, Stream stream);\n    public AssetDataProvider(AudioEngine engine, AudioFormat format, byte[] data);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; }\n    public int SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; private set; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public void Dispose();\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (always true for `AssetDataProvider`).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the provider.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio data.\n\n### Providers `StreamDataProvider`\n\n```csharp\npublic sealed class StreamDataProvider : ISoundDataProvider\n{\n    public StreamDataProvider(AudioEngine engine, AudioFormat format, Stream stream);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; private set; }\n    public int SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the stream into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio stream (if supported).\n*   `Dispose()`: Releases resources used by the provider.\n\n### Providers `MicrophoneDataProvider`\n\n```csharp\npublic class MicrophoneDataProvider : ISoundDataProvider\n{\n    public MicrophoneDataProvider(AudioDevice captureDevice, int bufferSize = 8);\n\n    public int Position { get; private set; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void StartCapture();\n    public void StopCapture();    \n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the captured audio data (in samples).\n*   `Length`: Returns -1, indicating an unknown length for the live microphone stream.\n*   `CanSeek`: Returns `false` because seeking is not supported for live microphone input.\n*   `SampleFormat`: The sample format of the captured audio data, which matches the `AudioEngine`'s sample format.\n*   `SampleRate`: The sample rate of the captured audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when `StopCapture()` is called, signaling the end of the microphone input stream.\n*   `PositionChanged`: Raised after reading data, indicating that the read position has changed.\n\n**Methods:**\n\n*   `MicrophoneDataProvider(int bufferSize = 8, int? sampleRate = null)`: Constructor that initializes the `MicrophoneDataProvider`. It sets the buffer queue size (default is 8), the sample rate (defaults to the `AudioEngine`'s sample rate), and subscribes to the `AudioEngine.OnAudioProcessed` event to capture audio data.\n    * `bufferSize`: The number of audio sample arrays to hold in internal queue. Higher values will lead to higher latency but will be more resilient to performance spikes.\n    * `sampleRate`: The sample rate of the microphone, will use the audioEngine sample rate if not set.\n*   `StartCapture()`: Starts capturing audio data from the microphone.\n*   `StopCapture()`: Stops capturing audio data and raises the `EndOfStreamReached` event.\n*   `ReadBytes(Span<float> buffer)`: Reads captured audio samples into the provided buffer. If not enough data is available in the queue it will fill the rest of the buffer with silence.\n*   `Seek(int offset)`: Throws `NotSupportedException` because seeking is not supported for live microphone input.\n*   `Dispose()`: Releases resources used by the `MicrophoneDataProvider`, unsubscribes from the `AudioEngine.OnAudioProcessed` event, and clears the internal buffer queue.\n\n### Providers `ChunkedDataProvider`\n\n```csharp\npublic sealed class ChunkedDataProvider : ISoundDataProvider\n{\n    public ChunkedDataProvider(AudioEngine engine, AudioFormat format, Stream stream, int chunkSize = DefaultChunkSize);\n    public ChunkedDataProvider(AudioEngine engine, AudioFormat format, string filePath, int chunkSize = DefaultChunkSize);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data in samples.\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a `Stream`. It creates an `ISoundDecoder` to decode the stream, sets the default chunk size (220500 samples per channel, which is 10 seconds at 44.1 kHz), and starts prefetching data.\n*   `ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a file path. It opens a `FileStream` and calls the other constructor.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is prefilled with decoded audio. If the buffer runs out, it decodes another chunk from the stream.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported by the stream and decoder). It disposes of the current decoder, creates a new one, and seeks within the stream. Then, it clears the internal buffer and refills it from the new position.\n*   `Dispose()`: Releases the resources used by the `ChunkedDataProvider`, including the decoder and the stream.\n\n**Remarks:**\n\nThe `ChunkedDataProvider` is designed to handle large audio files efficiently by reading and decoding them in chunks. This prevents the entire file from being loaded into memory at once. The default chunk size is set to 10 seconds of audio at 44.1 kHz, but you can adjust this value in the constructor. The class uses an internal buffer (`Queue<float>`) to store decoded audio samples and prefetches data in the background.\n\n### Providers `QueueDataProvider`\n\n```csharp\npublic enum QueueFullBehavior\n{\n    /// <summary>\n    ///     Throw an <see cref=\"InvalidOperationException\"/> when the queue is full. This is the default behavior.\n    /// </summary>\n    Throw,\n\n    /// <summary>\n    ///     Block the calling thread until space becomes available in the queue.\n    /// </summary>\n    Block,\n\n    /// <summary>\n    ///     Silently drop the incoming samples and return immediately.\n    /// </summary>\n    Drop\n}\n\npublic class QueueDataProvider : ISoundDataProvider\n{\n    public QueueDataProvider(AudioFormat format, int? maxSamples = null, QueueFullBehavior fullBehavior = QueueFullBehavior.Throw);\n\n    public int SamplesAvailable { get; }\n    public long TotalSamplesEnqueued { get; }\n    public bool IsDisposed { get; }\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void AddSamples(ReadOnlySpan<float> samples);\n    public void Reset();\n    public void CompleteAdding();\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from an in-memory queue that is fed samples externally. This provider is ideal for scenarios where audio data is generated or received in chunks. It supports configurable behavior for when the queue becomes full, specified by the `QueueFullBehavior` enum.\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: Always returns -1, indicating the total length is unknown as it's a dynamic queue.\n*   `CanSeek`: Always returns `false`, as seeking is not supported by this provider.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n*   `SamplesAvailable`: Gets the number of samples currently available in the queue.\n*   `TotalSamplesEnqueued`: Gets the total number of samples that have been enqueued into the provider since its creation or last reset.\n*   `IsDisposed`: Gets a value indicating whether the provider has been disposed.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached, which occurs when `CompleteAdding()` has been called and all samples have been read from the queue.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `AddSamples(ReadOnlySpan<float> samples)`: Adds audio samples to the internal queue. The behavior when the queue is full is determined by the `QueueFullBehavior` set during construction:\n*   If `QueueFullBehavior.Throw` (default), an `InvalidOperationException` is thrown if adding samples would exceed the maximum size.\n*   If `QueueFullBehavior.Block`, the calling thread will block until space becomes available in the queue for the entire block of samples.\n*   If `QueueFullBehavior.Drop`, the incoming samples are silently discarded.\n*   Throws `InvalidOperationException` if called after `CompleteAdding()` has been invoked.\n*   `Reset()`: Resets the provider to its initial state, clearing the sample queue, resetting the position, and allowing samples to be added again. Any threads previously blocked in `AddSamples` (if `QueueFullBehavior.Block` was active) will be unblocked.\n*   `CompleteAdding()`: Marks that no more samples will be added to the queue. Once called, subsequent calls to `AddSamples` will throw an `InvalidOperationException`. The `EndOfStreamReached` event will be raised when all remaining samples in the queue have been read.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the internal queue into the provided buffer. Returns the number of samples read. If the queue is empty, it returns 0. If `CompleteAdding()` has been called and the queue becomes empty, the `EndOfStreamReached` event will be fired.\n*   `Seek(int sampleOffset)`: Throws an `InvalidOperationException` because seeking is not supported by this queue-based provider.\n*   `Dispose()`: Releases the resources used by the `QueueDataProvider`, clears the sample queue, and unblocks any waiting threads.\n\n### Providers `NetworkDataProvider`\n\n```csharp\npublic sealed class NetworkDataProvider : ISoundDataProvider\n{\n    public NetworkDataProvider(AudioEngine engine, AudioFormat format, string url);\n\n    public int Position { get; }\n    public int Length { get; private set; }\n    public bool CanSeek { get; private set; }\n    public SampleFormat SampleFormat { get; private set; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Returns -1 for HLS streams without an `#EXT-X-ENDLIST` tag, indicating an unknown or continuously growing length.\n*   `CanSeek`: Indicates whether seeking is supported. It's `true` for direct audio URLs if the server supports range requests and for HLS streams with an `#EXT-X-ENDLIST` tag; otherwise, it's `false`.\n*   `SampleFormat`: The format of the audio samples. Determined after the initial connection to the stream.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `NetworkDataProvider(string url, int? sampleRate = null)`: Constructor that initializes the `NetworkDataProvider` with a network URL. It determines whether the URL points to a direct audio file or an HLS playlist.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is filled asynchronously.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported). The behavior differs for direct URLs and HLS streams:\n    *   **Direct URLs:** Performs an HTTP range request to fetch data starting from the desired offset.\n    *   **HLS Streams:** Locates the HLS segment containing the desired time offset and starts downloading from that segment.\n*   `Dispose()`: Releases the resources used by the `NetworkDataProvider`, including the `HttpClient`, decoder, and stream.\n\n**Remarks:**\n\nThe `NetworkDataProvider` can handle both direct audio URLs and HLS (HTTP Live Streaming) playlists. It automatically detects the stream type and behaves accordingly.\n\n**Direct Audio URLs:**\n\n*   It uses `HttpClient` to make requests to the URL.\n*   It supports seeking if the server responds with an \"Accept-Ranges: bytes\" header.\n*   It creates an `ISoundDecoder` to decode the audio stream.\n*   It buffers audio data asynchronously in a background thread.\n\n**HLS Playlists:**\n\n*   It downloads and parses the M3U(8) playlist file.\n*   It identifies the individual media segments (e.g., `.ts` files).\n*   It downloads and decodes segments sequentially.\n*   It refreshes the playlist periodically for live streams.\n*   It supports seeking by selecting the appropriate segment based on the desired time offset.\n*   It determines whether the playlist has a defined end by checking for the `#EXT-X-ENDLIST` tag, which affects whether `Length` is known and `CanSeek` is true.\n\nThe class uses an internal `Queue<float>` to buffer audio samples. The `ReadBytes` method waits for data to become available in the buffer if it's empty.\n\n### Providers `RawDataProvider`\n\n```csharp\npublic sealed class RawDataProvider : ISoundDataProvider, IDisposable\n{\n    public RawDataProvider(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from a stream containing raw PCM audio data.\n**Properties:**\n*   `Position`: The current read position in samples.\n*   `Length`: The total length of the stream in samples.\n*   `CanSeek`: Indicates if the underlying stream is seekable.\n*   `SampleFormat`: The sample format of the raw audio data.\n*   `SampleRate`: The sample rate of the raw audio data.\n    **Events:**\n*   `EndOfStreamReached`: Raised when the end of the stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n    **Methods:**\n*   `ReadBytes(Span<float> buffer)`: Reads raw PCM data from the stream and converts it to `float` if necessary.\n*   `Seek(int offset)`: Seeks to the specified offset in the underlying stream if `CanSeek` is true.\n*   `Dispose()`: Disposes the underlying stream.\n\n\n### Structs\n\n### `AudioFormat`\n```csharp\npublic record struct AudioFormat\n{\n    public SampleFormat Format;\n    public int Channels;\n    public int SampleRate;\n    public float InverseSampleRate { get; }\n\n    public static readonly AudioFormat Cd;\n    public static readonly AudioFormat Dvd;\n    public static readonly AudioFormat DvdHq;\n    public static readonly AudioFormat Studio;\n    public static readonly AudioFormat StudioHq;\n    public static readonly AudioFormat Broadcast;\n    public static readonly AudioFormat Telephony;\n}\n```\n**Description:** A record struct representing the format of an audio stream, including sample format, channel count, and sample rate. It is a value type and provides several common presets.\n\n**Fields:**\n\n*   `Format`: The sample format (e.g., `SampleFormat.S16`, `SampleFormat.F32`).\n*   `Channels`: The number of audio channels (e.g., 1 for mono, 2 for stereo).\n*   `SampleRate`: The sample rate in Hertz (e.g., 44100, 48000).\n\n**Properties:**\n\n*   `InverseSampleRate`: Gets the inverse of the sample rate, useful for calculations involving sample duration.\n\n**Static Presets:**\n\n*   `Cd`: Standard Compact Disc (CD) audio format (S16, 2 Channels, 44100 Hz).\n*   `Dvd`: Standard DVD-Video audio format (S16, 2 Channels, 48000 Hz).\n*   `DvdHq`: High-quality DVD-Video audio format (F32, 2 Channels, 48000 Hz).\n*   `Studio`: Common studio recording format (S24, 2 Channels, 96000 Hz).\n*   `StudioHq`: High-quality studio recording format (F32, 2 Channels, 96000 Hz).\n*   `Broadcast`: Standard broadcast audio format (S16, 1 Channel, 48000 Hz).\n*   `Telephony`: Telephony and VoIP audio format (U8, 1 Channel, 8000 Hz).\n\n#### `DeviceInfo`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct DeviceInfo\n{\n    public IntPtr Id;\n    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 256)]\n    public string Name;\n    [MarshalAs(UnmanagedType.U1)]\n    public bool IsDefault;\n    public uint NativeDataFormatCount;\n    public IntPtr NativeDataFormats; // Pointer to an array of NativeDataFormat\n}\n```\n**Description:** Represents information about an audio device, including its native ID, name, whether it's the default system device, and a count/pointer to its supported native data formats.\n\n#### `NativeDataFormat`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct NativeDataFormat\n{\n    public SampleFormat Format;\n    public uint Channels;\n    public uint SampleRate;\n    public uint Flags;\n}\n```\n**Description:** Represents a specific native data format (sample format, channels, sample rate) supported by an audio device. Accessed via the `NativeDataFormats` pointer in `DeviceInfo`.\n\n### Utils `Extensions`\n\n```csharp\npublic static class Extensions\n{\n    public static int GetBytesPerSample(this SampleFormat sampleFormat);\n    public static unsafe Span<T> GetSpan<T>(nint ptr, int length) where T : unmanaged;\n    public static T[] ReadArray<T>(this nint pointer, int count) where T : struct;\n}\n```\n**Methods:**\n*   `GetBytesPerSample(this SampleFormat sampleFormat)`: Returns the number of bytes per sample for a given sample format.\n*   `GetSpan<T>(nint ptr, int length)`: Creates a span of type `T` from a native memory pointer.\n*   `ReadArray<T>(this nint pointer, int count) where T : struct`: Reads an array of structures of type `T` from a native memory pointer.\n\n### `MathHelper`\n```csharp\npublic static class MathHelper\n{\n    public static bool EnableAvx { get; set; }\n    public static bool EnableSse { get; set; }\n\n    public static void InverseFft(Complex[] data);\n    public static void Fft(Complex[] data);\n\n    public static float[] HammingWindow(int size);\n    public static float[] HanningWindow(int size);\n\n    public static float Lerp(float a, float b, float t);\n    public static bool IsPowerOfTwo(long n);\n    public static double Mod(this double x, double y);\n    public static float PrincipalAngle(float angle);\n}\n```\n**Description:** Provides static helper methods for common mathematical operations, with a particular focus on signal processing functions like Fast Fourier Transforms (FFT/IFFT) and windowing functions. These methods leverage SIMD (Single Instruction, Multiple Data) instructions (AVX and SSE) for optimized performance on compatible hardware, with fallbacks to scalar implementations.\n\n**Properties:**\n\n*   `EnableAvx`: Gets or sets a value indicating whether to use AVX (Advanced Vector Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of AVX, forcing a fallback to SSE or scalar code.\n*   `EnableSse`: Gets or sets a value indicating whether to use SSE (Streaming SIMD Extensions) instructions for operations if the hardware supports them. Defaults to `true`. Setting this to `false` will prevent the use of SSE, forcing a fallback to scalar code. This also affects AVX routines that may utilize SSE for specific operations.\n\n**Methods:**\n\n*   `InverseFft(Complex[] data)`: Computes the Inverse Fast Fourier Transform (IFFT) of a complex array. It leverages the `Fft` method internally after conjugating the input and then scales and conjugates the output.\n*   `Fft(Complex[] data)`: Computes the Fast Fourier Transform (FFT) of a complex array. The length of the `data` array must be a power of two. This method automatically selects the most optimized implementation available:\n*   Prioritizes AVX (Advanced Vector Extensions) if `EnableAvx` is `true` and hardware supports it.\n*   Falls back to SSE (Streaming SIMD Extensions) if AVX is not used, `EnableSse` is `true`, and hardware supports SSE3.\n*   Uses a scalar (non-SIMD) implementation if neither AVX nor SSE is available or enabled.\n*   **Throws:** `ArgumentException` if the `data` array length is not a power of two.\n*   `HammingWindow(int size)`: Generates a Hamming window of the specified `size`. Utilizes SIMD (AVX or SSE) acceleration for performance if `EnableAvx` or `EnableSse` is `true` and hardware supports the necessary instructions (SSE4.1 for `Floor` intrinsic), otherwise falls back to a scalar implementation.\n*   `HanningWindow(int size)`: Generates a Hanning window of the specified `size`. Similar to `HammingWindow`, it uses SIMD acceleration (AVX or SSE) if enabled and supported, otherwise falls back to a scalar implementation.\n*   `Lerp(float a, float b, float t)`: Performs linear interpolation between two float values `a` and `b`. The interpolation factor `t` is clamped between 0 and 1, ensuring the result is always between `a` and `b`.\n*   `IsPowerOfTwo(long n)`: Checks if a given long integer `n` is a power of two (e.g., 2, 4, 8, 16, etc.). Returns `true` if `n` is positive and is a power of two, `false` otherwise.\n*   `Mod(this double x, double y)`: An extension method for `double` that returns the remainder of the division of `x` by `y`, ensuring the result is always in the range `[0, y)`. This differs from the standard `%` operator for negative `x`.\n*   `PrincipalAngle(float angle)`: Calculates the principal angle for a given `angle` in radians, ensuring the result lies within the range `[-PI, PI)`.\n\n### Visualization `LevelMeterAnalyzer`\n\n```csharp\npublic class LevelMeterAnalyzer : AudioAnalyzer\n{\n    public LevelMeterAnalyzer(AudioFormat format, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public float Peak { get; }\n    public float Rms { get; }\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Peak`: The peak level of the audio signal.\n*   `Rms`: The RMS (root mean square) level of the audio signal.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to calculate the RMS and peak levels.\n\n### Visualization `LevelMeterVisualizer`\n\n```csharp\npublic class LevelMeterVisualizer : IVisualizer\n{\n    public LevelMeterVisualizer(LevelMeterAnalyzer levelMeterAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public Color PeakHoldColor { get; set; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the level meter bar.\n*   `Name`: The name of the visualizer.\n*   `PeakHoldColor`: The color of the peak hold indicator.\n*   `Size`: The size of the level meter.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the level meter.\n*   `Render(IVisualizationContext context)`: Renders the level meter visualization.\n\n### Visualization `SpectrumAnalyzer`\n\n```csharp\npublic class SpectrumAnalyzer : AudioAnalyzer\n{\n    public SpectrumAnalyzer(AudioFormat format, int fftSize, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public ReadOnlySpan<float> SpectrumData { get; }\n\n    protected override void Analyze(Span<float> buffer, int channels);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `SpectrumData`: The calculated frequency spectrum data.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer, int channels)`: Analyzes the audio buffer to compute the frequency spectrum using an FFT.\n\n### Visualization `SpectrumVisualizer`\n\n```csharp\npublic class SpectrumVisualizer : IVisualizer\n{\n    public SpectrumVisualizer(SpectrumAnalyzer spectrumAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the spectrum bars.\n*   `Name`: The name of the visualizer.\n*   `Size`: The size of the spectrum visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the spectrum.\n*   `Render(IVisualizationContext context)`: Renders the spectrum visualization.\n\n### Visualization `WaveformVisualizer`\n\n```csharp\npublic class WaveformVisualizer : IVisualizer\n{\n    public WaveformVisualizer();\n\n    public string Name { get; }\n    public List<float> Waveform { get; }\n    public Color WaveformColor { get; set; }\n    public Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n*   `Waveform`: The waveform data.\n*   `WaveformColor`: The color of the waveform.\n*   `Size`: The size of the waveform visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the waveform.\n*   `Render(IVisualizationContext context)`: Renders the waveform visualization."
  },
  {
    "id": 6,
    "slug": "advanced-topics",
    "version": "1.2.0",
    "title": "Advanced Topics",
    "description": "Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.",
    "navOrder": 6,
    "category": "Core",
    "content": "---\nid: 6\ntitle: Advanced Topics\ndescription: Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.\nnavOrder: 6\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\nThis section delves into more advanced topics related to SoundFlow, including extending the engine with custom components, optimizing performance, and understanding threading considerations.\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Advanced Topics\">\n    <Tab\n        key=\"extending\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ph:puzzle-piece-bold' />\n                <span>Extending SoundFlow</span>\n            </div>\n        }\n    >\n        ## Extending SoundFlow\n\n        One of SoundFlow's key strengths is its extensibility. You can tailor the engine to your specific needs by creating custom:\n\n        *   **Sound Components (`SoundComponent`)**\n        *   **Sound Modifiers (`SoundModifier`)**\n        *   **Audio Analyzers (`AudioAnalyzer`)** and **Visualizers (`IVisualizer`)**\n        *   **Audio Backends (`AudioEngine`)**\n        *   **Sound Data Providers (`ISoundDataProvider`)**\n        *   **Extensions (e.g., for specific DSP libraries)**: SoundFlow supports integration with external audio processing libraries. For instance, the `SoundFlow.Extensions.WebRtc.Apm` package provides features like noise suppression and echo cancellation by wrapping the WebRTC Audio Processing Module. You can create similar extensions for other libraries.\n\n        ### Custom Sound Components\n\n        Creating custom `SoundComponent` classes allows you to implement unique audio processing logic and integrate it seamlessly into the SoundFlow audio graph.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundComponent\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundComponent` class. Your constructor must accept `AudioEngine` and `AudioFormat` parameters and pass them to the base constructor.\n            </Step>\n            <Step title=\"Implement GenerateAudio\" icon='lucide:audio-lines'>\n                Override the `GenerateAudio(Span<float> buffer, int channels)` method. This is where you'll write the core audio processing code for your component. The `buffer` passed to this method already contains the mixed output from all of the component's inputs.\n                *   If your component **generates** new audio (e.g., an oscillator), it should add its generated samples to the `buffer`.\n                *   If your component **modifies** incoming audio, it should process the samples within the `buffer` in-place.\n            </Step>\n            <Step title=\"Override other methods (optional)\" icon='icon-park-outline:switch-one'>\n                You can override methods like `ConnectInput`, `AddAnalyzer`, `AddModifier`, etc., to customize how your component interacts with the audio graph.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your component to expose configurable parameters that users can adjust.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Structs;\n        using System;\n\n        public class CustomGainComponent : SoundComponent\n        {\n            public float Gain { get; set; } = 1.0f; // Default gain\n\n            public override string Name { get; set; } = \"Custom Gain\";\n            \n            // The constructor must match the base class requirements.\n            public CustomGainComponent(AudioEngine engine, AudioFormat format) : base(engine, format) { }\n\n            protected override void GenerateAudio(Span<float> buffer, int channels)\n            {\n                // The buffer already contains mixed audio from any connected inputs.\n                // We simply modify it in-place.\n                for (int i = 0; i < buffer.Length; i++)\n                {\n                    buffer[i] *= Gain;\n                }\n            }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System.Linq;\n        \n        // 1. Create an instance of an audio engine.\n        using var engine = new MiniAudioEngine();\n\n        // 2. Define the desired audio format.\n        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\n\n        // 3. Get the default playback device info from the engine.\n        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\n\n        // 4. Initialize a playback device.\n        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);\n\n        // 5. Create the player and your custom component, passing the engine and format.\n        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(engine, audioFormat, dataProvider);\n        var gainComponent = new CustomGainComponent(engine, audioFormat) { Gain = 0.5f };\n\n        // 6. Connect the player as an input to the gain component.\n        gainComponent.ConnectInput(player);\n        \n        // 7. Add the final component in the chain to the device's master mixer.\n        device.MasterMixer.AddComponent(gainComponent);\n        \n        // 8. Start the device and play the sound.\n        device.Start();\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Sound Modifiers\n\n        Custom `SoundModifier` classes allow you to implement your own audio effects.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundModifier\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundModifier` class.\n            </Step>\n            <Step title=\"Implement ProcessSample\" icon='icon-park-outline:sound-wave'>\n                Implement `ProcessSample(float sample, int channel)` (or override `Process` for buffer-level):\n                *   `ProcessSample(float sample, int channel)`: This method takes a single audio sample and the channel index as input and returns the modified sample.\n                *   `Process(Span<float> buffer, int channels)`: Override this for more complex effects that operate on entire buffers (e.g., FFT-based effects). By default, it calls `ProcessSample` for each sample.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your modifier to expose configurable parameters.\n            </Step>\n            <Step title=\"Use 'Enabled' property\" icon='material-symbols:toggle-on-outline'>\n                Your modifier will have an `Enabled` property (defaulting to `true`) to allow toggling its effect.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using System;\n\n        public class CustomDistortionModifier : SoundModifier\n        {\n            public float Threshold { get; set; } = 0.5f;\n\n            public override string Name { get; set; } = \"Custom Distortion\";\n\n            public override float ProcessSample(float sample, int channel)\n            {\n                // Simple hard clipping distortion\n                if (sample > Threshold)\n                {\n                    return Threshold;\n                }\n                else if (sample < -Threshold)\n                {\n                    return -Threshold;\n                }\n                else\n                {\n                    return sample;\n                }\n            }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Providers;\n        using SoundFlow.Structs;\n        using System.Linq;\n\n        // 1. Create an engine and define the audio format.\n        using var engine = new MiniAudioEngine();\n        var audioFormat = new AudioFormat { Format = SampleFormat.F32, SampleRate = 48000, Channels = 2 };\n        \n        // 2. Get device info and initialize a playback device.\n        var deviceInfo = engine.PlaybackDevices.FirstOrDefault(d => d.IsDefault);\n        using var device = engine.InitializePlaybackDevice(deviceInfo, audioFormat);\n        \n        // 3. Create the data provider and player.\n        using var dataProvider = new StreamDataProvider(engine, audioFormat, File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(engine, audioFormat, dataProvider);\n\n        // 4. Create an instance of your custom modifier.\n        var distortion = new CustomDistortionModifier { Threshold = 0.7f };\n        // distortion.Enabled = false; // To disable it\n\n        // 5. Add the modifier to a SoundComponent (like a player).\n        player.AddModifier(distortion);\n        \n        // 6. Add the player to the device's master mixer.\n        device.MasterMixer.AddComponent(player);\n        \n        // 7. Start the device and play.\n        device.Start();\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Visualizers\n\n        Custom `IVisualizer` classes allow you to create unique visual representations of audio data.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement IVisualizer\" icon='ph:plugs-connected-bold'>\n                Create a new class that implements the `IVisualizer` interface.\n            </Step>\n            <Step title=\"Implement ProcessOnAudioData\" icon='carbon:data-vis-4'>\n                This method receives a `Span<float>` containing audio data. You should process this data and store the relevant information needed for rendering.\n            </Step>\n            <Step title=\"Implement Render\" icon='material-symbols:draw-outline'>\n                This method receives an `IVisualizationContext`. Use the drawing methods provided by the context (e.g., `DrawLine`, `DrawRectangle`) to render your visualization.\n            </Step>\n            <Step title=\"Raise VisualizationUpdated\" icon='mdi:bell-ring-outline'>\n                When the visualization data changes (e.g., after processing new audio data), raise the `VisualizationUpdated` event to notify the UI to update the display.\n            </Step>\n            <Step title=\"Implement Dispose\" icon='material-symbols:delete-outline'>\n                Release any unmanaged resources or unsubscribe from events.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Interfaces;\n        using System;\n        using System.Numerics;\n\n        public class CustomBarGraphVisualizer : IVisualizer\n        {\n            private float _level;\n\n            public string Name => \"Custom Bar Graph\";\n\n            public event EventHandler? VisualizationUpdated;\n\n            public void ProcessOnAudioData(Span<float> audioData)\n            {\n                if (audioData.IsEmpty) return;\n                // Calculate the average level (simplified for this example)\n                float sum = 0;\n                for (int i = 0; i < audioData.Length; i++)\n                {\n                    sum += Math.Abs(audioData[i]);\n                }\n                _level = sum / audioData.Length;\n\n                // Notify that the visualization needs to be updated\n                VisualizationUpdated?.Invoke(this, EventArgs.Empty);\n            }\n\n            public void Render(IVisualizationContext context)\n            {\n                // Clear the drawing area\n                context.Clear();\n\n                // Draw a simple bar graph based on the calculated level\n                float barHeight = _level * 200; // Scale the level for visualization\n                context.DrawRectangle(10, 200 - barHeight, 30, barHeight, new Color(0, 1, 0));\n            }\n\n            public void Dispose()\n            {\n                // Unsubscribe from events, release resources if any\n                VisualizationUpdated = null;\n            }\n        }\n        ```\n\n        ### Adding Audio Backends\n\n        SoundFlow is designed to support multiple audio backends. Currently, it includes a `MiniAudio` backend. You can add support for other audio APIs (e.g., WASAPI, ASIO, CoreAudio) by creating a new backend.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from AudioEngine\" icon='ph:engine-bold'>\n                Create a new class that inherits from the abstract `AudioEngine`. This class will manage the entire lifecycle of your custom backend.\n            </Step>\n            <Step title=\"Implement Abstract Methods\" icon='material-symbols:function'>\n                Implement all the abstract methods from `AudioEngine`:\n                *   `InitializeBackend()` and `CleanupBackend()`: Handle global setup/teardown of your native backend's context.\n                *   `InitializePlaybackDevice()`, `InitializeCaptureDevice()`, `InitializeFullDuplexDevice()`, `InitializeLoopbackDevice()`: These methods are the core of device management. You will implement the logic to initialize a device using your backend's API and return an instance of a class that inherits from the appropriate abstract device class.\n                *   `SwitchDevice(...)` (3 overloads): Implement logic to tear down an old device and initialize a new one while preserving the state (audio graph, event listeners).\n                *   `CreateEncoder(...)` and `CreateDecoder(...)`: Return your backend-specific implementations of the `ISoundEncoder` and `ISoundDecoder` interfaces.\n                *   `UpdateDevicesInfo()`: Implement logic to enumerate playback and capture devices using the backend's API, populating `PlaybackDevices` and `CaptureDevices`.\n            </Step>\n            <Step title=\"Create Device Wrappers\" icon='ph:package-bold'>\n                Create concrete classes inheriting from `AudioPlaybackDevice` and `AudioCaptureDevice`. These will wrap the native device handles and logic specific to your backend, including the audio callback that drives the processing graph.\n            </Step>\n            <Step title=\"Implement Codec Interfaces\" icon='mdi:file-code-outline'>\n                Create classes that implement `ISoundEncoder` and `ISoundDecoder` to handle audio encoding and decoding for your chosen backend.\n            </Step>\n        </Steps>\n\n        **Example (Skeleton):**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Abstracts.Devices;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Enums;\n        using SoundFlow.Structs;\n        using System;\n        using System.IO;\n        \n        // Custom Backend Engine\n        public class MyNewAudioEngine : AudioEngine\n        {\n            private nint _context; // Example native context handle\n            \n            // The parameterless constructor calls InitializeBackend() automatically.\n            public MyNewAudioEngine() { }\n\n            protected override void InitializeBackend()\n            {\n                // _context = NativeApi.InitContext();\n                // UpdateDevicesInfo(); // Initial device enumeration\n                Console.WriteLine(\"MyNewAudioEngine: Backend initialized.\");\n            }\n\n            protected override void CleanupBackend()\n            {\n                // NativeApi.UninitContext(_context);\n                Console.WriteLine(\"MyNewAudioEngine: Backend cleaned up.\");\n            }\n            \n            public override AudioPlaybackDevice InitializePlaybackDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)\n            {\n                // Create and return your backend-specific playback device wrapper\n                return new MyNewPlaybackDevice(this, _context, deviceInfo, format, config);\n            }\n\n            public override AudioCaptureDevice InitializeCaptureDevice(DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config = null)\n            {\n                // Similar logic for capture devices...\n                throw new NotImplementedException();\n            }\n\n            // Implement other abstract methods...\n            public override FullDuplexDevice InitializeFullDuplexDevice(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo, AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override AudioCaptureDevice InitializeLoopbackDevice(AudioFormat format, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, AudioFormat format) => throw new NotImplementedException();\n            public override ISoundDecoder CreateDecoder(Stream stream, AudioFormat format) => throw new NotImplementedException();\n            public override void UpdateDevicesInfo() { /* NativeApi.GetDevices(...); */ }\n            public override AudioPlaybackDevice SwitchDevice(AudioPlaybackDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override AudioCaptureDevice SwitchDevice(AudioCaptureDevice oldDevice, DeviceInfo newDeviceInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n            public override FullDuplexDevice SwitchDevice(FullDuplexDevice oldDevice, DeviceInfo? newPlaybackInfo, DeviceInfo? newCaptureInfo, DeviceConfig? config = null) => throw new NotImplementedException();\n        }\n\n        // Custom Playback Device Wrapper\n        public class MyNewPlaybackDevice : AudioPlaybackDevice\n        {\n            private nint _deviceHandle;\n            \n            public MyNewPlaybackDevice(AudioEngine engine, nint context, DeviceInfo? deviceInfo, AudioFormat format, DeviceConfig? config)\n                : base(engine, format, config ?? new MyDeviceConfig())\n            {\n                // _deviceHandle = NativeApi.InitDevice(context, deviceInfo?.Id, OnAudioCallback);\n                // this.Info = ... // Populate DeviceInfo from native API\n                this.Capability = Capability.Playback;\n            }\n\n            public override void Start() { /* NativeApi.StartDevice(_deviceHandle); */ IsRunning = true; }\n            public override void Stop() { /* NativeApi.StopDevice(_deviceHandle); */ IsRunning = false; }\n            public override void Dispose() \n            {\n                if (IsDisposed) return;\n                Stop();\n                // NativeApi.UninitDevice(_deviceHandle);\n                OnDisposedHandler();\n                IsDisposed = true;\n            }\n            \n            // This callback is invoked by the native backend on the audio thread.\n            private void OnAudioCallback(Span<float> buffer)\n            {\n                // 1. Clear the buffer\n                buffer.Clear();\n                // 2. Process the master mixer or a soloed component\n                var soloed = Engine.GetSoloedComponent();\n                if (soloed != null)\n                    soloed.Process(buffer, Format.Channels);\n                else\n                    MasterMixer.Process(buffer, Format.Channels);\n            }\n        }\n        \n        // Custom Device Configuration (Optional)\n        public class MyDeviceConfig : DeviceConfig { /* ... */ }\n        ```\n    </Tab>\n\n    <Tab\n        key=\"performance\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ic:round-speed' />\n                <span>Performance Optimization</span>\n            </div>\n        }\n    >\n        ## Performance Optimization\n\n        Here are some tips for optimizing the performance of your SoundFlow applications:\n\n        *   **Buffer Sizes:** Choose appropriate buffer sizes for your use case. Smaller buffers reduce latency but increase CPU overhead. Larger buffers can improve efficiency but may introduce latency. With the `MiniAudio` backend, you can specify this via `MiniAudioDeviceConfig` when initializing a device.\n        *   **SIMD:** SoundFlow uses SIMD instructions (when available) in critical paths like the `Mixer`, `SoundComponent` (for volume and panning), `MathHelper` (for FFTs and windowing), and in the `DeviceBufferHelper` for audio format conversions. Ensure your target platform supports SIMD for the best performance.\n        *   **Profiling:** Use a profiler (like the one built into Visual Studio) to identify performance bottlenecks in your audio processing pipeline.\n        *   **Asynchronous Operations:** For long-running operations (e.g., loading large files, network requests in `NetworkDataProvider`, project saving/loading), use asynchronous programming (`async` and `await`) to avoid blocking the main thread or the audio thread.\n        *   **Avoid Allocations:** Minimize memory allocations within the `GenerateAudio` method of `SoundComponent` and the `ProcessSample` or `Process` method of `SoundModifier`. SoundFlow internally uses `ArrayPool<T>.Shared` for many temporary buffers to reduce GC pressure.\n        *   **Efficient Algorithms:** Use efficient algorithms for audio processing, especially in performance-critical sections.\n        *   **Modifier Overhead:** Each `SoundModifier` added to a component or to the editing hierarchy (`AudioSegment`, `Track`, `Composition`) introduces some overhead. For very simple operations, integrating them directly into a custom `SoundComponent` might be more performant than using many tiny modifiers. However, modifiers offer better reusability and modularity.\n        *   **Effect Toggling:** Use the `Enabled` property on `SoundModifier`, `AudioAnalyzer`, `SoundComponent`, `AudioSegmentSettings`, and `TrackSettings` to non-destructively disable effects or entire processing paths instead of removing and re-adding them, which is more efficient.\n    </Tab>\n\n    <Tab\n        key=\"threading\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='carbon:thread' />\n                <span>Threading Considerations</span>\n            </div>\n        }\n    >\n        ## Threading Considerations\n\n        SoundFlow uses a dedicated, high-priority thread (managed by the audio backend, e.g., MiniAudio) for audio processing. This ensures that audio is processed in real time and minimizes the risk of glitches or dropouts.\n\n        **Key Considerations:**\n\n        *   **Audio Thread:** The audio processing logic is executed on a dedicated audio thread. This thread is responsible for calling the audio callback provided to the backend. In SoundFlow's `MiniAudio` implementation, this callback triggers the `Process` method on the appropriate `MiniAudioPlaybackDevice` or `MiniAudioCaptureDevice`, which in turn traverses the `SoundComponent` graph (e.g., calling `MasterMixer.Process(...)`). Therefore, all code within `GenerateAudio` (for `SoundComponent`) and `Process`/`ProcessSample` (for `SoundModifier`) runs on this critical audio thread. Avoid performing any long-running or blocking operations (like I/O, complex non-audio computations, or direct UI updates) on this thread.\n        *   **UI Thread:** Never perform audio processing directly on the UI thread. This can lead to unresponsiveness and glitches. Use the `AudioEngine`'s audio thread for all audio-related operations. For UI updates based on audio events (e.g., from an `IVisualizer`'s `VisualizationUpdated` event), you must marshal the calls to the UI thread (e.g., using `Dispatcher.Invoke` in WPF/Avalonia, or `Control.Invoke` in WinForms).\n        *   **Thread Safety:** If you need to access or modify shared data from both the audio thread and another thread (e.g., the UI thread updating a `SoundModifier`'s property), use appropriate synchronization mechanisms (like `lock`, `Monitor`, or thread-safe collections) to ensure data integrity and prevent race conditions. Many properties on `SoundComponent` and `SoundModifier` are internally locked for thread-safe access from different threads.\n    </Tab>\n</Tabs>"
  },
  {
    "id": 45,
    "slug": "webrtc-apm",
    "version": "1.1.2",
    "title": "WebRTC Audio Processing Module (APM) Extension",
    "description": "Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.",
    "navOrder": 7,
    "category": "Extensions",
    "content": "﻿---\r\ntitle: WebRTC Audio Processing Module (APM) Extension\r\ndescription: Integrate advanced voice processing capabilities like Echo Cancellation, Noise Suppression, and Automatic Gain Control into SoundFlow using the WebRTC APM Extension.\r\nnavOrder: 7\r\ncategory: Extensions\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# WebRTC Audio Processing Module (APM) Extension for SoundFlow\r\n\r\nThe `SoundFlow.Extensions.WebRtc.Apm` package integrates a native library based on the high-quality WebRTC Audio Processing Module (APM) into the SoundFlow audio engine. This extension provides advanced voice processing features essential for real-time communication and audio enhancement.\r\n\r\n## Features\r\n\r\nThe WebRTC APM extension offers several key audio processing features:\r\n\r\n*   **Acoustic Echo Cancellation (AEC):** Reduces or eliminates echoes that occur when audio played through speakers is picked up by the microphone.\r\n*   **Noise Suppression (NS):** Attenuates steady-state background noise (e.g., fans, hums) to improve speech clarity. Multiple suppression levels are available.\r\n*   **Automatic Gain Control (AGC):** Dynamically adjusts the microphone input volume to maintain a consistent audio level, preventing clipping or overly quiet audio. Supports different modes and target levels.\r\n*   **High Pass Filter (HPF):** Removes low-frequency components (typically below 80Hz) to reduce rumble and DC offset.\r\n*   **Pre-Amplifier:** Applies a configurable fixed gain to the audio signal before other APM processing steps.\r\n*   **Multi-channel Processing Configuration:** Allows specifying how multi-channel audio is handled and downmixed.\r\n\r\nThese features can be configured and applied primarily through the `WebRtcApmModifier` for real-time processing within the SoundFlow audio graph, or using the `NoiseSuppressor` component for offline batch processing.\r\n\r\n**Important Note on Sample Rates:** The WebRTC APM native library primarily supports specific sample rates: **8000 Hz, 16000 Hz, 32000 Hz, and 48000 Hz**. Ensure your SoundFlow `AudioEngine` is initialized with one of these sample rates when using this extension for optimal performance and compatibility.\r\n\r\n## Installation\r\n\r\nTo use this extension, you need to have the core `SoundFlow` library installed. Then, add the `SoundFlow.Extensions.WebRtc.Apm` package to your project:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\r\n    <Tab\r\n        key=\"nuget\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:nuget' />\r\n                <span>NuGet Package Manager</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        Install-Package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"cli\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='simple-icons:dotnet' />\r\n                <span>.NET CLI</span>\r\n            </div>\r\n        }\r\n    >\r\n        ```bash\r\n        dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\nThis package includes the necessary C# wrapper and the native WebRTC APM binaries for supported platforms.\r\n\r\n## Usage\r\n\r\n### Real-time Processing with `WebRtcApmModifier`\r\n\r\nThe `WebRtcApmModifier` is a `SoundModifier` that can be added to any `SoundComponent` to process its audio output in real-time. This is ideal for applications like voice chat, live audio input processing, etc.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Use a supported sample rate\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        Make sure to use a supported sample rate (e.g., 48000 Hz).\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n\r\n        // Initialize with a WebRTC APM compatible sample rate, e.g., 48kHz\r\n        // And enable mixed capability if you plan to use microphone input and playback for AEC.\r\n        var audioEngine = new MiniAudioEngine(48000, Capability.Mixed, channels: 1); // Mono for typical voice\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create SoundComponent\" description=\"Create a player for your audio source\" icon='ph:speaker-high-bold'>\r\n        ### 2. Create your `SoundComponent`\r\n        This could be a `SoundPlayer` playing microphone input, or any other component whose output you want to process.\r\n\r\n        ```csharp\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Providers;\r\n\r\n        // Example: Using microphone input\r\n        var microphoneDataProvider = new MicrophoneDataProvider();\r\n        var micPlayer = new SoundPlayer(microphoneDataProvider);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Configure APM Modifier\" description=\"Instantiate and set up the modifier\" icon='material-symbols:settings-outline'>\r\n        ### 3. Instantiate and Configure `WebRtcApmModifier`\r\n        The modifier's constructor allows setting initial states for all features. You can also adjust them dynamically via its public properties.\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Modifiers;\r\n\r\n        var apmModifier = new WebRtcApmModifier(\r\n        // Echo Cancellation (AEC) settings\r\n        aecEnabled: true,\r\n        aecMobileMode: false, // Desktop mode is generally more robust\r\n        aecLatencyMs: 40,     // Estimated system latency for AEC (tune this)\r\n\r\n        // Noise Suppression (NS) settings\r\n        nsEnabled: true,\r\n        nsLevel: NoiseSuppressionLevel.High,\r\n\r\n        // Automatic Gain Control (AGC) - Version 1 (legacy)\r\n        agc1Enabled: true,\r\n        agcMode: GainControlMode.AdaptiveDigital,\r\n        agcTargetLevel: -3,   // Target level in dBFS (0 is max, typical is -3 to -18)\r\n        agcCompressionGain: 9, // Only for FixedDigital mode\r\n        agcLimiter: true,\r\n\r\n        // Automatic Gain Control (AGC) - Version 2 (newer, often preferred)\r\n        agc2Enabled: false, // Set to true to use AGC2, potentially disable AGC1\r\n\r\n        // High Pass Filter (HPF)\r\n        hpfEnabled: true,\r\n\r\n        // Pre-Amplifier\r\n        preAmpEnabled: false,\r\n        preAmpGain: 1.0f,\r\n\r\n        // Pipeline settings for multi-channel audio (if numChannels > 1)\r\n        useMultichannelCapture: false, // Process capture (mic) as mono/stereo as configured by AudioEngine\r\n        useMultichannelRender: false,  // Process render (playback for AEC) as mono/stereo\r\n        downmixMethod: DownmixMethod.AverageChannels // Method if downmixing is needed\r\n        );\r\n\r\n        // Example of changing a setting dynamically:\r\n        // apmModifier.NoiseSuppression.Level = NoiseSuppressionLevel.VeryHigh;\r\n        ```\r\n    </Step>\r\n    <Step title=\"Add Modifier\" description=\"Attach the modifier to the component\" icon='ic:baseline-plus'>\r\n        ### 4. Add the Modifier to your `SoundComponent`\r\n\r\n        ```csharp\r\n        micPlayer.AddModifier(apmModifier);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Start Processing\" description=\"Add to mixer and start playback\" icon='mdi:play-box-outline'>\r\n        ### 5. Add the `SoundComponent` to the `Mixer` and start processing/playback\r\n\r\n        ```csharp\r\n        Mixer.Master.AddComponent(micPlayer);\r\n        microphoneDataProvider.StartCapture(); // If using microphone\r\n        micPlayer.Play(); // Start processing the microphone input\r\n\r\n        Console.WriteLine(\"WebRTC APM processing microphone input. Press any key to stop.\");\r\n        Console.ReadKey();\r\n\r\n        // Cleanup\r\n        microphoneDataProvider.StopCapture();\r\n        micPlayer.Stop();\r\n        Mixer.Master.RemoveComponent(micPlayer);\r\n        apmModifier.Dispose(); // Important to release native resources\r\n        microphoneDataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n**AEC Far-End (Playback) Signal:** For Acoustic Echo Cancellation to work effectively, the `WebRtcApmModifier` automatically listens to `AudioEngine.OnAudioProcessed` events for audio being played back (capability `Playback`). This playback audio is fed as the \"far-end\" or \"render\" signal to the AEC. Ensure your `AudioEngine` is initialized with `Capability.Mixed` if you're using AEC with live microphone input and simultaneous playback.\r\n\r\n### Offline Processing with `NoiseSuppressor`\r\n\r\nThe `NoiseSuppressor` component is designed for batch processing of audio from an `ISoundDataProvider` (e.g., an audio file). It applies only the WebRTC Noise Suppression feature.\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Initialize AudioEngine\" description=\"Required for encoding/decoding\" icon='ph:engine-bold'>\r\n        ### 1. Initialize SoundFlow `AudioEngine`\r\n        (Required for `ISoundDataProvider` decoding and `ISoundEncoder` encoding, even if not playing back). Use a supported sample rate.\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n\r\n        var audioEngine = new MiniAudioEngine(48000, Capability.Playback); // Or Record, if only encoding\r\n        ```\r\n    </Step>\r\n    <Step title=\"Create Data Provider\" description=\"Load your noisy audio file\" icon='mdi:file-music-outline'>\r\n        ### 2. Create an `ISoundDataProvider` for your noisy audio file\r\n\r\n        ```csharp\r\n        using SoundFlow.Interfaces;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Ensure this file's sample rate and channel count match what NoiseSuppressor expects\r\n        string noisyFilePath = \"path/to/your/noisy_audio.wav\";\r\n        var dataProvider = new StreamDataProvider(File.OpenRead(noisyFilePath));\r\n        ```\r\n    </Step>\r\n    <Step title=\"Instantiate NoiseSuppressor\" description=\"Set up the offline processor\" icon='icon-park-outline:sound-wave'>\r\n        ### 3. Instantiate `NoiseSuppressor`\r\n\r\n        ```csharp\r\n        using SoundFlow.Extensions.WebRtc.Apm;\r\n        using SoundFlow.Extensions.WebRtc.Apm.Components;\r\n\r\n        // Parameters for NoiseSuppressor: dataProvider, sampleRate, numChannels, suppressionLevel\r\n        // These MUST match the actual properties of the audio from dataProvider.\r\n        var noiseSuppressor = new NoiseSuppressor(dataProvider, 48000, 1, NoiseSuppressionLevel.VeryHigh);\r\n        ```\r\n    </Step>\r\n    <Step title=\"Process the Audio\" description=\"Process all at once or in chunks\" icon='carbon:cics-transaction-server-zos'>\r\n        ### 4. Process the audio\r\n        You can process all audio at once (for smaller files) or chunk by chunk.\r\n\r\n        **Option A: Process All (returns `float[]`)**\r\n        ```csharp\r\n        float[] cleanedAudio = noiseSuppressor.ProcessAll();\r\n        // Now 'cleanedAudio' contains the noise-suppressed audio data.\r\n        // You can save it using an ISoundEncoder:\r\n        var fileStream = new FileStream(\"cleaned_audio.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        var encoder = audioEngine.CreateEncoder(fileStream, EncodingFormat.Wav, SampleFormat.F32, 1, 48000);\r\n        encoder.Encode(cleanedAudio.AsSpan());\r\n        encoder.Dispose();\r\n        fileStream.Dispose();\r\n        ```\r\n\r\n        **Option B: Process Chunks (via event or direct handler)**\r\n        ```csharp\r\n        var chunkFileStream = new FileStream(\"cleaned_audio_chunked.wav\", FileMode.Create, FileAccess.Write, FileShare.None);\r\n        var chunkEncoder = audioEngine.CreateEncoder(chunkFileStream, EncodingFormat.Wav, SampleFormat.F32, 1, 48000);\r\n\r\n        noiseSuppressor.OnAudioChunkProcessed += (processedChunk) =>\r\n        {\r\n            if (!chunkEncoder.IsDisposed)\r\n        {\r\n            chunkEncoder.Encode(processedChunk.ToArray());\r\n        }\r\n        };\r\n\r\n        // ProcessChunks is a blocking call until the entire provider is processed.\r\n        noiseSuppressor.ProcessChunks();\r\n        chunkEncoder.Dispose(); // Finalize and save the encoded file\r\n        chunkFileStream.Dispose();\r\n        ```\r\n    </Step>\r\n    <Step title=\"Dispose Resources\" description=\"Clean up all IDisposable objects\" icon='material-symbols:delete-outline'>\r\n        ### 5. Dispose resources\r\n\r\n        ```csharp\r\n        noiseSuppressor.Dispose();\r\n        dataProvider.Dispose();\r\n        audioEngine.Dispose();\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n## Configuration Details\r\n\r\n### `WebRtcApmModifier` Properties:\r\n\r\n*   **`Enabled` (bool):** Enables/disables the entire APM modifier.\r\n*   **`EchoCancellation` (`EchoCancellationSettings`):**\r\n*   `Enabled` (bool): Enables/disables AEC.\r\n*   `MobileMode` (bool): Toggles between desktop (false) and mobile (true) AEC modes.\r\n*   `LatencyMs` (int): Estimated system audio latency in milliseconds. Crucial for AEC performance. Tune this value for your setup.\r\n*   **`NoiseSuppression` (`NoiseSuppressionSettings`):**\r\n*   `Enabled` (bool): Enables/disables NS.\r\n*   `Level` (`NoiseSuppressionLevel`): Sets the aggressiveness (Low, Moderate, High, VeryHigh).\r\n*   **`AutomaticGainControl` (`AutomaticGainControlSettings`):**\r\n*   `Agc1Enabled` (bool): Enables/disables the legacy AGC1.\r\n*   `Mode` (`GainControlMode`): Sets the mode for AGC1 (AdaptiveAnalog, AdaptiveDigital, FixedDigital).\r\n*   `TargetLevelDbfs` (int): Target level for AGC1 AdaptiveDigital mode (-31 to 0 dBFS).\r\n*   `CompressionGainDb` (int): Gain for AGC1 FixedDigital mode (0 to 90 dB).\r\n*   `LimiterEnabled` (bool): Enables/disables the limiter for AGC1.\r\n*   `Agc2Enabled` (bool): Enables/disables the newer AGC2.\r\n*   **`HighPassFilterEnabled` (bool):** Enables/disables the HPF.\r\n*   **`PreAmplifierEnabled` (bool):** Enables/disables the pre-amplifier.\r\n*   **`PreAmplifierGainFactor` (float):** Gain factor for the pre-amplifier (e.g., 1.0 is no change, 2.0 is +6dB).\r\n*   **`ProcessingPipeline` (`ProcessingPipelineSettings`):**\r\n*   `UseMultichannelCapture` (bool): If true and input is multi-channel, APM processes it as such. Otherwise, it might downmix.\r\n*   `UseMultichannelRender` (bool): Similar to capture, but for the far-end/render signal for AEC.\r\n*   `DownmixMethod` (`DownmixMethod`): Specifies how to downmix if multi-channel processing is disabled for a stream (AverageChannels, UseFirstChannel).\r\n*   **`PostProcessGain` (float):** A final gain applied after all APM processing (default 1.0f).\r\n\r\n### `NoiseSuppressor` Constructor:\r\n\r\n*   `dataProvider` (`ISoundDataProvider`): The audio source.\r\n*   `sampleRate` (int): Sample rate of the source audio (must be 8k, 16k, 32k, or 48k).\r\n*   `numChannels` (int): Number of channels in the source audio.\r\n*   `suppressionLevel` (`NoiseSuppressionLevel`): Desired noise suppression level.\r\n*   `useMultichannelProcessing` (bool): If true and `numChannels > 1`, attempts to process channels independently.\r\n\r\n## Licensing\r\n\r\n*   The C# code (`SoundFlow.Extensions.WebRtc.Apm` wrapper and components) is licensed under the **MIT License**.\r\n*   The native `webrtc-apm` library used by this extension is based on the WebRTC Audio Processing Module, which is typically licensed under the **BSD 3-Clause \"New\" or \"Revised\" License**. The specific version included is derived from the [PulseAudio project's extraction](https://gitlab.freedesktop.org/pulseaudio/webrtc-audio-processing).\r\n\r\n**Users must comply with the terms of both licenses.** This generally involves including the copyright notice and license text of the WebRTC code if distributing applications using this extension. Please consult the native library's specific distribution for exact requirements.\r\n\r\n## Troubleshooting\r\n\r\n*   **No effect or poor quality:**\r\n*   Verify the `AudioEngine` sample rate matches one supported by WebRTC APM (8k, 16k, 32k, 48k Hz).\r\n*   For AEC, ensure `aecLatencyMs` is tuned appropriately for your system. Too low or too high values can degrade performance.\r\n*   Ensure the far-end signal is correctly being captured if AEC is enabled (usually handled automatically by the modifier via `AudioEngine.OnAudioProcessed`).\r\n*   **Errors during initialization:** Check the console output for any specific error messages from the native APM library. Ensure the native binaries are correctly deployed with your application.\r\n*   **Performance issues:** While WebRTC APM is optimized, processing many channels or enabling all features at very high settings can be CPU intensive. Monitor performance and adjust settings if needed."
  },
  {
    "id": 46,
    "slug": "tutorials-and-examples",
    "version": "1.1.2",
    "title": "Tutorials and Examples",
    "description": "A collection of tutorials and examples to help you learn how to use SoundFlow for various audio processing tasks.",
    "navOrder": 5,
    "category": "Core",
    "content": "---\r\ntitle: Tutorials and Examples\r\ndescription: A collection of tutorials and examples to help you learn how to use SoundFlow for various audio processing tasks.\r\nnavOrder: 5\r\ncategory: Core\r\n---\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport {Steps, Step} from '/src/components/Shared/Steps';\r\n\r\n# Tutorials and Examples\r\n\r\nThis section provides a collection of tutorials and examples to help you learn how to use SoundFlow for various audio processing tasks. Each tutorial provides step-by-step instructions and explanations, while the examples offer ready-to-run code snippets that demonstrate specific features.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Tutorial categories\">\r\n    <Tab key=\"playback\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:play-circle\"/><span>Playback</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Playback tutorials\">\r\n            <Tab key=\"basic-playback\" title=\"Basic Playback\">\r\n                This tutorial demonstrates how to play an audio file from disk using `SoundPlayer` and\r\n                `StreamDataProvider`.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application:\r\n                        ```bash\r\n                        dotnet new console -o BasicPlayback\r\n                        cd BasicPlayback\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                        ### 2. Install the SoundFlow NuGet package:\r\n                        ```bash\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the basic player\" icon='ph:code-bold'>\r\n                        ### 3. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace BasicPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine with the MiniAudio backend.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                // Replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until playback finishes or the user presses a key.\r\n                                Console.WriteLine(\"Playing audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                // dataProvider is disposed automatically due to 'using' statement.\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file on your\r\n                        computer.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 4. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `StreamDataProvider` (which is `IDisposable` and\r\n                managed by a `using` statement) to load an audio file, creates a `SoundPlayer` with this provider, adds\r\n                the player to the `Master` mixer, and starts playback. The console application then waits for the user\r\n                to press a key before stopping playback and cleaning up.\r\n            </Tab>\r\n\r\n            <Tab key=\"web-playback\" title=\"Web Playback\">\r\n                This tutorial demonstrates how to play an audio stream from a URL using `SoundPlayer` and\r\n                `NetworkDataProvider`.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create Project\" description=\"Use the .NET CLI\" icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application:\r\n                        ```bash\r\n                        dotnet new console -o WebPlayback\r\n                        cd WebPlayback\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\r\n                        ### 2. Install the SoundFlow NuGet package:\r\n                        ```bash\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the network player\" icon='ph:code-bold'>\r\n                        ### 3. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n                        using System.IO;\r\n                        using System.Threading.Tasks;\r\n\r\n                        namespace WebPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static async Task Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a NetworkDataProvider. Replace \"your-audio-stream-url\"\r\n                                // with the actual URL (direct audio file or HLS .m3u8 playlist).\r\n                                // NetworkDataProvider is IDisposable.\r\n                                using var dataProvider = new NetworkDataProvider(\"your-audio-stream-url\");\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing stream... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                // dataProvider is disposed automatically.\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"your-audio-stream-url\"` with the actual URL of an audio stream (e.g., direct\r\n                        MP3/WAV or an HLS .m3u8 playlist).***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 4. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `NetworkDataProvider` for the given URL (which\r\n                handles direct files or HLS playlists), creates a `SoundPlayer`, adds it to the `Master` mixer, and\r\n                starts playback. `NetworkDataProvider` is `IDisposable` and managed with a `using` statement.\r\n            </Tab>\r\n\r\n            <Tab key=\"playback-control\" title=\"Playback Control\">\r\n                This tutorial demonstrates how to control audio playback using `Play`, `Pause`, `Stop`, `Seek`, and\r\n                `PlaybackSpeed`.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o PlaybackControl\r\n                        cd PlaybackControl\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the interactive player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace PlaybackControl;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider) {Volume = 0.8f}; // Example: set initial volume\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n                                Console.WriteLine(\"Playing audio... (p: pause/play, s: seek, +/-: speed, v/m: volume, any other: stop)\");\r\n\r\n                                // Handle user input for playback control.\r\n                                while (player.State != PlaybackState.Stopped)\r\n                                {\r\n                                    var keyInfo = Console.ReadKey(true);\r\n                                    switch (keyInfo.Key)\r\n                                    {\r\n                                    case ConsoleKey.P:\r\n                                        if (player.State == PlaybackState.Playing)\r\n                                            player.Pause();\r\n                                        else\r\n                                            player.Play();\r\n                                        Console.WriteLine(player.State == PlaybackState.Paused ? \"Paused\" : \"Playing\");\r\n                                        break;\r\n                                    case ConsoleKey.S:\r\n                                        Console.Write(\"Enter seek time (in seconds, e.g., 10.5): \");\r\n                                        if (float.TryParse(Console.ReadLine(), out var seekTimeSeconds))\r\n                                        {\r\n                                            if (player.Seek(TimeSpan.FromSeconds(seekTimeSeconds)))\r\n                                                Console.WriteLine($\"Seeked to {seekTimeSeconds:F1}s. Current time: {player.Time:F1}s\");\r\n                                            else\r\n                                                Console.WriteLine(\"Seek failed.\");\r\n                                        }\r\n                                        else\r\n                                            Console.WriteLine(\"Invalid seek time.\");\r\n                                    break;\r\n                                    case ConsoleKey.OemPlus:\r\n                                    case ConsoleKey.Add:\r\n                                        player.PlaybackSpeed = Math.Min(2.0f, player.PlaybackSpeed + 0.1f);\r\n                                        Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                    case ConsoleKey.OemMinus:\r\n                                    case ConsoleKey.Subtract:\r\n                                        player.PlaybackSpeed = Math.Max(0.1f, player.PlaybackSpeed - 0.1f);\r\n                                        Console.WriteLine($\"Playback speed: {player.PlaybackSpeed:F1}x\");\r\n                                    break;\r\n                                    case ConsoleKey.V:\r\n                                        player.Volume = Math.Min(1.5f, player.Volume + 0.1f); // Allow gain up to 150%\r\n                                        Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                    case ConsoleKey.M:\r\n                                        player.Volume = Math.Max(0.0f, player.Volume - 0.1f);\r\n                                        Console.WriteLine($\"Volume: {player.Volume:P0}\");\r\n                                    break;\r\n                                    default:\r\n                                        player.Stop();\r\n                                        Console.WriteLine(\"Stopped\");\r\n                                    break;\r\n                                }\r\n                                }\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, adds it to the `Master` mixer, and\r\n                starts playback. It then enters a loop that handles user input for playback control:\r\n\r\n                * `P`: Pauses or resumes playback.\r\n                * `S`: Prompts for a seek time (in seconds) and seeks using `TimeSpan.FromSeconds()`. The `Seek` method\r\n                now returns a boolean indicating success.\r\n                * `+`/`-`: Adjusts `PlaybackSpeed`.\r\n                * `V`/`M`: Adjusts `player.Volume`.\r\n                * Any other key: Stops playback.\r\n            </Tab>\r\n\r\n            <Tab key=\"looping\" title=\"Looping\">\r\n                This tutorial demonstrates how to enable looping for a `SoundPlayer` and how to set custom loop points.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o LoopingPlayback\r\n                        cd LoopingPlayback\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the looping player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace LoopingPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Enable looping.\r\n                                player.IsLooping = true;\r\n\r\n                                // **Optional: Set custom loop points**\r\n\r\n                                // Example 1: Loop from 2.5 seconds to 7.0 seconds (using float seconds)\r\n                                // player.SetLoopPoints(2.5f, 7.0f);\r\n\r\n                                // Example 2: Loop from sample 110250 to sample 308700 (using samples)\r\n                                // player.SetLoopPoints(110250, 308700); // Assuming 44.1kHz stereo, these are example values\r\n\r\n                                // Example 3: Loop from 1.5 seconds to the natural end of the audio (using TimeSpan, end point is optional)\r\n                                player.SetLoopPoints(TimeSpan.FromSeconds(1.5));\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio in a loop... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code builds upon the basic playback example and introduces audio looping.\r\n                * **`player.IsLooping = true;`**: Enables looping.\r\n                * **`player.SetLoopPoints(...)`**: Configures the loop region.\r\n                * Overloads accept `float` seconds, `int` samples, or `TimeSpan`.\r\n                * If `endTime` (or `endSample`) is omitted or set to `-1f` (or `-1`), the loop goes to the natural end\r\n                of the audio.\r\n            </Tab>\r\n\r\n            <Tab key=\"surround-sound\" title=\"Surround Sound\">\r\n                This tutorial demonstrates how to use `SurroundPlayer` to play audio with surround sound configurations.\r\n\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o SurroundPlayback\r\n                        cd SurroundPlayback\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the surround player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.Numerics;\r\n                        using System.IO;\r\n\r\n                        namespace SurroundPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine with appropriate channels for surround.\r\n                                // For 7.1, use 8 channels.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback, channels: 8);\r\n\r\n                                // Create a SurroundPlayer. Load a mono or stereo file for surround upmixing,\r\n                                // or a multi-channel file if your source is already surround.\r\n                                // The SurroundPlayer will attempt to pan mono/stereo to the configured speakers.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\")); // Can be mono/stereo\r\n                                var player = new SurroundPlayer(dataProvider);\r\n\r\n                                // Configure the SurroundPlayer for 7.1 surround sound.\r\n                                player.SpeakerConfig = SurroundPlayer.SpeakerConfiguration.Surround71;\r\n\r\n                                // Set the panning method (VBAP is often good for surround).\r\n                                player.Panning = SurroundPlayer.PanningMethod.Vbap;\r\n\r\n                                // Set the listener position (optional, (0,0) is center).\r\n                                player.ListenerPosition = new Vector2(0, 0);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until playback finishes or the user presses a key.\r\n                                Console.WriteLine(\"Playing surround sound audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file (mono, stereo,\r\n                        or multi-channel).***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes `AudioEngine` with 8 channels for 7.1. A `SurroundPlayer` is created. If the input\r\n                `audiofile.wav` is mono or stereo, the `SurroundPlayer` will pan it across the configured 7.1 speaker\r\n                layout. `SpeakerConfig` and `Panning` method are set. The `ListenerPosition` can also be adjusted.\r\n            </Tab>\r\n\r\n            <Tab key=\"chunked-data\" title=\"Chunked Data\">\r\n                This tutorial demonstrates how to use the `ChunkedDataProvider` for efficient playback of large audio\r\n                files.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o ChunkedPlayback\r\n                        cd ChunkedPlayback\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the chunked data player\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace ChunkedPlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                                // Create a ChunkedDataProvider and load a large audio file.\r\n                                // Replace \"path/to/your/large/audiofile.wav\" with the actual path.\r\n                                using var dataProvider = new ChunkedDataProvider(\"path/to/your/large/audiofile.wav\");\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                Mixer.Master.AddComponent(player);\r\n                                player.Play();\r\n\r\n                                Console.WriteLine(\"Playing audio with ChunkedDataProvider... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/large/audiofile.wav\"` with the path to a large audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                The `ChunkedDataProvider` reads and decodes audio in chunks, suitable for large files. It's\r\n                `IDisposable` and managed with `using`.\r\n            </Tab>\r\n\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"recording\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:mic\"/><span>Recording</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Recording tutorials\">\r\n            <Tab key=\"basic-recording\" title=\"Basic Recording\">\r\n                This tutorial demonstrates how to record audio from the default recording device and save it to a WAV\r\n                file.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o BasicRecording\r\n                        cd BasicRecording\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement the basic recorder\" icon='ph:code-bold'>\r\n                        ### 2. Replace the contents of `Program.cs`:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace BasicRecording;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize for recording, e.g., 48kHz.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Record);\r\n\r\n                                string outputFilePath = Path.Combine(Directory.GetCurrentDirectory(), \"output.wav\");\r\n                                using var fileStream = new FileStream(outputFilePath, FileMode.Create, FileAccess.Write, FileShare.None);\r\n                                using var recorder = new Recorder(fileStream, sampleRate: 48000, encodingFormat: EncodingFormat.Wav);\r\n\r\n                                Console.WriteLine(\"Recording... Press any key to stop.\");\r\n                                recorder.StartRecording();\r\n                                Console.ReadKey();\r\n                                recorder.StopRecording();\r\n\r\n                                Console.WriteLine($\"Recording stopped. Saved to {outputFilePath}\");\r\n                            }\r\n                        }\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                Initializes `AudioEngine` for recording, creates a `Recorder` to save to \"output.wav\", starts recording,\r\n                waits for a key, then stops.\r\n            </Tab>\r\n\r\n            <Tab key=\"custom-processing\" title=\"Custom Processing\">\r\n                This tutorial demonstrates using a callback to process recorded audio in real-time.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow.\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement real-time processing\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs`:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace CustomProcessing;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Record);\r\n                                using var recorder = new Recorder(ProcessAudio, sampleRate: 48000);\r\n\r\n                                Console.WriteLine(\"Recording with custom processing... Press any key to stop.\");\r\n                                recorder.StartRecording();\r\n                                Console.ReadKey();\r\n                                recorder.StopRecording();\r\n                                Console.WriteLine(\"Recording stopped.\");\r\n                            }\r\n\r\n                                // This method will be called for each chunk of recorded audio.\r\n                            private static void ProcessAudio(Span<float> samples)\r\n                            {\r\n                                // Perform custom processing on the audio samples.\r\n                                // For example, calculate the average level:\r\n                                float sum = 0;\r\n                                for (int i = 0; i < samples.Length; i++)\r\n                                {\r\n                                    sum += Math.Abs(samples[i]);\r\n                                }\r\n                                float averageLevel = sum / samples.Length;\r\n\r\n                                Console.WriteLine($\"Average level: {averageLevel:F4}\");\r\n                            }\r\n                        }\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application.\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                A `Recorder` is created with a `ProcessAudio` callback that gets called with chunks of recorded audio.\r\n            </Tab>\r\n\r\n            <Tab key=\"mic-playback\" title=\"Mic Playback (Monitor)\">\r\n                This tutorial demonstrates capturing microphone audio and playing it back in real-time.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow.\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement microphone loopback\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs`:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System;\r\n\r\n                        namespace MicrophonePlayback;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Mixed capability for simultaneous record & playback.\r\n                                using var audioEngine = new MiniAudioEngine(48000, Capability.Mixed);\r\n                                using var microphoneDataProvider = new MicrophoneDataProvider();\r\n\r\n                                // Create a SoundPlayer and connect the MicrophoneDataProvider.\r\n                                var player = new SoundPlayer(microphoneDataProvider);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start capturing audio from the microphone.\r\n                                microphoneDataProvider.StartCapture();\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                Console.WriteLine(\"Playing live microphone audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and capture.\r\n                                player.Stop();\r\n                                microphoneDataProvider.StopCapture(); // Stop capture before provider is disposed by 'using'\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Build and run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application.\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n                Uses `MicrophoneDataProvider` as a source for a `SoundPlayer` to achieve real-time microphone\r\n                monitoring. `AudioEngine` needs `Capability.Mixed`.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"effects\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:wand-2\"/><span>Effects</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Effects tutorials\">\r\n            <Tab key=\"reverb\" title=\"Reverb\">\r\n                Demonstrates how to apply a reverb effect using the `AlgorithmicReverbModifier`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o ReverbEffect\r\n                        cd ReverbEffect\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with reverb\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace ReverbEffect;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create an AlgorithmicReverbModifier.\r\n                                var reverb = new AlgorithmicReverbModifier\r\n                                {\r\n                                    RoomSize = 0.8f,\r\n                                    Damp = 0.5f,\r\n                                    Wet = 0.3f,\r\n                                    Dry = 0.7f,\r\n                                    Width = 1f\r\n                                };\r\n\r\n                                // Add the reverb modifier to the player.\r\n                                player.AddModifier(reverb);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with reverb... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates an\r\n                `AlgorithmicReverbModifier` with custom settings, adds the modifier to the player, adds the player to\r\n                the `Master` mixer, and starts playback. You will hear the audio with the reverb effect applied.\r\n                Experiment with different values for `RoomSize`, `Damp`, `Wet`, `Dry`, and `Width` to change the\r\n                characteristics of the reverb.\r\n            </Tab>\r\n\r\n            <Tab key=\"equalization\" title=\"Equalization\">\r\n                Demonstrates how to use the `ParametricEqualizer` to adjust frequency balance.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o Equalization\r\n                        cd Equalization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with EQ\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace Equalization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a ParametricEqualizer.\r\n                                var equalizer = new ParametricEqualizer(AudioEngine.Channels);\r\n\r\n                                // Add some equalizer bands:\r\n                                // Boost low frequencies (bass)\r\n                                equalizer.AddBand(FilterType.LowShelf, 100, 6, 0.7f, 0);\r\n                                // Cut mid frequencies\r\n                                equalizer.AddBand(FilterType.Peaking, 1000, -4, 2, 0);\r\n                                // Boost high frequencies (treble)\r\n                                equalizer.AddBand(FilterType.HighShelf, 10000, 5, 0.7f, 0);\r\n\r\n                                // Add the equalizer to the player.\r\n                                player.AddModifier(equalizer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with equalization... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust bands\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `ParametricEqualizer`, adds three equalizer bands (low-shelf boost, peaking cut, high-shelf boost), adds\r\n                the equalizer to the player, adds the player to the `Master` mixer, and starts playback. You will hear\r\n                the audio with the equalization applied. Experiment with different filter types (`FilterType`),\r\n                frequencies, gain values, and Q values to shape the sound to your liking.\r\n            </Tab>\r\n\r\n            <Tab key=\"chorus-delay\" title=\"Chorus & Delay\">\r\n                Demonstrates how to apply chorus and delay effects using `ChorusModifier` and `DelayModifier`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o ChorusDelay\r\n                        cd ChorusDelay\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with effects\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace ChorusDelay;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a ChorusModifier.\r\n                                var chorus = new ChorusModifier(\r\n                                depth: 25f,           // Depth (in milliseconds)\r\n                                rate: 0.8f,          // Rate (in Hz)\r\n                                feedback: 0.5f,      // Feedback amount\r\n                                wetDryMix: 0.5f,     // Wet/dry mix (0 = dry, 1 = wet)\r\n                                maxDelayLength: 500  // Maximum delay length (in milliseconds)\r\n                                );\r\n\r\n                                // Create a DelayModifier.\r\n                                var delay = new DelayModifier(\r\n                                delayLength: 500,   // Delay length (in milliseconds)\r\n                                feedback: 0.6f,      // Feedback amount\r\n                                wetMix: 0.4f,       // Wet/dry mix\r\n                                cutoffFrequency: 4000 // Cutoff frequency for the low-pass filter\r\n                                );\r\n\r\n                                // Add the chorus and delay modifiers to the player.\r\n                                player.AddModifier(chorus);\r\n                                player.AddModifier(delay);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with chorus and delay... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust effects\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates\r\n                `ChorusModifier` and `DelayModifier` instances with custom settings, adds both modifiers to the player\r\n                (they will be applied in the order they are added), adds the player to the `Master` mixer, and starts\r\n                playback. You will hear the audio with both chorus and delay effects applied. Experiment with different\r\n                parameter values for the chorus and delay modifiers to create a wide range of sonic textures.\r\n            </Tab>\r\n\r\n            <Tab key=\"compression\" title=\"Compression\">\r\n                Demonstrates how to use the `CompressorModifier` to reduce the dynamic range of an audio stream.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o Compression\r\n                        cd Compression\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with compressor\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace Compression;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a CompressorModifier.\r\n                                var compressor = new CompressorModifier(\r\n                                threshold: -20f, // Threshold (in dB)\r\n                                ratio: 4f,        // Compression ratio\r\n                                attack: 10f,      // Attack time (in milliseconds)\r\n                                release: 100f,    // Release time (in milliseconds)\r\n                                knee: 5f,         // Knee width (in dB)\r\n                                makeupGain: 6f   // Makeup gain (in dB)\r\n                                );\r\n\r\n                                // Add the compressor to the player.\r\n                                player.AddModifier(compressor);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with compression... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust parameters\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `CompressorModifier` with specific settings, adds the compressor to the player, adds the player to the\r\n                `Master` mixer, and starts playback. You will hear the audio with the compression effect applied,\r\n                resulting in a more consistent volume level. Experiment with different values for `threshold`, `ratio`,\r\n                `attack`, `release`, `knee`, and `makeupGain` to understand how they affect the compression.\r\n            </Tab>\r\n\r\n            <Tab key=\"noise-reduction\" title=\"Noise Reduction\">\r\n                Demonstrates how to use the `NoiseReductionModifier` to reduce noise in an audio stream.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o NoiseReduction\r\n                        cd NoiseReduction\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with noise reduction\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Modifiers;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace NoiseReduction;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load a noisy audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/noisy/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a NoiseReductionModifier.\r\n                                var noiseReducer = new NoiseReductionModifier(\r\n                                fftSize: 2048,       // FFT size (power of 2)\r\n                                alpha: 3f,          // Smoothing factor for noise estimation\r\n                                beta: 0.001f,        // Minimum gain for noise reduction\r\n                                smoothingFactor: 0.8f, // Smoothing factor for gain\r\n                                gain: 1.0f,         // Post-processing gain\r\n                                noiseFrames: 10     // Number of initial frames to use for noise estimation\r\n                                );\r\n\r\n                                // Add the noise reducer to the player.\r\n                                player.AddModifier(noiseReducer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio with noise reduction... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback.\r\n                                player.Stop();\r\n\r\n                                // Remove the player from the mixer.\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/noisy/audiofile.wav\"` with the actual path to a noisy audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and fine-tune parameters\"\r\n                          icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads a noisy audio file, creates a\r\n                `NoiseReductionModifier` with specific settings, adds the noise reducer to the player, adds the player\r\n                to the `Master` mixer, and starts playback. You should hear a reduction in the noise level of the audio.\r\n                Experiment with different values for `fftSize`, `alpha`, `beta`, `smoothingFactor`, `gain`, and\r\n                `noiseFrames` to fine-tune the noise reduction.\r\n            </Tab>\r\n\r\n            <Tab key=\"mixing\" title=\"Mixing\">\r\n                Demonstrates how to use the `Mixer` to combine multiple audio sources.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o Mixing\r\n                        cd Mixing\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Combine multiple audio sources\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace Mixing;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create two SoundPlayer instances and load different audio files.\r\n                                using var dataProvider1 = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile1.wav\"));\r\n                                var player1 = new SoundPlayer(dataProvider1);\r\n\r\n                                using var dataProvider2 = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile2.wav\"));\r\n                                var player2 = new SoundPlayer(dataProvider2);\r\n\r\n                                // Create an Oscillator that generates a sine wave.\r\n                                var oscillator = new Oscillator\r\n                                {\r\n                                    Frequency = 440, // 440 Hz (A4 note)\r\n                                    Amplitude = 0.5f,\r\n                                    Type = Oscillator.WaveformType.Sine\r\n                                };\r\n\r\n                                // Add the players and the oscillator to the master mixer.\r\n                                Mixer.Master.AddComponent(player1);\r\n                                Mixer.Master.AddComponent(player2);\r\n                                Mixer.Master.AddComponent(oscillator);\r\n\r\n                                // Start playback for both players.\r\n                                player1.Play();\r\n                                player2.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing mixed audio... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback for both players.\r\n                                player1.Stop();\r\n                                player2.Stop();\r\n\r\n                                // Remove the components from the mixer.\r\n                                Mixer.Master.RemoveComponent(player1);\r\n                                Mixer.Master.RemoveComponent(player2);\r\n                                Mixer.Master.RemoveComponent(oscillator);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile1.wav\"` and `\"path/to/your/audiofile2.wav\"` with the actual\r\n                        paths to two different audio files.***\r\n                    </Step>\r\n                    <Step title=\"Run & Experiment\" description=\"Run the app and adjust levels\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates two `SoundPlayer` instances, loads two different audio\r\n                files, creates an `Oscillator` that generates a sine wave, adds all three components to the `Master`\r\n                mixer, and starts playback for the players. You will hear the two audio files and the sine wave mixed\r\n                together. Experiment with adding more sound sources to the mixer and adjusting their individual volumes\r\n                and panning using the `Volume` and `Pan` properties of each `SoundComponent`.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"advanced-processing\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"mdi:webrtc\"/><span>WebRTC APM</span></div>}>\r\n        This tutorial shows how to use the `WebRtcApmModifier` for real-time noise suppression and echo cancellation on\r\n        microphone input. For offline file processing, refer to the [WebRTC APM Extension\r\n        documentation](./extensions/webrtc-apm).\r\n\r\n        **Prerequisites:**\r\n        * SoundFlow core package.\r\n        * `SoundFlow.Extensions.WebRtc.Apm` NuGet package.\r\n\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & both packages\"\r\n                  icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a console app and install both SoundFlow packages:\r\n                ```bash\r\n                dotnet new console -o WebRtcApmDemo\r\n                cd WebRtcApmDemo\r\n                dotnet add package SoundFlow\r\n                dotnet add package SoundFlow.Extensions.WebRtc.Apm\r\n                ```\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement microphone processing\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs`:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Extensions.WebRtc.Apm; // For enums like NoiseSuppressionLevel\r\n                using SoundFlow.Extensions.WebRtc.Apm.Modifiers; // For WebRtcApmModifier\r\n                using System;\r\n\r\n                namespace WebRtcApmDemo;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize AudioEngine. 48kHz is good for WebRTC APM.\r\n                        // Capability.Mixed is needed for AEC (to get playback audio for far-end).\r\n                        using var audioEngine = new MiniAudioEngine(48000, Capability.Mixed, channels: 1); // Mono for voice\r\n\r\n                        // Setup microphone input\r\n                        using var micProvider = new MicrophoneDataProvider();\r\n                        var micPlayer = new SoundPlayer(micProvider) {Name = \"MicrophoneInput\"};\r\n\r\n                        // Instantiate and configure WebRtcApmModifier\r\n                        var apmModifier = new WebRtcApmModifier(\r\n                        aecEnabled: true,       // Enable Acoustic Echo Cancellation\r\n                        aecMobileMode: false,\r\n                        aecLatencyMs: 40,       // Adjust based on your system's latency\r\n\r\n                        nsEnabled: true,        // Enable Noise Suppression\r\n                        nsLevel: NoiseSuppressionLevel.High,\r\n\r\n                        agc1Enabled: true,      // Enable Automatic Gain Control (AGC1)\r\n                        agcMode: GainControlMode.AdaptiveDigital,\r\n                        agcTargetLevel: -6,     // Target level in dBFS\r\n                        agcLimiter: true,\r\n\r\n                        hpfEnabled: true        // Enable High Pass Filter\r\n                        );\r\n                        micPlayer.AddModifier(apmModifier);\r\n\r\n                        // To test AEC, you might want to play some audio simultaneously\r\n                        // For simplicity, this example focuses on mic input processing.\r\n                        // If you play audio through Mixer.Master, AEC will use it as far-end.\r\n\r\n                        Mixer.Master.AddComponent(micPlayer);\r\n\r\n                        micProvider.StartCapture();\r\n                        micPlayer.Play();\r\n\r\n                        Console.WriteLine(\"Processing microphone with WebRTC APM (AEC, NS, AGC, HPF)...\");\r\n                        Console.WriteLine(\"Speak into your microphone. Press any key to stop.\");\r\n                        Console.ReadKey();\r\n\r\n                        micPlayer.Stop();\r\n                        micProvider.StopCapture();\r\n                        Mixer.Master.RemoveComponent(micPlayer);\r\n                        apmModifier.Dispose(); // Important!\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run & Test\" description=\"Speak into your microphone\" icon='lucide:play'>\r\n                ### 3. Build and run. Speak into your microphone to hear the processing.\r\n                **Explanation:**\r\n                This setup processes microphone audio in real-time.\r\n                * `AudioEngine` is set to `Capability.Mixed` and a compatible sample rate (48kHz).\r\n                * `WebRtcApmModifier` is configured with AEC, NS, AGC, and HPF enabled.\r\n                * AEC requires a far-end signal. The modifier automatically listens to `AudioEngine.OnAudioProcessed`\r\n                for `Playback` capability audio to use as the far-end reference. If you were playing music through\r\n                another `SoundPlayer` added to `Mixer.Master`, that would be the far-end signal.\r\n                * ***Remember to `Dispose()` the `WebRtcApmModifier` to release native resources.***\r\n            </Step>\r\n        </Steps>\r\n    </Tab>\r\n\r\n    <Tab key=\"analysis\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:activity\"/><span>Analysis</span></div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Analysis tutorials\">\r\n            <Tab key=\"level-metering\" title=\"Level Metering\">\r\n                This tutorial demonstrates how to use the `LevelMeterAnalyzer` to measure the RMS and peak levels of an\r\n                audio stream.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o LevelMetering\r\n                        cd LevelMetering\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System.IO;\r\n\r\n                        namespace LevelMetering;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a LevelMeterAnalyzer.\r\n                                var levelMeter = new LevelMeterAnalyzer();\r\n\r\n                                // Connect the player's output to the level meter's input.\r\n                                player.AddAnalyzer(levelMeter);\r\n\r\n                                // Add the player to the master mixer (the level meter doesn't produce output).\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Create a timer to periodically display the RMS and peak levels.\r\n                                var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    Console.WriteLine($\"RMS Level: {levelMeter.Rms:F4}, Peak Level: {levelMeter.Peak:F4}\");\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `LevelMeterAnalyzer`, connects the player's output to the analyzer's input, adds the player to the\r\n                `Master` mixer, and starts playback. It then creates a timer that fires every 100 milliseconds, printing\r\n                the current RMS and peak levels to the console.\r\n            </Tab>\r\n\r\n            <Tab key=\"spectrum-analysis\" title=\"Spectrum Analysis\">\r\n                This tutorial demonstrates how to use the `SpectrumAnalyzer` to analyze frequency content using FFT.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o SpectrumAnalysis\r\n                        cd SpectrumAnalysis\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with analyzer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace SpectrumAnalysis;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                                var spectrumAnalyzer = new SpectrumAnalyzer(fftSize: 2048);\r\n\r\n                                // Connect the player's output to the spectrum analyzer's input.\r\n                                player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                                // Add the player to the master mixer (the spectrum analyzer doesn't produce output).\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Create a timer to periodically display the spectrum data.\r\n                                var timer = new System.Timers.Timer(100); // Update every 100 milliseconds\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    // Get the spectrum data from the analyzer.\r\n                                    var spectrumData = spectrumAnalyzer.SpectrumData;\r\n\r\n                                    // Print the magnitude of the first few frequency bins.\r\n                                    if (spectrumData.Length > 0)\r\n                                    {\r\n                                        Console.Write(\"Spectrum: \");\r\n                                        for (int i = 0; i < Math.Min(10, spectrumData.Length); i++)\r\n                                        {\r\n                                            Console.Write($\"{spectrumData[i]:F2} \");\r\n                                        }\r\n                                        Console.WriteLine();\r\n                                    }\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying spectrum data... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `SpectrumAnalyzer` with an FFT size of 2048, connects the player's output to the analyzer's input, adds\r\n                the player to the `Master` mixer, and starts playback. It then creates a timer that fires every 100\r\n                milliseconds, printing the magnitude of the first 10 frequency bins of the spectrum data to the console.\r\n            </Tab>\r\n\r\n            <Tab key=\"vad\" title=\"Voice Activity Detection\">\r\n                This tutorial demonstrates how to use the `VoiceActivityDetector` to detect the presence of human voice.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o VoiceActivityDetection\r\n                        cd VoiceActivityDetection\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement VAD on a source\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using System.IO;\r\n\r\n                        namespace VoiceActivityDetection;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine (either for playback or recording).\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback); // Or Capability.Record for microphone input\r\n\r\n                                // Create a SoundPlayer and load an audio file (if using playback).\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a VoiceActivityDetector.\r\n                                var vad = new VoiceActivityDetector();\r\n\r\n                                // Connect the VAD as an analyzer to the player's output (or microphone input).\r\n                                player.AddAnalyzer(vad);\r\n\r\n                                // Subscribe to the SpeechDetected event.\r\n                                vad.SpeechDetected += isDetected => Console.WriteLine($\"Speech detected: {isDetected}\");\r\n\r\n                                // Add the player to the master mixer (if using playback).\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback (if using playback).\r\n                                player.Play();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Analyzing audio for voice activity... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback (if using playback) and clean up.\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine` (either for playback or recording), creates a `SoundPlayer` and\r\n                loads an audio file (if using playback), creates a `VoiceActivityDetector`, connects the player's output\r\n                (or microphone input) to the VAD, subscribes to the `SpeechDetected` event to print messages to the\r\n                console when speech is detected or not detected, adds the player to the `Master` mixer (if using\r\n                playback), and starts playback.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"visualization\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:bar-chart-3\"/><span>Visualization</span>\r\n         </div>}>\r\n        <Tabs color=\"secondary\" variant=\"underlined\" aria-label=\"Visualization tutorials\">\r\n            <Tab key=\"level-meter-viz\" title=\"Level Meter\">\r\n                Demonstrates creating a console-based level meter using the `LevelMeterAnalyzer` and\r\n                `LevelMeterVisualizer`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o LevelMeterVisualization\r\n                        cd LevelMeterVisualization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement player with visualizer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System.Diagnostics;\r\n                        using System.IO;\r\n\r\n                        namespace LevelMeterVisualization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a LevelMeterAnalyzer.\r\n                                var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                                // Create a LevelMeterVisualizer.\r\n                                var levelMeterVisualizer = new LevelMeterVisualizer(levelMeterAnalyzer);\r\n\r\n                                // Connect the player's output to the level meter analyzer's input.\r\n                                player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                                levelMeterVisualizer.VisualizationUpdated += (sender, e) =>\r\n                                {\r\n                                    DrawLevelMeter(levelMeterAnalyzer.Rms, levelMeterAnalyzer.Peak);\r\n                                };\r\n\r\n                                // Start a timer to update the visualization.\r\n                                var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    levelMeterVisualizer.ProcessOnAudioData(System.Array.Empty<float>());\r\n                                    levelMeterVisualizer.Render(new ConsoleVisualizationContext());\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying level meter... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                levelMeterVisualizer.Dispose();\r\n                            }\r\n\r\n                            // Helper method to draw a simple console-based level meter.\r\n                            private static void DrawLevelMeter(float rms, float peak)\r\n                            {\r\n                                int barLength = (int)(rms * 40); // Scale the RMS value to a bar length\r\n                                int peakBarLength = (int)(peak * 40); // Scale the peak value to a bar length\r\n\r\n                                Console.SetCursorPosition(0, 0);\r\n                                Console.Write(\"RMS:  \");\r\n                                Console.Write(new string('#', barLength));\r\n                                Console.Write(new string(' ', 40 - barLength));\r\n                                Console.Write(\"|\\n\");\r\n\r\n                                Console.SetCursorPosition(0, 1);\r\n                                Console.Write(\"Peak: \");\r\n                                Console.Write(new string('#', peakBarLength));\r\n                                Console.Write(new string(' ', 40 - peakBarLength));\r\n                                Console.Write(\"|\");\r\n\r\n                                Console.SetCursorPosition(0, 3);\r\n                            }\r\n                        }\r\n\r\n                        // Simple IVisualizationContext implementation for console output.\r\n                        public class ConsoleVisualizationContext : IVisualizationContext\r\n                        {\r\n                            public void Clear() {}\r\n                            public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1) {}\r\n                            public void DrawRectangle(float x, float y, float width, float height, Color color) {}\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `LevelMeterAnalyzer` and a `LevelMeterVisualizer`, connects the player's output to the analyzer, adds\r\n                the player to the `Master` mixer, and starts playback. It then subscribes to the `VisualizationUpdated`\r\n                event of the visualizer to redraw the level meter when the data changes. Finally, it starts a timer that\r\n                calls `ProcessOnAudioData` and `Render` on the visualizer approximately 60 times per second. The\r\n                `DrawLevelMeter` method is a helper function that draws a simple console-based level meter using `#`\r\n                characters.\r\n            </Tab>\r\n\r\n            <Tab key=\"waveform-viz\" title=\"Waveform\">\r\n                Demonstrates using `WaveformVisualizer` to display an audio waveform.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o WaveformVisualization\r\n                        cd WaveformVisualization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement waveform visualizer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System;\r\n                        using System.Collections.Generic;\r\n                        using System.IO;\r\n\r\n                        namespace WaveformVisualization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a LevelMeterAnalyzer or any analyzer you want.\r\n                                var levelMeterAnalyzer = new LevelMeterAnalyzer();\r\n\r\n                                // Create a WaveformVisualizer.\r\n                                var waveformVisualizer = new WaveformVisualizer();\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                                waveformVisualizer.VisualizationUpdated += (sender, e) =>\r\n                                {\r\n                                    DrawWaveform(waveformVisualizer.Waveform);\r\n                                };\r\n\r\n                                // Connect the player's output to the level meter analyzer's input.\r\n                                player.AddAnalyzer(levelMeterAnalyzer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Start a timer to update the visualization.\r\n                                var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    waveformVisualizer.Render(new ConsoleVisualizationContext()); // ConsoleVisualizationContext is just a placeholder\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying waveform... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                waveformVisualizer.Dispose();\r\n                            }\r\n\r\n                            // Helper method to draw a simple console-based waveform.\r\n                            private static void DrawWaveform(List<float> waveform)\r\n                            {\r\n                                Console.Clear();\r\n                                int consoleWidth = Console.WindowWidth;\r\n                                int consoleHeight = Console.WindowHeight;\r\n\r\n                                if (waveform.Count == 0) return;\r\n\r\n                                for (int i = 0; i < consoleWidth; i++)\r\n                                {\r\n                                    int waveformIndex = (int)(i * (waveform.Count / (float)consoleWidth));\r\n                                    waveformIndex = Math.Clamp(waveformIndex, 0, waveform.Count - 1);\r\n\r\n                                    float sampleValue = waveform[waveformIndex];\r\n                                    int consoleY = (int)((sampleValue + 1) * 0.5 * consoleHeight);\r\n                                    consoleY = Math.Clamp(consoleY, 0, consoleHeight - 1);\r\n\r\n                                    if (i < Console.WindowWidth && (consoleHeight - consoleY - 1) < Console.WindowHeight)\r\n                                    {\r\n                                        Console.SetCursorPosition(i, consoleHeight - consoleY - 1);\r\n                                        Console.Write(\"*\");\r\n                                    }\r\n                                }\r\n                                Console.SetCursorPosition(0, consoleHeight - 1);\r\n                            }\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `WaveformVisualizer`, adds the player to the `Master` mixer, and starts playback. It subscribes to the\r\n                `VisualizationUpdated` event of the visualizer to redraw the waveform when the data changes. The\r\n                `DrawWaveform` method is a helper function that draws a simple console-based waveform using `*`\r\n                characters. The `AudioEngine.OnAudioProcessed` is used to send chunks of processed audio data to the\r\n                `WaveformVisualizer`.\r\n            </Tab>\r\n\r\n            <Tab key=\"spectrum-viz\" title=\"Spectrum Analyzer\">\r\n                Demonstrates creating a console-based spectrum analyzer using the `SpectrumAnalyzer` and\r\n                `SpectrumVisualizer`.\r\n                <Steps>\r\n                    <Step title=\"Create & Install\" description=\"Setup project & package\"\r\n                          icon='ic:outline-create-new-folder'>\r\n                        ### 1. Create a new console application and install SoundFlow:\r\n                        ```bash\r\n                        dotnet new console -o SpectrumAnalyzerVisualization\r\n                        cd SpectrumAnalyzerVisualization\r\n                        dotnet add package SoundFlow\r\n                        ```\r\n                    </Step>\r\n                    <Step title=\"Write Code\" description=\"Implement spectrum visualizer\" icon='ph:code-bold'>\r\n                        ### 2. Replace `Program.cs` with the following code:\r\n                        ```csharp\r\n                        using SoundFlow.Abstracts;\r\n                        using SoundFlow.Backends.MiniAudio;\r\n                        using SoundFlow.Components;\r\n                        using SoundFlow.Enums;\r\n                        using SoundFlow.Providers;\r\n                        using SoundFlow.Visualization;\r\n                        using System;\r\n                        using System.IO;\r\n\r\n                        namespace SpectrumAnalyzerVisualization;\r\n\r\n                        internal static class Program\r\n                        {\r\n                            private static void Main(string[] args)\r\n                            {\r\n                                // Initialize the audio engine.\r\n                                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\r\n\r\n                                // Create a SoundPlayer and load an audio file.\r\n                                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\r\n                                var player = new SoundPlayer(dataProvider);\r\n\r\n                                // Create a SpectrumAnalyzer with an FFT size of 2048.\r\n                                var spectrumAnalyzer = new SpectrumAnalyzer(fftSize: 2048);\r\n\r\n                                // Create a SpectrumVisualizer.\r\n                                var spectrumVisualizer = new SpectrumVisualizer(spectrumAnalyzer);\r\n\r\n                                // Connect the player's output to the spectrum analyzer's input.\r\n                                player.AddAnalyzer(spectrumAnalyzer);\r\n\r\n                                // Add the player to the master mixer.\r\n                                Mixer.Master.AddComponent(player);\r\n\r\n                                // Start playback.\r\n                                player.Play();\r\n\r\n                                // Subscribe to the VisualizationUpdated event to trigger a redraw.\r\n                                spectrumVisualizer.VisualizationUpdated += (sender, e) =>\r\n                                {\r\n                                    DrawSpectrum(spectrumAnalyzer.SpectrumData);\r\n                                };\r\n\r\n                                // Start a timer to update the visualization.\r\n                                var timer = new System.Timers.Timer(1000 / 60); // Update at approximately 60 FPS\r\n                                timer.Elapsed += (sender, e) =>\r\n                                {\r\n                                    spectrumVisualizer.ProcessOnAudioData(Array.Empty<float>());\r\n                                    spectrumVisualizer.Render(new ConsoleVisualizationContext());\r\n                                };\r\n                                timer.Start();\r\n\r\n                                // Keep the console application running until the user presses a key.\r\n                                Console.WriteLine(\"Playing audio and displaying spectrum analyzer... Press any key to stop.\");\r\n                                Console.ReadKey();\r\n\r\n                                // Stop playback and clean up.\r\n                                timer.Stop();\r\n                                player.Stop();\r\n                                Mixer.Master.RemoveComponent(player);\r\n                                spectrumVisualizer.Dispose();\r\n                            }\r\n\r\n                            // Helper method to draw a simple console-based spectrum analyzer.\r\n                            private static void DrawSpectrum(ReadOnlySpan<float> spectrumData)\r\n                            {\r\n                                Console.Clear();\r\n                                int consoleWidth = Console.WindowWidth;\r\n                                int consoleHeight = Console.WindowHeight;\r\n\r\n                                if (spectrumData.IsEmpty) return;\r\n\r\n                                int barWidth = Math.Max(1, consoleWidth / spectrumData.Length);\r\n\r\n                                for (int i = 0; i < spectrumData.Length; i++)\r\n                                {\r\n                                    float magnitude = spectrumData[i];\r\n                                    int barHeight = (int)(magnitude * consoleHeight / 2);\r\n                                    barHeight = Math.Clamp(barHeight, 0, consoleHeight - 1);\r\n\r\n                                    for (int j = 0; j < barHeight; j++)\r\n                                    {\r\n                                        for (int w = 0; w < barWidth; w++)\r\n                                        {\r\n                                            if ((i * barWidth + w) < consoleWidth - 1)\r\n                                            {\r\n                                                Console.SetCursorPosition(i * barWidth + w, consoleHeight - 1 - j);\r\n                                                Console.Write(\"*\");\r\n                                            }\r\n                                        }\r\n                                    }\r\n                                }\r\n                                Console.SetCursorPosition(0, consoleHeight - 1);\r\n                            }\r\n                        }\r\n\r\n                        // Simple IVisualizationContext implementation for console output.\r\n                        public class ConsoleVisualizationContext : IVisualizationContext\r\n                        {\r\n                            public void Clear() {}\r\n                            public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f) {}\r\n                            public void DrawRectangle(float x, float y, float width, float height, Color color) {}\r\n                        }\r\n                        ```\r\n                        ***Replace `\"path/to/your/audiofile.wav\"` with the actual path to an audio file.***\r\n                    </Step>\r\n                    <Step title=\"Run Application\" description=\"Run the app\" icon='lucide:play'>\r\n                        ### 3. Build and run the application:\r\n                        ```bash\r\n                        dotnet run\r\n                        ```\r\n                    </Step>\r\n                </Steps>\r\n\r\n                **Explanation:**\r\n\r\n                This code initializes the `AudioEngine`, creates a `SoundPlayer`, loads an audio file, creates a\r\n                `SpectrumAnalyzer` and a `SpectrumVisualizer`, connects the player's output to the analyzer, adds the\r\n                player to the `Master` mixer, and starts playback. It subscribes to the `VisualizationUpdated` event of\r\n                the visualizer to redraw the spectrum when the data changes. The `DrawSpectrum` method is a helper\r\n                function that draws a simple console-based spectrum analyzer using `*` characters. The height of each\r\n                bar represents the magnitude of the corresponding frequency bin.\r\n            </Tab>\r\n        </Tabs>\r\n    </Tab>\r\n\r\n    <Tab key=\"ui-integration\" title={<div className=\"flex items-center gap-2\"><Icon\r\n        icon=\"lucide:layout-template\"/><span>UI Integration</span></div>}>\r\n        These examples use basic console output for simplicity. To integrate SoundFlow's visualizers with a GUI\r\n        framework (like WPF, WinForms, Avalonia, or MAUI), you'll need to:\r\n        <Steps layout='vertical'>\r\n            <Step title=\"Implement IVisualizationContext\" description=\"Wrap your UI framework's drawing primitives\"\r\n                  icon='material-symbols:draw-outline'>\r\n                This class will wrap the drawing primitives of your chosen UI framework. For example, in WPF, you might\r\n                use `DrawingContext` methods to draw shapes on a `Canvas`.\r\n            </Step>\r\n            <Step title=\"Update UI from Event\" description=\"Trigger a redraw on the UI thread\" icon='mdi:update'>\r\n                In the `VisualizationUpdated` event handler, trigger a redraw of your UI element that hosts the\r\n                visualization. Make sure to marshal the update to the UI thread using `Dispatcher.Invoke` or a similar\r\n                mechanism if the event is raised from a different thread.\r\n            </Step>\r\n            <Step title=\"Call Render Method\" description=\"Pass your context to the visualizer\" icon='lucide:render'>\r\n                In your UI's rendering logic, call the `Render` method of the visualizer, passing your\r\n                `IVisualizationContext` implementation.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Example (Conceptual WPF):**\r\n\r\n        ```csharp\r\n        // In your XAML:\r\n        // <Canvas x:Name=\"VisualizationCanvas\"/>\r\n\r\n        // In your code-behind:\r\n        public partial class MainWindow : Window\r\n        {\r\n            private readonly WaveformVisualizer _visualizer;\r\n\r\n            public MainWindow()\r\n            {\r\n                InitializeComponent();\r\n\r\n                // ... Initialize AudioEngine, SoundPlayer, etc. ...\r\n\r\n                _visualizer = new WaveformVisualizer();\r\n                _visualizer.VisualizationUpdated += OnVisualizationUpdated;\r\n\r\n                // ...\r\n            }\r\n\r\n            private void OnVisualizationUpdated(object? sender, EventArgs e)\r\n            {\r\n                // Marshal the update to the UI thread\r\n                Dispatcher.Invoke(() =>\r\n                {\r\n                    VisualizationCanvas.Children.Clear(); // Clear previous drawing\r\n\r\n                    // Create a custom IVisualizationContext that wraps the Canvas\r\n                    var context = new WpfVisualizationContext(VisualizationCanvas);\r\n\r\n                    // Render the visualization\r\n                    _visualizer.Render(context);\r\n                });\r\n            }\r\n\r\n            // ...\r\n        }\r\n\r\n        // IVisualizationContext implementation for WPF\r\n        public class WpfVisualizationContext : IVisualizationContext\r\n        {\r\n            private readonly Canvas _canvas;\r\n\r\n            public WpfVisualizationContext(Canvas canvas)\r\n            {\r\n                _canvas = canvas;\r\n            }\r\n\r\n            public void Clear()\r\n            {\r\n                _canvas.Children.Clear();\r\n            }\r\n\r\n            public void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)\r\n            {\r\n                var line = new Line\r\n                {\r\n                    X1 = x1,\r\n                    Y1 = y1,\r\n                    X2 = x2,\r\n                    Y2 = y2,\r\n                    Stroke = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255))),\r\n                    StrokeThickness = thickness\r\n                };\r\n                _canvas.Children.Add(line);\r\n            }\r\n\r\n            public void DrawRectangle(float x, float y, float width, float height, Color color)\r\n            {\r\n                var rect = new Rectangle\r\n                {\r\n                    Width = width,\r\n                    Height = height,\r\n                    Fill = new SolidColorBrush(System.Windows.Media.Color.FromArgb((byte)(color.A * 255), (byte)(color.R * 255), (byte)(color.G * 255), (byte)(color.B * 255)))\r\n                };\r\n                Canvas.SetLeft(rect, x);\r\n                Canvas.SetTop(rect, y);\r\n                _canvas.Children.Add(rect);\r\n            }\r\n        }\r\n        ```\r\n\r\n        Remember to adapt this conceptual example to your specific UI framework and project structure.\r\n    </Tab>\r\n\r\n    <Tab key=\"device-management\"\r\n         title={<div className=\"flex items-center gap-2\"><Icon icon=\"lucide:audio-lines\"/><span>Device Management</span>\r\n         </div>}>\r\n        This tutorial demonstrates how to list available audio devices and switch the playback device.\r\n        <Steps>\r\n            <Step title=\"Create & Install\" description=\"Setup project & package\" icon='ic:outline-create-new-folder'>\r\n                ### 1. Create a new console application and install SoundFlow.\r\n            </Step>\r\n            <Step title=\"Write Code\" description=\"Implement the device switcher\" icon='ph:code-bold'>\r\n                ### 2. Replace `Program.cs`:\r\n                ```csharp\r\n                using SoundFlow.Abstracts;\r\n                using SoundFlow.Backends.MiniAudio;\r\n                using SoundFlow.Components;\r\n                using SoundFlow.Enums;\r\n                using SoundFlow.Providers;\r\n                using SoundFlow.Structs;\r\n                using System;\r\n                using System.IO;\r\n                using System.Linq;\r\n\r\n                namespace DeviceSwitcher;\r\n\r\n                internal static class Program\r\n                {\r\n                    private static void Main(string[] args)\r\n                    {\r\n                        // Initialize engine (MiniAudioEngine used here)\r\n                        using var engine = new MiniAudioEngine(48000, Capability.Playback);\r\n\r\n                        void PrintDevices()\r\n                        {\r\n                            engine.UpdateDevicesInfo();\r\n                            Console.WriteLine(\"\\nAvailable Playback Devices:\");\r\n                            for (int i = 0; i < engine.PlaybackDeviceCount; i++)\r\n                            {\r\n                                Console.WriteLine($\"{i}: {engine.PlaybackDevices[i].Name} {(engine.PlaybackDevices[i].IsDefault ? \"(Default)\" : \"\")}\");\r\n                            }\r\n                                Console.WriteLine($\"Current Playback Device: {engine.CurrentPlaybackDevice?.Name ?? \"None selected\"}\");\r\n                        }\r\n\r\n                        PrintDevices();\r\n\r\n                        // Simple audio playback setup\r\n                        Console.Write(\"Enter path to an audio file to play: \");\r\n                        string? filePath = Console.ReadLine()?.Trim('\"');\r\n                        if (string.IsNullOrEmpty(filePath) || !File.Exists(filePath))\r\n                        {\r\n                            Console.WriteLine(\"Invalid file path. Exiting.\");\r\n                            return;\r\n                        }\r\n\r\n                        using var dataProvider = new StreamDataProvider(File.OpenRead(filePath));\r\n                        var player = new SoundPlayer(dataProvider);\r\n                        Mixer.Master.AddComponent(player);\r\n                        player.Play();\r\n                        Console.WriteLine($\"Playing on {engine.CurrentPlaybackDevice?.Name ?? \"default device\"}.\");\r\n\r\n                        while (true)\r\n                        {\r\n                            Console.Write(\"Enter device number to switch to, 'r' to refresh list, or 'q' to quit: \");\r\n                            string? input = Console.ReadLine();\r\n\r\n                            if (input?.ToLower() == \"q\") break;\r\n                            if (input?.ToLower() == \"r\")\r\n                            {\r\n                                PrintDevices();\r\n                                continue;\r\n                            }\r\n\r\n                            if (int.TryParse(input, out int deviceIndex) && deviceIndex >= 0 && deviceIndex < engine.PlaybackDeviceCount)\r\n                            {\r\n                                try\r\n                                {\r\n                                    Console.WriteLine($\"Switching to {engine.PlaybackDevices[deviceIndex].Name}...\");\r\n                                    engine.SwitchDevice(engine.PlaybackDevices[deviceIndex], DeviceType.Playback);\r\n                                    Console.WriteLine($\"Successfully switched to {engine.CurrentPlaybackDevice?.Name}.\");\r\n                                }\r\n                                catch (Exception ex)\r\n                                {\r\n                                    Console.WriteLine($\"Error switching device: {ex.Message}\");\r\n                                }\r\n                            }\r\n                            else\r\n                            {\r\n                                Console.WriteLine(\"Invalid input.\");\r\n                            }\r\n                        }\r\n\r\n                        player.Stop();\r\n                        Mixer.Master.RemoveComponent(player);\r\n                    }\r\n                }\r\n                ```\r\n            </Step>\r\n            <Step title=\"Run & Test\" description=\"Switch devices during playback\" icon='lucide:play'>\r\n                ### 3. Build and run.\r\n                You'll see a list of playback devices. Enter the number of the device you want to switch to while audio\r\n                is playing.\r\n            </Step>\r\n        </Steps>\r\n\r\n        **Explanation:**\r\n        The `AudioEngine` (here `MiniAudioEngine`) provides `UpdateDevicesInfo()` to get device lists\r\n        (`PlaybackDevices`, `CaptureDevices`). `SwitchDevice()` or `SwitchDevices()` can then be used to change the\r\n        active audio output/input.\r\n    </Tab>\r\n\r\n    <Tab key=\"editing\" title={<div className=\"flex items-center gap-2\"><Icon icon=\"mdi:content-cut\"/><span>Editing & Persistence</span>\r\n    </div>}>\r\n        This new section covers the powerful non-destructive editing engine introduced in SoundFlow v1.1.0. See the\r\n        dedicated [Editing Engine & Persistence Guide](./editing-engine) for comprehensive details and examples.\r\n\r\n        **Key features demonstrated in the guide:**\r\n        * Creating `Composition`s, `Track`s, and `AudioSegment`s.\r\n        * Manipulating segment properties: `SourceStartTime`, `SourceDuration`, `TimelineStartTime`.\r\n        * Using `AudioSegmentSettings`: volume, pan, reverse, looping, fades (`FadeCurveType`).\r\n\r\n        A simple example of creating a composition and adding a segment:\r\n\r\n        ```csharp\r\n        using SoundFlow.Abstracts;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Components;\r\n        using SoundFlow.Enums;\r\n        using SoundFlow.Providers;\r\n        using SoundFlow.Editing; // New namespace\r\n        using System;\r\n        using System.IO;\r\n        using System.Threading.Tasks;\r\n\r\n        namespace BasicComposition;\r\n\r\n        internal static class Program\r\n        {\r\n            private static async Task Main(string[] args)\r\n            {\r\n                // Initialize the audio engine.\r\n                using var audioEngine = new MiniAudioEngine(48000, Capability.Playback, channels: 1);\r\n\r\n                // Create a composition\r\n                var composition = new Composition(\"My First Song\") {SampleRate = 48000, TargetChannels = 1};\r\n\r\n                // Create a track\r\n                var track1 = new Track(\"Vocals\");\r\n                composition.AddTrack(track1);\r\n\r\n                // Load an audio file for a segment\r\n                string audioPath = \"path/to/your/audio.wav\";\r\n                if (!File.Exists(audioPath))\r\n                {\r\n                    Console.WriteLine($\"Audio file not found: {audioPath}\");\r\n                    return;\r\n                }\r\n                var provider = new StreamDataProvider(File.OpenRead(audioPath));\r\n\r\n                // Create an audio segment\r\n                // Play from 0s of source, for 5s duration, place at 1s on timeline\r\n                var segment1 = new AudioSegment(provider,\r\n                TimeSpan.Zero,\r\n                TimeSpan.FromSeconds(5),\r\n                TimeSpan.FromSeconds(1),\r\n                \"Intro\",\r\n                ownsDataProvider: true);\r\n\r\n                // Optionally, modify segment settings\r\n                segment1.Settings.Volume = 0.9f;\r\n                segment1.Settings.FadeInDuration = TimeSpan.FromMilliseconds(500);\r\n\r\n                track1.AddSegment(segment1);\r\n\r\n                // Create a SoundPlayer for the composition\r\n                var compositionPlayer = new SoundPlayer(composition); // Composition itself is an ISoundDataProvider\r\n                Mixer.Master.AddComponent(compositionPlayer);\r\n                compositionPlayer.Play();\r\n\r\n                Console.WriteLine($\"Playing composition '{composition.Name}' for {composition.CalculateTotalDuration().TotalSeconds:F1}s... Press any key to stop.\");\r\n                Console.ReadKey();\r\n\r\n                compositionPlayer.Stop();\r\n                Mixer.Master.RemoveComponent(compositionPlayer);\r\n\r\n                composition.Dispose();\r\n            }\r\n        }\r\n        ```\r\n        *For this example to run, you'd need an audio file. The new `SoundFlow.Samples.EditingMixer` project contains\r\n        sample audio files and more complex editing examples.*\r\n    </Tab>\r\n</Tabs>\r\n\r\n---\r\n\r\nThese tutorials and examples provide a starting point for using SoundFlow in your own audio applications.Explore the different components, modifiers, analyzers, and visualizers to create a wide range of audio processing and visualization solutions.Refer to the ** Core Concepts ** and ** API Reference ** sections of the Wiki for more detailed information about each class and interface."
  },
  {
    "id": 47,
    "slug": "getting-started",
    "version": "1.1.2",
    "title": "Getting Started with SoundFlow",
    "description": "Learn how to install the library, set up your development environment, and write your first SoundFlow application.",
    "navOrder": 1,
    "category": "Core",
    "content": "---\ntitle: Getting Started with SoundFlow\ndescription: Learn how to install the library, set up your development environment, and write your first SoundFlow application.\nnavOrder: 1\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\n# Getting Started with SoundFlow\n\nThis guide will help you get up and running with SoundFlow quickly. You'll learn how to install the library, set up your development environment, and write your first SoundFlow application.\n\n## Prerequisites\n\nBefore you begin, make sure you have the following installed:\n\n*   **[.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0) or later:** SoundFlow is built on .NET 8.0, so you'll need the corresponding SDK to build and run SoundFlow projects.\n*   **An IDE or code editor:** You can use any IDE or code editor that supports .NET development. Popular choices include:\n    *   [Visual Studio](https://visualstudio.microsoft.com/) (Recommended for Windows)\n    *   [Visual Studio Code](https://code.visualstudio.com/) (Cross-platform)\n    *   [JetBrains Rider](https://www.jetbrains.com/rider/) (Cross-platform)\n*   **Basic knowledge of C# and .NET:** Familiarity with C# programming and .NET concepts will be helpful.\n\n**Supported Operating Systems:**\n\n*   Windows\n*   macOS\n*   Linux\n*   Android\n*   iOS\n*   FreeBSD\n\n## Installation\n\nYou can install SoundFlow in several ways:\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Installation options\">\n    <Tab\n        key=\"nuget\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:nuget' />\n                <span>NuGet Package Manager</span>\n            </div>\n        }\n    >\n        ### Option 1: Using the NuGet Package Manager (Recommended)\n\n        This is the easiest and recommended way to add SoundFlow to your .NET projects.\n\n        1. **Open the NuGet Package Manager Console:** In Visual Studio, go to `Tools` > `NuGet Package Manager` > `Package Manager Console`.\n        2. **Run the installation command:**\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        This command will download and install the latest version of SoundFlow and its dependencies into your current project.\n    </Tab>\n\n    <Tab\n        key=\"cli\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:dotnet' />\n                <span>.NET CLI</span>\n            </div>\n        }\n    >\n        ### Option 2: Using the .NET CLI\n\n        1. **Open your terminal or command prompt.**\n        2. **Navigate to your project directory.**\n        3. **Run the following command:**\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n\n        This command will add a reference to the SoundFlow package in your project file (`.csproj`).\n    </Tab>\n\n    <Tab\n        key=\"source\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='simple-icons:git' />\n                <span>Building from Source</span>\n            </div>\n        }\n    >\n        ### Option 3: Building from Source\n\n        If you want to use the latest development version of SoundFlow or contribute to the project, you can build it from source:\n\n        1. **Clone the SoundFlow repository:**\n\n        ```bash\n        git clone https://github.com/LSXPrime/SoundFlow.git\n        ```\n\n        2. **Navigate to the cloned directory:**\n\n        ```bash\n        cd SoundFlow\n        ```\n\n        3. **Build the project using the .NET CLI:**\n\n        ```bash\n        dotnet build\n        ```\n    </Tab>\n</Tabs>\n\n## Basic Usage Example\n\nLet's create a simple console application that plays an audio file using SoundFlow.\n\n<Steps layout='horizontal' variant='glow' nextLabel='Got it, Next' finishLabel='All Done!' resetLabel='Start Again' summaryMessage=\"Congratulations! You've built and run your first audio application with SoundFlow. Feel free to experiment with the code or start over.\">\n    <Step title=\"Create Project\" description=\"Use VS or .NET CLI\" icon='ic:outline-create-new-folder'>\n        ### 1. Create a new console application:\n        *   In Visual Studio, go to `File` > `New` > `Project`. Select `Console App` and give it a name (e.g., `SoundFlowExample`).\n        *   Or, use the .NET CLI:\n\n        ```bash\n        dotnet new console -o SoundFlowExample\n        cd SoundFlowExample\n        ```\n    </Step>\n\n    <Step title=\"Install Package\" description=\"Add SoundFlow via NuGet\" icon='lucide:download'>\n        ### 2. Install the SoundFlow NuGet package: (If you haven't already)\n\n        ```bash\n        Install-Package SoundFlow\n        ```\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        ```bash\n        dotnet add package SoundFlow\n        ```\n    </Step>\n\n    <Step title=\"Write Code\" description=\"Implement the audio player\" icon='ph:code-bold'>\n        ### 3. Replace the contents of `Program.cs` with the following code:\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Backends.MiniAudio;\n        using SoundFlow.Components;\n        using SoundFlow.Enums;\n        using SoundFlow.Providers;\n\n        namespace SoundFlowExample;\n\n        internal static class Program\n        {\n            private static void Main(string[] args)\n            {\n                // Initialize the audio engine with the MiniAudio backend, 44.1kHz sample rate, and playback capability.\n                using var audioEngine = new MiniAudioEngine(44100, Capability.Playback);\n\n                // Create a SoundPlayer and load an audio file.\n                // Make sure you replace \"path/to/your/audiofile.wav\" with the actual path to your audio file.\n                using var dataProvider = new StreamDataProvider(File.OpenRead(\"path/to/your/audiofile.wav\"));\n                var player = new SoundPlayer(dataProvider);\n\n                // Add the player to the master mixer. This connects the player's output to the audio engine's output.\n                Mixer.Master.AddComponent(player);\n\n                // Start playback.\n                player.Play();\n\n                // Keep the console application running until playback finishes or the user presses a key.\n                Console.WriteLine(\"Playing audio... Press any key to stop.\");\n                Console.ReadKey();\n\n                // Stop playback.\n                player.Stop();\n\n                // Remove the player from the mixer.\n                Mixer.Master.RemoveComponent(player);\n            }\n        }\n        ```\n        ***Replace `path/to/your/audiofile.wav` with the actual path to an audio file on your computer.***\n    </Step>\n\n    <Step title=\"Run\" description=\"Build and run the app\" icon='lucide:audio-lines'>\n        ### 4. Build and run the application:\n        *   In Visual Studio, press `F5` or go to `Debug` > `Start Debugging`.\n\n        <div style={{textAlign: 'center'}}>OR</div>\n\n        *  use the .NET CLI:\n\n        ```bash\n        dotnet run\n        ```\n    </Step>\n</Steps>\n\nYou should now hear the audio file playing through your default audio output device.\n\n**Code Explanation:**\n\n*   `using SoundFlow...`: These lines import the necessary namespaces from the SoundFlow library.\n*   `using var audioEngine = new MiniAudioEngine(...)`: This creates an instance of the `MiniAudioEngine`, which is the default audio backend for SoundFlow. It's initialized with a sample rate of 44100 Hz and playback capability. The `using` statement ensures that the engine is properly disposed of when it's no longer needed.\n*   `using var dataProvider = new StreamDataProvider(...)`: Creates a `StreamDataProvider` to load the audio file. `ISoundDataProvider` (which `StreamDataProvider` implements) is now `IDisposable`, so it's good practice to use a `using` statement.\n*   `var player = new SoundPlayer(dataProvider)`: This creates a `SoundPlayer` instance with the loaded audio data.\n*   `Mixer.Master.AddComponent(player)`: This adds the `SoundPlayer` to the `Master` mixer. The `Master` mixer is the root of the audio graph and represents the final output of the audio engine.\n*   `player.Play()`: This starts the playback of the audio file.\n*   `Console.WriteLine(...)` and `Console.ReadKey()`: These lines keep the console application running and wait for the user to press a key.\n*   `player.Stop()`: This stops the playback.\n*   `Mixer.Master.RemoveComponent(player)`: This removes the `SoundPlayer` from the `Master` mixer, disconnecting it from the audio graph.\n\n\n## Troubleshooting\n\n*   **\"Could not load file or assembly 'SoundFlow'...\"**: Make sure you have installed the SoundFlow NuGet package or added a reference to the SoundFlow library if you built it from source.\n*   **No audio output**:\n    *   Verify that your audio device is properly configured and selected as the default output device in your operating system's sound settings.\n    *   Check the volume levels in your operating system and in the SoundFlow application.\n    *   Ensure that the audio file you are trying to play is in a supported format and is not corrupted.\n*   **Errors during installation**: If you encounter errors while installing the NuGet package, try clearing your NuGet cache (`dotnet nuget locals all --clear`) and try again.\n\nIf you encounter any other issues, please open an issue on the [GitHub repository](https://github.com/LSXPrime/SoundFlow).\n\n## Next Steps\n\nNow that you have successfully set up SoundFlow and played your first audio file, you can explore the more advanced features and concepts covered in this Wiki:\n\n*   [Core Concepts](./core-concepts): Learn more about the fundamental building blocks of SoundFlow.\n*   [Editing Engine & Persistence](./editing-engine): Discover the powerful new non-destructive editing and project saving capabilities.\n*   [API Reference](./api-reference): Dive into the detailed documentation for each class and interface.\n*   [Tutorials and Examples](./tutorials-and-examples): Get hands-on experience with various SoundFlow features.\n*   [WebRTC APM Extension](./extensions/webrtc-apm): Explore advanced voice processing features like noise suppression and echo cancellation.\n\nHappy coding!"
  },
  {
    "id": 48,
    "slug": "editing-engine",
    "version": "1.1.2",
    "title": "Editing Engine & Persistence",
    "description": "Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.",
    "navOrder": 3,
    "category": "Core",
    "content": "---\r\ntitle: Editing Engine & Persistence\r\ndescription: Dive deep into SoundFlow's non-destructive audio editing engine, project management, time stretching, and media handling capabilities.\r\nnavOrder: 3\r\ncategory: Core\r\n---\r\n\r\nimport {Icon} from \"@iconify/react\";\r\nimport {Tab, Tabs} from \"@heroui/react\";\r\nimport { Steps, Step } from '/src/components/Shared/Steps';\r\n\r\n# SoundFlow Editing Engine & Persistence\r\n\r\nSoundFlow v1.1.0 introduces a comprehensive, non-destructive audio editing engine and a robust project persistence system. This allows developers to programmatically build, manipulate, and save complex audio timelines, complete with effects, advanced timing controls, and media management.\r\n\r\n## Core Editing Concepts\r\n\r\nThe editing engine revolves around a few key classes:\r\n\r\n<Steps layout='vertical'>\r\n    <Step title=\"Composition\" description=\"The top-level project container\" icon='ph:stack-bold'>\r\n        ### `Composition` (`SoundFlow.Editing.Composition`)\r\n\r\n        The `Composition` is the top-level container for an audio project. Think of it as the main \"session\" or \"project file\" in a Digital Audio Workstation (DAW).\r\n\r\n        *   **Holds Tracks:** A `Composition` contains one or more `Track` objects.\r\n        *   **Master Settings:** It has master volume control (`MasterVolume`) and can have master effects (modifiers and analyzers) applied to the final mix.\r\n        *   **Renderable:** A `Composition` itself implements `ISoundDataProvider`, meaning the entire composed project can be played back directly using a `SoundPlayer` or rendered to an audio file.\r\n        *   **Project Properties:** Stores overall project settings like `Name`, `TargetSampleRate`, and `TargetChannels`.\r\n        *   **Dirty Flag:** Tracks unsaved changes via an `IsDirty` property.\r\n        *   **IDisposable:** Manages the disposal of resources within its scope.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Backends.MiniAudio;\r\n        using SoundFlow.Enums;\r\n\r\n        // Create a new composition\r\n        var composition = new Composition(\"My Awesome Project\")\r\n        {\r\n            TargetSampleRate = 48000,\r\n            TargetChannels = 2, // Stereo\r\n            MasterVolume = 0.9f\r\n        };\r\n\r\n        // Add master effects (optional)\r\n        // composition.AddModifier(new SomeMasterReverb());\r\n\r\n        // ... (add tracks and segments) ...\r\n\r\n        // To play the composition:\r\n        // using var audioEngine = new MiniAudioEngine(composition.TargetSampleRate, Capability.Playback, channels: composition.TargetChannels);\r\n        // var player = new SoundPlayer(composition);\r\n        // Mixer.Master.AddComponent(player);\r\n        // player.Play();\r\n\r\n        // To render the composition to a float array:\r\n        // float[] renderedAudio = composition.Render(TimeSpan.Zero, composition.CalculateTotalDuration());\r\n        ```\r\n    </Step>\r\n    <Step title=\"Track\" description=\"A single audio timeline\" icon='lucide:audio-lines'>\r\n        ### `Track` (`SoundFlow.Editing.Track`)\r\n\r\n        A `Track` represents a single audio track within a `Composition`, similar to a track in a DAW.\r\n\r\n        *   **Holds Segments:** A `Track` contains a list of `AudioSegment` objects, which are the actual audio clips placed on the track's timeline.\r\n        *   **Track-Level Settings (`TrackSettings`):** Each track has its own settings:\r\n        *   `Volume`, `Pan`\r\n        *   `IsMuted`, `IsSoloed`, `IsEnabled`\r\n        *   Track-specific `Modifiers` and `Analyzers`.\r\n        *   **Timeline Management:** Tracks manage the arrangement of their segments.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n\r\n        var track1 = new Track(\"Lead Vocals\");\r\n        track1.Settings.Volume = 0.8f;\r\n        track1.Settings.Pan = -0.1f; // Slightly to the left\r\n\r\n        var track2 = new Track(\"Background Music\");\r\n        track2.Settings.Volume = 0.5f;\r\n        track2.Settings.IsMuted = true; // Mute this track for now\r\n\r\n        composition.AddTrack(track1);\r\n        composition.AddTrack(track2);\r\n        ```\r\n    </Step>\r\n    <Step title=\"AudioSegment\" description=\"The fundamental audio clip\" icon='mdi:content-cut'>\r\n        ### `AudioSegment` (`SoundFlow.Editing.AudioSegment`)\r\n\r\n        The `AudioSegment` is the fundamental building block for audio content on a `Track`. It represents a specific portion of an audio source placed at a particular time on the track's timeline.\r\n\r\n        *   **Source Reference:** Points to an `ISoundDataProvider` for its audio data.\r\n        *   **Timeline Placement:**\r\n        *   `SourceStartTime`: The time offset within the `ISoundDataProvider` from which this segment begins.\r\n        *   `SourceDuration`: The duration of audio to use from the `ISoundDataProvider`.\r\n        *   `TimelineStartTime`: The time at which this segment starts on the parent `Track`'s timeline.\r\n        *   **Segment-Level Settings (`AudioSegmentSettings`):** Each segment has incredibly granular control:\r\n        *   `Volume`, `Pan`\r\n        *   `IsEnabled`\r\n        *   `IsReversed`: Play the segment's audio backward.\r\n        *   `Loop` (`LoopSettings`): Control repetitions or loop to fill a target duration.\r\n        *   `FadeInDuration`, `FadeInCurve`, `FadeOutDuration`, `FadeOutCurve`: Apply various fade shapes (`Linear`, `Logarithmic`, `S-Curve`).\r\n        *   `SpeedFactor`: Classic varispeed, affects pitch and tempo.\r\n        *   **Pitch-Preserved Time Stretching:**\r\n        *   `TimeStretchFactor`: Lengthen or shorten the segment without changing pitch (e.g., 0.5 for half duration, 2.0 for double duration).\r\n        *   `TargetStretchDuration`: Stretch the segment to fit a specific duration, preserving pitch.\r\n        *   Segment-specific `Modifiers` and `Analyzers`.\r\n        *   **Non-Destructive:** All operations (trimming, fades, stretching) are applied at runtime and do not alter the original audio source.\r\n        *   **IDisposable:** Can own and dispose its `ISoundDataProvider` if specified.\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Providers;\r\n        using System.IO;\r\n\r\n        // Assuming 'track1' and 'composition' from previous examples\r\n        // And an audio file \"vocals.wav\" exists.\r\n        using var vocalProvider = new StreamDataProvider(File.OpenRead(\"vocals.wav\"));\r\n\r\n        // Create a segment: use 10 seconds of \"vocals.wav\" starting from 5s into the file,\r\n        // and place it at 2 seconds on track1's timeline.\r\n        var vocalSegment = new AudioSegment(\r\n        sourceDataProvider: vocalProvider,\r\n        sourceStartTime: TimeSpan.FromSeconds(5),\r\n        sourceDuration: TimeSpan.FromSeconds(10),\r\n        timelineStartTime: TimeSpan.FromSeconds(2),\r\n        name: \"Verse 1 Vocals\",\r\n        ownsDataProvider: false // vocalProvider is managed by 'using' here\r\n        );\r\n\r\n        // Apply settings\r\n        vocalSegment.Settings.Volume = 0.95f;\r\n        vocalSegment.Settings.FadeInDuration = TimeSpan.FromMilliseconds(200);\r\n        vocalSegment.Settings.FadeInCurve = FadeCurveType.SCurve;\r\n        vocalSegment.Settings.TimeStretchFactor = 1.1f; // Make it 10% longer without pitch change\r\n\r\n        track1.AddSegment(vocalSegment);\r\n        ```\r\n    </Step>\r\n</Steps>\r\n\r\n### Duration Calculations\r\n\r\n*   `AudioSegment.StretchedSourceDuration`: The duration of the segment's content *after* pitch-preserved time stretching is applied (but before `SpeedFactor`).\r\n*   `AudioSegment.EffectiveDurationOnTimeline`: The duration a single instance of the segment takes on the timeline, considering both `StretchedSourceDuration` and `SpeedFactor`.\r\n*   `AudioSegment.GetTotalLoopedDurationOnTimeline()`: The total duration the segment occupies on the timeline, including all loops.\r\n*   `AudioSegment.TimelineEndTime`: `TimelineStartTime + GetTotalLoopedDurationOnTimeline()`.\r\n*   `Track.CalculateDuration()`: The time of the latest `TimelineEndTime` among all its segments.\r\n*   `Composition.CalculateTotalDuration()`: The time of the latest `TimelineEndTime` among all its tracks.\r\n\r\n## Time Manipulation\r\n\r\nSoundFlow's editing engine offers sophisticated time manipulation capabilities for `AudioSegment`s:\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Time manipulation options\">\r\n    <Tab\r\n        key=\"time-stretch\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:timelapse-outline' />\r\n                <span>Pitch-Preserved Time Stretching</span>\r\n            </div>\r\n        }\r\n    >\r\n        This feature allows you to change the duration of an audio segment without affecting its pitch. It's ideal for:\r\n        *   Fitting dialogue or music to a specific time slot.\r\n        *   Creative sound design by drastically stretching or compressing audio.\r\n\r\n        It's controlled by two properties in `AudioSegmentSettings`:\r\n\r\n        *   **`TimeStretchFactor` (float):**\r\n        *   `1.0`: No stretching.\r\n        *   `> 1.0`: Makes the segment longer (e.g., `2.0` doubles the duration).\r\n        *   `< 1.0` and `> 0.0`: Makes the segment shorter (e.g., `0.5` halves the duration).\r\n        *   **`TargetStretchDuration` (TimeSpan?):**\r\n        *   If set, this overrides `TimeStretchFactor`. The segment will be stretched or compressed to match this exact duration.\r\n        *   Set to `null` to use `TimeStretchFactor` instead.\r\n\r\n        Internally, SoundFlow uses a high-quality **WSOLA (Waveform Similarity Overlap-Add)** algorithm implemented in the `WsolaTimeStretcher` class.\r\n\r\n        ```csharp\r\n        // Make a segment 50% shorter while preserving pitch\r\n        mySegment.Settings.TimeStretchFactor = 0.5f;\r\n\r\n        // Make a segment exactly 3.75 seconds long, preserving pitch\r\n        mySegment.Settings.TargetStretchDuration = TimeSpan.FromSeconds(3.75);\r\n        ```\r\n    </Tab>\r\n    <Tab\r\n        key=\"varispeed\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='mdi:speedometer' />\r\n                <span>Classic Speed Control (Varispeed)</span>\r\n            </div>\r\n        }\r\n    >\r\n        The `SpeedFactor` property in `AudioSegmentSettings` provides traditional speed control, affecting both the tempo and the pitch of the audio, similar to changing the playback speed of a tape machine.\r\n\r\n        *   **`SpeedFactor` (float):**\r\n        *   `1.0`: Normal speed and pitch.\r\n        *   `> 1.0`: Faster playback, higher pitch.\r\n        *   `< 1.0` and `> 0.0`: Slower playback, lower pitch.\r\n\r\n        ```csharp\r\n        // Play segment at double speed (and an octave higher)\r\n        mySegment.Settings.SpeedFactor = 2.0f;\r\n\r\n        // Play segment at half speed (and an octave lower)\r\n        mySegment.Settings.SpeedFactor = 0.5f;\r\n        ```\r\n\r\n        **Interaction:** Time stretching is applied to the source audio *first*, and then the `SpeedFactor` is applied to the time-stretched result.\r\n    </Tab>\r\n</Tabs>\r\n\r\n## Project Persistence (`SoundFlow.Editing.Persistence`)\r\n\r\nThe `CompositionProjectManager` class provides static methods for saving and loading your `Composition` objects. Projects are saved in a JSON-based format with the `.sfproj` extension.\r\n\r\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Project persistence options\">\r\n    <Tab\r\n        key=\"save\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:save-outline' />\r\n                <span>Saving a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        ### Saving a Project\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Editing.Persistence;\r\n        using System.Threading.Tasks;\r\n\r\n        public async Task SaveMyProject(Composition composition, string filePath)\r\n        {\r\n            await CompositionProjectManager.SaveProjectAsync(\r\n                composition,\r\n                filePath,\r\n                consolidateMedia: true,  // Recommended for portability\r\n            embedSmallMedia: true   // Embeds small audio files directly\r\n            );\r\n            Console.WriteLine($\"Project saved to {filePath}\");\r\n        }\r\n        ```\r\n\r\n        **Saving Options:**\r\n\r\n        *   **`consolidateMedia` (bool):**\r\n        *   If `true` (default), SoundFlow will attempt to copy all unique external audio files referenced by segments into an `Assets` subfolder next to your `.sfproj` file. This makes the project self-contained and portable.\r\n        *   In-memory `ISoundDataProvider`s (like `RawDataProvider` from generated audio) will also be saved as WAV files in the `Assets` folder if `consolidateMedia` is true.\r\n        *   The project file will then store relative paths to these consolidated assets.\r\n        *   **`embedSmallMedia` (bool):**\r\n        *   If `true` (default), audio sources smaller than a certain threshold (currently 1MB) will be embedded directly into the `.sfproj` file as Base64-encoded strings. This is useful for short sound effects or jingles, avoiding the need for separate files.\r\n        *   Embedding takes precedence over consolidation for small files.\r\n    </Tab>\r\n    <Tab\r\n        key=\"load\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='material-symbols:folder-open-outline' />\r\n                <span>Loading a Project</span>\r\n            </div>\r\n        }\r\n    >\r\n        ### Loading a Project\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Editing.Persistence;\r\n        using System.Threading.Tasks;\r\n        using System.Collections.Generic; // For List\r\n\r\n        public async Task<(Composition?, List<ProjectSourceReference>)> LoadMyProject(string filePath)\r\n        {\r\n            if (!File.Exists(filePath))\r\n        {\r\n            Console.WriteLine($\"Project file not found: {filePath}\");\r\n            return (null, new List<ProjectSourceReference>());\r\n        }\r\n\r\n            var (loadedComposition, unresolvedSources) = await CompositionProjectManager.LoadProjectAsync(filePath);\r\n\r\n            if (unresolvedSources.Any())\r\n        {\r\n            Console.WriteLine(\"Warning: Some media sources could not be found:\");\r\n            foreach (var missing in unresolvedSources)\r\n        {\r\n            Console.WriteLine($\" - Missing ID: {missing.Id}, Original Path: {missing.OriginalAbsolutePath ?? \"N/A\"}\");\r\n            // Here you could trigger a UI for relinking\r\n        }\r\n        }\r\n\r\n            Console.WriteLine($\"Project '{loadedComposition.Name}' loaded successfully!\");\r\n            return (loadedComposition, unresolvedSources);\r\n        }\r\n        ```\r\n\r\n        When loading, `LoadProjectAsync` returns a tuple:\r\n        *   The loaded `Composition` object.\r\n        *   A `List<ProjectSourceReference>` detailing any audio sources that could not be found (based on embedded data, consolidated paths, or original absolute paths).\r\n    </Tab>\r\n    <Tab\r\n        key=\"relink\"\r\n        title={\r\n            <div className=\"flex items-center gap-2\">\r\n                <Icon icon='ph:link-bold' />\r\n                <span>Media Management & Relinking</span>\r\n            </div>\r\n        }\r\n    >\r\n        ### Media Management & Relinking\r\n\r\n        SoundFlow's persistence system attempts to locate media in this order:\r\n        1.  **Embedded Data:** If the `ProjectSourceReference` indicates embedded data, it's decoded.\r\n        2.  **Consolidated Relative Path:** If not embedded, it looks for the file in the `Assets` folder relative to the project file.\r\n        3.  **Original Absolute Path:** If still not found, it tries the original absolute path stored during the save.\r\n\r\n        If a source is still missing, it's added to the `unresolvedSources` list. You can then use `CompositionProjectManager.RelinkMissingMediaAsync` to update the project with the new location of a missing file:\r\n\r\n        ```csharp\r\n        using SoundFlow.Editing;\r\n        using SoundFlow.Editing.Persistence;\r\n        using System.Threading.Tasks;\r\n\r\n        public async Task AttemptRelink(ProjectSourceReference missingSource, string newFilePath, string projectDirectory)\r\n        {\r\n            bool success = CompositionProjectManager.RelinkMissingMediaAsync(\r\n            missingSource,\r\n            newFilePath,\r\n            projectDirectory\r\n            );\r\n\r\n            if (success)\r\n        {\r\n            Console.WriteLine($\"Successfully relinked '{missingSource.Id}' to '{newFilePath}'.\");\r\n            // You might need to re-resolve or update segments in your loaded composition\r\n            // that use this missingSourceReference. One way is to reload the project:\r\n            // (var reloadedComposition, var newMissing) = await CompositionProjectManager.LoadProjectAsync(projectFilePath);\r\n            // Or, manually update ISoundDataProvider instances in affected AudioSegments.\r\n        }\r\n            else\r\n        {\r\n            Console.WriteLine($\"Failed to relink '{missingSource.Id}'. File at new path might be invalid or inaccessible.\");\r\n        }\r\n        }\r\n        ```\r\n    </Tab>\r\n</Tabs>\r\n\r\n**Note on `ownsDataProvider` in `AudioSegment`:**\r\n*   When you create `AudioSegment`s manually for a new composition, you manage the lifecycle of their `ISoundDataProvider`s. If you pass `ownsDataProvider: true`, the segment will dispose of the provider when the segment itself (or its parent `Composition`) is disposed.\r\n*   When a `Composition` is loaded from a project file, the `AudioSegment`s created during loading will typically have `ownsDataProvider: true` set for the `ISoundDataProvider`s that were resolved (from file, embedded, or consolidated assets), as the loading process instantiates these providers.\r\n\r\n## Dirty Flag (`IsDirty`)\r\n\r\n`Composition`, `Track`, and `AudioSegment` (via its `Settings`) have an `IsDirty` property.\r\n*   This flag is automatically set to `true` when any significant property that affects playback or persistence is changed.\r\n*   `CompositionProjectManager.SaveProjectAsync` calls `composition.ClearDirtyFlag()` internally upon successful save.\r\n*   You can use this flag to prompt users to save changes before closing an application, for example.\r\n\r\n## Examples in Action\r\n\r\nThe `SoundFlow.Samples.EditingMixer` project in the SoundFlow GitHub repository provides extensive, runnable examples demonstrating:\r\n*   Building compositions with dialogue and generated audio.\r\n*   Using various `AudioSegmentSettings` like fades, loops, reverse, speed, and time stretching.\r\n*   Saving projects with different media handling strategies (consolidation, embedding).\r\n*   Loading projects and handling missing media by relinking.\r\n\r\nExploring this sample project is highly recommended to see these concepts applied in practical scenarios."
  },
  {
    "id": 49,
    "slug": "core-concepts",
    "version": "1.1.2",
    "title": "Core Concepts",
    "description": "Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.",
    "navOrder": 2,
    "category": "Core",
    "content": "---\ntitle: Core Concepts\ndescription: Learn the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline.\nnavOrder: 2\ncategory: Core\n---\n\n# Core Concepts\n\nThis section explains the fundamental building blocks of SoundFlow and how they interact to create a powerful and flexible audio processing pipeline. Understanding these core concepts is essential for effectively using and extending the SoundFlow audio engine.\n\n## Audio Engine (`AudioEngine`)\n\nThe `AudioEngine` is the heart of SoundFlow. It's responsible for:\n\n*   **Initializing and managing the audio backend:** SoundFlow supports multiple audio backends (e.g., `MiniAudio`), which handle the low-level interaction with the operating system's audio API. The `AudioEngine` abstracts away the backend details, providing a consistent interface for higher-level components.\n*   **Enumerating and Managing Audio Devices:** The engine can list available playback and capture devices and allows switching between them during runtime.\n*   **Controlling audio device settings:** The engine allows you to configure parameters like sample rate, channel count, and buffer size.\n*   **Driving the audio processing loop:** The `AudioEngine` runs a dedicated, high-priority thread (or uses backend-driven callbacks) that continuously processes audio data. This ensures real-time performance and minimizes latency.\n*   **Providing the root of the audio graph:** The engine hosts the `Master` mixer, which is the starting point for building complex audio processing pipelines.\n\n**Key Properties:**\n\n*   `SampleRate`: The audio sample rate (e.g., 44100 Hz, 48000 Hz).\n*   `Channels`: The number of audio channels (e.g., 2 for stereo).\n*   `Capability`:  Indicates whether the engine is configured for `Playback`, `Recording`, `Mixed` (both), or `Loopback`.\n*   `SampleFormat`: The format of audio samples (e.g., `F32` for 32-bit floating-point).\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n*   `CurrentPlaybackDevice`, `CurrentCaptureDevice`: Information about the currently active audio devices.\n*   `PlaybackDevices`, `CaptureDevices`: Lists of available audio devices.\n\n**Key Methods:**\n\n*   `CreateEncoder(...)`: Creates an instance of an `ISoundEncoder` for the current backend.\n*   `CreateDecoder(...)`: Creates an instance of an `ISoundDecoder` for the current backend.\n*   `UpdateDevicesInfo()`: Refreshes the list of available audio devices.\n*   `SwitchDevice(...)`, `SwitchDevices(...)`: Changes the active playback and/or capture device.\n*   `SoloComponent(...)`: Isolates a specific `SoundComponent` in the audio graph for debugging or monitoring.\n*   `UnsoloComponent(...)`: Removes a component from the soloed state.\n*   `Dispose()`: Releases the resources used by the `AudioEngine`.\n\n**Example:**\n\n```csharp\n// Initialize a MiniAudioEngine with a 48kHz sample rate, stereo output, and 32-bit float samples.\n// 48kHz is a common rate and compatible with extensions like WebRTC APM.\nusing var audioEngine = new MiniAudioEngine(48000, Capability.Playback, SampleFormat.F32, 2);\n\n// List playback devices\naudioEngine.UpdateDevicesInfo();\nforeach(var device in audioEngine.PlaybackDevices)\n{\n    Console.WriteLine($\"Device: {device.Name}, Default: {device.IsDefault}\");\n}\n```\n\n\n## Sound Components (`SoundComponent`)\n\n`SoundComponent` is the abstract base class for all audio processing units in SoundFlow. Each component represents a node in a directed acyclic graph (DAG), known as the **audio graph**.\n\n**Key Features:**\n\n*   **Modular Processing:** Components encapsulate specific audio processing logic, making the system modular and extensible.\n*   **Input and Output Connections:** Components can have zero or more input and output connections, allowing data to flow between them.\n*   **`GenerateAudio(Span<float> buffer)`:** This is the core method that derived classes must implement. It's called repeatedly by the audio engine to either:\n    *   **Generate new audio samples:** For source components like oscillators or file players.\n    *   **Modify existing audio samples:** For effects, filters, or analyzers.\n*   **Properties:**\n    *   `Name`: A descriptive name for the component.\n    *   `Volume`: Controls the output gain.\n    *   `Pan`: Controls the stereo panning (0.0 for full left, 0.5 for center, 1.0 for full right).\n    *   `Enabled`: Enables or disables the component's processing.\n    *   `Solo`: Isolates the component for debugging.\n    *   `Mute`: Silences the component's output.\n    *   `Parent`: The `Mixer` to which this component belongs (if any).\n*   **Methods:**\n    *   `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n    *   `DisconnectInput(SoundComponent input)`: Disconnects an input connection.\n    *   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an `AudioAnalyzer` to this component.\n    *   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an `AudioAnalyzer` from this component.\n    *   `AddModifier(SoundModifier modifier)`: Adds a `SoundModifier` to this component.\n    *   `RemoveModifier(SoundModifier modifier)`: Removes a `SoundModifier` from this component.\n\n**Example:**\n\n```csharp\n// A simple custom SoundComponent that generates a sine wave\npublic class SineWaveGenerator : SoundComponent\n{\n    public float Frequency { get; set; } = 440f; // Frequency in Hz\n    private float _phase;\n\n    protected override void GenerateAudio(Span<float> buffer)\n    {\n        var sampleRate = AudioEngine.Instance.SampleRate;\n        for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] = MathF.Sin(_phase); // Assumes mono output or fills one channel\n            _phase += 2 * MathF.PI * Frequency / sampleRate;\n            if (_phase > 2 * MathF.PI) _phase -= 2 * MathF.PI;\n        }\n    }\n}\n```\n\n\n## Mixer (`Mixer`)\n\nThe `Mixer` is a specialized `SoundComponent` that combines the output of multiple `SoundComponent` instances into a single audio stream.\n\n**Key Features:**\n\n*   **`Master` Mixer:** The `Mixer.Master` static property provides access to the default root mixer, which is automatically created by the `AudioEngine`. All audio ultimately flows through the `Master` mixer before reaching the output device.\n*   **Adding and Removing Components:**\n    *   `AddComponent(SoundComponent component)`: Adds a component to the mixer's inputs.\n    *   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n*   **Efficient Mixing:** The `Mixer` uses SIMD instructions (when available) to perform mixing operations very efficiently.\n\n**Example:**\n\n```csharp\n// Create a SoundPlayer and an Oscillator\nusing var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(dataProvider);\nvar oscillator = new Oscillator { Frequency = 220, Type = Oscillator.WaveformType.Square };\n\n// Add both to the Master mixer\nMixer.Master.AddComponent(player);\nMixer.Master.AddComponent(oscillator);\n// ...\n// Don't forget to dispose dataProvider when done if not using 'using' on player scope\n```\n\n\n## Sound Modifiers (`SoundModifier`)\n\n`SoundModifier` is an abstract base class for creating audio effects that modify the audio stream. Modifiers are applied to `SoundComponent` instances or to `AudioSegment`, `Track`, `Composition` and process the audio data.\n\n**Key Features:**\n\n*   **`ProcessSample(float sample, int channel)`:** This is the core method that derived classes can implement to process audio on a sample-by-sample basis.\n*   **`Process(Span<float> buffer)`:** This method can be overridden for buffer-based processing, which is often more efficient for complex effects. By default, it calls `ProcessSample` for each sample.\n*   **`Enabled` Property:** Allows dynamically enabling or disabling the modifier's effect.\n*   **Chaining:** Modifiers can be chained together on a `SoundComponent` to create complex effect pipelines.\n\n**Built-in Modifiers:**\n\nSoundFlow provides a variety of built-in modifiers, including:\n*   Algorithmic Reverb Modifier: Simulates reverberation.\n*   Ambient Reverb Modifier: Creates a sense of spaciousness.\n*   Bass Boost Modifier: Enhances low frequencies.\n*   Chorus Modifier: Creates a chorus effect.\n*   Compressor Modifier: Reduces dynamic range.\n*   Delay Modifier: Applies a delay effect.\n*   Frequency Band Modifier: Boosts or cuts frequency bands.\n*   Noise Reduction Modifier: Reduces noise.\n*   Parametric Equalizer: Provides precise EQ control.\n*   Stereo Chorus Modifier: Creates a stereo chorus.\n*   Treble Boost Modifier: Enhances high frequencies.\n*   And potentially external modifiers like `WebRtcApmModifier` via extensions.\n\n\n**Example:**\n\n```csharp\n// Create a SoundPlayer and a Reverb modifier\nusing var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\nvar player = new SoundPlayer(dataProvider);\nvar reverb = new AlgorithmicReverbModifier { RoomSize = 0.8f, Wet = 0.2f };\n\n// Add the reverb modifier to the player\nplayer.AddModifier(reverb);\n\n// Add the player to the Master mixer\nMixer.Master.AddComponent(player);\n// ...\n```\n\n## Sound Player Base (`SoundPlayerBase`)\n\n`SoundPlayerBase` is a new abstract class that provides common functionality for sound playback components like `SoundPlayer` and `SurroundPlayer`.\n\n**Key Features (inherited by `SoundPlayer` and `SurroundPlayer`):**\n\n*   Implements `ISoundPlayer`.\n*   Handles core playback logic: reading from an `ISoundDataProvider`, managing playback state (Play, Pause, Stop).\n*   Supports playback speed adjustment via `PlaybackSpeed` property.\n*   Manages looping with `IsLooping`, `LoopStartSamples`/`Seconds`, `LoopEndSamples`/`Seconds`.\n*   Provides seeking capabilities via `Seek` methods (accepting time in seconds, sample offset, or `TimeSpan`).\n*   `Volume` control (inherited from `SoundComponent`).\n*   `PlaybackEnded` event.\n\n## Audio Playback (`SoundPlayer`, `SurroundPlayer`)\n\nSoundFlow provides concrete classes for audio playback, deriving from `SoundPlayerBase`:\n\n*   **`SoundPlayer`:** A `SoundPlayerBase` implementation for standard mono or stereo audio playback from an `ISoundDataProvider`.\n*   **`SurroundPlayer`:** An extended `SoundPlayerBase` implementation that supports advanced surround sound configurations.\n\n**Key Features (`SoundPlayer`):**\n*   All features from `SoundPlayerBase` and `ISoundPlayer`.\n\n**Key Features (`SurroundPlayer`):**\n*   All features from `SoundPlayerBase` and `ISoundPlayer`.\n*   `SpeakerConfiguration`: Allows you to define the speaker setup (e.g., Stereo, Quad, 5.1, 7.1, or a custom configuration).\n*   `PanningMethod`: Selects the panning algorithm to use (Linear, EqualPower, or VBAP).\n*   `ListenerPosition`: Sets the listener's position relative to the speakers.\n*   `VbapParameters`: Provides fine-grained control over VBAP (Vector Base Amplitude Panning) settings.\n\n\n## Audio Recording (`Recorder`)\n\nThe `Recorder` class allows you to capture audio input from a recording device.\n\n**Key Features:**\n*   `StartRecording()`: Begins the recording process.\n*   `PauseRecording()`: Pauses the recording.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `StopRecording()`: Stops the recording and finalizes the output.\n*   `State`: Indicates the current recording state (Playing, Paused, Stopped).\n*   `SampleFormat`: The sample format to use for recording.\n*   `EncodingFormat`: The encoding format to use when saving to a file (e.g., WAV, FLAC), Currently only WAV supported by miniaudio backend.\n*   `SampleRate`: The sample rate for recording.\n*   `Channels`: The number of channels to record.\n*   `Stream`: The output `Stream` where recorded audio is written.\n*   `ProcessCallback`: A delegate that can be used to process recorded audio samples in real time.\n\n\n## Audio Providers (`ISoundDataProvider`)\n\n`ISoundDataProvider` is an interface that defines a standard way to access audio data, regardless of its source.\n\n**Key Features:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples).\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio.\n*   `ReadBytes(Span<float> buffer)`: Reads a chunk of audio data into the provided buffer.\n*   `Seek(int offset)`: Moves the read position to a specific offset (in samples).\n*   `EndOfStreamReached`: An event that is raised when the end of the audio data is reached.\n*   `PositionChanged`: An event that is raised when the read position changes.\n*   `Dispose()`: Implementations should release underlying resources (e.g., file streams).\n\n**Built-in Providers:**\n\n*   `AssetDataProvider`: Loads audio data from a byte array or `Stream`.\n*   `StreamDataProvider`: Reads audio data from a `Stream`.\n*   `MicrophoneDataProvider`: Captures audio data from the microphone in real-time.\n*   `ChunkedDataProvider`: Reads audio data from a file or stream in chunks.\n*   `NetworkDataProvider`: Reads audio data from a network source (URL, HLS).\n*   `RawDataProvider`: Reads audio data from a raw PCM stream or various raw array types (`float[]`, `byte[]`, `int[]`, `short[]`).\n\nIt's good practice to dispose of `ISoundDataProvider` instances when they are no longer needed, for example, using a `using` statement.\n\n```csharp\nusing var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\n// Use dataProvider\n```\n\n## Audio Encoding/Decoding (`ISoundEncoder`, `ISoundDecoder`)\n\n`ISoundEncoder` and `ISoundDecoder` are interfaces for encoding and decoding audio data to and from different formats. Both are `IDisposable`.\n\n*   **`ISoundEncoder`:** Encodes raw audio samples into a specific format (e.g., WAV, FLAC, MP3). Currently only WAV supported by miniaudio backend.\n*   **`ISoundDecoder`:** Decodes audio data from a specific format into raw audio samples.\n\n**`MiniAudio` Backend:**\n\nThe `MiniAudio` backend provides implementations of these interfaces using the `miniaudio` library:\n\n*   `MiniAudioEncoder`\n*   `MiniAudioDecoder`\n\n\n## Audio Analysis (`AudioAnalyzer`)\n\n`AudioAnalyzer` is an abstract base class for creating components that analyze audio data. Analyzers typically extract information from the audio stream without modifying it.\n\n**Key Features:**\n\n*   `Analyze(Span<float> buffer)`: An abstract method that derived classes must implement to perform their specific analysis.\n*   `Enabled`: If false, the `Analyze` step might be skipped by the `SoundComponent` it's attached to.\n*   **Integration with Visualizers:** Analyzers are often used in conjunction with `IVisualizer` implementations to display the analysis results visually.\n\n**Built-in Analyzers:**\n\n*   Level Meter Analyzer: Measures the RMS (root mean square) and peak levels of an audio signal.\n*   Spectrum Analyzer: Computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).\n*   Voice Activity Detector: Detects the presence of human voice in an audio stream.\n\n\n## Audio Visualization (`IVisualizer`)\n\n`IVisualizer` is an interface for creating components that visualize audio data. Visualizers typically don't modify the audio stream but instead render a graphical representation of the data. It implements `IDisposable`.\n\n**Key Features:**\n\n*   `Name`: A descriptive name for the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: This method is called by the audio engine to provide the visualizer with a chunk of audio data to process.\n*   `Render(IVisualizationContext context)`: This method is called to render the visualization. It receives an `IVisualizationContext` instance, which provides drawing methods.\n*   `VisualizationUpdated`: An event that is raised when the visualization needs to be redrawn (e.g., when new audio data has been processed).\n*   `Dispose()`: Releases resources held by the visualizer.\n\n## Visualization Context (`IVisualizationContext`):\n\nThis interface provides a set of drawing methods for rendering the visualization. The specific implementation of `IVisualizationContext` will depend on the UI framework you are using.\n\n**Built-in Visualizers:**\n\n*   Level Meter Visualizer: Displays a level meter that shows the current RMS or peak level of the audio.\n*   Spectrum Visualizer: Renders a bar graph representing the frequency spectrum of the audio.\n*   Waveform Visualizer: Draws the waveform of the audio signal.\n\n## Editing Engine & Persistence (`SoundFlow.Editing`, `SoundFlow.Editing.Persistence`)\n\nSoundFlow v1.1.0 introduces a powerful non-destructive audio editing engine. For a detailed guide, please see the [Editing Engine & Persistence](./editing-engine.mdx) documentation.\n\n**Key Concepts:**\n\n*   **`Composition`**: The main container for an audio project, holding multiple `Track`s. It can be rendered or played back.\n*   **`Track`**: Represents a single audio track within a `Composition`. Contains `AudioSegment`s and has its own settings (volume, pan, mute, solo, effects).\n*   **`AudioSegment`**: A clip of audio placed on a `Track`'s timeline. It references a portion of an `ISoundDataProvider` and has its own extensive settings.\n    *   **`AudioSegmentSettings`**: Controls volume, pan, fades (with `FadeCurveType`), looping (`LoopSettings`), reverse playback, speed, and **pitch-preserved time stretching** (via `TimeStretchFactor` or `TargetStretchDuration`, powered by `WsolaTimeStretcher`).\n    *   Supports segment-level modifiers and analyzers.\n*   **Non-Destructive:** Edits do not alter the original audio source files. All operations are applied at runtime during playback or rendering.\n*   **Project Persistence (`CompositionProjectManager`)**:\n    *   Save and load entire compositions as `.sfproj` files.\n    *   **Media Consolidation**: Option to copy all external audio files into an `Assets` folder within the project.\n    *   **Embed Small Media**: Option to embed small audio files (e.g., SFX) directly into the project file.\n    *   **Relink Missing Media**: If an audio file is moved, the project can be relinked to its new location.\n\nThis new engine allows for programmatic creation and manipulation of complex audio timelines, effects processing at multiple levels (segment, track, master), and robust project management."
  },
  {
    "id": 50,
    "slug": "api-reference",
    "version": "1.1.2",
    "title": "API Reference",
    "description": "A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.",
    "navOrder": 4,
    "category": "Core",
    "content": "---\ntitle: API Reference\ndescription: A detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members.\nnavOrder: 4\ncategory: Core\n---\n\n# API Reference\n\nThis section provides a detailed overview of the SoundFlow API, including namespaces, classes, interfaces, and their members. It serves as a comprehensive reference for developers working with the SoundFlow library.\n\n## Namespaces\n\nSoundFlow is organized into the following namespaces:\n\n*   **`SoundFlow.Abstracts`:** Contains abstract classes and interfaces that define the core framework of SoundFlow. These classes provide the foundation for building custom components, modifiers, and other extensions.\n*   **`SoundFlow.Backends`:** Provides backend-specific implementations for audio input/output. The primary backend currently supported is `SoundFlow.Backends.MiniAudio`, which uses the `miniaudio` library.\n*   **`SoundFlow.Components`:** Contains concrete `SoundComponent` classes that provide various audio processing functionalities, including playback, recording, mixing, and analysis.\n*   **`SoundFlow.Editing`:** Contains classes for non-destructive audio editing, including `Composition`, `Track`, `AudioSegment`, `AudioSegmentSettings`, `LoopSettings`, and `FadeCurveType`.\n*   **`SoundFlow.Editing.Persistence`:** Contains classes for saving and loading audio compositions, such as `CompositionProjectManager` and various project data DTOs.\n*   **`SoundFlow.Enums`:** Contains enumerations used throughout the SoundFlow library to represent different states, options, and capabilities.\n*   **`SoundFlow.Exceptions`:** Contains custom exception classes used for error handling within SoundFlow.\n*   **`SoundFlow.Extensions`:** Namespace for official extensions.\n    *   **`SoundFlow.Extensions.WebRtc.Apm`:** Provides integration with the WebRTC Audio Processing Module for features like echo cancellation, noise suppression, and automatic gain control.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Components`:** Contains components utilizing the WebRTC APM, like `NoiseSuppressor`.\n        *   **`SoundFlow.Extensions.WebRtc.Apm.Modifiers`:** Contains modifiers utilizing the WebRTC APM, like `WebRtcApmModifier`.\n*   **`SoundFlow.Interfaces`:** Contains interfaces that define contracts for various functionalities, such as audio data providers, encoders, decoders, and visualizers.\n*   **`SoundFlow.Modifiers`:** Contains concrete `SoundModifier` classes that implement various audio effects.\n*   **`SoundFlow.Providers`:** Contains classes that implement the `ISoundDataProvider` interface, providing ways to load audio data from different sources.\n*   **`SoundFlow.Structs`:** Contains custom struct types used within SoundFlow, often for interop or specific data representation.\n*   **`SoundFlow.Utils`:** Contains utility classes and extension methods that provide helpful functionalities for working with audio data and performing common operations.\n*   **`SoundFlow.Visualization`:** Contains classes related to audio visualization, including analyzers and visualizers.\n\n## Key Classes and Interfaces\n\nBelow is a summary of the key classes and interfaces in SoundFlow.\n\n\n### Abstracts\n\n| Class/Interface                               | Description                                                                                                     |\n| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n| [`AudioAnalyzer`](#abstracts-audioanalyzer)   | Abstract base class for audio analysis components. Inherits from `SoundComponent`.                              |\n| [`AudioEngine`](#abstracts-audioengine)       | Abstract base class for audio engine implementations. Manages audio device, processing thread, and audio graph. |\n| [`SoundComponent`](#abstracts-soundcomponent) | Abstract base class for all audio processing units in SoundFlow. Represents a node in the audio graph.          |\n| [`SoundModifier`](#abstracts-soundmodifier)   | Abstract base class for audio effects that modify audio samples.                                                |\n| [`SoundPlayerBase`](#abstracts-soundplayerbase) | Abstract base class providing common functionality for sound playback components. Inherits from `SoundComponent` and implements `ISoundPlayer`. Includes support for time-stretching. |\n\n### Backends.MiniAudio\n\n| Class/Interface                                           | Description                                                                                                                                |\n| --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`MiniAudioDecoder`](#backendsminiaudio-miniaudiodecoder) | `ISoundDecoder` implementation using the `miniaudio` library.                                                                              |\n| [`MiniAudioEncoder`](#backendsminiaudio-miniaudiodecoder) | `ISoundEncoder` implementation using the `miniaudio` library.                                                                              |\n| [`MiniAudioEngine`](#backendsminiaudio-miniaudioengine)   | `AudioEngine` implementation that uses the `miniaudio` library for audio I/O. Provides concrete implementations for encoding and decoding. |\n\n### Components\n\n| Class/Interface                                                | Description                                                                                                                                            |\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`EnvelopeGenerator`](#components-envelopegenerator)           | `SoundComponent` that generates an ADSR (Attack, Decay, Sustain, Release) envelope signal.                                                             |\n| [`Filter`](#components-filter)                                 | `SoundComponent` that applies a digital filter (low-pass, high-pass, band-pass, notch) to the audio signal.                                            |\n| [`LowFrequencyOscillator`](#components-lowfrequencyoscillator) | `SoundComponent` that generates a low-frequency oscillator (LFO) signal with various waveforms.                                                        |\n| [`Mixer`](#components-mixer)                                   | `SoundComponent` that mixes multiple audio streams together. The `Mixer.Master` property provides access to the default root mixer.                    |\n| [`Oscillator`](#components-oscillator)                         | `SoundComponent` that generates various waveforms (sine, square, sawtooth, triangle, noise, pulse).                                                    |\n| [`Recorder`](#components-recorder)                             | `SoundComponent` that captures audio input from a recording device and allows saving it to a stream or processing it via a callback.                     |\n| [`SoundPlayer`](#components-soundplayer)                       | `SoundPlayerBase` implementation that plays audio from an `ISoundDataProvider`.                                                                        |\n| [`SurroundPlayer`](#components-surroundplayer)                 | `SoundPlayerBase` implementation that extends `SoundPlayer` to support surround sound configurations with customizable speaker positions, delays, and panning methods. |\n| [`VoiceActivityDetector`](#components-voiceactivitydetector)   | `SoundComponent` and `AudioAnalyzer` that detects the presence of human voice in an audio stream using spectral features and energy thresholds.        |\n| [`WsolaTimeStretcher`](#components-wsolatimestretcher)\t | Implements WSOLA algorithm for real-time, pitch-preserved time stretching. Used internally by `AudioSegment`. |\n\n### Editing\n\n| Class/Interface                                       | Description                                                                                                                                            |\n| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`Composition`](#editing-composition)                 | Top-level container for audio tracks, representing a complete project. Implements `ISoundDataProvider` for rendering. `IDisposable`.                 |\n| [`Track`](#editing-track)                             | Represents a single audio track within a `Composition`, containing `AudioSegment`s and track-level settings.                                         |\n| [`AudioSegment`](#editing-audiosegment)               | Represents a single audio clip on a `Track`'s timeline, referencing a portion of an audio source and applying playback settings. `IDisposable`.      |\n| [`AudioSegmentSettings`](#editing-audiosegmentsettings) | Configurable settings for an `AudioSegment` (volume, pan, fades, loop, reverse, speed, time stretch, modifiers, analyzers).                          |\n| [`TrackSettings`](#editing-tracksettings)             | Configurable settings for a `Track` (volume, pan, mute, solo, enabled, modifiers, analyzers).                                                          |\n| [`LoopSettings`](#editing-loopsettings)               | (struct) Defines looping behavior for an `AudioSegment` (repetitions, target duration).                                                                |\n| [`FadeCurveType`](#editing-fadecurvetype)             | (enum) Defines curve types for fade effects (Linear, Logarithmic, SCurve).                                                                             |\n\n### Editing.Persistence\n\n| Class/Interface                                                        | Description                                                                                                                            |\n| ---------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| [`CompositionProjectManager`](#editingpersistence-compositionprojectmanager) | Static class for saving and loading `Composition` projects to/from `.sfproj` files. Handles media consolidation and relinking.       |\n| [`ProjectData`](#editingpersistence-projectdata)                         | DTO representing the root of a saved project file.                                                                                     |\n| [`ProjectTrack`](#editingpersistence-projecttrack)                       | DTO for a `Track` within a saved project.                                                                                              |\n| [`ProjectSegment`](#editingpersistence-projectsegment)                   | DTO for an `AudioSegment` within a saved project.                                                                                      |\n| [`ProjectAudioSegmentSettings`](#editingpersistence-projectaudiosegmentsettings) | DTO for `AudioSegmentSettings` within a saved project.                                                                                 |\n| [`ProjectTrackSettings`](#editingpersistence-projecttracksettings)       | DTO for `TrackSettings` within a saved project.                                                                                        |\n| [`ProjectSourceReference`](#editingpersistence-projectsourcereference)   | DTO representing how an audio source is referenced in a project (file path, embedded data, consolidation).                             |\n| [`ProjectEffectData`](#editingpersistence-projecteffectdata)             | DTO for serializing `SoundModifier` or `AudioAnalyzer` instances (type name, parameters).                                              |\n\n### Enums\n\n| Enum                                             | Description                                                                                                                                 |\n| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Capability`](#enums-capability)                | Specifies the capabilities of an `AudioEngine` instance (Playback, Recording, Mixed, Loopback).                                           |\n| [`DeviceType`](#enums-devicetype)                | Specifies the type of audio device (Playback, Capture).                                                                                     |\n| [`EncodingFormat`](#enums-encodingformat)        | Specifies the audio encoding format to use (e.g., WAV, FLAC, MP3, Vorbis). *Note: MiniAudio backend currently only supports WAV for encoding.* |\n| [`PlaybackState`](#enums-playbackstate)          | Specifies the current playback state of a `SoundPlayer` or `SurroundPlayer` (Stopped, Playing, Paused).                                    |\n| [`Result`](#enums-result)                        | Represents the result of an operation, including success and various error codes.                                                           |\n| [`SampleFormat`](#enums-sampleformat)            | Specifies the format of audio samples (e.g., U8, S16, S24, S32, F32).                                                                     |\n| [`FilterType`](#enums-filtertype)                | Specifies the type of filter to use in the `Filter` and `ParametricEqualizer` components (Peaking, LowShelf, HighShelf, BandPass, Notch, LowPass, HighPass) |\n| [`EnvelopeGenerator.EnvelopeState`](#enums-envelopegenerator-envelopestate) | Specifies the current state of the envelope generator (Idle, Attack, Decay, Sustain, Release) |\n| [`EnvelopeGenerator.TriggerMode`](#enums-envelopegenerator-triggermode) | Specifies how the envelope generator is triggered (NoteOn, Gate, Trigger) |\n| [`LowFrequencyOscillator.WaveformType`](#enums-lowfrequencyoscillator-waveformtype) | Specifies the waveform type for the low-frequency oscillator (Sine, Square, Triangle, Sawtooth, ReverseSawtooth, Random, SampleAndHold) |\n| [`LowFrequencyOscillator.TriggerMode`](#enums-lowfrequencyoscillator-triggermode) | Specifies how the LFO is triggered (FreeRunning, NoteTrigger) |\n| [`Oscillator.WaveformType`](#enums-oscillator-waveformtype) | Specifies the waveform type for the oscillator (Sine, Square, Sawtooth, Triangle, Noise, Pulse) |\n| [`SurroundPlayer.SpeakerConfiguration`](#enums-surroundplayer-speakerconfiguration) | Specifies the speaker configuration for the surround player (Stereo, Quad, Surround51, Surround71, Custom) |\n| [`SurroundPlayer.PanningMethod`](#enums-surroundplayer-panningmethod) | Specifies the panning method for the surround player (Linear, EqualPower, Vbap) |\n| [`FadeCurveType`](#editing-fadecurvetype)\t   | Specifies curve types for fade effects (Linear, Logarithmic, SCurve). |\n| **`SoundFlow.Extensions.WebRtc.Apm` Enums**          |                                                                                                                                             |\n| [`ApmError`](#extensions-webrtc-apm-apmerror)    | Error codes returned by the WebRTC Audio Processing Module.                                                                                |\n| [`NoiseSuppressionLevel`](#extensions-webrtc-apm-noisesuppressionlevel) | Specifies noise suppression levels (Low, Moderate, High, VeryHigh).                                                                  |\n| [`GainControlMode`](#extensions-webrtc-apm-gaincontrolmode) | Specifies gain controller modes (AdaptiveAnalog, AdaptiveDigital, FixedDigital).                                                        |\n| [`DownmixMethod`](#extensions-webrtc-apm-downmixmethod) | Specifies methods for downmixing audio channels (AverageChannels, UseFirstChannel).                                                    |\n| [`RuntimeSettingType`](#extensions-webrtc-apm-runtimesettingtype) | Specifies types of runtime settings for the WebRTC APM.                                                                        |\n\n### Exceptions\n\n| Class                                           | Description                                                                                   |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [`BackendException`](#exceptions-backendexception) | Thrown when an error occurs in a specific audio backend.                                     |\n\n### Extensions.WebRtc.Apm\n\n| Class/Interface                                                                 | Description                                                                                                                                       |\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AudioProcessingModule`](#extensions-webrtc-apm-audioprocessingmodule)         | Provides access to the native WebRTC Audio Processing Module for advanced audio processing like AEC, NS, AGC.                                     |\n| [`ApmConfig`](#extensions-webrtc-apm-apmconfig)                                 | Represents a configuration for the `AudioProcessingModule`, allowing enabling/disabling and setting parameters for various APM features.            |\n| [`StreamConfig`](#extensions-webrtc-apm-streamconfig)                           | Represents a stream configuration (sample rate, channels) for audio processing within the APM.                                                    |\n| [`ProcessingConfig`](#extensions-webrtc-apm-processingconfig)                   | Holds multiple `StreamConfig` instances for input, output, and reverse streams for the APM.                                                       |\n| **Components Namespace**                                                        |                                                                                                                                                   |\n| [`NoiseSuppressor`](#extensions-webrtc-apm-components-noisesuppressor)          | A component for offline/batch noise suppression using WebRTC APM, processing audio from an `ISoundDataProvider`.                                  |\n| **Modifiers Namespace**                                                         |                                                                                                                                                   |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier)       | A `SoundModifier` that applies WebRTC APM features (AEC, NS, AGC, etc.) in real-time to an audio stream within the SoundFlow graph. Configurable. |\n\n### Interfaces\n\n| Interface                                           | Description                                                                                                                                  |\n| --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`ISoundDataProvider`](#interfaces-isounddataprovider) | Defines a standard way to access audio data from various sources. Implements `IDisposable`.                                                |\n| [`ISoundDecoder`](#interfaces-isounddecoder)         | Defines the contract for decoding audio data from a specific format into raw audio samples.                                                 |\n| [`ISoundEncoder`](#interfaces-isoundencoder)         | Defines the contract for encoding raw audio samples into a specific format.                                                                 |\n| [`ISoundPlayer`](#interfaces-isoundplayer)           | Defines the contract for controlling audio playback (Play, Pause, Stop, Seek, Looping, Speed, Volume).                                       |\n| [`IVisualizationContext`](#interfaces-ivisualizationcontext) | Provides drawing methods for rendering audio visualizations. The implementation depends on the specific UI framework used.               |\n| [`IVisualizer`](#interfaces-ivisualizer)             | Defines the contract for components that visualize audio data.                                                                            |\n\n### Modifiers\n\n| Class                                                               | Description                                                                                                                                                                                           |\n| ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AlgorithmicReverbModifier`](#modifiers-algorithmicreverbmodifier) | Simulates reverberation using a network of comb and all-pass filters.                                                                                                                                 |\n| [`AmbientReverbModifier`](#modifiers-ambientreverbmodifier)         | Creates a sense of spaciousness by simulating ambient reflections.                                                                                                                                    |\n| [`BassBoostModifier`](#modifiers-bassboostmodifier)                 | Enhances low-frequency content using a low-pass filter.                                                                                                                                               |\n| [`ChorusModifier`](#modifiers-chorusmodifier)                       | Creates a chorus effect by mixing delayed and modulated copies of the signal.                                                                                                                         |\n| [`CompressorModifier`](#modifiers-compressormodifier)               | Reduces the dynamic range of the audio signal using a compressor algorithm.                                                                                                                           |\n| [`DelayModifier`](#modifiers-delaymodifier)                         | Applies a delay effect with feedback and optional low-pass filtering of the delayed signal.                                                                                                           |\n| [`FrequencyBandModifier`](#modifiers-frequencybandmodifier)         | Allows boosting or cutting specific frequency bands using a combination of low-pass and high-pass filters.                                                                                            |\n| [`NoiseReductionModifier`](#modifiers-noisereductionmodifier)       | Reduces noise in an audio stream using spectral subtraction.                                                                                                                                          |\n| [`ParametricEqualizer`](#modifiers-parametricequalizer)             | Provides precise control over the frequency spectrum with multiple configurable bands, each of which can be set as a peaking, low-shelf, high-shelf, band-pass, notch, low-pass, or high-pass filter. |\n| [`StereoChorusModifier`](#modifiers-stereochorusmodifier)           | Creates a stereo chorus effect with independent processing for the left and right channels.                                                                                                           |\n| [`TrebleBoostModifier`](#modifiers-trebleboostmodifier)             | Enhances high-frequency content using a high-pass filter.                                                                                                                                             |\n| [`WebRtcApmModifier`](#extensions-webrtc-apm-modifiers-webrtcapmmodifier) (from `SoundFlow.Extensions.WebRtc.Apm.Modifiers`) | Applies WebRTC APM features like echo cancellation, noise suppression, and AGC in real-time. |\n\n### Providers\n\n| Class                                                         | Description                                                                                                                                  |\n| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`AssetDataProvider`](#providers-assetdataprovider)           | `ISoundDataProvider` implementation that reads audio data from a byte array (useful for in-memory assets). Implements `IDisposable`.            |\n| [`StreamDataProvider`](#providers-streamdataprovider)         | `ISoundDataProvider` implementation that reads audio data from a generic `Stream` (supports seeking if the stream is seekable). Implements `IDisposable`. |\n| [`MicrophoneDataProvider`](#providers-microphonedataprovider) | `ISoundDataProvider` implementation that captures and provides audio data from the microphone in real-time. Implements `IDisposable`.            |\n| [`ChunkedDataProvider`](#providers-chunkeddataprovider)       | `ISoundDataProvider` implementation that reads and decodes audio data from a file or stream in chunks, improving efficiency for large files. Implements `IDisposable`. |\n| [`NetworkDataProvider`](#providers-networkdataprovider)       | `ISoundDataProvider` implementation that provides audio data from a network source (direct URL or HLS playlist). Implements `IDisposable`.   |\n| [`RawDataProvider`](#providers-rawdataprovider)               | `ISoundDataProvider` implementation for reading raw PCM audio data from a stream. Implements `IDisposable`.                                   |\n\n### Structs\n\n| Struct                                       | Description                                                                                    |\n| -------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| [`DeviceInfo`](#structs-deviceinfo)          | Represents information about an audio device, including ID, name, and supported formats.         |\n| [`NativeDataFormat`](#structs-nativedataformat) | Represents a native data format supported by an audio device (format, channels, sample rate). |\n\n### Utils\n\n| Class                                       | Description                                                                                                                                                                                                 |\n| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Extensions`](#utils-extensions)           | Provides extension methods for working with audio data and other utility functions, including `ReadArray<T>` for reading structures from native memory.                                                                                                                             |\n| [`MathHelper`](#utils-mathhelper)           | Provides mathematical functions and algorithms used in audio processing, including optimized FFT, window functions, `Mod`, and `PrincipalAngle`. |\n\n### Visualization\n\n| Class/Interface                                         | Description                                                                                                                                                                                      |\n| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [`LevelMeterAnalyzer`](#visualization-levelmeteranalyzer) | `AudioAnalyzer` that calculates the RMS (root mean square) and peak levels of an audio signal.                                                                                                  |\n| [`LevelMeterVisualizer`](#visualization-levelmetervisualizer) | `IVisualizer` that displays a level meter showing the current RMS or peak level of the audio.                                                                                                 |\n| [`SpectrumAnalyzer`](#visualization-spectrumanalyzer)    | `AudioAnalyzer` that computes the frequency spectrum of the audio using the Fast Fourier Transform (FFT).                                                                                         |\n| [`SpectrumVisualizer`](#visualization-spectrumvisualizer)  | `IVisualizer` that renders a bar graph representing the frequency spectrum of the audio.                                                                                                        |\n| [`WaveformVisualizer`](#visualization-waveformvisualizer)  | `IVisualizer` that draws the waveform of the audio signal.                                                                                                                                     |\n\n## Detailed Class and Interface Documentation\n\nThis section provides more in-depth information about some of the key classes and interfaces.\n\n### Abstracts `AudioAnalyzer`\n\n```csharp\npublic abstract class AudioAnalyzer : SoundComponent\n{\n    protected AudioAnalyzer(IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    protected abstract void Analyze(Span<float> buffer);\n    protected override void GenerateAudio(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Enabled`: Gets or sets whether the analyzer is active. If false, `Analyze` might be skipped.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer)`: Abstract method to be implemented by derived classes to perform audio analysis on the provided buffer.\n*   `GenerateAudio(Span<float> buffer)`: Overrides `SoundComponent`'s `GenerateAudio`, internally calls `Analyze` and then processes the visualizer if one is attached.\n\n### Abstracts `AudioEngine`\n\n```csharp\npublic abstract class AudioEngine : IDisposable\n{\n    protected AudioEngine(int sampleRate, Capability capability, SampleFormat sampleFormat, int channels);\n\n    public static int Channels { get; protected set; }\n    public Capability Capability { get; }\n    public static AudioEngine Instance { get; }\n    public bool IsDisposed { get; protected set; }\n    public float InverseSampleRate { get; }\n    public SampleFormat SampleFormat { get; }\n    public int SampleRate { get; }\n    public DeviceInfo? CurrentPlaybackDevice { get; protected set; }\n    public DeviceInfo? CurrentCaptureDevice { get; protected set; }\n    public int CaptureDeviceCount { get; protected set; }\n    public int PlaybackDeviceCount { get; protected set; }\n    public DeviceInfo[] PlaybackDevices { get; protected set; }\n    public DeviceInfo[] CaptureDevices { get; protected set; }\n\n    public static event AudioProcessCallback? OnAudioProcessed;\n\n    ~AudioEngine();\n\n    protected abstract void CleanupAudioDevice();\n    public abstract ISoundDecoder CreateDecoder(Stream stream);\n    public abstract ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate);\n    protected virtual void Dispose(bool disposing);\n    protected abstract void InitializeAudioDevice();\n    protected abstract void ProcessAudioData();\n    protected void ProcessAudioInput(nint input, int length);\n    protected void ProcessGraph(nint output, int length);\n    public void SoloComponent(SoundComponent component);\n    public void UnsoloComponent(SoundComponent component);\n    public abstract void SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback);\n    public abstract void SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo);\n    public abstract void UpdateDevicesInfo();\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Channels`: The number of audio channels.\n*   `Capability`: The audio engine's capabilities (Playback, Recording, Mixed, Loopback).\n*   `Instance`: Gets the singleton instance of the `AudioEngine`.\n*   `IsDisposed`: Indicates whether the engine has been disposed.\n*   `InverseSampleRate`: The inverse of the sample rate (1 / `SampleRate`).\n*   `SampleFormat`: The audio sample format.\n*   `SampleRate`: The audio sample rate.\n*   `CurrentPlaybackDevice`: Gets the currently selected playback device.\n*   `CurrentCaptureDevice`: Gets the currently selected capture device.\n*   `CaptureDeviceCount`: Gets the number of available capture devices.\n*   `PlaybackDeviceCount`: Gets the number of available playback devices.\n*   `PlaybackDevices`: Gets an array of available playback devices.\n*   `CaptureDevices`: Gets an array of available capture devices.\n\n\n**Events:**\n\n*   `OnAudioProcessed`: Event that is raised after each audio processing cycle.\n\n**Methods:**\n\n*   `CleanupAudioDevice()`: Abstract method to be implemented by derived classes to clean up audio device resources.\n*   `CreateDecoder(Stream stream)`: Creates an `ISoundDecoder` for the specific backend.\n*   `CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate)`: Creates an `ISoundEncoder` for the specific backend.\n*   `Dispose(bool disposing)`: Releases resources used by the engine.\n*   `InitializeAudioDevice()`: Abstract method to be implemented by derived classes to initialize the audio device.\n*   `ProcessAudioData()`: Abstract method to be implemented by derived classes to perform the main audio processing loop.\n*   `ProcessAudioInput(nint input, int length)`: Processes the audio input buffer.\n*   `ProcessGraph(nint output, int length)`: Processes the audio graph and outputs to the specified buffer.\n*   `SoloComponent(SoundComponent component)`: Solos a component in the audio graph.\n*   `UnsoloComponent(SoundComponent component)`: Unsolos a component in the audio graph.\n*   `SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback)`: Switches the audio engine to use the specified device.\n*   `SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo)`: Switches playback and/or capture devices.\n*   `UpdateDevicesInfo()`: Retrieves and updates the list of available audio devices.\n*   `Dispose()`: Public method to dispose of the engine and its resources.\n\n### Abstracts `SoundComponent`\n\n```csharp\npublic abstract class SoundComponent\n{\n    protected SoundComponent();\n\n    public virtual float Pan { get; set; } // Range 0.0 (Left) to 1.0 (Right), 0.5 is Center\n    public virtual string Name { get; set; }\n    public Mixer? Parent { get; set; }\n    public virtual bool Solo { get; set; }\n    public virtual float Volume { get; set; }\n    public virtual bool Enabled { get; set; }\n    public virtual bool Mute { get; set; }\n\n    public IReadOnlyList<SoundComponent> Inputs { get; }\n    public IReadOnlyList<SoundModifier> Modifiers { get; }\n    public IReadOnlyList<AudioAnalyzer> Analyzers { get; }\n\n    public void AddModifier(SoundModifier modifier);\n    public void ConnectInput(SoundComponent input);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void DisconnectInput(SoundComponent input);\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    protected abstract void GenerateAudio(Span<float> buffer);\n    internal void Process(Span<float> outputBuffer);\n    public void RemoveModifier(SoundModifier modifier);\n}\n```\n\n**Properties:**\n\n* `Pan`: The panning of the component's output (0.0 for full left, 0.5 for center, 1.0 for full right, using equal-power panning).\n* `Name`: The name of the component.\n* `Parent`: The parent mixer of this component.\n* `Solo`: Whether the component is soloed.\n* `Volume`: The volume of the component's output.\n* `Enabled`: Whether the component is enabled.\n* `Mute`: Whether the component is muted.\n* `Inputs`: Read-only list of connected input components.\n* `Modifiers`: Read-only list of applied modifiers.\n* `Analyzers`: Read-only list of attached audio analyzers.\n\n**Methods:**\n\n* `AddModifier(SoundModifier modifier)`: Adds a sound modifier to the component.\n* `RemoveModifier(SoundModifier modifier)`: Removes a sound modifier from the component.\n* `ConnectInput(SoundComponent input)`: Connects another component's output to this component's input.\n* `DisconnectInput(SoundComponent input)`: Disconnects an input from this component.\n* `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an audio analyzer to the component.\n* `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes an audio analyzer from the component.\n* `GenerateAudio(Span<float> buffer)`: Abstract method that derived classes must implement to generate or modify audio data.\n* `Process(Span<float> outputBuffer)`: Processes the component's audio, including applying modifiers and handling input/output connections.\n\n### Abstracts `SoundModifier`\n\n```csharp\npublic abstract class SoundModifier\n{\n    public SoundModifier();\n\n    public virtual string Name { get; set; }\n    public bool Enabled { get; set; } = true;\n\n    public abstract float ProcessSample(float sample, int channel);\n    public virtual void Process(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n*   `Enabled`: Gets or sets whether the modifier is active and should process audio. Defaults to true.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Abstract method to be implemented by derived classes to process a single audio sample.\n*   `Process(Span<float> buffer)`: Processes a buffer of audio data. Applies `ProcessSample` to each sample if not overridden.\n\n### Abstracts `SoundPlayerBase`\n\n```csharp\npublic abstract class SoundPlayerBase : SoundComponent, ISoundPlayer\n{\n    protected SoundPlayerBase(ISoundDataProvider dataProvider);\n\n    public float PlaybackSpeed { get; set; }\n    public PlaybackState State { get; private set; }\n    public bool IsLooping { get; set; }\n    public float Time { get; }\n    public float SourceTimeSeconds { get; } // Time in normal playback speed (1.0)\n    public float Duration { get; }\n    public int LoopStartSamples { get; }\n    public int LoopEndSamples { get; }\n    public float LoopStartSeconds { get; }\n    public float LoopEndSeconds { get; }\n    // Volume is inherited from SoundComponent\n\n    public event EventHandler<EventArgs>? PlaybackEnded;\n\n    protected override void GenerateAudio(Span<float> output);\n    protected virtual void HandleEndOfStream(Span<float> remainingOutputBuffer);\n    protected virtual void OnPlaybackEnded();\n\n    public void Play();\n    public void Pause();\n    public void Stop();\n    public bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    public bool Seek(float time);\n    public bool Seek(int sampleOffset);\n    public void SetLoopPoints(float startTime, float? endTime = -1f);\n    public void SetLoopPoints(int startSample, int endSample = -1);\n    public void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `PlaybackSpeed`: Gets or sets the playback speed (1.0 is normal).\n*   `State`: Gets the current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Gets or sets whether looping is enabled.\n*   `Time`: Gets the current playback position in seconds, affected by `PlaybackSpeed`.\n*   `SourceTimeSeconds`: Gets the current playback position in seconds as if `PlaybackSpeed` were 1.0.\n*   `Duration`: Gets the total duration of the audio in seconds.\n*   `LoopStartSamples`: Gets the loop start point in samples.\n*   `LoopEndSamples`: Gets the loop end point in samples (-1 for end of audio).\n*   `LoopStartSeconds`: Gets the loop start point in seconds.\n*   `LoopEndSeconds`: Gets the loop end point in seconds (-1 for end of audio).\n*   `Volume`: (Inherited from `SoundComponent`) Gets or sets the volume of the player.\n\n**Events:**\n\n*   `PlaybackEnded`: Occurs when playback reaches the end of the audio (not raised during looping).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> output)`: (Protected Override) Core audio generation logic, handles reading from data provider, resampling for playback speed, and looping.\n*   `HandleEndOfStream(Span<float> remainingOutputBuffer)`: (Protected Virtual) Handles logic when the data provider reaches its end (looping or stopping).\n*   `OnPlaybackEnded()`: (Protected Virtual) Invokes the `PlaybackEnded` event.\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to a specific time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to a specific time in seconds. Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to a specific sample offset. Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures loop points using start/end times in seconds.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures loop points using start/end sample indices.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures loop points using `TimeSpan`.\n\n### Abstracts `WsolaTimeStretcher`\n```csharp\npublic class WsolaTimeStretcher\n{\n    public WsolaTimeStretcher(int initialChannels = 2, float initialSpeed = 1.0f);\n\n    public void SetChannels(int channels);\n    public void SetSpeed(float speed);\n    public int MinInputSamplesToProcess { get; }\n    public void Reset();\n    public float GetTargetSpeed();\n    public int Process(ReadOnlySpan<float> input, Span<float> output, out int samplesConsumedFromInputBuffer, out int sourceSamplesRepresentedByOutput);\n    public int Flush(Span<float> output);\n}\n```\n**Description:** Implements the WSOLA (Waveform Similarity Overlap-Add) algorithm for real-time, pitch-preserved time stretching of audio. Allows changing playback speed without altering pitch. Primarily used internally by `AudioSegment`.\n\n\n### Backends.MiniAudio `MiniAudioDecoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioDecoder : ISoundDecoder\n{\n    internal MiniAudioDecoder(Stream stream);\n\n    public bool IsDisposed { get; private set; }\n    public int Length { get; private set; } // Length can be updated after initial check\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int Decode(Span<float> samples);\n    public void Dispose();\n    public bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n* `IsDisposed`: Indicates whether the decoder has been disposed.\n* `Length`: The total length of the decoded audio data in samples. *Note: Can be updated after initial checks if the stream length was not immediately available.*\n* `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n* `EndOfStreamReached`: Occurs when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n* `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer. Internally synchronized.\n* `Dispose()`: Releases the resources used by the decoder.\n* `Seek(int offset)`: Seeks to the specified offset within the audio stream (in samples). Internally synchronized.\n\n### Backends.MiniAudio `MiniAudioEncoder`\n\n```csharp\ninternal sealed unsafe class MiniAudioEncoder : ISoundEncoder\n{\n    public MiniAudioEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public bool IsDisposed { get; private set; }\n\n    public void Dispose();\n    public int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the encoder.\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples and writes them to the output file or stream.\n\n### Backends.MiniAudio `MiniAudioEngine`\n\n```csharp\npublic sealed class MiniAudioEngine : AudioEngine\n{\n    public MiniAudioEngine(int sampleRate, Capability capability, SampleFormat sampleFormat = SampleFormat.F32, int channels = 2);\n\n    protected override void CleanupAudioDevice();\n    public override ISoundDecoder CreateDecoder(Stream stream);\n    public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate); // Now public\n    protected override void InitializeAudioDevice();\n    protected override void ProcessAudioData();\n    public override void SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback);\n    public override void SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo);\n    public override void UpdateDevicesInfo();\n}\n```\n\n**Methods:**\n\n*   `CleanupAudioDevice()`: Cleans up the audio device resources.\n*   `CreateDecoder(Stream stream)`: Creates a `MiniAudioDecoder` instance.\n*   `CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate)`: Creates a `MiniAudioEncoder` instance.\n*   `InitializeAudioDevice()`: Initializes the audio device using `miniaudio`, including context initialization.\n*   `ProcessAudioData()`: Implements the main audio processing loop using `miniaudio` (typically via callbacks).\n*   `SwitchDevice(DeviceInfo deviceInfo, DeviceType type = DeviceType.Playback)`: Switches the playback or capture device.\n*   `SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo)`: Switches both playback and capture devices if specified.\n*   `UpdateDevicesInfo()`: Retrieves and updates the list of available playback and capture devices from MiniAudio.\n\n### Components `EnvelopeGenerator`\n\n```csharp\npublic class EnvelopeGenerator : SoundComponent\n{\n    public EnvelopeGenerator();\n\n    public float AttackTime { get; set; }\n    public float DecayTime { get; set; }\n    public override string Name { get; set; }\n    public float ReleaseTime { get; set; }\n    public bool Retrigger { get; set; }\n    public float SustainLevel { get; set; }\n    public TriggerMode Trigger { get; set; }\n\n    public event Action<float>? LevelChanged;\n\n    protected override void GenerateAudio(Span<float> buffer);\n    public void TriggerOff();\n    public void TriggerOn();\n}\n```\n\n**Properties:**\n\n*   `AttackTime`: The attack time of the envelope (in seconds).\n*   `DecayTime`: The decay time of the envelope (in seconds).\n*   `Name`: The name of the envelope generator.\n*   `ReleaseTime`: The release time of the envelope (in seconds).\n*   `Retrigger`: Whether to retrigger the envelope on each new trigger.\n*   `SustainLevel`: The sustain level of the envelope.\n*   `Trigger`: The trigger mode (`NoteOn`, `Gate`, `Trigger`).\n\n**Events:**\n\n*   `LevelChanged`: Occurs when the envelope level changes.\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Generates the envelope signal.\n*   `TriggerOff()`: Triggers the release stage of the envelope (if in `Gate` mode).\n*   `TriggerOn()`: Triggers the attack stage of the envelope.\n\n### Components `Filter`\n\n```csharp\npublic class Filter : SoundComponent\n{\n    public Filter();\n\n    public float CutoffFrequency { get; set; }\n    public override string Name { get; set; }\n    public float Resonance { get; set; }\n    public FilterType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency of the filter.\n*   `Name`: The name of the filter.\n*   `Resonance`: The resonance of the filter.\n*   `Type`: The filter type (`LowPass`, `HighPass`, `BandPass`, `Notch`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Applies the filter to the audio buffer.\n\n### Components `LowFrequencyOscillator`\n\n```csharp\npublic class LowFrequencyOscillator : SoundComponent\n{\n    public LowFrequencyOscillator();\n\n    public float Depth { get; set; }\n    public TriggerMode Mode { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float Rate { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer);\n    public float GetLastOutput();\n    public void Trigger();\n}\n```\n\n**Properties:**\n\n*   `Depth`: The depth of the LFO's modulation.\n*   `Mode`: The trigger mode (`FreeRunning`, `NoteTrigger`).\n*   `Name`: The name of the LFO.\n*   `Phase`: The initial phase of the LFO.\n*   `Rate`: The rate (frequency) of the LFO.\n*   `Type`: The waveform type (`Sine`, `Square`, `Triangle`, `Sawtooth`, `ReverseSawtooth`, `Random`, `SampleAndHold`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Generates the LFO signal.\n*   `GetLastOutput()`: Returns the last generated output sample.\n*   `Trigger()`: Triggers the LFO (if in `NoteTrigger` mode).\n\n### Components `Mixer`\n\n```csharp\npublic sealed class Mixer : SoundComponent\n{\n    public Mixer();\n\n    public static Mixer Master { get; }\n    public override string Name { get; set; }\n\n    public void AddComponent(SoundComponent component);\n    protected override void GenerateAudio(Span<float> buffer);\n    public void RemoveComponent(SoundComponent component);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Master`: The static instance of the master mixer.\n*   `Name`: The name of the mixer.\n\n**Methods:**\n\n*   `AddComponent(SoundComponent component)`: Adds a component to the mixer.\n*   `GenerateAudio(Span<float> buffer)`: Mixes the audio from all connected components.\n*   `RemoveComponent(SoundComponent component)`: Removes a component from the mixer.\n\n### Components `Oscillator`\n\n```csharp\npublic class Oscillator : SoundComponent\n{\n    public Oscillator();\n\n    public float Amplitude { get; set; }\n    public float Frequency { get; set; }\n    public override string Name { get; set; }\n    public float Phase { get; set; }\n    public float PulseWidth { get; set; }\n    public WaveformType Type { get; set; }\n\n    protected override void GenerateAudio(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Amplitude`: The amplitude of the oscillator.\n*   `Frequency`: The frequency of the oscillator.\n*   `Name`: The name of the oscillator.\n*   `Phase`: The initial phase of the oscillator.\n*   `PulseWidth`: The pulse width (for pulse waveforms).\n*   `Type`: The waveform type (`Sine`, `Square`, `Sawtooth`, `Triangle`, `Noise`, `Pulse`).\n\n**Methods:**\n\n*   `GenerateAudio(Span<float> buffer)`: Generates the oscillator's output.\n\n### Components `Recorder`\n\n```csharp\npublic class Recorder : IDisposable\n{\n    public Recorder(Stream stream, SampleFormat sampleFormat = SampleFormat.F32, EncodingFormat encodingFormat = EncodingFormat.Wav, int sampleRate = 44100, int channels = 2, VoiceActivityDetector? vad = null);\n    public Recorder(AudioProcessCallback callback, SampleFormat sampleFormat = SampleFormat.F32, EncodingFormat encodingFormat = EncodingFormat.Wav, int sampleRate = 44100, int channels = 2, VoiceActivityDetector? vad = null);\n\n    public ReadOnlyCollection<AudioAnalyzer> Analyzers { get; }\n    public int Channels { get; }\n    public EncodingFormat EncodingFormat { get; }\n    public Stream Stream { get; }\n    public ReadOnlyCollection<SoundModifier> Modifiers { get; }\n    public AudioProcessCallback? ProcessCallback { get; set; }\n    public int SampleRate { get; }\n    public PlaybackState State { get; }\n    public SampleFormat SampleFormat {get;}\n\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public void AddModifier(SoundModifier modifier);\n    public void Dispose();\n    public void PauseRecording();\n    public void RemoveAnalyzer(AudioAnalyzer analyzer);\n    public void RemoveModifier(SoundModifier modifier);\n    public void ResumeRecording();\n    public void StartRecording();\n    public void StopRecording();\n}\n```\n\n**Properties:**\n\n*   `Analyzers`: Gets a read-only collection of <see cref=\"AudioAnalyzer\"/> components applied to the recorder. Analyzers are used to process and extract data from the audio stream during recording.\n*   `Channels`: The number of channels to record.\n*   `EncodingFormat`: The encoding format for the recorded audio.\n*   `Stream`: The stream to write encoded recorded audio to.\n*   `Modifiers`: Gets a read-only collection of <see cref=\"SoundModifier\"/> components applied to the recorder. Modifiers are applied to the audio data before encoding or processing via callback, allowing for real-time audio effects during recording.\n*   `ProcessCallback`: A callback for processing recorded audio in real time.\n*   `SampleRate`: The sample rate for recording.\n*   `State`: The current recording state (`Stopped`, `Playing`, `Paused`).\n*   `SampleFormat`: The sample format for recording.\n\n**Methods:**\n\n*   `AddAnalyzer(AudioAnalyzer analyzer)`: Adds an <see cref=\"AudioAnalyzer\"/> to the recording pipeline. Analyzers process audio data during recording, enabling real-time analysis.\n*   `AddModifier(SoundModifier modifier)`: Adds a <see cref=\"SoundModifier\"/> to the recording pipeline. Modifiers apply effects to the audio data in real-time as it's being recorded.\n*   `Dispose()`: Releases resources used by the recorder.\n*   `PauseRecording()`: Pauses the recording.\n*   `RemoveAnalyzer(AudioAnalyzer analyzer)`: Removes a specific <see cref=\"AudioAnalyzer\"/> from the recording pipeline.\n*   `RemoveModifier(SoundModifier modifier)`: Removes a specific <see cref=\"SoundModifier\"/> from the recording pipeline.\n*   `ResumeRecording()`: Resumes a paused recording.\n*   `StartRecording()`: Starts the recording.\n*   `StopRecording()`: Stops the recording.\n### Components `SoundPlayer`\n\n```csharp\npublic sealed class SoundPlayer : SoundPlayerBase\n{\n    public SoundPlayer(ISoundDataProvider dataProvider);\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n}\n```\nInherits all playback functionality, properties, and events from `SoundPlayerBase`.\n\n**Properties:**\n* `Name`: The name of the sound player component (default: \"Sound Player\").\n\n### Components `SurroundPlayer`\n\n```csharp\npublic sealed class SurroundPlayer : SoundPlayerBase\n{\n    public SurroundPlayer(ISoundDataProvider dataProvider);\n\n    public override string Name { get; set; } // Overrides SoundPlayerBase's default\n    public Vector2 ListenerPosition { get; set; }\n    public PanningMethod Panning { get; set; }\n    public VbapParameters VbapParameters { get; set; }\n    public SurroundConfiguration SurroundConfig { get; set; }\n    public SpeakerConfiguration SpeakerConfig { get; set; }\n\n    protected override void GenerateAudio(Span<float> output);\n    public void SetSpeakerConfiguration(SpeakerConfiguration config);\n    // Inherits Play, Pause, Stop, Seek, IsLooping, etc. from SoundPlayerBase\n}\n```\nInherits base playback functionality from `SoundPlayerBase` and adds surround-specific features.\n\n**Properties:**\n*   `Name`: The name of the surround player component (default: \"Surround Player\").\n*   `ListenerPosition`: The position of the listener in the surround sound field (Vector2).\n*   `Panning`: Gets or sets the panning method to use for surround sound (`Linear`, `EqualPower`, `VBAP`).\n*   `SpeakerConfig`: Gets or sets the speaker configuration (`Stereo`, `Quad`, `Surround51`, `Surround71`, `Custom`).\n*   `VbapParameters`: Gets or sets parameters for Vector Base Amplitude Panning (VBAP).\n*   `SurroundConfig`: Gets or sets the custom surround configuration when `SpeakerConfig` is set to `Custom`.\n\n**Methods:**\n*   `GenerateAudio(Span<float> output)`: (Overrides `SoundPlayerBase`) Reads audio data, applies resampling, then applies surround processing and looping if enabled.\n*   `SetSpeakerConfiguration(SpeakerConfiguration config)`: Sets the speaker configuration for surround sound playback.\n*   `Seek(int sampleOffset)`: (Overrides `SoundPlayerBase`) Seeks and re-initializes delay lines for surround processing.\n\n\n### Components `VoiceActivityDetector`\n\n```csharp\npublic class VoiceActivityDetector : AudioAnalyzer\n{\n    public VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null);\n\n    public bool IsVoiceActive { get; }\n    public int SpeechHighBand { get; set; }\n    public int SpeechLowBand { get; set; }\n    public double Threshold { get; set; }\n\n    public override string Name { get; set; }\n\n    public event Action<bool>? SpeechDetected;\n\n    protected override void Analyze(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `IsVoiceActive`: Indicates whether voice activity is currently detected. This is a read-only property that reflects the detector's current state.\n*   `Name`: Gets or sets the name of the voice activity detector component, useful for identification and debugging.\n*   `SpeechHighBand`: Gets or sets the upper bound of the frequency range (in Hz) that the detector uses to analyze for speech. Frequencies above this band are ignored in the voice activity detection process. Default is 3400 Hz.\n*   `SpeechLowBand`: Gets or sets the lower bound of the frequency range (in Hz) that the detector focuses on for speech detection. Frequencies below this band are not considered for voice activity. Default is 300 Hz.\n*   `Threshold`: Gets or sets the detection sensitivity threshold. This value determines how sensitive the detector is to voice activity. A lower threshold value increases sensitivity, making the detector more likely to identify quieter sounds as voice activity. Default is 0.01.\n\n**Events:**\n\n*   `SpeechDetected`: An event that is raised whenever the voice activity state changes (i.e., when speech is detected or ceases to be detected). Listeners can subscribe to this event to respond in real-time to changes in voice activity.\n\n**Methods:**\n\n*   `VoiceActivityDetector(int fftSize = 1024, float threshold = 0.01f, IVisualizer? visualizer = null)`: Constructor for the VoiceActivityDetector class. Initializes a new instance of the voice activity detector with configurable FFT size, detection threshold and optional visualizer for audio analysis visualization.\n    *   `fftSize`: `int` – The size of the FFT (Fast Fourier Transform) window used for spectral analysis. Must be a power of two. Larger FFT sizes provide finer frequency resolution but may increase processing latency. Default is 1024.\n    *   `threshold`: `float` – The sensitivity threshold for voice detection. A lower value increases sensitivity. Default is 0.01.\n    *   `visualizer`: `IVisualizer?` – An optional visualizer instance that can be attached to the analyzer for visualizing audio processing data, useful for debugging and tuning. Default is `null`.\n*   `Analyze(Span<float> buffer)`: перевіряє audio buffer та оновлює `IsVoiceActive` property на основі алгоритму детекції.\n    *   `buffer`: `Span<float>` – The audio buffer to analyze for voice activity. The audio data in this buffer is processed to determine if voice is present.\n*   `GenerateAudio(Span<float> buffer)`: This method is inherited from `AudioAnalyzer` but not directly used in `VoiceActivityDetector`. Voice Activity Detector works by analyzing the input audio buffer provided to the `Analyze` method and does not generate audio; use `Process` method instead.\n\n**Remarks:**\n\n*   **Frequency Range:** The `SpeechLowBand` and `SpeechHighBand` properties allow you to customize the frequency range that the VAD focuses on for speech detection. Speech typically falls within the 300Hz to 3400Hz range, but you may need to adjust these values depending on the characteristics of your audio and the type of speech you are detecting.\n*   **Threshold Sensitivity:** The `Threshold` property is crucial for controlling the sensitivity of the voice activity detection. Adjusting this threshold may be necessary to achieve optimal performance in different environments and with varying audio input levels.\n*   **FFT Size:** The `fftSize` parameter in the constructor determines the FFT window size. A larger FFT size provides better frequency resolution, which can be beneficial in noisy environments or when detecting subtle voice activity. However, it also increases the computational cost and latency. Ensure that the FFT size is always a power of 2 for optimal performance and compatibility with FFT algorithms.\n*   **Performance Tuning:** For optimal performance, especially in real-time applications, carefully tune the `fftSize` and `Threshold` parameters. Larger FFT sizes are more computationally intensive but offer better frequency resolution. Adjust the `Threshold` based on the ambient noise level and the desired sensitivity of voice detection.\n*   **Environment Considerations:** The ideal settings for `fftSize`, `Threshold`, `SpeechLowBand`, and `SpeechHighBand` may vary depending on the environment in which the voice activity detector is used. In noisy environments, you might need to increase the `fftSize` and adjust the `Threshold` to minimize false positives.\n*   **Visualizer for Debugging:** The optional `visualizer` parameter in the constructor is highly useful for debugging and tuning the voice activity detector. By attaching a visualizer, you can visually inspect the audio data and the detector's response, which can help in understanding and adjusting the detector's parameters for optimal performance in your specific use case.\n\n### Enums `Capability`\n\n```csharp\n[Flags]\npublic enum Capability\n{\n    Playback = 1,\n    Record = 2,\n    Mixed = Playback | Record,\n    Loopback = 4\n}\n```\n\n**Values:**\n\n*   `Playback`: Indicates playback capability.\n*   `Record`: Indicates recording capability.\n*   `Mixed`: Indicates both playback and recording capability.\n*   `Loopback`: Indicates loopback capability (recording system audio output).\n\n### Enums `DeviceType`\n\n```csharp\npublic enum DeviceType\n{\n    Playback,\n    Capture\n}\n```\n**Values:**\n*   `Playback`: Device used for audio playback.\n*   `Capture`: Device used for audio capture.\n\n### Enums `EncodingFormat`\n\n```csharp\npublic enum EncodingFormat\n{\n    Unknown = 0,\n    Wav,\n    Flac,\n    Mp3,\n    Vorbis\n}\n```\n\n**Values:**\n\n*   `Unknown`: Unknown encoding format.\n*   `Wav`: Waveform Audio File Format (WAV).\n*   `Flac`: Free Lossless Audio Codec (FLAC).\n*   `Mp3`: MPEG-1 Audio Layer III (MP3).\n*   `Vorbis`: Ogg Vorbis.\n\n### Enums `PlaybackState`\n\n```csharp\npublic enum PlaybackState\n{\n    Stopped,\n    Playing,\n    Paused\n}\n```\n\n**Values:**\n\n*   `Stopped`: Playback is stopped.\n*   `Playing`: Playback is currently in progress.\n*   `Paused`: Playback is paused.\n\n### Enums `Result`\n\n```csharp\npublic enum Result\n{\n    Success = 0,\n    Error = -1,\n    // ... (other error codes)\n    CrcMismatch = -100,\n    FormatNotSupported = -200,\n    // ... (other backend-specific error codes)\n    DeviceNotInitialized = -300,\n    // ... (other device-related error codes)\n    FailedToInitBackend = -400\n    // ... (other backend initialization error codes)\n}\n```\n\n**Values:**\n\n*   `Success`: The operation was successful.\n*   `Error`: A generic error occurred.\n*   `CrcMismatch`: CRC checksum mismatch.\n*   `FormatNotSupported`: The requested audio format is not supported.\n*   `DeviceNotInitialized`: The audio device is not initialized.\n*   `FailedToInitBackend`: Failed to initialize the audio backend.\n*   **(Many other error codes representing various error conditions)**\n\n### Enums `SampleFormat`\n\n```csharp\npublic enum SampleFormat\n{\n    Default = 0,\n    U8 = 1,\n    S16 = 2,\n    S24 = 3,\n    S32 = 4,\n    F32 = 5\n}\n```\n\n**Values:**\n\n*   `Default`: The default sample format (typically `F32`).\n*   `U8`: Unsigned 8-bit integer.\n*   `S16`: Signed 16-bit integer.\n*   `S24`: Signed 24-bit integer packed in 3 bytes.\n*   `S32`: Signed 32-bit integer.\n*   `F32`: 32-bit floating-point.\n\n### Enums `FilterType`\n\n```csharp\npublic enum FilterType\n{\n    Peaking,\n    LowShelf,\n    HighShelf,\n    BandPass,\n    Notch,\n    LowPass,\n    HighPass\n}\n```\n\n**Values:**\n\n*   `Peaking`: Peaking filter.\n*   `LowShelf`: Low-shelf filter.\n*   `HighShelf`: High-shelf filter.\n*   `BandPass`: Band-pass filter.\n*   `Notch`: Notch filter.\n*   `LowPass`: Low-pass filter.\n*   `HighPass`: High-pass filter.\n\n### Enums `EnvelopeGenerator.EnvelopeState`\n\n```csharp\npublic enum EnvelopeState\n{\n    Idle,\n    Attack,\n    Decay,\n    Sustain,\n    Release\n}\n```\n\n**Values:**\n\n*   `Idle`: The envelope is inactive.\n*   `Attack`: The attack stage of the envelope.\n*   `Decay`: The decay stage of the envelope.\n*   `Sustain`: The sustain stage of the envelope.\n*   `Release`: The release stage of the envelope.\n\n### Enums `EnvelopeGenerator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    NoteOn,\n    Gate,\n    Trigger\n}\n```\n\n**Values:**\n\n* `NoteOn`: The envelope will go directly from attack to sustain, without a decay stage.\n* `Gate`: The envelope will progress normally, and will only enter release stage when trigger is off.\n* `Trigger`: The envelope will always progress to the end, including the release stage.\n\n### Enums `LowFrequencyOscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Triangle,\n    Sawtooth,\n    ReverseSawtooth,\n    Random,\n    SampleAndHold\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Triangle`: Triangle wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `ReverseSawtooth`: Reverse sawtooth wave.\n*   `Random`: Random values.\n*   `SampleAndHold`: Sample and hold random values.\n\n### Enums `LowFrequencyOscillator.TriggerMode`\n\n```csharp\npublic enum TriggerMode\n{\n    FreeRunning,\n    NoteTrigger\n}\n```\n\n**Values:**\n\n* `FreeRunning`: The LFO will run continuously without needing a trigger.\n* `NoteTrigger`: The LFO will only start when triggered.\n\n### Enums `Oscillator.WaveformType`\n\n```csharp\npublic enum WaveformType\n{\n    Sine,\n    Square,\n    Sawtooth,\n    Triangle,\n    Noise,\n    Pulse\n}\n```\n\n**Values:**\n\n*   `Sine`: Sine wave.\n*   `Square`: Square wave.\n*   `Sawtooth`: Sawtooth wave.\n*   `Triangle`: Triangle wave.\n*   `Noise`: White noise.\n*   `Pulse`: Pulse wave.\n\n### Enums `SurroundPlayer.SpeakerConfiguration`\n\n```csharp\npublic enum SpeakerConfiguration\n{\n    Stereo,\n    Quad,\n    Surround51,\n    Surround71,\n    Custom\n}\n```\n\n**Values:**\n\n*   `Stereo`: Standard stereo configuration (2 speakers).\n*   `Quad`: Quadraphonic configuration (4 speakers).\n*   `Surround51`: 5.1 surround sound configuration (6 speakers).\n*   `Surround71`: 7.1 surround sound configuration (8 speakers).\n* *   `Custom`: A custom speaker configuration defined by the user.\n\n### Enums `SurroundPlayer.PanningMethod`\n\n```csharp\npublic enum PanningMethod\n{\n    Linear,\n    EqualPower,\n    Vbap\n}\n```\n\n**Values:**\n\n*   `Linear`: Linear panning.\n*   `EqualPower`: Equal power panning.\n*   `Vbap`: Vector Base Amplitude Panning (VBAP).\n\n### Editing `Composition`\nSee [Editing and Persistence Guide](./editing-engine.mdx#composition) for details.\n```csharp\npublic class Composition : ISoundDataProvider, IDisposable\n{\n    public Composition(string name = \"Composition\", int? targetChannels = null);\n\n    public string Name { get; set; }\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public List<Track> Tracks { get; }\n    public float MasterVolume { get; set; }\n    public bool IsDirty { get; }\n    public int SampleRate { get; set; } // Target sample rate for rendering\n    public int TargetChannels { get; set; }\n\n    // ISoundDataProvider implementation\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public bool IsDisposed { get; private set; }\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    \n    public void AddTrack(Track track);\n    public bool RemoveTrack(Track track);\n    public TimeSpan CalculateTotalDuration();\n    public float[] Render(TimeSpan startTime, TimeSpan duration);\n    public int Render(TimeSpan startTime, TimeSpan duration, Span<float> outputBuffer);\n    public void MarkDirty();\n    internal void ClearDirtyFlag();\n    public void Dispose();\n    // ... other methods like ReplaceSegment, RemoveSegment, SilenceSegment, InsertSegment ...\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `Track`\nSee [Editing and Persistence Guide](./editing-engine.mdx#track) for details.\n```csharp\npublic class Track\n{\n    public Track(string name = \"Track\", TrackSettings? settings = null);\n\n    public string Name { get; set; }\n    public List<AudioSegment> Segments { get; }\n    public TrackSettings Settings { get; set; }\n    internal Composition? ParentComposition { get; set; }\n\n    public void MarkDirty();\n    public void AddSegment(AudioSegment segment);\n    public bool RemoveSegment(AudioSegment segment, bool shiftSubsequent = false);\n    public void InsertSegmentAt(AudioSegment segmentToInsert, TimeSpan insertionTime, bool shiftSubsequent = true);\n    public TimeSpan CalculateDuration();\n    public int Render(TimeSpan overallStartTime, TimeSpan durationToRender, Span<float> outputBuffer, int targetSampleRate, int targetChannels);\n}\n```\n\n### Editing `AudioSegment`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegment) for details.\n```csharp\npublic class AudioSegment : IDisposable\n{\n    public AudioSegment(\n        ISoundDataProvider sourceDataProvider,\n        TimeSpan sourceStartTime,\n        TimeSpan sourceDuration,\n        TimeSpan timelineStartTime,\n        string name = \"Segment\",\n        AudioSegmentSettings? settings = null,\n        bool ownsDataProvider = false);\n\n    public string Name { get; set; }\n    public ISoundDataProvider SourceDataProvider { get; private set; }\n    public TimeSpan SourceStartTime { get; set; }\n    public TimeSpan SourceDuration { get; set; }\n    public TimeSpan TimelineStartTime { get; set; }\n    public AudioSegmentSettings Settings { get; set; }\n    internal Track? ParentTrack { get; set; }\n\n    public TimeSpan StretchedSourceDuration { get; }\n    public TimeSpan EffectiveDurationOnTimeline { get; }\n    public TimeSpan TimelineEndTime { get; }\n    public TimeSpan GetTotalLoopedDurationOnTimeline();\n    public AudioSegment Clone(TimeSpan? newTimelineStartTime = null);\n    internal void ReplaceSource(ISoundDataProvider newSource, TimeSpan newSourceStartTime, TimeSpan newSourceDuration);\n    public int ReadProcessedSamples(TimeSpan segmentTimelineOffset, TimeSpan durationToRead, Span<float> outputBuffer, int outputBufferOffset, int targetSampleRate, int targetChannels);\n    internal void FullResetState();\n    public void Dispose();\n    public void MarkDirty();\n}\n```\n\n### Editing `AudioSegmentSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#audiosegmentsettings) for details.\n```csharp\npublic class AudioSegmentSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public TimeSpan FadeInDuration { get; set; }\n    public FadeCurveType FadeInCurve { get; set; }\n    public TimeSpan FadeOutDuration { get; set; }\n    public FadeCurveType FadeOutCurve { get; set; }\n    public bool IsReversed { get; set; }\n    public LoopSettings Loop { get; set; }\n    public float SpeedFactor { get; set; }\n    public float TimeStretchFactor { get; set; } // Overridden by TargetStretchDuration if set\n    public TimeSpan? TargetStretchDuration { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public AudioSegmentSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `TrackSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#tracksettings) for details.\n```csharp\npublic class TrackSettings\n{\n    public List<SoundModifier> Modifiers { get; init; }\n    public List<AudioAnalyzer> Analyzers { get; init; }\n    public float Volume { get; set; }\n    public float Pan { get; set; }\n    public bool IsMuted { get; set; }\n    public bool IsSoloed { get; set; }\n    public bool IsEnabled { get; set; }\n\n    public TrackSettings Clone();\n    public void AddModifier(SoundModifier modifier);\n    public bool RemoveModifier(SoundModifier modifier);\n    public void ReorderModifier(SoundModifier modifier, int newIndex);\n    public void AddAnalyzer(AudioAnalyzer analyzer);\n    public bool RemoveAnalyzer(AudioAnalyzer analyzer);\n}\n```\n\n### Editing `LoopSettings`\nSee [Editing and Persistence Guide](./editing-engine.mdx#loopsettings) for details.\n```csharp\npublic record struct LoopSettings\n{\n    public int Repetitions { get; }\n    public TimeSpan? TargetDuration { get; }\n    public LoopSettings(int repetitions = 0, TimeSpan? targetDuration = null);\n    public static LoopSettings PlayOnce { get; }\n}\n```\n\n### Editing `FadeCurveType`\nSee [Editing and Persistence Guide](./editing-engine.mdx#fadecurvetype) for details.\n```csharp\npublic enum FadeCurveType\n{\n    Linear,\n    Logarithmic,\n    SCurve\n}\n```\n\n### Editing.Persistence\nThese are Data Transfer Objects (DTOs) for serialization. See [Editing and Persistence Guide](./editing-engine.mdx#project-persistence) for their purpose.\n*   `CompositionProjectManager` (static class): `SaveProjectAsync`, `LoadProjectAsync`, `RelinkMissingMediaAsync`.\n*   `ProjectData`\n*   `ProjectTrack`\n*   `ProjectSegment`\n*   `ProjectAudioSegmentSettings`\n*   `ProjectTrackSettings`\n*   `ProjectSourceReference`\n*   `ProjectEffectData`\n\n\n### Extensions.WebRtc.Apm\n\n#### `AudioProcessingModule` (Class)\n```csharp\npublic class AudioProcessingModule : IDisposable\n{\n    public AudioProcessingModule();\n    public ApmError ApplyConfig(ApmConfig config);\n    public ApmError Initialize();\n    public ApmError ProcessStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    public ApmError ProcessReverseStream(float[][] src, StreamConfig inputConfig, StreamConfig outputConfig, float[][] dest);\n    // ... other methods for setting delays, levels, runtime settings, getting info, AEC dump ...\n    public static int GetFrameSize(int sampleRateHz);\n    public void Dispose();\n}\n```\n**Description:** Provides low-level access to the WebRTC Audio Processing Module. It's responsible for initializing the APM, applying configurations, and processing audio frames. Generally used internally by `WebRtcApmModifier` and `NoiseSuppressor`.\n\n#### `ApmConfig` (Class)\n```csharp\npublic class ApmConfig : IDisposable\n{\n    public ApmConfig();\n    public void SetEchoCanceller(bool enabled, bool mobileMode);\n    public void SetNoiseSuppression(bool enabled, NoiseSuppressionLevel level);\n    public void SetGainController1(bool enabled, GainControlMode mode, int targetLevelDbfs, int compressionGainDb, bool enableLimiter);\n    public void SetGainController2(bool enabled);\n    public void SetHighPassFilter(bool enabled);\n    public void SetPreAmplifier(bool enabled, float fixedGainFactor);\n    public void SetPipeline(int maxInternalRate, bool multiChannelRender, bool multiChannelCapture, DownmixMethod downmixMethod);\n    public void Dispose();\n}\n```\n**Description:** Used to configure the features of the `AudioProcessingModule` such as echo cancellation, noise suppression, gain control, etc.\n\n#### `StreamConfig` (Class)\n```csharp\npublic class StreamConfig : IDisposable\n{\n    public StreamConfig(int sampleRateHz, int numChannels);\n    public int SampleRateHz { get; }\n    public int NumChannels { get; }\n    public void Dispose();\n}\n```\n**Description:** Defines the properties (sample rate, number of channels) of an audio stream being processed by the APM.\n\n#### `ProcessingConfig` (Class)\nThis class holds multiple `StreamConfig` instances for different parts of the APM pipeline (input, output, reverse input, reverse output).\n\n#### `NoiseSuppressor` (Component - `SoundFlow.Extensions.WebRtc.Apm.Components`)\n```csharp\npublic class NoiseSuppressor : IDisposable\n{\n    public NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, NoiseSuppressionLevel suppressionLevel = NoiseSuppressionLevel.High, bool useMultichannelProcessing = false);\n    public event ProcessedAudioChunkHandler? OnAudioChunkProcessed;\n    public float[] ProcessAll();\n    public void ProcessChunks(Action<ReadOnlyMemory<float>>? chunkHandler = null);\n    public void Dispose();\n}\n```\n**Description:** A component for offline/batch noise suppression using WebRTC APM. It takes an `ISoundDataProvider`, processes its audio, and outputs the cleaned audio either as a whole or in chunks.\n**Key Members:**\n*   `NoiseSuppressor(ISoundDataProvider dataProvider, int sampleRate, int numChannels, ...)`: Constructor.\n*   `OnAudioChunkProcessed` (event): Raised when a chunk of audio is processed.\n*   `ProcessAll()`: Processes the entire audio stream and returns it.\n*   `ProcessChunks()`: Processes audio in chunks, raising `OnAudioChunkProcessed`.\n\n#### `WebRtcApmModifier` (Modifier - `SoundFlow.Extensions.WebRtc.Apm.Modifiers`)\n```csharp\npublic sealed class WebRtcApmModifier : SoundModifier, IDisposable\n{\n    public WebRtcApmModifier(\n        bool aecEnabled = false, bool aecMobileMode = false, int aecLatencyMs = 40,\n        bool nsEnabled = false, NoiseSuppressionLevel nsLevel = NoiseSuppressionLevel.High,\n        // ... other AGC, HPF, PreAmp, Pipeline settings ...\n    );\n\n    public override string Name { get; set; }\n    public EchoCancellationSettings EchoCancellation { get; }\n    public NoiseSuppressionSettings NoiseSuppression { get; }\n    public AutomaticGainControlSettings AutomaticGainControl { get; }\n    public ProcessingPipelineSettings ProcessingPipeline { get; }\n    public bool HighPassFilterEnabled { get; set; }\n    public bool PreAmplifierEnabled { get; set; }\n    public float PreAmplifierGainFactor { get; set; }\n    public float PostProcessGain { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel); // Throws NotSupportedException\n    public void Dispose();\n}\n```\n**Description:** A `SoundModifier` that applies various WebRTC APM features (AEC, NS, AGC, HPF, PreAmp) to an audio stream in real-time.\n**Key Members:**\n*   Constructor with detailed initial settings.\n*   Properties for configuring each APM feature (`EchoCancellation`, `NoiseSuppression`, `AutomaticGainControl`, `ProcessingPipeline`, `HighPassFilterEnabled`, etc.).\n*   `Process(Span<float> buffer)`: Core processing logic.\n*   `Dispose()`: Releases native APM resources.\n\n#### Enums for WebRTC APM\n*   `ApmError`: Error codes.\n*   `NoiseSuppressionLevel`: Low, Moderate, High, VeryHigh.\n*   `GainControlMode`: AdaptiveAnalog, AdaptiveDigital, FixedDigital.\n*   `DownmixMethod`: AverageChannels, UseFirstChannel.\n*   `RuntimeSettingType`: Types for runtime APM settings.\n\n### Interfaces `ISoundDataProvider`\n\n```csharp\npublic interface ISoundDataProvider : IDisposable\n{\n    int Position { get; }\n    int Length { get; }\n    bool CanSeek { get; }\n    SampleFormat SampleFormat { get; }\n    int SampleRate { get; set; }\n    bool IsDisposed { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n    event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    int ReadBytes(Span<float> buffer);\n    void Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples).\n*   `CanSeek`: Indicates whether seeking is supported.\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio data.\n*   `Dispose()`: Releases resources held by the data provider.\n\n### Interfaces `ISoundDecoder`\n\n```csharp\npublic interface ISoundDecoder : IDisposable\n{\n    bool IsDisposed { get; }\n    int Length { get; }\n    SampleFormat SampleFormat { get; }\n\n    event EventHandler<EventArgs>? EndOfStreamReached;\n\n    int Decode(Span<float> samples);\n    bool Seek(int offset);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the decoder has been disposed.\n*   `Length`: The total length of the decoded audio data (in samples).\n*   `SampleFormat`: The sample format of the decoded audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached during decoding.\n\n**Methods:**\n\n*   `Decode(Span<float> samples)`: Decodes a portion of the audio stream into the provided buffer.\n*   `Seek(int offset)`: Seeks to the specified offset (in samples) within the audio stream.\n*   `Dispose()`: Releases the resources used by the decoder.\n\n### Interfaces `ISoundEncoder`\n\n```csharp\npublic interface ISoundEncoder : IDisposable\n{\n    bool IsDisposed { get; }\n\n    int Encode(Span<float> samples);\n}\n```\n\n**Properties:**\n\n*   `IsDisposed`: Indicates whether the encoder has been disposed.\n\n**Methods:**\n\n*   `Encode(Span<float> samples)`: Encodes the provided audio samples.\n*   `Dispose()`: Releases the resources used by the encoder.\n\n\n### Interfaces `ISoundPlayer`\n\n```csharp\npublic interface ISoundPlayer\n{\n    PlaybackState State { get; }\n    bool IsLooping { get; set; }\n    float PlaybackSpeed { get; set; }\n    float Volume { get; set; }\n    float Time { get; }\n    float Duration { get; }\n    float LoopStartSeconds { get; }\n    float LoopEndSeconds { get; }\n    int LoopStartSamples { get; }\n    int LoopEndSamples { get; }\n\n    void Play();\n    void Pause();\n    void Stop();\n    bool Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin);\n    bool Seek(float time);\n    bool Seek(int sampleOffset);\n    void SetLoopPoints(float startTime, float? endTime = -1f);\n    void SetLoopPoints(int startSample, int endSample = -1);\n    void SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null);\n}\n```\n\n**Properties:**\n\n*   `State`: The current playback state (`Stopped`, `Playing`, `Paused`).\n*   `IsLooping`: Whether looping is enabled or disabled (`get`, `set`).\n*   `PlaybackSpeed`: Gets or sets the playback speed. 1.0 is normal speed.\n*   `Volume`: Gets or sets the volume of the sound player (0.0 to 1.0 or higher for gain).\n*   `Time`: The current playback position (in seconds).\n*   `Duration`: The total duration of the audio (in seconds).\n*   `LoopStartSeconds`: Gets the configured loop start point in seconds.\n*   `LoopEndSeconds`: Gets the configured loop end point in seconds.  -1 indicates looping to the natural end.\n*   `LoopStartSamples`: Gets the configured loop start point in samples.\n*   `LoopEndSamples`: Gets the configured loop end point in samples. -1 indicates looping to the natural end.\n\n**Methods:**\n\n*   `Play()`: Starts or resumes playback.\n*   `Pause()`: Pauses playback.\n*   `Stop()`: Stops playback and resets the position to the beginning.\n*   `Seek(TimeSpan time, SeekOrigin seekOrigin = SeekOrigin.Begin)`: Seeks to the specified time using `TimeSpan`. Returns `true` if successful.\n*   `Seek(float time)`: Seeks to the specified time (in seconds). Returns `true` if successful.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset (in samples). Returns `true` if successful.\n*   `SetLoopPoints(float startTime, float? endTime = -1f)`: Configures custom loop points using start and end times in seconds. `endTime` is optional;  use -1 or `null` to loop to the natural end.\n*   `SetLoopPoints(int startSample, int endSample = -1)`: Configures custom loop points using start and end sample indices. `endSample` is optional; use -1 to loop to the natural end.\n*   `SetLoopPoints(TimeSpan startTime, TimeSpan? endTime = null)`: Configures custom loop points using `TimeSpan`.\n\n### Interfaces `IVisualizationContext`\n\n```csharp\npublic interface IVisualizationContext\n{\n    void Clear();\n    void DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f);\n    void DrawRectangle(float x, float y, float width, float height, Color color);\n}\n```\n\n**Methods:**\n\n*   `Clear()`: Clears the drawing surface.\n*   `DrawLine(float x1, float y1, float x2, float y2, Color color, float thickness = 1f)`: Draws a line from (`x1`, `y1`) to (`x2`, `y2`) with the specified color and thickness.\n*   `DrawRectangle(float x, float y, float width, float height, Color color)`: Draws a rectangle with the specified position, dimensions, and color.\n\n### Interfaces `IVisualizer`\n\n```csharp\npublic interface IVisualizer : IDisposable\n{\n    string Name { get; }\n\n    event EventHandler VisualizationUpdated;\n\n    void ProcessOnAudioData(Span<float> audioData);\n    void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes a chunk of audio data for visualization.\n*   `Render(IVisualizationContext context)`: Renders the visualization using the provided `IVisualizationContext`.\n*   `Dispose()`: Releases the resources used by the visualizer.\n\n### Modifiers `AlgorithmicReverbModifier`\n\n```csharp\npublic class AlgorithmicReverbModifier : SoundModifier\n{\n    public AlgorithmicReverbModifier();\n\n    public float Damp { get; set; }\n    public override string Name { get; set; }\n    public float PreDelay { get; set; }\n    public float RoomSize { get; set; }\n    public float Wet { get; set; }\n    public float Width { get; set; }\n    public float Mix { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Damp`: The damping factor of the reverb.\n*   `Name`: The name of the modifier.\n*   `PreDelay`: The pre-delay time (in milliseconds).\n*   `RoomSize`: The simulated room size.\n*   `Wet`: The wet/dry mix of the reverb (0 = dry, 1 = wet).\n*   `Width`: The stereo width of the reverb.\n*   `Mix`: The mix level of the reverb.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the reverb effect.\n\n### Modifiers `AmbientReverbModifier`\n\n```csharp\npublic class AmbientReverbModifier : SoundModifier\n{\n    public AmbientReverbModifier(int baseDelayLength, float baseDecayFactor, float roomSize, float stereoWidth, float diffusion);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `AmbientReverbModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `baseDelayLength`: The base delay length (in samples) for the reverb.\n*   `baseDecayFactor`: The base decay factor for the reverb.\n*   `roomSize`: The simulated room size.\n*   `stereoWidth`: The stereo width of the reverb.\n*   `diffusion`: The diffusion factor of the reverb.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the ambient reverb effect.\n\n### Modifiers `BassBoostModifier`\n\n```csharp\npublic class BassBoostModifier : SoundModifier\n{\n    public BassBoostModifier(float cutoffFrequency);\n\n    public float CutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency below which the bass boost is applied.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the bass boost effect.\n\n### Modifiers `ChorusModifier`\n\n```csharp\npublic class ChorusModifier : SoundModifier\n{\n    public ChorusModifier(float depth, float rate, float feedback, float wetDryMix, int maxDelayLength);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `ChorusModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `depth`: The depth of the chorus effect.\n*   `rate`: The rate of the chorus effect.\n*   `feedback`: The feedback amount of the chorus effect.\n*   `wetDryMix`: The wet/dry mix of the chorus effect.\n*   `maxDelayLength`: The maximum delay length (in samples) used by the chorus effect.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the chorus effect.\n\n### Modifiers `CompressorModifier`\n\n```csharp\npublic class CompressorModifier : SoundModifier\n{\n    public CompressorModifier(float threshold, float ratio, float attack, float release, float knee, float makeupGain);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `CompressorModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `threshold`: The threshold (in dB) above which compression is applied.\n*   `ratio`: The compression ratio.\n*   `attack`: The attack time (in milliseconds).\n*   `release`: The release time (in milliseconds).\n*   `knee`: The knee width (in dB).\n*   `makeupGain`: The amount of makeup gain to apply after compression.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the compression effect.\n\n### Modifiers `DelayModifier`\n\n```csharp\npublic class DelayModifier : SoundModifier\n{\n    public DelayModifier(int delayLength, float feedback, float wetMix, float cutoffFrequency);\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `DelayModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `delayLength`: The delay length (in samples).\n*   `feedback`: The feedback amount of the delay.\n*   `wetMix`: The wet/dry mix of the delay (0 = dry, 1 = wet).\n*   `cutoffFrequency`: The cutoff frequency for the low-pass filter applied to the delayed signal.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the delay effect.\n\n### Modifiers `FrequencyBandModifier`\n\n```csharp\npublic class FrequencyBandModifier : SoundModifier\n{\n    public FrequencyBandModifier(float lowCutoffFrequency, float highCutoffFrequency);\n\n    public float HighCutoffFrequency { get; set; }\n    public float LowCutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `HighCutoffFrequency`: The high cutoff frequency of the frequency band.\n*   `LowCutoffFrequency`: The low cutoff frequency of the frequency band.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the frequency band modification.\n\n### Modifiers `NoiseReductionModifier`\n\n```csharp\npublic class NoiseReductionModifier : SoundModifier\n{\n    public NoiseReductionModifier(int fftSize = 2048, float alpha = 3f, float beta = 0.001f, float smoothingFactor = 0.9f, float gain = 1.5f, int noiseFrames = 5, VoiceActivityDetector? vad = null);\n\n    public override string Name { get; set; }\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying noise reduction.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `NoiseReductionModifier` operates on buffers, not individual samples.\n\n### Modifiers `ParametricEqualizer`\n\n```csharp\npublic class ParametricEqualizer : SoundModifier\n{\n    public ParametricEqualizer(int channels);\n\n    public override string Name { get; set; }\n    \n    public void AddBand(EqualizerBand band);\n    public void AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth);\n    public void AddBands(IEnumerable<EqualizerBand> bands);\n    public void ClearBands();\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n    public void RemoveBand(EqualizerBand band);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the modifier.\n\n**Methods:**\n\n*   `AddBand(EqualizerBand band)`: Adds an `EqualizerBand` to the equalizer.\n*   `AddBand(FilterType filterType, float frequency, float gain, float q, float bandwidth)`: Adds an equalizer band with the specified parameters.\n*   `AddBands(IEnumerable<EqualizerBand> bands)`: Adds multiple equalizer bands.\n*   `ClearBands()`: Removes all equalizer bands.\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying equalization.\n*   `ProcessSample(float sample, int channel)`: This method throws a `NotSupportedException` because `ParametricEqualizer` operates on buffers, not individual samples.\n*   `RemoveBand(EqualizerBand band)`: Removes a specific equalizer band.\n\n### Modifiers `StereoChorusModifier`\n\n```csharp\npublic class StereoChorusModifier : SoundModifier\n{\n    public StereoChorusModifier(float depthLeft, float depthRight, float rateLeft, float rateRight, float feedbackLeft, float feedbackRight, float wetDryMix, int maxDelayLength);\n\n    public override void Process(Span<float> buffer);\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\nThere are no public properties exposed by `StereoChorusModifier`. The constructor takes the following parameters that act as its properties:\n\n*   `depthLeft`: The depth of the chorus effect for the left channel.\n*   `depthRight`: The depth of the chorus effect for the right channel.\n*   `rateLeft`: The rate of the chorus effect for the left channel.\n*   `rateRight`: The rate of the chorus effect for the right channel.\n*   `feedbackLeft`: The feedback amount for the left channel.\n*   `feedbackRight`: The feedback amount for the right channel.\n*   `wetDryMix`: The wet/dry mix of the chorus effect.\n*   `maxDelayLength`: The maximum delay length (in samples) used by the chorus effect.\n\n**Methods:**\n\n*   `Process(Span<float> buffer)`: Processes an entire buffer of audio, applying the stereo chorus effect.\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the stereo chorus effect.\n\n### Modifiers `TrebleBoostModifier`\n\n```csharp\npublic class TrebleBoostModifier : SoundModifier\n{\n    public TrebleBoostModifier(float cutoffFrequency);\n\n    public float CutoffFrequency { get; set; }\n\n    public override float ProcessSample(float sample, int channel);\n}\n```\n\n**Properties:**\n\n*   `CutoffFrequency`: The cutoff frequency above which the treble boost is applied.\n\n**Methods:**\n\n*   `ProcessSample(float sample, int channel)`: Processes a single audio sample and applies the treble boost effect.\n\n### Providers `AssetDataProvider`\n\n```csharp\npublic sealed class AssetDataProvider : ISoundDataProvider, IDisposable\n{\n    public AssetDataProvider(Stream stream, int? sampleRate = null);\n    public AssetDataProvider(byte[] data, int? sampleRate = null);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; }\n    public int? SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; private set; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public void Dispose();\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (always true for `AssetDataProvider`).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio data is reached.\n\n**Methods:**\n\n*   `Dispose()`: Releases the resources used by the provider.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio data.\n\n### Providers `StreamDataProvider`\n\n```csharp\npublic sealed class StreamDataProvider : ISoundDataProvider\n{\n    public StreamDataProvider(Stream stream, int? sampleRate = null);\n\n    public bool CanSeek { get; }\n    public int Length { get; }\n    public int Position { get; private set; }\n    public int? SampleRate { get; set; }\n    public SampleFormat SampleFormat { get; }\n\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `Length`: The total length of the audio data (in samples).\n*   `Position`: The current read position within the audio data (in samples).\n*   `SampleRate`: The sample rate of the audio data.\n*   `SampleFormat`: The format of the audio samples.\n\n**Events:**\n\n*   `PositionChanged`: Raised when the read position changes.\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n\n**Methods:**\n\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples from the stream into the provided buffer.\n*   `Seek(int sampleOffset)`: Seeks to the specified offset (in samples) within the audio stream (if supported).\n*   `Dispose()`: Releases resources used by the provider.\n\n### Providers `MicrophoneDataProvider`\n\n```csharp\npublic class MicrophoneDataProvider : ISoundDataProvider, IDisposable\n{\n    public MicrophoneDataProvider(int bufferSize = 8, int? sampleRate = null);\n\n    public int Position { get; private set; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public void StartCapture();\n    public void StopCapture();    \n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the captured audio data (in samples).\n*   `Length`: Returns -1, indicating an unknown length for the live microphone stream.\n*   `CanSeek`: Returns `false` because seeking is not supported for live microphone input.\n*   `SampleFormat`: The sample format of the captured audio data, which matches the `AudioEngine`'s sample format.\n*   `SampleRate`: The sample rate of the captured audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when `StopCapture()` is called, signaling the end of the microphone input stream.\n*   `PositionChanged`: Raised after reading data, indicating that the read position has changed.\n\n**Methods:**\n\n*   `MicrophoneDataProvider(int bufferSize = 8, int? sampleRate = null)`: Constructor that initializes the `MicrophoneDataProvider`. It sets the buffer queue size (default is 8), the sample rate (defaults to the `AudioEngine`'s sample rate), and subscribes to the `AudioEngine.OnAudioProcessed` event to capture audio data.\n    * `bufferSize`: The number of audio sample arrays to hold in internal queue. Higher values will lead to higher latency but will be more resilient to performance spikes.\n    * `sampleRate`: The sample rate of the microphone, will use the audioEngine sample rate if not set.\n*   `StartCapture()`: Starts capturing audio data from the microphone.\n*   `StopCapture()`: Stops capturing audio data and raises the `EndOfStreamReached` event.\n*   `ReadBytes(Span<float> buffer)`: Reads captured audio samples into the provided buffer. If not enough data is available in the queue it will fill the rest of the buffer with silence.\n*   `Seek(int offset)`: Throws `NotSupportedException` because seeking is not supported for live microphone input.\n*   `Dispose()`: Releases resources used by the `MicrophoneDataProvider`, unsubscribes from the `AudioEngine.OnAudioProcessed` event, and clears the internal buffer queue.\n\n### Providers `ChunkedDataProvider`\n\n```csharp\npublic sealed class ChunkedDataProvider : ISoundDataProvider, IDisposable\n{\n    public ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize);\n    public ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data in samples.\n*   `CanSeek`: Indicates whether seeking is supported (depends on the underlying stream).\n*   `SampleFormat`: The format of the audio samples.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `ChunkedDataProvider(Stream stream, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a `Stream`. It creates an `ISoundDecoder` to decode the stream, sets the default chunk size (220500 samples per channel, which is 10 seconds at 44.1 kHz), and starts prefetching data.\n*   `ChunkedDataProvider(string filePath, int? sampleRate = null, int chunkSize = DefaultChunkSize)`: Constructor that initializes the `ChunkedDataProvider` with a file path. It opens a `FileStream` and calls the other constructor.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is prefilled with decoded audio. If the buffer runs out, it decodes another chunk from the stream.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported by the stream and decoder). It disposes of the current decoder, creates a new one, and seeks within the stream. Then, it clears the internal buffer and refills it from the new position.\n*   `Dispose()`: Releases the resources used by the `ChunkedDataProvider`, including the decoder and the stream.\n\n**Remarks:**\n\nThe `ChunkedDataProvider` is designed to handle large audio files efficiently by reading and decoding them in chunks. This prevents the entire file from being loaded into memory at once. The default chunk size is set to 10 seconds of audio at 44.1 kHz, but you can adjust this value in the constructor. The class uses an internal buffer (`Queue<float>`) to store decoded audio samples and prefetches data in the background.\n\n\n### Providers `NetworkDataProvider`\n\n```csharp\npublic sealed class NetworkDataProvider : ISoundDataProvider, IDisposable\n{\n    public NetworkDataProvider(string url, int? sampleRate = null);\n\n    public int Position { get; }\n    public int Length { get; private set; }\n    public bool CanSeek { get; private set; }\n    public SampleFormat SampleFormat { get; private set; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int sampleOffset);\n    public void Dispose();\n}\n```\n\n**Properties:**\n\n*   `Position`: The current read position within the audio data (in samples).\n*   `Length`: The total length of the audio data (in samples). Returns -1 for HLS streams without an `#EXT-X-ENDLIST` tag, indicating an unknown or continuously growing length.\n*   `CanSeek`: Indicates whether seeking is supported. It's `true` for direct audio URLs if the server supports range requests and for HLS streams with an `#EXT-X-ENDLIST` tag; otherwise, it's `false`.\n*   `SampleFormat`: The format of the audio samples. Determined after the initial connection to the stream.\n*   `SampleRate`: The sample rate of the audio data.\n\n**Events:**\n\n*   `EndOfStreamReached`: Raised when the end of the audio stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n\n**Methods:**\n\n*   `NetworkDataProvider(string url, int? sampleRate = null)`: Constructor that initializes the `NetworkDataProvider` with a network URL. It determines whether the URL points to a direct audio file or an HLS playlist.\n*   `ReadBytes(Span<float> buffer)`: Reads audio samples into the provided buffer. It reads data from an internal buffer that is filled asynchronously.\n*   `Seek(int sampleOffset)`: Seeks to the specified sample offset within the audio data (if supported). The behavior differs for direct URLs and HLS streams:\n    *   **Direct URLs:** Performs an HTTP range request to fetch data starting from the desired offset.\n    *   **HLS Streams:** Locates the HLS segment containing the desired time offset and starts downloading from that segment.\n*   `Dispose()`: Releases the resources used by the `NetworkDataProvider`, including the `HttpClient`, decoder, and stream.\n\n**Remarks:**\n\nThe `NetworkDataProvider` can handle both direct audio URLs and HLS (HTTP Live Streaming) playlists. It automatically detects the stream type and behaves accordingly.\n\n**Direct Audio URLs:**\n\n*   It uses `HttpClient` to make requests to the URL.\n*   It supports seeking if the server responds with an \"Accept-Ranges: bytes\" header.\n*   It creates an `ISoundDecoder` to decode the audio stream.\n*   It buffers audio data asynchronously in a background thread.\n\n**HLS Playlists:**\n\n*   It downloads and parses the M3U(8) playlist file.\n*   It identifies the individual media segments (e.g., `.ts` files).\n*   It downloads and decodes segments sequentially.\n*   It refreshes the playlist periodically for live streams.\n*   It supports seeking by selecting the appropriate segment based on the desired time offset.\n*   It determines whether the playlist has a defined end by checking for the `#EXT-X-ENDLIST` tag, which affects whether `Length` is known and `CanSeek` is true.\n\nThe class uses an internal `Queue<float>` to buffer audio samples. The `ReadBytes` method waits for data to become available in the buffer if it's empty.\n\n### Providers `RawDataProvider`\n\n```csharp\npublic sealed class RawDataProvider : ISoundDataProvider, IDisposable\n{\n    public RawDataProvider(Stream stream, SampleFormat sampleFormat, int channels, int sampleRate);\n\n    public int Position { get; }\n    public int Length { get; }\n    public bool CanSeek { get; }\n    public SampleFormat SampleFormat { get; }\n    public int? SampleRate { get; set; }\n\n    public event EventHandler<EventArgs>? EndOfStreamReached;\n    public event EventHandler<PositionChangedEventArgs>? PositionChanged;\n\n    public int ReadBytes(Span<float> buffer);\n    public void Seek(int offset);\n    public void Dispose();\n}\n```\n**Description:** Provides audio data from a stream containing raw PCM audio data.\n**Properties:**\n*   `Position`: The current read position in samples.\n*   `Length`: The total length of the stream in samples.\n*   `CanSeek`: Indicates if the underlying stream is seekable.\n*   `SampleFormat`: The sample format of the raw audio data.\n*   `SampleRate`: The sample rate of the raw audio data.\n    **Events:**\n*   `EndOfStreamReached`: Raised when the end of the stream is reached.\n*   `PositionChanged`: Raised when the read position changes.\n    **Methods:**\n*   `ReadBytes(Span<float> buffer)`: Reads raw PCM data from the stream and converts it to `float` if necessary.\n*   `Seek(int offset)`: Seeks to the specified offset in the underlying stream if `CanSeek` is true.\n*   `Dispose()`: Disposes the underlying stream.\n\n\n### Structs\n\n#### `DeviceInfo`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct DeviceInfo\n{\n    public IntPtr Id;\n    [MarshalAs(UnmanagedType.ByValTStr, SizeConst = 256)]\n    public string Name;\n    [MarshalAs(UnmanagedType.U1)]\n    public bool IsDefault;\n    public uint NativeDataFormatCount;\n    public IntPtr NativeDataFormats; // Pointer to an array of NativeDataFormat\n}\n```\n**Description:** Represents information about an audio device, including its native ID, name, whether it's the default system device, and a count/pointer to its supported native data formats.\n\n#### `NativeDataFormat`\n```csharp\n[StructLayout(LayoutKind.Sequential, CharSet = CharSet.Ansi)]\npublic struct NativeDataFormat\n{\n    public SampleFormat Format;\n    public uint Channels;\n    public uint SampleRate;\n    public uint Flags;\n}\n```\n**Description:** Represents a specific native data format (sample format, channels, sample rate) supported by an audio device. Accessed via the `NativeDataFormats` pointer in `DeviceInfo`.\n\n### Utils `Extensions`\n\n```csharp\npublic static class Extensions\n{\n    public static int GetBytesPerSample(this SampleFormat sampleFormat);\n    public static unsafe Span<T> GetSpan<T>(nint ptr, int length) where T : unmanaged;\n    public static T[] ReadArray<T>(this nint pointer, int count) where T : struct; // New method\n}\n```\n**New Methods:**\n*   `ReadArray<T>(this nint pointer, int count) where T : struct`: Reads an array of structures of type `T` from a native memory pointer.\n\n### Utils `MathHelper`\n```csharp\npublic static class MathHelper\n{\n    public static void Fft(Complex[] data);\n    public static float[] HammingWindow(int size);\n    public static void InverseFft(Complex[] data);\n    public static double Mod(this double x, double y); // New method\n    public static float PrincipalAngle(float angle);   // New method\n    // ... other existing methods ...\n}\n```\n**New Methods:**\n*   `Mod(this double x, double y)`: Returns the remainder after division, in the range [-0.5, 0.5).\n*   `PrincipalAngle(float angle)`: Returns the principal angle of a number in the range [-PI, PI).\n\n### Visualization `LevelMeterAnalyzer`\n\n```csharp\npublic class LevelMeterAnalyzer : AudioAnalyzer\n{\n    public LevelMeterAnalyzer(IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public float Peak { get; }\n    public float Rms { get; }\n\n    protected override void Analyze(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `Peak`: The peak level of the audio signal.\n*   `Rms`: The RMS (root mean square) level of the audio signal.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer)`: Analyzes the audio buffer to calculate the RMS and peak levels.\n\n### Visualization `LevelMeterVisualizer`\n\n```csharp\npublic class LevelMeterVisualizer : IVisualizer\n{\n    public LevelMeterVisualizer(LevelMeterAnalyzer levelMeterAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public Color PeakHoldColor { get; set; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the level meter bar.\n*   `Name`: The name of the visualizer.\n*   `PeakHoldColor`: The color of the peak hold indicator.\n*   `Size`: The size of the level meter.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the level meter.\n*   `Render(IVisualizationContext context)`: Renders the level meter visualization.\n\n### Visualization `SpectrumAnalyzer`\n\n```csharp\npublic class SpectrumAnalyzer : AudioAnalyzer\n{\n    public SpectrumAnalyzer(int fftSize, IVisualizer? visualizer = null);\n\n    public override string Name { get; set; }\n    public ReadOnlySpan<float> SpectrumData { get; }\n\n    protected override void Analyze(Span<float> buffer);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the analyzer.\n*   `SpectrumData`: The calculated frequency spectrum data.\n\n**Methods:**\n\n*   `Analyze(Span<float> buffer)`: Analyzes the audio buffer to compute the frequency spectrum using an FFT.\n\n### Visualization `SpectrumVisualizer`\n\n```csharp\npublic class SpectrumVisualizer : IVisualizer\n{\n    public SpectrumVisualizer(SpectrumAnalyzer spectrumAnalyzer);\n\n    public Color BarColor { get; set; }\n    public string Name { get; }\n    public static Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `BarColor`: The color of the spectrum bars.\n*   `Name`: The name of the visualizer.\n*   `Size`: The size of the spectrum visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the spectrum.\n*   `Render(IVisualizationContext context)`: Renders the spectrum visualization.\n\n### Visualization `WaveformVisualizer`\n\n```csharp\npublic class WaveformVisualizer : IVisualizer\n{\n    public WaveformVisualizer();\n\n    public string Name { get; }\n    public List<float> Waveform { get; }\n    public Color WaveformColor { get; set; }\n    public Vector2 Size { get; }\n\n    public event EventHandler? VisualizationUpdated;\n\n    public void Dispose();\n    public void ProcessOnAudioData(Span<float> audioData);\n    public void Render(IVisualizationContext context);\n}\n```\n\n**Properties:**\n\n*   `Name`: The name of the visualizer.\n*   `Waveform`: The waveform data.\n*   `WaveformColor`: The color of the waveform.\n*   `Size`: The size of the waveform visualizer.\n\n**Events:**\n\n*   `VisualizationUpdated`: Raised when the visualization needs to be updated.\n\n**Methods:**\n\n*   `Dispose()`: Releases resources used by the visualizer.\n*   `ProcessOnAudioData(Span<float> audioData)`: Processes audio data to update the waveform.\n*   `Render(IVisualizationContext context)`: Renders the waveform visualization."
  },
  {
    "id": 51,
    "slug": "advanced-topics",
    "version": "1.1.2",
    "title": "Advanced Topics",
    "description": "Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.",
    "navOrder": 6,
    "category": "Core",
    "content": "---\ntitle: Advanced Topics\ndescription: Learn how to extend the SoundFlow engine, optimize performance, and understand threading considerations.\nnavOrder: 6\ncategory: Core\n---\n\nimport {Icon} from \"@iconify/react\";\nimport {Tab, Tabs} from \"@heroui/react\";\nimport { Steps, Step } from '/src/components/Shared/Steps';\n\nThis section delves into more advanced topics related to SoundFlow, including extending the engine with custom components, optimizing performance, and understanding threading considerations.\n\n<Tabs color=\"primary\" variant=\"bordered\" aria-label=\"Advanced Topics\">\n    <Tab\n        key=\"extending\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ph:puzzle-piece-bold' />\n                <span>Extending SoundFlow</span>\n            </div>\n        }\n    >\n        ## Extending SoundFlow\n\n        One of SoundFlow's key strengths is its extensibility. You can tailor the engine to your specific needs by creating custom:\n\n        *   **Sound Components (`SoundComponent`)**\n        *   **Sound Modifiers (`SoundModifier`)**\n        *   **Visualizers (`IVisualizer`)**\n        *   **Audio Backends (`AudioEngine`)**\n        *   **Sound Data Providers (`ISoundDataProvider`)**\n        *   **Extensions (e.g., for specific DSP libraries)**: SoundFlow supports integration with external audio processing libraries. For instance, the `SoundFlow.Extensions.WebRtc.Apm` package provides features like noise suppression and echo cancellation by wrapping the WebRTC Audio Processing Module. You can create similar extensions for other libraries.\n\n        ### Custom Sound Components\n\n        Creating custom `SoundComponent` classes allows you to implement unique audio processing logic and integrate it seamlessly into the SoundFlow audio graph.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundComponent\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundComponent` class.\n            </Step>\n            <Step title=\"Implement GenerateAudio\" icon='lucide:audio-lines'>\n                Override the `GenerateAudio(Span<float> buffer)` method. This is where you'll write the core audio processing code for your component.\n                *   If your component generates audio (e.g., an oscillator), write samples to the provided `buffer`.\n                *   If your component modifies audio, read from connected input components (using a temporary buffer if necessary), process the audio, and then write to the provided `buffer`.\n            </Step>\n            <Step title=\"Override other methods (optional)\" icon='icon-park-outline:switch-one'>\n                You can override methods like `ConnectInput`, `AddAnalyzer`, `AddModifier`, etc., to customize how your component interacts with the audio graph.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your component to expose configurable parameters that users can adjust.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using System;\n\n        public class CustomGainComponent : SoundComponent\n        {\n            public float Gain { get; set; } = 1.0f; // Default gain\n\n            public override string Name { get; set; } = \"Custom Gain\";\n\n            protected override void GenerateAudio(Span<float> buffer)\n        {\n            // Multiply each sample by the gain factor\n            for (int i = 0; i < buffer.Length; i++)\n        {\n            buffer[i] *= Gain;\n        }\n        }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        // Create an instance of your custom component\n        var gainComponent = new CustomGainComponent { Gain = 0.5f };\n\n        // Connect it to the audio graph\n        var player = new SoundPlayer(new StreamDataProvider(File.OpenRead(\"audio.wav\")));\n        gainComponent.AddInput(player);\n        Mixer.Master.AddComponent(gainComponent);\n\n        // ...\n        ```\n\n        ### Custom Sound Modifiers\n\n        Custom `SoundModifier` classes allow you to implement your own audio effects.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from SoundModifier\" icon='ph:git-fork-bold'>\n                Create a new class that inherits from the abstract `SoundModifier` class.\n            </Step>\n            <Step title=\"Implement ProcessSample\" icon='icon-park-outline:sound-wave'>\n                Implement `ProcessSample(float sample, int channel)` (or override `Process` for buffer-level):\n                *   `ProcessSample(float sample, int channel)`: This method takes a single audio sample and the channel index as input and returns the modified sample.\n                *   `Process(Span<float> buffer)`: Override this for more complex effects that operate on entire buffers (e.g., FFT-based effects). By default, it calls `ProcessSample` for each sample.\n            </Step>\n            <Step title=\"Add properties (optional)\" icon='material-symbols:settings-outline'>\n                Add properties to your modifier to expose configurable parameters.\n            </Step>\n            <Step title=\"Use 'Enabled' property\" icon='material-symbols:toggle-on-outline'>\n                Your modifier will have an `Enabled` property (defaulting to `true`) to allow toggling its effect.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using System;\n\n        public class CustomDistortionModifier : SoundModifier\n        {\n            public float Threshold { get; set; } = 0.5f;\n\n            public override string Name { get; set; } = \"Custom Distortion\";\n\n            public override float ProcessSample(float sample, int channel)\n        {\n            // Simple hard clipping distortion\n            if (sample > Threshold)\n        {\n            return Threshold;\n        }\n            else if (sample < -Threshold)\n        {\n            return -Threshold;\n        }\n            else\n        {\n            return sample;\n        }\n        }\n        }\n        ```\n\n        **Usage:**\n\n        ```csharp\n        // Create an instance of your custom modifier\n        var distortion = new CustomDistortionModifier { Threshold = 0.7f };\n        // distortion.Enabled = false; // To disable it\n\n        // Add it to a SoundComponent\n        using var dataProvider = new StreamDataProvider(File.OpenRead(\"audio.wav\"));\n        var player = new SoundPlayer(dataProvider);\n        player.AddModifier(distortion);\n        Mixer.Master.AddComponent(player);\n        player.Play();\n        // ...\n        ```\n\n        ### Custom Visualizers\n\n        Custom `IVisualizer` classes allow you to create unique visual representations of audio data.\n\n        <Steps layout='vertical'>\n            <Step title=\"Implement IVisualizer\" icon='ph:plugs-connected-bold'>\n                Create a new class that implements the `IVisualizer` interface.\n            </Step>\n            <Step title=\"Implement ProcessOnAudioData\" icon='carbon:data-vis-4'>\n                This method receives a `Span<float>` containing audio data. You should process this data and store the relevant information needed for rendering.\n            </Step>\n            <Step title=\"Implement Render\" icon='material-symbols:draw-outline'>\n                This method receives an `IVisualizationContext`. Use the drawing methods provided by the context (e.g., `DrawLine`, `DrawRectangle`) to render your visualization.\n            </Step>\n            <Step title=\"Raise VisualizationUpdated\" icon='mdi:bell-ring-outline'>\n                When the visualization data changes (e.g., after processing new audio data), raise the `VisualizationUpdated` event to notify the UI to update the display.\n            </Step>\n            <Step title=\"Implement Dispose\" icon='material-symbols:delete-outline'>\n                Release any unmanaged resources or unsubscribe from events.\n            </Step>\n        </Steps>\n\n        **Example:**\n\n        ```csharp\n        using SoundFlow.Interfaces;\n        using System;\n        using System.Numerics;\n\n        // Assuming a simple Color struct/class exists:\n        // public struct Color { public float R, G, B, A; ... }\n\n        public class CustomBarGraphVisualizer : IVisualizer\n        {\n            private float _level;\n\n            public string Name => \"Custom Bar Graph\";\n\n            public event EventHandler? VisualizationUpdated;\n\n            public void ProcessOnAudioData(Span<float> audioData)\n        {\n            if (audioData.IsEmpty) return;\n            // Calculate the average level (simplified for this example)\n            float sum = 0;\n            for (int i = 0; i < audioData.Length; i++)\n        {\n            sum += Math.Abs(audioData[i]);\n        }\n            _level = sum / audioData.Length;\n\n            // Notify that the visualization needs to be updated\n            VisualizationUpdated?.Invoke(this, EventArgs.Empty);\n        }\n\n            public void Render(IVisualizationContext context)\n        {\n            // Clear the drawing area\n            context.Clear();\n\n            // Draw a simple bar graph based on the calculated level\n            float barHeight = _level * 200; // Scale the level for visualization\n            // Assuming Color constructor: new Color(r,g,b) or similar\n            context.DrawRectangle(10, 200 - barHeight, 30, barHeight, new Color(0, 1, 0));\n        }\n\n            public void Dispose()\n        {\n            // Unsubscribe from events, release resources if any\n            VisualizationUpdated = null;\n        }\n        }\n        ```\n\n        ### Adding Audio Backends\n\n        SoundFlow is designed to support multiple audio backends. Currently, it includes a `MiniAudio` backend. You can add support for other audio APIs (e.g., WASAPI, ASIO, CoreAudio) by creating a new backend.\n\n        <Steps layout='vertical'>\n            <Step title=\"Inherit from AudioEngine\" icon='ph:engine-bold'>\n                Create a new class that inherits from `AudioEngine`.\n            </Step>\n            <Step title=\"Implement Abstract Methods\" icon='material-symbols:function'>\n                Implement the abstract methods:\n                *   `InitializeAudioDevice()`: Initialize the audio device using the new backend's API, including context creation if needed.\n                *   `ProcessAudioData()`: Implement the main audio processing loop, or set up callbacks if the backend works that way.\n                *   `CleanupAudioDevice()`: Clean up any resources used by the audio device and context.\n                *   `CreateEncoder(...)`: Create an `ISoundEncoder` implementation for the new backend.\n                *   `CreateDecoder(...)`: Create an `ISoundDecoder` implementation for the new backend.\n                *   `UpdateDevicesInfo()`: Implement logic to enumerate playback and capture devices using the backend's API, populating `PlaybackDevices`, `CaptureDevices`, `PlaybackDeviceCount`, and `CaptureDeviceCount`.\n                *   `SwitchDevice(...)` and `SwitchDevices(...)`: Implement logic to reinitialize or reconfigure the audio device to use the specified new device(s).\n            </Step>\n            <Step title=\"Implement Codec Interfaces\" icon='mdi:file-code-outline'>\n                Create classes that implement `ISoundEncoder` and `ISoundDecoder` to handle audio encoding and decoding for your chosen backend.\n            </Step>\n        </Steps>\n\n        **Example (Skeleton):**\n\n        ```csharp\n        using SoundFlow.Abstracts;\n        using SoundFlow.Interfaces;\n        using SoundFlow.Enums;\n        using SoundFlow.Structs;\n        using System;\n        using System.IO;\n        using System.Linq;\n\n        public class MyNewAudioEngine : AudioEngine\n        {\n            private nint _context; // Example: native context handle\n            private nint _device;  // Example: native device handle\n\n            public MyNewAudioEngine(int sampleRate, Capability capability, SampleFormat sampleFormat, int channels)\n            : base(sampleRate, capability, sampleFormat, channels)\n        {\n            // Base constructor calls InitializeAudioDevice\n        }\n\n            protected override void InitializeAudioDevice()\n        {\n            // Initialize your audio device using the new backend's API\n\n            // 1. Initialize backend context (e.g., ma_context_init for MiniAudio)\n            // _context = NativeApi.InitContext();\n            // 2. Get default device IDs or use null for system default\n            // UpdateDevicesInfo(); // Populate device lists\n            // var defaultPlaybackId = PlaybackDevices.FirstOrDefault(d => d.IsDefault)?.Id ?? IntPtr.Zero;\n            // var defaultCaptureId = CaptureDevices.FirstOrDefault(d => d.IsDefault)?.Id ?? IntPtr.Zero;\n            // 3. Initialize the actual device with these IDs\n            // _device = NativeApi.InitDevice(_context, ..., defaultPlaybackId, defaultCaptureId);\n            // NativeApi.StartDevice(_device);\n            Console.WriteLine(\"MyNewAudioEngine: Audio device initialized.\");\n        }\n\n            protected override void ProcessAudioData()\n        {\n            // If backend uses callbacks, this might be empty.\n            // If backend needs a manual loop, implement it here.\n        }\n\n            protected override void CleanupAudioDevice()\n        {\n            // NativeApi.StopDevice(_device);\n            // NativeApi.UninitDevice(_device);\n            // NativeApi.UninitContext(_context);\n            Console.WriteLine(\"MyNewAudioEngine: Audio device cleaned up.\");\n        }\n\n            public override ISoundEncoder CreateEncoder(Stream stream, EncodingFormat encodingFormat, SampleFormat sampleFormat, int encodingChannels, int sampleRate)\n        {\n            // Return an instance of your custom ISoundEncoder implementation\n            // return new MyNewAudioEncoder(stream, encodingFormat, sampleFormat, encodingChannels, sampleRate);\n            throw new NotImplementedException();\n        }\n\n            public override ISoundDecoder CreateDecoder(Stream stream)\n        {\n            // Return an instance of your custom ISoundDecoder implementation\n            // return new MyNewAudioDecoder(stream);\n            throw new NotImplementedException();\n        }\n\n            public override void UpdateDevicesInfo()\n        {\n            // Use backend API to get list of playback and capture devices\n            // Populate this.PlaybackDevices, this.CaptureDevices,\n            // this.PlaybackDeviceCount, this.CaptureDeviceCount.\n            // Example:\n            // (var nativePlaybackDevices, var nativeCaptureDevices) = NativeApi.GetDeviceList(_context);\n            // this.PlaybackDevices = ConvertNativeToDeviceInfo(nativePlaybackDevices);\n            // ...\n            Console.WriteLine(\"MyNewAudioEngine: Device info updated.\");\n        }\n\n            public override void SwitchDevice(DeviceInfo deviceInfo, DeviceType type)\n        {\n            // CleanupCurrentDevice(); // Stop and uninit current device\n            // if (type == DeviceType.Playback) InitializeWithDeviceIds(deviceInfo.Id, CurrentCaptureDevice?.Id ?? IntPtr.Zero);\n            // else InitializeWithDeviceIds(CurrentPlaybackDevice?.Id ?? IntPtr.Zero, deviceInfo.Id);\n            // Update CurrentPlaybackDevice/CurrentCaptureDevice properties\n            Console.WriteLine($\"MyNewAudioEngine: Switched {type} device to {deviceInfo.Name}.\");\n        }\n\n            public override void SwitchDevices(DeviceInfo? playbackDeviceInfo, DeviceInfo? captureDeviceInfo)\n        {\n            // IntPtr playbackId = playbackDeviceInfo?.Id ?? CurrentPlaybackDevice?.Id ?? IntPtr.Zero;\n            // IntPtr captureId = captureDeviceInfo?.Id ?? CurrentCaptureDevice?.Id ?? IntPtr.Zero;\n            // CleanupCurrentDevice();\n            // InitializeWithDeviceIds(playbackId, captureId);\n            Console.WriteLine(\"MyNewAudioEngine: Switched devices.\");\n        }\n            // Helper method for device initialization logic\n            // private void InitializeWithDeviceIds(IntPtr playbackId, IntPtr captureId) { /* ... */ }\n            // private void CleanupCurrentDevice() { /* ... */ }\n        }\n        ```\n    </Tab>\n\n    <Tab\n        key=\"performance\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='ic:round-speed' />\n                <span>Performance Optimization</span>\n            </div>\n        }\n    >\n        ## Performance Optimization\n\n        Here are some tips for optimizing the performance of your SoundFlow applications:\n\n        *   **Buffer Sizes:** Choose appropriate buffer sizes for your use case. Smaller buffers reduce latency but increase CPU overhead. Larger buffers can improve efficiency but may introduce latency. Experiment to find the optimal balance. The audio backend (e.g., MiniAudio) often manages its own internal buffer sizes based on system capabilities and requests.\n        *   **SIMD:** SoundFlow uses SIMD instructions (when available) in the `Mixer` and `MathHelper` classes, and for some operations within `SoundComponent` and the audio conversion pipeline. Ensure your target platform supports SIMD for better performance.\n        *   **Profiling:** Use a profiler (like the one built into Visual Studio) to identify performance bottlenecks in your audio processing pipeline.\n        *   **Asynchronous Operations:** For long-running operations (e.g., loading large files, network requests in `NetworkDataProvider`, project saving/loading), use asynchronous programming (`async` and `await`) to avoid blocking the main thread or the audio thread.\n        *   **Avoid Allocations:** Minimize memory allocations within the `GenerateAudio` method of `SoundComponent` and the `ProcessSample` or `Process` method of `SoundModifier`. Allocate buffers and other resources in advance, if possible. Use `ArrayPool<T>.Shared` for temporary buffers when unavoidable.\n        *   **Efficient Algorithms:** Use efficient algorithms for audio processing, especially in performance-critical sections.\n        *   **Modifier Overhead:** Each `SoundModifier` added to a `SoundComponent` or `AudioSegment`, `Track`, `Composition` introduces some overhead. For very simple operations, integrating them directly into a custom `SoundComponent` might be more performant than using many tiny modifiers. However, modifiers offer better reusability and modularity.\n        *   **Effect Toggling:** Use the `Enabled` property on `SoundModifier`, `AudioAnalyzer`, `AudioSegmentSettings`, and `TrackSettings` to non-destructively disable effects or entire processing paths instead of removing and re-adding them, which can be more efficient.\n    </Tab>\n\n    <Tab\n        key=\"threading\"\n        title={\n            <div className=\"flex items-center gap-2\">\n                <Icon icon='carbon:thread' />\n                <span>Threading Considerations</span>\n            </div>\n        }\n    >\n        ## Threading Considerations\n\n        SoundFlow uses a dedicated, high-priority thread for audio processing. This ensures that audio is processed in real time and minimizes the risk of glitches or dropouts.\n\n        **Key Considerations:**\n\n        *   **Audio Thread:** The `AudioEngine`'s audio processing logic (e.g., within `ProcessGraph`, `ProcessAudioInput`, or backend callbacks which then call these) and consequently the `GenerateAudio` method of your `SoundComponent` and `Process` method of your `AudioAnalyzer` classes, are all called from the audio thread(s). Avoid performing any long-running or blocking operations (like I/O, complex non-audio computations, or UI updates) on this thread.\n        *   **UI Thread:** Never perform audio processing directly on the UI thread. This can lead to unresponsiveness and glitches. Use the `AudioEngine`'s audio thread for all audio-related operations. For UI updates based on audio events (e.g., from an `IVisualizer`), marshal the calls to the UI thread (e.g., `Dispatcher.Invoke` in WPF, `Control.Invoke` in WinForms).\n        *   **Thread Safety:** If you need to access or modify shared data from both the audio thread and another thread (e.g., the UI thread updating a `SoundModifier`'s property), use appropriate synchronization mechanisms (like `lock`, `Monitor`, or thread-safe collections) to ensure data integrity and prevent race conditions. Many properties on `SoundComponent` and `SoundModifier` are internally locked for thread-safe access.\n    </Tab>\n</Tabs>"
  }
]
